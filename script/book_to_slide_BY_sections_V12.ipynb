{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9771e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION SUMMARY ---\n",
      "Processing Mode: EPUB\n",
      "Unit ID: ICT312\n",
      "Unit Outline Path: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "Book Path: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "Parsed UO Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "Parsed ToC Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n",
      "Vector DB Path: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "Vector DB Collection: book_toc_guided_chunks_epub_v2\n",
      "--- SETUP COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "import logging\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "EXTRACT_UO = False\n",
    "\n",
    "CREATE_RAG_BOOK = False\n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "\n",
    "\n",
    "\n",
    "# New configuration file paths\n",
    "CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "SETTINGS_DECK_PATH = os.path.join(CONFIG_DIR, \"settings_deck.json\")\n",
    "TEACHING_FLOWS_PATH = os.path.join(CONFIG_DIR, \"teaching_flows.json\")\n",
    "\n",
    "# to check the layauts \n",
    "LAYOUT_MAPPING_PATH = os.path.join(CONFIG_DIR, \"layout_mapping.json\")\n",
    "\n",
    "\n",
    "# New output path for the processed settings\n",
    "PROCESSED_SETTINGS_PATH = os.path.join(CONFIG_DIR, \"processed_settings.json\")\n",
    "\n",
    "# to Save the individual FINAL plan to a file\n",
    "PLAN_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_plans\")\n",
    "os.makedirs(PLAN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#to Save the individual FINAL Content to a file\n",
    "CONTENT_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_content\")\n",
    "os.makedirs(CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CONTENT_LLM_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_content_llm\")\n",
    "os.makedirs(CONTENT_LLM_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "SLIDE_TEMPLATE_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/slide_style/slide_style_test_2.pptx\"\n",
    "FINAL_PRESENTATION_DIR = os.path.join(PROJECT_BASE_DIR, \"final_presentations\")\n",
    "os.makedirs(FINAL_PRESENTATION_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "# --- Helper Functions ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0852ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a new cell after your imports, or within Cell 3 before the functions.\n",
    "# This code is based on the schema from your screenshot on page 4.\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# Define Pydantic models that match your JSON schema\n",
    "class UnitInformation(BaseModel):\n",
    "    unitCode: Optional[str] = None\n",
    "    unitName: Optional[str] = None\n",
    "    creditPoints: Optional[int] = None\n",
    "    unitRationale: Optional[str] = None\n",
    "    prerequisites: Optional[str] = None\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    taskName: str\n",
    "    description: str\n",
    "    dueWeek: Optional[str] = None\n",
    "    weightingPercent: Optional[int] = None\n",
    "    learningOutcomesAssessed: Optional[str] = None\n",
    "\n",
    "class WeeklyScheduleItem(BaseModel):\n",
    "    week: str\n",
    "    contentTopic: str\n",
    "    requiredReading: Optional[str] = None\n",
    "\n",
    "class ParsedUnitOutline(BaseModel):\n",
    "    unitInformation: UnitInformation\n",
    "    learningOutcomes: List[str]\n",
    "    assessments: List[Assessment]\n",
    "    weeklySchedule: List[WeeklyScheduleItem] \n",
    "    requiredReadings: List[str]\n",
    "    recommendedReadings: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a090c5",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521c4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Configuration and Scoping for Content Generation (Corrected)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "# --- Global Test Overrides (for easy testing) ---\n",
    "TEST_OVERRIDE_WEEKS = None\n",
    "TEST_OVERRIDE_FLOW_ID = None\n",
    "TEST_OVERRIDE_SESSIONS_PER_WEEK = None\n",
    "TEST_OVERRIDE_DISTRIBUTION_STRATEGY = None\n",
    "\n",
    "\n",
    "def process_and_load_configurations():\n",
    "    \"\"\"\n",
    "    PHASE 1: Loads configurations, calculates a PRELIMINARY time-based slide budget,\n",
    "    and saves the result as 'processed_settings.json' for the Planning Agent.\n",
    "    \"\"\"\n",
    "    print_header(\"Phase 1: Configuration and Scoping Process\", char=\"-\")\n",
    "    \n",
    "    # --- Load all input files ---\n",
    "    logger.info(\"Loading all necessary configuration and data files...\")\n",
    "    try:\n",
    "        os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f: unit_outline = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: book_toc = json.load(f)\n",
    "        with open(SETTINGS_DECK_PATH, 'r', encoding='utf-8') as f: settings_deck = json.load(f)\n",
    "        with open(TEACHING_FLOWS_PATH, 'r', encoding='utf-8') as f: teaching_flows = json.load(f)\n",
    "        logger.info(\"All files loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"FATAL: A required configuration file was not found: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Pre-process and Refine Settings ---\n",
    "    logger.info(\"Pre-processing settings_deck for definitive plan...\")\n",
    "    processed_settings = json.loads(json.dumps(settings_deck))\n",
    "\n",
    "    unit_info = unit_outline.get(\"unitInformation\", {})\n",
    "    processed_settings['course_id'] = unit_info.get(\"unitCode\", \"UNKNOWN_COURSE\")\n",
    "    processed_settings['unit_name'] = unit_info.get(\"unitName\", \"Unknown Unit Name\")\n",
    "    \n",
    "    # --- Apply test overrides IF they are not None ---\n",
    "    logger.info(\"Applying overrides if specified...\")\n",
    "    # This block now correctly sets the teaching_flow_id based on the interactive flag.\n",
    "    if TEST_OVERRIDE_FLOW_ID is not None:\n",
    "        processed_settings['teaching_flow_id'] = TEST_OVERRIDE_FLOW_ID\n",
    "        logger.info(f\"OVERRIDE: teaching_flow_id set to '{TEST_OVERRIDE_FLOW_ID}'\")\n",
    "    else:\n",
    "        # If no override, use the 'interactive' boolean from the file as the source of truth.\n",
    "        is_interactive = processed_settings.get('interactive', False)\n",
    "        if is_interactive:\n",
    "            processed_settings['teaching_flow_id'] = 'apply_topic_interactive'\n",
    "        else:\n",
    "            processed_settings['teaching_flow_id'] = 'standard_lecture'\n",
    "        logger.info(f\"Loaded from settings: 'interactive' is {is_interactive}. Set teaching_flow_id to '{processed_settings['teaching_flow_id']}'.\")\n",
    "\n",
    "    # The 'interactive' flag is now always consistent with the teaching_flow_id.\n",
    "    processed_settings['interactive'] = \"interactive\" in processed_settings['teaching_flow_id'].lower()\n",
    "    \n",
    "    if TEST_OVERRIDE_SESSIONS_PER_WEEK is not None:\n",
    "        processed_settings['week_session_setup']['sessions_per_week'] = TEST_OVERRIDE_SESSIONS_PER_WEEK\n",
    "        logger.info(f\"OVERRIDE: sessions_per_week set to {TEST_OVERRIDE_SESSIONS_PER_WEEK}\")\n",
    "\n",
    "    if TEST_OVERRIDE_DISTRIBUTION_STRATEGY is not None:\n",
    "        processed_settings['week_session_setup']['distribution_strategy'] = TEST_OVERRIDE_DISTRIBUTION_STRATEGY\n",
    "        logger.info(f\"OVERRIDE: distribution_strategy set to '{TEST_OVERRIDE_DISTRIBUTION_STRATEGY}'\")\n",
    "\n",
    "    if TEST_OVERRIDE_WEEKS is not None:\n",
    "        processed_settings['generation_scope']['weeks'] = TEST_OVERRIDE_WEEKS\n",
    "        logger.info(f\"OVERRIDE: generation_scope weeks set to {TEST_OVERRIDE_WEEKS}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --- DYNAMIC SLIDE BUDGET CALCULATION (Phase 1) ---\n",
    "    logger.info(\"Calculating preliminary slide budget based on session time...\")\n",
    "    \n",
    "    params = processed_settings.get('parameters_slides', {})\n",
    "    SLIDES_PER_HOUR = params.get('slides_per_hour', 18)\n",
    "    \n",
    "    duration_hours = processed_settings['week_session_setup'].get('session_time_duration_in_hour', 1.0)\n",
    "    sessions_per_week = processed_settings['week_session_setup'].get('sessions_per_week', 1)\n",
    "    \n",
    "    logger.info(f\"⚠️Duration Hours {duration_hours}.\")\n",
    "    logger.info(f\"⚠️Sessions per week {sessions_per_week}.\")\n",
    "    \n",
    "    slides_content_per_session = int(duration_hours * SLIDES_PER_HOUR)\n",
    "    target_total_slides = slides_content_per_session * sessions_per_week\n",
    "    \n",
    "    processed_settings['slide_count_strategy']['target_total_slides'] = target_total_slides\n",
    "    processed_settings['slide_count_strategy']['slides_content_per_session'] = slides_content_per_session\n",
    "    logger.info(f\"⚠️Preliminary weekly content slide target calculated: #️⃣{target_total_slides} slides.\")\n",
    "    \n",
    "    # --- Resolve Generation Scope if not overridden ---\n",
    "    if TEST_OVERRIDE_WEEKS is None and processed_settings.get('generation_scope', {}).get('weeks') == \"all\":\n",
    "        num_weeks = len(unit_outline.get('weeklySchedule', []))\n",
    "        processed_settings['generation_scope']['weeks'] = list(range(1, num_weeks + 1))\n",
    "    \n",
    "    # --- Save the processed settings to disk ---\n",
    "    logger.info(f\"Saving preliminary processed configuration to: {PROCESSED_SETTINGS_PATH}\")\n",
    "    with open(PROCESSED_SETTINGS_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_settings, f, indent=2)\n",
    "    logger.info(\"File saved successfully.\")\n",
    "\n",
    "    # --- Assemble master config for optional preview ---\n",
    "    master_config = {\n",
    "        \"processed_settings\": processed_settings,\n",
    "        \"unit_outline\": unit_outline,\n",
    "        \"book_toc\": book_toc,\n",
    "        \"teaching_flows\": teaching_flows\n",
    "    }\n",
    "    \n",
    "    print_header(\"Phase 1 Configuration Complete\", char=\"-\")\n",
    "    logger.info(\"Master configuration object is ready for the Planning Agent.\")\n",
    "    return master_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d3789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2497c5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae61cd82",
   "metadata": {},
   "source": [
    "### **Component: Definitive PowerPoint Layout Inspector**\n",
    "\n",
    "Component: Definitive PowerPoint Layout Inspector\n",
    "Primary Purpose\n",
    "The inspect_and_generate_layout_config function serves as a critical pre-processing utility for the automated presentation generation system. Its primary purpose is to bridge the gap between a visual PowerPoint template and the programmatic logic of the content generation agents.\n",
    "It achieves this by performing a deep inspection of a given PowerPoint (.pptx) template file and auto-generating a detailed, structured, and human-readable JSON configuration file (layout_mapping.json). This configuration file acts as the \"API documentation\" for the presentation template, allowing both human users and a Large Language Model (LLM) to understand and utilize the available slide layouts effectively.\n",
    "Key Functions and GoOf course. Here is a formal description of the purpose and functionality of the \"Definitive PowerPoint Layout Inspector\" script. This description is suitable for project documentation, a README file, or for explaining its role to other developers.\n",
    "\n",
    "#### **Primary Purpose**\n",
    "\n",
    "The `inspect_and_generate_layout_config` function serves as a critical pre-processing utility for the automated presentation generation system. Its primary purpose is to **bridge the gap between a visual PowerPoint template and the programmatic logic of the content generation agents**.\n",
    "\n",
    "It achieves this by performing a deep inspection of a given PowerPoint (`.pptx`) template file and auto-generating a detailed, structured, and human-readable JSON configuration file (`layout_mapping.json`). This configuration file acts as the \"API documentation\" for the presentation template, allowing both human users and a Large Language Model (LLM) to understand and utilize the available slide layouts effectively.\n",
    "\n",
    "#### **Key Functions and Goals**\n",
    "\n",
    "1.  **Comprehensive Layout Discovery:**\n",
    "    *   The script guarantees that **every single slide layout** present in the PowerPoint template's Slide Master is detected and analyzed. This prevents a common problem where unused or unconventionally named layouts might be missed by simpler scripts.\n",
    "\n",
    "2.  **Detailed Placeholder Analysis:**\n",
    "    *   For each layout, the script extracts an exhaustive list of all its placeholders. For every placeholder, it records crucial metadata:\n",
    "        *   **`type`**: The functional role of the placeholder (e.g., `TITLE`, `BODY`, `OBJECT`, `TABLE`, `PICTURE`).\n",
    "        *   **`name`**: The unique name given to the placeholder in the PowerPoint interface (e.g., \"Title 1\", \"Content Placeholder 2\").\n",
    "        *   **`idx`**: The internal identification number of the placeholder.\n",
    "        *   **`position` and `size`**: The physical coordinates (`left`, `top`) and dimensions (`width`, `height`), converted to an intuitive unit (inches) for easy comprehension of the layout's visual structure.\n",
    "\n",
    "3.  **Intelligent Capability Summarization:**\n",
    "    *   The script's core innovation is its ability to generate a **machine-readable capabilities summary** for each layout. Instead of just listing raw data, it synthesizes the placeholder information into a concise description of what the layout is designed for. For example:\n",
    "        *   `{\"title_support\": \"standard_title\", \"body_layout\": \"2_column\"}`\n",
    "        *   `{\"title_support\": \"centered_title_with_subtitle\", \"body_layout\": \"no_body\"}`\n",
    "        *   `{\"specific_content_types\": [\"TABLE\", \"CHART\"]}`\n",
    "    *   This summary is specifically designed to be passed to an LLM as part of a prompt, enabling the LLM to make an informed, logical choice about the best layout for presenting a given piece of information.\n",
    "\n",
    "4.  **User-Friendly Configuration:**\n",
    "    *   While providing a detailed summary for the LLM, the script also generates a simplified `user_selections` section. This allows a human operator to easily map the system's semantic slide types (e.g., \"Agenda\", \"Summary\") to a specific layout index, providing a robust fallback and manual override capability.\n",
    "\n",
    "#### **How It Solves Critical Problems**\n",
    "\n",
    "*   **Eliminates Ambiguity:** By capturing the name, index, and position of every placeholder, it solves the problem of layouts with multiple placeholders of the same type (e.g., two content boxes). The system can now programmatically target the \"left column\" vs. the \"right column\".\n",
    "*   **Decouples Logic from Design:** The presentation generation agent no longer needs hard-coded assumptions about the template's design. All the logic for choosing and populating layouts is driven by the generated JSON file. This means the visual template can be updated or completely replaced without requiring changes to the core Python code.\n",
    "*   **Empowers the LLM:** It transforms a visual, unstructured design asset (the `.pptx` file) into a structured, well-defined set of \"tools\" (the layouts) that an LLM can reason about. This is the key to enabling more advanced tasks where the LLM doesn't just fill in content, but also makes decisions about the *visual structure* of the presentation.\n",
    "\n",
    "In summary, the Definitive PowerPoint Layout Inspector is the foundational component that makes the entire presentation generation process intelligent, configurable, and robust. It translates the abstract design of a template into concrete, actionable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5029d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this process the layauts from the slides provide to use them to process the plan (This require be change to the course of the system⚠️⚠️⚠️⚠️)\n",
    "# https://aistudio.google.com/prompts/18YaU5pG96eFMbM1l6yeG1v9j63PkLc5H\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER  \n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def generate_layout_capabilities(layout_name: str, placeholders: list) -> dict:\n",
    "    \n",
    "    essential_placeholders = [p for p in placeholders if p['type'] != 'SLIDE_NUMBER']\n",
    "    capabilities = {\"title_support\": \"none\", \"body_layout\": \"none\", \"specific_content_types\": []}\n",
    "    has_center_title = any(p['type'] == 'CENTER_TITLE' for p in essential_placeholders)\n",
    "    has_standard_title = any(p['type'] == 'TITLE' for p in essential_placeholders)\n",
    "    has_subtitle = any(p['type'] == 'SUBTITLE' for p in essential_placeholders)\n",
    "    if has_center_title: capabilities[\"title_support\"] = \"centered_title\"\n",
    "    elif has_standard_title: capabilities[\"title_support\"] = \"standard_title\"\n",
    "    if has_subtitle: capabilities[\"title_support\"] += \"_with_subtitle\"\n",
    "    body_placeholders = [p for p in essential_placeholders if p['type'] not in ('TITLE', 'CENTER_TITLE', 'SUBTITLE')]\n",
    "    if len(body_placeholders) == 0: capabilities[\"body_layout\"] = \"no_body\"\n",
    "    elif len(body_placeholders) == 1: capabilities[\"body_layout\"] = \"single_column\"\n",
    "    elif len(body_placeholders) > 1:\n",
    "        body_placeholders.sort(key=lambda p: p['left'])\n",
    "        if len(body_placeholders) > 1 and body_placeholders[1]['left'] > (body_placeholders[0]['left'] + body_placeholders[0]['width'] * 0.5):\n",
    "            capabilities[\"body_layout\"] = f\"{len(body_placeholders)}_column\"\n",
    "        else: capabilities[\"body_layout\"] = \"stacked_sections\"\n",
    "    specific_types = {p['type'] for p in body_placeholders if p['type'] not in ('BODY', 'OBJECT')}\n",
    "    if specific_types: capabilities[\"specific_content_types\"] = sorted(list(specific_types))\n",
    "    return capabilities\n",
    "\n",
    "\n",
    "def inspect_and_generate_layout_config(template_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Inspects a template, generates a machine-readable capabilities summary, and creates\n",
    "    a complete and definitive JSON configuration file for user and LLM use.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prs = Presentation(template_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not open or parse the presentation template at '{template_path}'. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Analyze all Layouts ---\n",
    "   \n",
    "    available_layouts = []\n",
    "    for i, layout in enumerate(prs.slide_layouts):\n",
    "        placeholder_details = []\n",
    "        for p in layout.placeholders:\n",
    "            placeholder_details.append({\n",
    "                \"idx\": p.placeholder_format.idx, \"type\": p.placeholder_format.type.name,\n",
    "                \"name\": p.name, \"left\": round(p.left.inches, 2), \"top\": round(p.top.inches, 2),\n",
    "                \"width\": round(p.width.inches, 2), \"height\": round(p.height.inches, 2)\n",
    "            })\n",
    "        capabilities = generate_layout_capabilities(layout.name, placeholder_details)\n",
    "        layout_info = {\"layout_index\": i, \"layout_name\": layout.name, \"capabilities\": capabilities, \"placeholders\": placeholder_details}\n",
    "        available_layouts.append(layout_info)\n",
    "\n",
    "    # --- 2. Create the Complete User-Facing Configuration Structure ---\n",
    "    def find_default_by_capability(layouts, capability_key, capability_value, fallback_index=1):\n",
    "        for layout in layouts:\n",
    "            if layout['capabilities'].get(capability_key) == capability_value:\n",
    "                return layout['layout_index']\n",
    "        # If no perfect match is found, return a safe fallback index\n",
    "        return fallback_index if len(layouts) > fallback_index else 0\n",
    "\n",
    "    # ** This map is now complete, explicitly including all required keys **\n",
    "    user_selection_map = {\n",
    "        \"Title\": { # title and subtitle \n",
    "            \"description\": \"Main title slide of the deck.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"title_support\", \"centered_title_with_subtitle\")\n",
    "        },\n",
    "        \"Agenda\": { # title and object  \n",
    "            \"description\": \"Agenda/Table of Contents slide.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Content\": { # title and object \n",
    "            \"description\": \"Default slide for a standard topic with bullet points.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Content_Two_Column\": { # title and 2 objects \n",
    "            \"description\": \"Slide for side-by-side content, like comparisons.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"2_column\")\n",
    "        },\n",
    "        \"Content_child\": { ## # title, subtitle and object\n",
    "            \"description\": \"Default slide for a standard topic with bullet points.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Content_Two_Column_child\": { ## title, subtitle and 2 objects \n",
    "            \"description\": \"Slide for side-by-side content, like comparisons.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"2_column\")\n",
    "        },\n",
    "        \"Application\": {# title and  object \n",
    "            \"description\": \"Slide for interactive questions ('Let's Apply This!').\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Application_Two_Column \": { # title and 2 objects \n",
    "            \"description\": \"Slide for side-by-side content ('Let's Apply This!').\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"2_column\")\n",
    "        },\n",
    "        \"Summary\": { # title and object\n",
    "            \"description\": \"The final summary slide of the deck.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"End\": { # title \n",
    "            \"description\": \"The final 'Thank You / Questions?' slide.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"title_support\", \"centered_title\", fallback_index=7)\n",
    "        },\n",
    "        \"Divider\": { # title and object\n",
    "            \"description\": \"Slide that divide sections general use base on the agenda.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"title_support\", \"centered_title\", fallback_index=7)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_to_save = {\n",
    "        \"//_INSTRUCTIONS\": \"This file describes the available slide layouts. The 'available_layouts' section is for an LLM to read. The 'user_selections' section is for you to edit. Please verify the 'selected_layout_index' for each slide type.\",\n",
    "        \"template_file\": os.path.basename(template_path),\n",
    "        \"user_selections\": user_selection_map,\n",
    "        \"available_layouts\": available_layouts\n",
    "    }\n",
    "\n",
    "    # --- 3. Save the Configuration File ---\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_to_save, f, indent=4)\n",
    "        print_header(\"Configuration Generated Successfully\", char=\"=\")\n",
    "        print(f\"A new, human-readable configuration file has been saved to:\\n{output_path}\")\n",
    "        print(\"\\nPlease open this file to review the classifications and edit your layout selections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write the layout configuration file to '{output_path}'. Error: {e}\")\n",
    "        \n",
    "# # --- Execution ---\n",
    "# SLIDE_TEMPLATE_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/slide_style/slide_style_test_2.pptx\"\n",
    "# LAYOUT_MAPPING_PATH = \"/home/sebas_dev_linux/projects/course_generator/configs/layout_mapping_test.json\"\n",
    "# # You run this function to generate the config file.\n",
    "# # Make sure SLIDE_TEMPLATE_PATH and LAYOUT_MAPPING_PATH are defined.\n",
    "# inspect_and_generate_layout_config(SLIDE_TEMPLATE_PATH, LAYOUT_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline_robust(\n",
    "    input_filepath: str, \n",
    "    output_filepath: str, \n",
    "    prompt_template: str,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    logger.info(f\"Starting to robustly process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from file: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    client = ollama.Client(host=OLLAMA_HOST)\n",
    "    current_prompt = prompt_template.format(outline_text=outline_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{max_retries} to parse outline.\")\n",
    "        \n",
    "        try:\n",
    "            # Call the LLM\n",
    "            llm_output_str = call_ollama_with_retry(client, current_prompt)\n",
    "            \n",
    "            # Find the JSON blob in the response\n",
    "            json_blob = parse_llm_json_output(llm_output_str) # Your existing helper\n",
    "            if not json_blob:\n",
    "                raise ValueError(\"LLM did not return a parsable JSON object.\")\n",
    "\n",
    "            # *** THE KEY VALIDATION STEP ***\n",
    "            # Try to parse the dictionary into your Pydantic model.\n",
    "            # This will raise a `ValidationError` if keys are wrong, types are wrong, or fields are missing.\n",
    "            parsed_data = ParsedUnitOutline.model_validate(json_blob)\n",
    "            \n",
    "            # If successful, save the validated data and exit the loop\n",
    "            logger.info(\"Successfully validated JSON structure against Pydantic model.\")\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                # Use .model_dump_json() for clean, validated output\n",
    "                f.write(parsed_data.model_dump_json(indent=2)) \n",
    "\n",
    "            logger.info(f\"Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "            return # Exit function on success\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Validation failed on attempt {attempt + 1}. Error: {e}\")\n",
    "            # Formulate a new prompt with the error message for self-correction\n",
    "            error_feedback = (\n",
    "                f\"\\n\\nYour previous attempt failed. You MUST correct the following errors:\\n\"\n",
    "                f\"{e}\\n\\n\"\n",
    "                f\"Please regenerate the entire JSON object, ensuring it strictly adheres to the schema \"\n",
    "                f\"and corrects these specific errors. Do not change any key names.\"\n",
    "            )\n",
    "            current_prompt = current_prompt + error_feedback # Append the error to the prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch other errors like network issues from call_ollama_with_retry\n",
    "            logger.error(f\"An unexpected error occurred on attempt {attempt + 1}: {e}\", exc_info=True)\n",
    "            # You might want to wait before retrying for non-validation errors\n",
    "            time.sleep(5)\n",
    "\n",
    "    logger.error(f\"Failed to get valid structured data from the LLM after {max_retries} attempts.\")\n",
    "\n",
    "\n",
    "# --- In your execution block, call the new function ---\n",
    "# parse_and_save_outline(...) becomes:\n",
    "\n",
    "if EXTRACT_UO:\n",
    "    parse_and_save_outline_robust(\n",
    "        input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "        output_filepath=PARSED_UO_JSON_PATH,\n",
    "        prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EPUB ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\n",
      "✅ Successfully wrote EPUB ToC with IDs and links to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Book Table of Contents (ToC) with Pre-assigned IDs & Links in Order\n",
    "\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import urllib.parse # Needed to clean up links\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS (MODIFIED TO INCLUDE ID ASSIGNMENT AND LINK EXTRACTION)\n",
    "# ==============================================================================\n",
    "\n",
    "def clean_epub_href(href: str) -> str:\n",
    "    \"\"\"Removes URL fragments and decodes URL-encoded characters.\"\"\"\n",
    "    if not href: return \"\"\n",
    "    # Remove fragment identifier (e.g., '#section1')\n",
    "    cleaned_href = href.split('#')[0]\n",
    "    # Decode any URL-encoded characters (e.g., %20 -> space)\n",
    "    return urllib.parse.unquote(cleaned_href)\n",
    "\n",
    "# --- EPUB Extraction Logic ---\n",
    "def parse_navpoint(navpoint: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 2 navPoints and assigns a toc_id and link_filename.\"\"\"\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    if not title: return None\n",
    "    \n",
    "    # --- MODIFICATION: Extract the linked filename ---\n",
    "    content_tag = navpoint.find('content', recursive=False)\n",
    "    link_filename = clean_epub_href(content_tag['src']) if content_tag else \"\"\n",
    "    \n",
    "    node = {\n",
    "        \"level\": level,\n",
    "        \"toc_id\": counter[0],\n",
    "        \"title\": title,\n",
    "        \"link_filename\": link_filename, # Add the cleaned link\n",
    "        \"children\": []\n",
    "    }\n",
    "    counter[0] += 1\n",
    "    \n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, counter, level + 1)\n",
    "        if child_node: node[\"children\"].append(child_node)\n",
    "        \n",
    "    return node\n",
    "\n",
    "def parse_li(li_element: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 3 <li> elements and assigns a toc_id and link_filename.\"\"\"\n",
    "    a_tag = li_element.find('a', recursive=False)\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        if not title: return None\n",
    "        \n",
    "        # --- MODIFICATION: Extract the linked filename ---\n",
    "        link_filename = clean_epub_href(a_tag.get('href'))\n",
    "        \n",
    "        node = {\n",
    "            \"level\": level,\n",
    "            \"toc_id\": counter[0],\n",
    "            \"title\": title,\n",
    "            \"link_filename\": link_filename, # Add the cleaned link\n",
    "            \"children\": []\n",
    "        }\n",
    "        counter[0] += 1\n",
    "        \n",
    "        nested_ol = li_element.find('ol', recursive=False)\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, counter, level + 1)\n",
    "                if child_node: node[\"children\"].append(child_node)\n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_epub_toc(epub_path, output_json_path):\n",
    "    print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "    toc_data = []\n",
    "    book = epub.read_epub(epub_path)\n",
    "    id_counter = [0]\n",
    "    \n",
    "    for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "        soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "        # Logic to handle both EPUB 2 (NCX) and EPUB 3 (XHTML)\n",
    "        if nav_item.get_name().endswith('.ncx'):\n",
    "            print(\"INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\")\n",
    "            navmap = soup.find('navMap')\n",
    "            if navmap:\n",
    "                for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                    node = parse_navpoint(navpoint, id_counter, level=0)\n",
    "                    if node: toc_data.append(node)\n",
    "        else: # Assumes EPUB 3\n",
    "            print(\"INFO: Found EPUB 3 (XHTML) Table of Contents. Parsing...\")\n",
    "            toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "            if toc_nav:\n",
    "                top_ol = toc_nav.find('ol', recursive=False)\n",
    "                if top_ol:\n",
    "                    for li in top_ol.find_all('li', recursive=False):\n",
    "                        node = parse_li(li, id_counter, level=0)\n",
    "                        if node: toc_data.append(node)\n",
    "        if toc_data: break\n",
    "    \n",
    "    if toc_data:\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote EPUB ToC with IDs and links to: {output_json_path}\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# --- PDF Extraction Logic (Unchanged) ---\n",
    "def build_pdf_hierarchy_with_ids(toc_list: List) -> List[Dict]:\n",
    "    root = []\n",
    "    parent_stack = {-1: {\"children\": root}}\n",
    "    id_counter = [0]\n",
    "    for level, title, page in toc_list:\n",
    "        normalized_level = level - 1\n",
    "        node = {\"level\": normalized_level, \"toc_id\": id_counter[0], \"title\": title.strip(), \"page\": page, \"children\": []}\n",
    "        id_counter[0] += 1\n",
    "        parent_node = parent_stack.get(normalized_level - 1)\n",
    "        if parent_node: parent_node[\"children\"].append(node)\n",
    "        parent_stack[normalized_level] = node\n",
    "    return root\n",
    "\n",
    "def extract_pdf_toc(pdf_path, output_json_path):\n",
    "    print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        hierarchical_toc = []\n",
    "        if not toc: print(\"❌ WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "        else:\n",
    "            print(f\"INFO: Found {len(toc)} bookmark entries. Building hierarchy and assigning IDs...\")\n",
    "            hierarchical_toc = build_pdf_hierarchy_with_ids(toc)\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote PDF ToC with assigned IDs to: {output_json_path}\")\n",
    "    except Exception as e: print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if PROCESS_EPUB:\n",
    "    extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "else:\n",
    "    extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effd9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Hierarchical Vector Database (with Sequential ToC ID and Chunk ID)\n",
    "# This cell processes the book, enriches it with hierarchical and sequential metadata,\n",
    "# chunks it, and creates the final vector database.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper: Clean metadata values for ChromaDB ---\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "    if isinstance(value, list): return \", \".join(map(str, value))\n",
    "    if isinstance(value, dict): return json.dumps(value)\n",
    "    if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    return str(value)\n",
    "\n",
    "# --- Core Function to Process Book with Pre-extracted ToC ---\n",
    "def process_book_with_extracted_toc(\n",
    "    book_path: str,\n",
    "    extracted_toc_json_path: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "    logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "    # 1. Load the pre-extracted hierarchical ToC\n",
    "    try:\n",
    "        with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        if not hierarchical_toc:\n",
    "            logger.error(f\"Pre-extracted ToC at '{extracted_toc_json_path}' is empty or invalid.\")\n",
    "            return [], []\n",
    "        logger.info(f\"Successfully loaded pre-extracted ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pre-extracted ToC JSON: {e}\", exc_info=True)\n",
    "        return [], []\n",
    "\n",
    "    # 2. Load all text elements/pages from the book\n",
    "    all_raw_book_docs: List[Document] = []\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "    if file_extension == \".epub\":\n",
    "        loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} text elements from EPUB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EPUB content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    elif file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} pages from PDF.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    else:\n",
    "        logger.error(f\"Unsupported book file format: {file_extension}\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    if not all_raw_book_docs:\n",
    "        logger.error(\"No text elements/pages loaded from the book.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    # 3. Create enriched LangChain Documents by matching ToC to content\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "    # Flatten the ToC, AND add a unique sequential ID for sorting and validation.\n",
    "    flat_toc_entries: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def _add_ids_and_flatten_recursive(nodes: List[Dict[str, Any]], current_titles_path: List[str], counter: List[int]):\n",
    "        \"\"\"\n",
    "        Recursively traverses ToC nodes to flatten them and assign a unique, sequential toc_id.\n",
    "        \"\"\"\n",
    "        for node in nodes:\n",
    "            toc_id = counter[0]\n",
    "            counter[0] += 1\n",
    "            title = node.get(\"title\", \"\").strip()\n",
    "            if not title: continue\n",
    "            new_titles_path = current_titles_path + [title]\n",
    "            entry = {\n",
    "                \"titles_path\": new_titles_path,\n",
    "                \"level\": node.get(\"level\"),\n",
    "                \"full_title_for_matching\": title,\n",
    "                \"toc_id\": toc_id\n",
    "            }\n",
    "            if \"page\" in node: entry[\"page\"] = node[\"page\"]\n",
    "            flat_toc_entries.append(entry)\n",
    "            if node.get(\"children\"):\n",
    "                _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter)\n",
    "\n",
    "    toc_id_counter = [0]\n",
    "    _add_ids_and_flatten_recursive(hierarchical_toc, [], toc_id_counter)\n",
    "    logger.info(f\"Flattened ToC and assigned sequential IDs to {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "    # Logic for PDF metadata assignment\n",
    "    if file_extension == \".pdf\" and any(\"page\" in entry for entry in flat_toc_entries):\n",
    "        logger.info(\"Assigning metadata to PDF pages based on ToC page numbers...\")\n",
    "        flat_toc_entries.sort(key=lambda x: x.get(\"page\", -1) if x.get(\"page\") is not None else -1)\n",
    "        for page_doc in all_raw_book_docs:\n",
    "            page_num_0_indexed = page_doc.metadata.get(\"page\", -1)\n",
    "            page_num_1_indexed = page_num_0_indexed + 1\n",
    "            assigned_metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "            best_match_toc_entry = None\n",
    "            for toc_entry in flat_toc_entries:\n",
    "                toc_page = toc_entry.get(\"page\")\n",
    "                if toc_page is not None and toc_page <= page_num_1_indexed:\n",
    "                    if best_match_toc_entry is None or toc_page > best_match_toc_entry.get(\"page\", -1):\n",
    "                        best_match_toc_entry = toc_entry\n",
    "                elif toc_page is not None and toc_page > page_num_1_indexed:\n",
    "                    break\n",
    "            if best_match_toc_entry:\n",
    "                for i, title_in_path in enumerate(best_match_toc_entry[\"titles_path\"]):\n",
    "                    assigned_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                assigned_metadata['toc_id'] = best_match_toc_entry.get('toc_id')\n",
    "            else:\n",
    "                assigned_metadata[\"level_1_title\"] = \"Uncategorized PDF Page\"\n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in assigned_metadata.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=cleaned_meta))\n",
    "\n",
    "    # Logic for EPUB metadata assignment\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Assigning metadata to EPUB elements by matching ToC titles in text...\")\n",
    "        toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "        current_hierarchy_metadata = {}\n",
    "        for element_doc in all_raw_book_docs:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            if not element_text: continue\n",
    "            for toc_entry in toc_titles_for_search:\n",
    "                if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "                    current_hierarchy_metadata = {\"source\": os.path.basename(book_path)}\n",
    "                    for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "                        current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                    current_hierarchy_metadata['toc_id'] = toc_entry.get('toc_id')\n",
    "                    if \"page\" in toc_entry: current_hierarchy_metadata[\"epub_toc_page\"] = toc_entry[\"page\"]\n",
    "                    break\n",
    "            if not current_hierarchy_metadata:\n",
    "                doc_metadata_to_assign = {\"source\": os.path.basename(book_path), \"level_1_title\": \"EPUB Preamble\", \"toc_id\": -1}\n",
    "            else:\n",
    "                doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata_to_assign.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=element_text, metadata=cleaned_meta))\n",
    "    \n",
    "    else: # Fallback\n",
    "        final_documents_with_metadata = all_raw_book_docs\n",
    "\n",
    "    if not final_documents_with_metadata:\n",
    "        logger.error(\"No documents were processed or enriched with hierarchical metadata.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks, inheriting hierarchical metadata.\")\n",
    "    \n",
    "    # --- MODIFICATION START: Add a unique, sequential chunk_id to each chunk ---\n",
    "    logger.info(\"Assigning sequential chunk_id to all final chunks...\")\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        chunk.metadata['chunk_id'] = i\n",
    "    logger.info(f\"Assigned chunk_ids from 0 to {len(final_chunks) - 1}.\")\n",
    "    # --- MODIFICATION END ---\n",
    "\n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "# --- Main Execution Block for this Cell ---\n",
    "if CREATE_RAG_BOOK:\n",
    "    if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "        logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "        logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "    else:\n",
    "        final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "            book_path=BOOK_PATH,\n",
    "            extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "\n",
    "        if final_chunks_for_db:\n",
    "            if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "                logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "                shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "            logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "            embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "            \n",
    "            vector_db = Chroma.from_documents(\n",
    "                documents=final_chunks_for_db,\n",
    "                embedding=embedding_model,\n",
    "                persist_directory=CHROMA_PERSIST_DIR,\n",
    "                collection_name=CHROMA_COLLECTION_NAME\n",
    "            )\n",
    "            \n",
    "            reloaded_db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model, collection_name=CHROMA_COLLECTION_NAME)\n",
    "            count = reloaded_db._collection.count()\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            logger.info(f\"✅ Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "            logger.info(f\"✅ Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            logger.error(\"❌ Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41a6d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:18,039 - INFO - Loading pre-extracted ToC and raw EPUB elements...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "                         EPUB Pre-Processing Inspection                         \n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-07-14 17:26:22,623 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-07-14 17:26:22,624 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-07-14 17:26:26,150 - INFO - Successfully loaded 11815 raw text elements from the EPUB.\n",
      "2025-07-14 17:26:26,151 - INFO - Flattening the hierarchical ToC for matching...\n",
      "2025-07-14 17:26:26,152 - INFO - Flattened ToC into 877 entries.\n",
      "2025-07-14 17:26:26,152 - INFO - Assigning metadata to EPUB elements by matching ToC titles...\n",
      "2025-07-14 17:26:26,350 - INFO - Processing complete. Generated 11483 documents with assigned metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                 INSPECTION RESULTS: Documents Before Chunking                  \n",
      "================================================================================\n",
      "Total documents created: 11483\n",
      "\n",
      "--- Document [1] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Guide to Computer Forensics and Investigations: Processing Digital Evidence'\n",
      "-------------------------\n",
      "\n",
      "--- Document [2] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Copyright Statement'\n",
      "-------------------------\n",
      "\n",
      "--- Document [3] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Guide to Computer Forensics and Investigations: Processing Digital Evidence'\n",
      "-------------------------\n",
      "\n",
      "--- Document [4] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'COPYRIGHT © 2019, 2016 Cengage Learning, Inc.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [5] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'ALL RIGHTS RESERVED. No part of this work covered by the copyright herein may be reproduced or distributed in any form or by any means, except as permitted by U.S. copyright law, without the prior written permission of the copyright owner.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [6] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'For product information and technology assistance, contact us at Cengage Customer & Sales Support, 1-800-354-9706 or support.cengage.com.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [7] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'For permission to use material from this text or product, submit all requests online at www.cengage.com/permissions.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [8] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'SOURCE FOR ILLUSTRATIONS: Copyright © Cengage.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [9] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> ', Microsoft® is a registered trademark of the Microsoft Corporation.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [10] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Library of Congress Control Number: 2018936389'\n",
      "-------------------------\n",
      "\n",
      "--- Document [11] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'ISBN: 978-1-337-56894-4'\n",
      "-------------------------\n",
      "\n",
      "--- Document [12] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Cengage'\n",
      "-------------------------\n",
      "\n",
      "--- Document [13] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> '20 Channel Center Street'\n",
      "-------------------------\n",
      "\n",
      "--- Document [14] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Boston MA 02210'\n",
      "-------------------------\n",
      "\n",
      "--- Document [15] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'USA'\n",
      "-------------------------\n",
      "\n",
      "--- Document [16] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Cengage is a leading provider of customized learning solutions with employees residing in nearly 40 different countries and sales in more than 125 countries around the world. Find your local representative at www.cengage.com.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [17] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Cengage products are represented in Canada by Nelson Education, Ltd.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [18] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'To learn more about Cengage platforms and services, visit www.cengage.com.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [19] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'To register or access your online learning solution or purchase materials for your course, visit www.cengagebrain.com.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [20] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'EPUB Preamble', 'toc_id': -1}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Notice to the Reader Publisher does not warrant or guarantee any of the products described herein or perform any independent analysis in connection with any of the product information contained herein. Publisher does not assume, and expressly disclaims, any obligation to obtain and include information other than that provided to it by the manufacturer. The reader is expressly warned to consider and adopt all safety precautions that might be indicated by the activities described herein and to avoid all potential hazards. By following the instructions contained herein, the reader willingly assumes all risks in connection with such instructions. The publisher makes no representations or warranties of any kind, including but not limited to, the warranties of fitness for particular purpose or merchantability, nor are any such representations implied with respect to the material set forth herein, and the publisher takes no responsibility with respect to such material. The publisher shall not be liable for any special, consequential, or exemplary damages resulting, in whole or part, from the readers’ use of, or reliance upon, this material.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [21] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Preface'\n",
      "-------------------------\n",
      "\n",
      "--- Document [22] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Guide to Computer Forensics and Investigations is now in its sixth edition. As digital technology and cyberspace have evolved from their early roots as basic communication platforms into the hyper-connected world we live in today, so has the demand for people who have the knowledge and skills to investigate legal and technical issues involving computers and digital technology. My sincere compliments to the authors and publishing staff who have made this textbook such a remarkable resource for thousands of students and practitioners worldwide.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [23] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Computers, the Internet, and the world’s digital ecosystem are all instrumental in how we conduct our daily lives. When the founding fathers of the modern computing era were designing the digital infrastructure as we know it today, security and temporal accountability issues were not at the top of their list of things to do. The technological advancement of these systems over the past 10 years has changed the way we learn, socialize, and conduct business. Finding digital data that can be used as evidence to incriminate or exonerate a suspect accused in a legal or administrative proceeding is not an easy task.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [24] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Cyberthreats have become pervasive in modern society. They range from simple computer viruses to complex ransomware and cyber extortion schemes. The ability to conduct sophisticated digital forensics investigations has become a requirement in both the government and commercial sectors. Currently, the organizations and agencies whose job it is to investigate both criminal and civil matters involving the use of rapidly developing digital technology often struggle to keep up with the ever-changing digital landscape. Additionally, finding trained and qualified people to conduct these types of inquiries has been challenging.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [25] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Today, an entire industry has evolved for the purpose of investigating events occurring in cyberspace to include incidents involving international and corporate espionage, massive data breaches, and even cyberterrorism. The opportunities for employment in this field are expanding every day. Professionals in this exciting field of endeavor are now in high demand and are expected to have multiple skill sets in areas such as malware analysis, cloud computing, social media, and mobile device forensics.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [26] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Guide to Computer Forensics and Investigations can now be found in both academic and professional environments as a reliable source of current technical information and practical exercises concerning investigations involving the latest digital technologies. It’s my belief that this book, combined with an enthusiastic and knowledgeable facilitator, makes for a fascinating course of instruction.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [27] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'As I have stated to many of my students in the past, it’s not just laptop computers and servers that harbor the binary code of ones and zeros, but an infinite array of digital devices. If one of these devices retains evidence of a crime, it’s up to newly trained and educated digital detectives to find the evidence in a forensically sound manner. This book will assist both students and practitioners in accomplishing this goal.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [28] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Respectfully,'\n",
      "-------------------------\n",
      "\n",
      "--- Document [29] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'John A. Sgromolo'\n",
      "-------------------------\n",
      "\n",
      "--- Document [30] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Preface', 'toc_id': 3}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'As a Senior Special Agent, John was one of the founding members of the NCIS Computer Crime Investigations Group. John left government service to run his own company, Digital Forensics, Inc., and has taught hundreds of law enforcement and corporate students nationwide in the art and science of digital forensics investigations. Currently, he serves as a senior consultant for Verizon’s Global Security Services, where he helps manage the Threat Intel Response Service.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [31] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Introduction'\n",
      "-------------------------\n",
      "\n",
      "--- Document [32] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Computer forensics, now most commonly called “digital forensics,” has been a professional field for many years, but most well-established experts in the field have been self-taught. The growth of the Internet and the worldwide proliferation of computers have increased the need for digital investigations. Computers can be used to commit crimes, and crimes can be recorded on computers, including company policy violations, embezzlement, e-mail harassment, murder, leaks of proprietary information, and even terrorism. Law enforcement, network administrators, attorneys, and private investigators now rely on the skills of professional digital forensics experts to investigate criminal and civil cases.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [33] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'This book is not intended to provide comprehensive training in digital forensics. It does, however, give you a solid foundation by introducing digital forensics to those who are new to the field. Other books on digital forensics are targeted to experts; this book is intended for novices who have a thorough grounding in computer and networking basics.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [34] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The new generation of digital forensics experts needs more initial training because operating systems, computer and mobile device hardware, and forensics software tools are changing more quickly. This book covers current and past operating systems and a range of hardware, from basic workstations and high-end network servers to a wide array of mobile devices. Although this book focuses on a few forensics software tools, it also reviews and discusses other currently available tools.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [35] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The purpose of this book is to guide you toward becoming a skilled digital forensics investigator. A secondary goal is to help you pass related certification exams. As the field of digital forensics and investigations matures, keep in mind that certifications will change. You can find more information on certifications in Chapter 2 and Appendix A.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [36] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Intended Audience'\n",
      "-------------------------\n",
      "\n",
      "--- Document [37] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Although this book can be used by people with a wide range of backgrounds, it’s intended for those with A+ and Network+ certifications or the equivalent. A networking background is necessary so that you understand how computers operate in a networked environment and can work with a network administrator when needed. In addition, you must know how to use a computer from the command line and how to use common operating systems, including Windows, Linux, and macOS, and their related hardware.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [38] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'This book can be used at any educational level, from technical high schools and community colleges to graduate students. Current professionals in the public and private sectors can also use this book. Each group will approach investigative problems from a different perspective, but all will benefit from the coverage.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [39] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'What’s New in This Edition'\n",
      "-------------------------\n",
      "\n",
      "--- Document [40] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The chapter flow of this book is organized so that you’re first exposed to what happens in a forensics lab and how to set one up before you get into the nuts and bolts. Coverage of several GUI tools has been added to give you a familiarity with some widely used software. In addition, Chapter 11 has additional coverage of social media forensics, Chapter 12 has been expanded to include more information on smartphones and tablets, and Chapter 13 on forensics procedures for information stored in the cloud has been updated. Corrections have been made to this edition based on feedback from users, and all software tools and Web sites have been updated to reflect what’s current at the time of publication. Finally, a new digital lab manual is being offered in MindTap for Guide to Computer Forensics and Investigationsto go with the sixth edition textbook.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [41] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter Descriptions'\n",
      "-------------------------\n",
      "\n",
      "--- Document [42] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Here is a summary of the topics covered in each chapter of this book:'\n",
      "-------------------------\n",
      "\n",
      "--- Document [43] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 1 , “Understanding the Digital Forensics Profession and Investigations,” introduces you to the history of digital forensics and explains how the use of electronic evidence developed. It also reviews legal issues and compares public and private sector cases. This chapter also explains how to take a systematic approach to preparing a digital investigation, describes how to conduct an investigation, and summarizes requirements for workstations and software.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [44] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 2 , “The Investigator’s Office and Laboratory,” outlines physical requirements and equipment for digital forensics labs, from small private investigators’ labs to the regional FBI lab. It also covers certifications for digital investigators and building a business case for a forensics lab.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [45] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 3 , “Data Acquisition,” explains how to prepare to acquire data from a suspect’s drive and discusses available Linux and GUI acquisition tools. This chapter also discusses acquiring data from RAID systems and gives you an overview of tools for remote acquisitions.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [46] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 4 , “Processing Crime and Incident Scenes,” explains search warrants and the nature of a typical digital forensics case. It discusses when to use outside professionals, how to assemble a team, and how to evaluate a case and explains the correct procedures for searching and seizing evidence. This chapter also introduces you to calculating hashes to verify data you collect.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [47] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 5 , “Working with Windows and CLI Systems,” discusses the most common operating systems. You learn what happens and what files are altered during computer startup and how file systems deal with deleted and slack space. In addition, this chapter covers some options for decrypting drives encrypted with whole disk encryption and explains the purpose of using virtual machines.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [48] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 6 , “Current Digital Forensics Tools,” explores current digital forensics software and hardware tools, including those that might not be readily available, and evaluates their strengths and weaknesses.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [49] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 7 , “Linux and Macintosh File Systems,” continues the operating system discussion from Chapter 5 by examining Macintosh and Linux OSs and file systems. It also gives you practice in using Linux forensics tools.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [50] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 8 , “Recovering Graphics Files,” explains how to recover graphics files and examines data compression, carving data, reconstructing file fragments, and steganography and copyright issues.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [51] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 9 , “Digital Forensics Analysis and Validation,” covers determining what data to collect and analyze and refining investigation plans. It also explains validation with hex editors and forensics software and data-hiding techniques.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [52] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 10 , “Virtual Machine Forensics, Live Acquisitions, and Network Forensics,” covers tools and methods for conducting forensic analysis of virtual machines, performing live acquisitions, reviewing network logs for evidence, and using network-monitoring tools to detect unauthorized access. It also examines using Linux tools and the Honeynet Project’s resources.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [53] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 11 , “E-mail and Social Media Investigations,” examines e-mail crimes and violations and reviews some specialized e-mail and social media forensics tools. It also explains how to approach investigating social media communications and handling the challenges this content poses.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [54] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 12 , “Mobile Device Forensics and The Internet of Anything,” covers investigation techniques and acquisition procedures for smartphones, other mobile devices, Internet of Anything devices, and sensors. You learn where data might be stored or backed up and what tools are available for these investigations.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [55] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 13 , “Cloud Forensics,” summarizes the legal and technical challenges in conducting cloud forensics. It also describes how to acquire cloud data and explains how remote acquisition tools can be used in cloud investigations.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [56] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 14 , “Report Writing for High-Tech Investigations,” discusses the importance of report writing in digital forensics examinations; offers guidelines on report content, structure, and presentation; and explains how to generate report findings with forensics software tools.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [57] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 15 , “Expert Testimony in Digital Investigations,” explores the role of an expert witness or a fact witness, including developing a curriculum vitae, understanding the trial process, and preparing forensics evidence for testimony. It also offers guidelines for testifying in court and at depositions and hearings.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [58] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter 16 , “Ethics for the Expert Witness,” provides guidance in the principles and practice of ethics for digital forensics investigators and examines other professional organizations’ codes of ethics.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [59] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Appendix A , “Certification Test References,” provides information on the National Institute of Standards and Technology (NIST) testing processes for validating digital forensics tools and covers digital forensics certifications and training programs.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [60] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Appendix B , “Digital Forensics References,” lists recommended books, journals, e-mail lists, and Web sites for additional information and further study. It also covers the latest ISO 27000 standards that apply to digital forensics.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [61] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Appendix C , “Digital Forensics Lab Considerations,” provides more information on considerations for forensics labs, including certifications, ergonomics, structural design, and communication and fire-suppression systems. It also covers applicable ISO standards.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [62] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Appendix D , “Legacy File System and Forensics Tools,” reviews FAT file system basics and Mac legacy file systems and explains using DOS forensics tools, creating forensic boot media, and using scripts. It also has an overview of the hexadecimal numbering system and how it’s applied to digital information.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [63] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Features'\n",
      "-------------------------\n",
      "\n",
      "--- Document [64] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'To help you fully understand digital forensics, this book includes many features designed to enhance your learning experience:'\n",
      "-------------------------\n",
      "\n",
      "--- Document [65] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter objectives—Each chapter begins with a detailed list of the concepts to be mastered in that chapter. This list gives you a quick reference to the chapter’s contents and is a useful study aid.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [66] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Figures and tables—Screenshots are used as guidelines for stepping through commands and forensics tools. For tools not included with the book or that aren’t offered in free demo versions, figures have been added when possible to illustrate the tool’s interface. Tables are used throughout the book to present information in an organized, easy-to-grasp manner.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [67] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Chapter summaries—Each chapter’s material is followed by a summary of the concepts introduced in that chapter. These summaries are a helpful way to review the ideas covered in each chapter.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [68] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Key terms—Following the chapter summary, all new terms introduced in the chapter with boldfaced text are gathered together in the Key Terms list. This list encourages a more thorough understanding of the chapter’s key concepts and is a useful reference.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [69] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Review questions—The end-of-chapter assessment begins with a set of review questions that reinforce the main concepts in each chapter. These questions help you evaluate and apply the material you have learned.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [70] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Hands-on projects—Although understanding the theory behind digital technology is important, nothing can improve on real-world experience. To this end, each chapter offers several hands-on projects with software supplied as free downloads on the student companion site and in MindTap. You can explore a variety of ways to acquire and even hide evidence. For the conceptual chapters, research projects are supplied.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [71] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Case projects—At the end of each chapter are case projects. To do these projects, you must draw on real-world common sense as well as your knowledge of the technical topics covered to that point in the book. Your goal for each project is to come up with answers to problems similar to those you’ll face as a working digital forensics investigator.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [72] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Software and student data files—Student data files are available for download from the student companion site and the MindTap for this book and are used for activities and projects in the chapters. Demo and freeware software used in this book can be downloaded from the Web sites specified in activities and projects or in “Digital Forensics Software” later in this introduction.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [73] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Student companion site—To access the student companion site, go to www.cengagebrain.com and search for the sixth edition by entering the title, author’s name, or ISBN. On the product page, click the Free Materials tab, and then click Save to MyHome. Then you can sign in as a returning student or choose to create a new account. After you’ve logged on, you can begin accessing your free study tools.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [74] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Text and Graphic Conventions'\n",
      "-------------------------\n",
      "\n",
      "--- Document [75] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'When appropriate, additional information and exercises have been added to this book to help you better understand the topic at hand. The following icons used in this book alert you to additional materials:'\n",
      "-------------------------\n",
      "\n",
      "--- Document [76] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Note'\n",
      "-------------------------\n",
      "\n",
      "--- Document [77] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Note Box'\n",
      "-------------------------\n",
      "\n",
      "--- Document [78] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The Note icon draws your attention to additional helpful material related to the subject being covered.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [79] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Tip'\n",
      "-------------------------\n",
      "\n",
      "--- Document [80] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Tips based on the authors’ experiences offer extra information about how to attack a problem or what to do in real-world situations.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [81] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Caution'\n",
      "-------------------------\n",
      "\n",
      "--- Document [82] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Caution Box'\n",
      "-------------------------\n",
      "\n",
      "--- Document [83] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Introduction', 'toc_id': 4}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The Caution icon warns you about potential mistakes or problems and explains how to avoid them.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [84] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Hands-On Projects', 'toc_id': 53}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Hands-On Projects'\n",
      "-------------------------\n",
      "\n",
      "--- Document [85] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Hands-On Projects', 'toc_id': 53}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The hands-on icon indicates that the projects following it give you a chance to practice using software tools and get hands-on experience.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [86] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Case Projects'\n",
      "-------------------------\n",
      "\n",
      "--- Document [87] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'This icon marks the start of projects that require you to apply common sense and knowledge to solving problems involving that chapter’s concepts.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [88] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Student Resources'\n",
      "-------------------------\n",
      "\n",
      "--- Document [89] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'MindTap for Guide to Computer Forensics and Investigations helps you learn on your terms.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [90] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Instant access in your pocket: Take advantage of the MindTap Mobile App to learn on your terms. Read or listen to textbooks and study with the aid of instructor notifications, flashcards, and practice quizzes.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [91] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'MindTap helps you create your own potential. Gear up for ultimate success: Track your scores and stay motivated toward your goals. Whether you have more work to do or are ahead of the curve, you’ll know where you need to focus your efforts. The MindTap Green Dot will charge your confidence along the way.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [92] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'MindTap helps you own your progress. Make your textbook yours; no one knows what works for you better than you. Highlight key text, add notes, and create custom flashcards. When it’s time to study, everything you’ve flagged or noted can be gathered into a guide you can organize.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [93] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Lab Manual'\n",
      "-------------------------\n",
      "\n",
      "--- Document [94] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'A new digital lab manual is being offered in the MindTap for Guide to Computer Forensics and Investigations to give you additional hands-on experience.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [95] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Lab Requirements'\n",
      "-------------------------\n",
      "\n",
      "--- Document [96] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'The hands-on projects in this book help you apply what you have learned about digital forensics techniques. The following sections list the minimum requirements for completing all the projects in this book. In addition to the items listed, you must be able to download and install demo versions of software.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [97] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Note'\n",
      "-------------------------\n",
      "\n",
      "--- Document [98] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Note Box'\n",
      "-------------------------\n",
      "\n",
      "--- Document [99] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Magnet AXIOM has a 30-day trial for download. If you aren’t purchasing its academic license or are planning to do the labs at home, it’s recommended that you wait until Chapter 11 to begin using this tool so that your trial doesn’t expire before you finish the projects. In addition, wait until Chapter 11 before doing Hands-On Project 6–5, installing Magnet AXIOM.'\n",
      "-------------------------\n",
      "\n",
      "--- Document [100] ---\n",
      "  Assigned Metadata: {'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_1_title': 'Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'level_2_title': 'Chapter Review', 'level_3_title': 'Case Projects', 'toc_id': 54}\n",
      "  Content (Un-chunked Element):\n",
      "  >> 'Note'\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5a: Inspecting EPUB Documents and Metadata BEFORE Chunking\n",
    "\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- Setup Logger for this inspection cell ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def inspect_epub_preprocessing():\n",
    "    \"\"\"\n",
    "    This function replicates the pre-chunking logic from Cell 5 for EPUB files\n",
    "    to show the list of large documents with their assigned ToC metadata.\n",
    "    \"\"\"\n",
    "    if not PROCESS_EPUB:\n",
    "        print(\"This inspection cell is for EPUB processing. Please set PROCESS_EPUB = True in Cell 1.\")\n",
    "        return\n",
    "\n",
    "    print_header(\"EPUB Pre-Processing Inspection\", char=\"~\")\n",
    "\n",
    "    # --- 1. Load the necessary data (replicating start of Cell 5) ---\n",
    "    logger.info(\"Loading pre-extracted ToC and raw EPUB elements...\")\n",
    "    try:\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        \n",
    "        loader = UnstructuredEPubLoader(BOOK_PATH, mode=\"elements\", strategy=\"fast\")\n",
    "        all_raw_book_docs = loader.load()\n",
    "        logger.info(f\"Successfully loaded {len(all_raw_book_docs)} raw text elements from the EPUB.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load necessary files: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Flatten the ToC (replicating logic from Cell 5) ---\n",
    "    logger.info(\"Flattening the hierarchical ToC for matching...\")\n",
    "    flat_toc_entries = []\n",
    "    def _add_ids_and_flatten_recursive(nodes, current_titles_path, counter):\n",
    "        for node in nodes:\n",
    "            toc_id = counter[0]\n",
    "            counter[0] += 1\n",
    "            title = node.get(\"title\", \"\").strip()\n",
    "            if not title: continue\n",
    "            new_titles_path = current_titles_path + [title]\n",
    "            entry = {\n",
    "                \"titles_path\": new_titles_path,\n",
    "                \"level\": node.get(\"level\"),\n",
    "                \"full_title_for_matching\": title,\n",
    "                \"toc_id\": toc_id\n",
    "            }\n",
    "            flat_toc_entries.append(entry)\n",
    "            if node.get(\"children\"):\n",
    "                _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter)\n",
    "    \n",
    "    _add_ids_and_flatten_recursive(hierarchical_toc, [], [0])\n",
    "    logger.info(f\"Flattened ToC into {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "    # --- 3. The Core Matching Logic for EPUB (the part you want to see) ---\n",
    "    logger.info(\"Assigning metadata to EPUB elements by matching ToC titles...\")\n",
    "    \n",
    "    final_documents_with_metadata = []\n",
    "    toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "    current_hierarchy_metadata = {}\n",
    "\n",
    "    for element_doc in all_raw_book_docs:\n",
    "        element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "        if not element_text:\n",
    "            continue\n",
    "\n",
    "        # Check if this element is a heading that matches a ToC entry\n",
    "        is_heading = False\n",
    "        for toc_entry in toc_titles_for_search:\n",
    "            if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "                # It's a heading! Update the current context.\n",
    "                current_hierarchy_metadata = {\"source\": os.path.basename(BOOK_PATH)}\n",
    "                for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "                    current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                current_hierarchy_metadata['toc_id'] = toc_entry.get('toc_id')\n",
    "                is_heading = True\n",
    "                break # Found the match, no need to search further\n",
    "\n",
    "        # Assign metadata\n",
    "        if not current_hierarchy_metadata:\n",
    "            # Content before the first ToC entry (e.g., cover, title page)\n",
    "            doc_metadata_to_assign = {\"source\": os.path.basename(BOOK_PATH), \"level_1_title\": \"EPUB Preamble\", \"toc_id\": -1}\n",
    "        else:\n",
    "            doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "\n",
    "        final_documents_with_metadata.append(Document(page_content=element_text, metadata=doc_metadata_to_assign))\n",
    "    \n",
    "    logger.info(f\"Processing complete. Generated {len(final_documents_with_metadata)} documents with assigned metadata.\")\n",
    "    \n",
    "    # --- 4. Print the result for inspection ---\n",
    "    print_header(\"INSPECTION RESULTS: Documents Before Chunking\", char=\"=\")\n",
    "    print(f\"Total documents created: {len(final_documents_with_metadata)}\\n\")\n",
    "\n",
    "    for i, doc in enumerate(final_documents_with_metadata[:100]): # Print first 30 to avoid flooding the output\n",
    "        print(f\"--- Document [{i+1}] ---\")\n",
    "        print(f\"  Assigned Metadata: {doc.metadata}\")\n",
    "        print(f\"  Content (Un-chunked Element):\")\n",
    "        print(f\"  >> '{doc.page_content}'\")\n",
    "        print(\"-\" * 25 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Execute the inspection ---\n",
    "inspect_epub_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d38d",
   "metadata": {},
   "source": [
    "### Full Database Health & Hierarchy Diagnostic Report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9902b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:26,372 - INFO - Connecting to the vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:26,397 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-07-14 17:26:26,496 - INFO - Successfully connected to the database.\n",
      "2025-07-14 17:26:26,667 - INFO - Retrieving metadata for all 11774 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "               Full Database Health & Hierarchy Diagnostic Report               \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:27,278 - INFO - Successfully retrieved all metadata.\n",
      "2025-07-14 17:26:27,279 - INFO - Reconstructing hierarchy from chunk metadata...\n",
      "2025-07-14 17:26:27,289 - INFO - Hierarchy reconstruction complete.\n",
      "2025-07-14 17:26:27,291 - INFO - Verifying chunk order and reassembling content for a random ToC section.\n",
      "2025-07-14 17:26:27,292 - INFO - Selected random section for testing: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Command-Line Forensics Tools' (toc_id: 246)\n",
      "2025-07-14 17:26:27,295 - INFO - Retrieved 14 document chunks for toc_id 246.\n",
      "2025-07-14 17:26:27,296 - INFO - ✅ TEST PASSED: Chunk IDs for the section are sequential and content is reassembled.\n",
      "2025-07-14 17:26:27,296 - WARNING - Found 21 chunks MISSING a valid 'toc_id'. Check 'Orphaned' sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                  Reconstructed Hierarchy Report (Book Order)                   \n",
      "--------------------------------------------------------------------------------\n",
      "|-- Preface [ID: 3] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "|-- Introduction [ID: 4] (Total Chuck in branch: 73, Direct Chunk: 73)\n",
      "|-- About the Authors [ID: 5] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "|-- Acknowledgments [ID: 6] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "|-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: 7] (Total Chuck in branch: 4566, Direct Chunk: 23)\n",
      "  |-- An Overview of Digital Forensics [ID: 9] (Total Chuck in branch: 60, Direct Chunk: 18)\n",
      "    |-- Digital Forensics and Other Related Disciplines [ID: 10] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- A Brief History of Digital Forensics [ID: 11] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Understanding Case Law [ID: 12] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Developing Digital Forensics Resources [ID: 13] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Preparing for Digital Investigations [ID: 14] (Total Chuck in branch: 84, Direct Chunk: 5)\n",
      "    |-- Understanding Law Enforcement Agency Investigations [ID: 15] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Following Legal Processes [ID: 16] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Understanding Private-Sector Investigations [ID: 17] (Total Chuck in branch: 56, Direct Chunk: 3)\n",
      "      |-- Establishing Company Policies [ID: 18] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- Displaying Warning Banners [ID: 19] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "      |-- Designating an Authorized Requester [ID: 20] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "      |-- Conducting Security Investigations [ID: 21] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "      |-- Distinguishing Personal and Company Property [ID: 22] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "  |-- Maintaining Professional Conduct [ID: 23] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Preparing a Digital Forensics Investigation [ID: 24] (Total Chuck in branch: 97, Direct Chunk: 4)\n",
      "    |-- An Overview of a Computer Crime [ID: 25] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- An Overview of a Company Policy Violation [ID: 26] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Taking a Systematic Approach [ID: 27] (Total Chuck in branch: 77, Direct Chunk: 16)\n",
      "      |-- Assessing the Case [ID: 28] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "      |-- Planning Your Investigation [ID: 29] (Total Chuck in branch: 41, Direct Chunk: 41)\n",
      "      |-- Securing Your Evidence [ID: 30] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Procedures for Private-Sector High-Tech Investigations [ID: 31] (Total Chuck in branch: 124, Direct Chunk: 2)\n",
      "    |-- Employee Termination Cases [ID: 32] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Internet Abuse Investigations [ID: 33] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "    |-- E-mail Abuse Investigations [ID: 34] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "    |-- Attorney-Client Privilege Investigations [ID: 35] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "    |-- Industrial Espionage Investigations [ID: 36] (Total Chuck in branch: 52, Direct Chunk: 41)\n",
      "      |-- Interviews and Interrogations in High-Tech Investigations [ID: 37] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Understanding Data Recovery Workstations and Software [ID: 38] (Total Chuck in branch: 37, Direct Chunk: 18)\n",
      "    |-- Setting Up Your Workstation for Digital Forensics [ID: 39] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "  |-- Conducting an Investigation [ID: 40] (Total Chuck in branch: 109, Direct Chunk: 8)\n",
      "    |-- Gathering the Evidence [ID: 41] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Understanding Bit-stream Copies [ID: 42] (Total Chuck in branch: 8, Direct Chunk: 6)\n",
      "      |-- Acquiring an Image of Evidence Media [ID: 43] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Analyzing Your Digital Evidence [ID: 44] (Total Chuck in branch: 48, Direct Chunk: 44)\n",
      "      |-- Some Additional Features of Autopsy [ID: 45] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Completing the Case [ID: 46] (Total Chuck in branch: 22, Direct Chunk: 12)\n",
      "      |-- Autopsy’s Report Generator [ID: 47] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Critiquing the Case [ID: 48] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Chapter Review [ID: inf] (Total Chuck in branch: 4022, Direct Chunk: 0)\n",
      "    |-- Chapter Summary [ID: 50] (Total Chuck in branch: 211, Direct Chunk: 211)\n",
      "    |-- Key Terms [ID: 51] (Total Chuck in branch: 309, Direct Chunk: 309)\n",
      "    |-- Review Questions [ID: 52] (Total Chuck in branch: 1785, Direct Chunk: 1785)\n",
      "    |-- Hands-On Projects [ID: 53] (Total Chuck in branch: 1527, Direct Chunk: 1527)\n",
      "    |-- Case Projects [ID: 54] (Total Chuck in branch: 190, Direct Chunk: 190)\n",
      "|-- Chapter 2. The Investigator’s Office and Laboratory [ID: 55] (Total Chuck in branch: 331, Direct Chunk: 22)\n",
      "  |-- Understanding Forensics Lab Accreditation Requirements [ID: 57] (Total Chuck in branch: 86, Direct Chunk: 7)\n",
      "    |-- Identifying Duties of the Lab Manager and Staff [ID: 58] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Lab Budget Planning [ID: 59] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Acquiring Certification and Training [ID: 60] (Total Chuck in branch: 54, Direct Chunk: 4)\n",
      "      |-- International Association of Computer Investigative Specialists [ID: 61] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "      |-- ISC2 Certified Cyber Forensics Professional [ID: 62] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- High Tech Crime Network [ID: 63] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "      |-- EnCase Certified Examiner Certification [ID: 64] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- AccessData Certified Examiner [ID: 65] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Other Training and Certifications [ID: 66] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "  |-- Determining the Physical Requirements for a Digital Forensics Lab [ID: 67] (Total Chuck in branch: 68, Direct Chunk: 3)\n",
      "    |-- Identifying Lab Security Needs [ID: 68] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Conducting High-Risk Investigations [ID: 69] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Using Evidence Containers [ID: 70] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "    |-- Overseeing Facility Maintenance [ID: 71] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Considering Physical Security Needs [ID: 72] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Auditing a Digital Forensics Lab [ID: 73] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Determining Floor Plans for Digital Forensics Labs [ID: 74] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Selecting a Basic Forensic Workstation [ID: 75] (Total Chuck in branch: 51, Direct Chunk: 2)\n",
      "    |-- Selecting Workstations for a Lab [ID: 76] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Selecting Workstations for Private-Sector Labs [ID: 77] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Stocking Hardware Peripherals [ID: 78] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Maintaining Operating Systems and Software Inventories [ID: 79] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Using a Disaster Recovery Plan [ID: 80] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Planning for Equipment Upgrades [ID: 81] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Building a Business Case for Developing a Forensics Lab [ID: 82] (Total Chuck in branch: 104, Direct Chunk: 11)\n",
      "    |-- Preparing a Business Case for a Digital Forensics Lab [ID: 83] (Total Chuck in branch: 93, Direct Chunk: 2)\n",
      "      |-- Justification [ID: 84] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Budget Development [ID: 85] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Facility Cost [ID: 86] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "      |-- Hardware Requirements [ID: 87] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "      |-- Software Requirements [ID: 88] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "      |-- Miscellaneous Budget Needs [ID: 89] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Approval and Acquisition [ID: 90] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Implementation [ID: 91] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Acceptance Testing [ID: 92] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Correction for Acceptance [ID: 93] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Production [ID: 94] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "|-- Chapter 3. Data Acquisition [ID: 101] (Total Chuck in branch: 390, Direct Chunk: 28)\n",
      "  |-- Understanding Storage Formats for Digital Evidence [ID: 103] (Total Chuck in branch: 31, Direct Chunk: 5)\n",
      "    |-- Raw Format [ID: 104] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Proprietary Formats [ID: 105] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "    |-- Advanced Forensic Format [ID: 106] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Determining the Best Acquisition Method [ID: 107] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "  |-- Contingency Planning for Image Acquisitions [ID: 108] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Using Acquisition Tools [ID: 109] (Total Chuck in branch: 173, Direct Chunk: 5)\n",
      "    |-- Mini-WinFE Boot CDs and USB Drives [ID: 110] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Acquiring Data with a Linux Boot CD [ID: 111] (Total Chuck in branch: 113, Direct Chunk: 5)\n",
      "      |-- Using Linux Live CD Distributions [ID: 112] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "      |-- Preparing a Target Drive for Acquisition in Linux [ID: 113] (Total Chuck in branch: 45, Direct Chunk: 45)\n",
      "      |-- Acquiring Data with dd in Linux [ID: 114] (Total Chuck in branch: 32, Direct Chunk: 32)\n",
      "      |-- Acquiring Data with dcfldd in Linux [ID: 115] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Capturing an Image with AccessData FTK Imager Lite [ID: 116] (Total Chuck in branch: 46, Direct Chunk: 46)\n",
      "  |-- Validating Data Acquisitions [ID: 117] (Total Chuck in branch: 32, Direct Chunk: 5)\n",
      "    |-- Linux Validation Methods [ID: 118] (Total Chuck in branch: 21, Direct Chunk: 3)\n",
      "      |-- Validating dd-Acquired Data [ID: 119] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "      |-- Validating dcfldd-Acquired Data [ID: 120] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Windows Validation Methods [ID: 121] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Performing RAID Data Acquisitions [ID: 122] (Total Chuck in branch: 30, Direct Chunk: 2)\n",
      "    |-- Understanding RAID [ID: 123] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Acquiring RAID Disks [ID: 124] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "  |-- Using Remote Network Acquisition Tools [ID: 125] (Total Chuck in branch: 39, Direct Chunk: 5)\n",
      "    |-- Remote Acquisition with ProDiscover [ID: 126] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Remote Acquisition with EnCase Enterprise [ID: 127] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Remote Acquisition with R-Tools R-Studio [ID: 128] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Remote Acquisition with WetStone US-LATT PRO [ID: 129] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Remote Acquisition with F-Response [ID: 130] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Using Other Forensics Acquisition Tools [ID: 131] (Total Chuck in branch: 27, Direct Chunk: 2)\n",
      "    |-- PassMark Software ImageUSB [ID: 132] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- ASR Data SMART [ID: 133] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Runtime Software [ID: 134] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- ILookIX IXImager [ID: 135] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- SourceForge [ID: 136] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "|-- Chapter 4. Processing Crime and Incident Scenes [ID: 143] (Total Chuck in branch: 413, Direct Chunk: 29)\n",
      "  |-- Identifying Digital Evidence [ID: 145] (Total Chuck in branch: 76, Direct Chunk: 13)\n",
      "    |-- Understanding Rules of Evidence [ID: 146] (Total Chuck in branch: 63, Direct Chunk: 63)\n",
      "  |-- Collecting Evidence in Private-Sector Incident Scenes [ID: 147] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "  |-- Processing Law Enforcement Crime Scenes [ID: 148] (Total Chuck in branch: 24, Direct Chunk: 6)\n",
      "    |-- Understanding Concepts and Terms Used in Warrants [ID: 149] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "  |-- Preparing for a Search [ID: 150] (Total Chuck in branch: 40, Direct Chunk: 2)\n",
      "    |-- Identifying the Nature of the Case [ID: 151] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Identifying the Type of OS or Digital Device [ID: 152] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Determining Whether You Can Seize Computers and Digital Devices [ID: 153] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Getting a Detailed Description of the Location [ID: 154] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Determining Who Is in Charge [ID: 155] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Using Additional Technical Expertise [ID: 156] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Determining the Tools You Need [ID: 157] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "    |-- Preparing the Investigation Team [ID: 158] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Securing a Digital Incident or Crime Scene [ID: 159] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Seizing Digital Evidence at the Scene [ID: 160] (Total Chuck in branch: 72, Direct Chunk: 4)\n",
      "    |-- Preparing to Acquire Digital Evidence [ID: 161] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Processing Incident or Crime Scenes [ID: 162] (Total Chuck in branch: 34, Direct Chunk: 34)\n",
      "    |-- Processing Data Centers with RAID Systems [ID: 163] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Using a Technical Advisor [ID: 164] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Documenting Evidence in the Lab [ID: 165] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Processing and Handling Digital Evidence [ID: 166] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Storing Digital Evidence [ID: 167] (Total Chuck in branch: 18, Direct Chunk: 7)\n",
      "    |-- Evidence Retention and Media Storage Needs [ID: 168] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Documenting Evidence [ID: 169] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Obtaining a Digital Hash [ID: 170] (Total Chuck in branch: 42, Direct Chunk: 42)\n",
      "  |-- Reviewing a Case [ID: 171] (Total Chuck in branch: 79, Direct Chunk: 8)\n",
      "    |-- Sample Civil Investigation [ID: 172] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "    |-- An Example of a Criminal Investigation [ID: 173] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Reviewing Background Information for a Case [ID: 174] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Planning the Investigation [ID: 175] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Conducting the Investigation: Acquiring Evidence with OSForensics [ID: 176] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "|-- Chapter 5. Working with Windows and CLI Systems [ID: 183] (Total Chuck in branch: 471, Direct Chunk: 22)\n",
      "  |-- Understanding File Systems [ID: 185] (Total Chuck in branch: 33, Direct Chunk: 3)\n",
      "    |-- Understanding the Boot Sequence [ID: 186] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Understanding Disk Drives [ID: 187] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Solid-State Storage Devices [ID: 188] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Exploring Microsoft File Structures [ID: 189] (Total Chuck in branch: 71, Direct Chunk: 5)\n",
      "    |-- Disk Partitions [ID: 190] (Total Chuck in branch: 39, Direct Chunk: 39)\n",
      "    |-- Examining FAT Disks [ID: 191] (Total Chuck in branch: 27, Direct Chunk: 24)\n",
      "      |-- Deleting FAT Files [ID: 192] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Examining NTFS Disks [ID: 193] (Total Chuck in branch: 168, Direct Chunk: 14)\n",
      "    |-- NTFS System Files [ID: 194] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- MFT and File Attributes [ID: 195] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- MFT Structures for File Data [ID: 196] (Total Chuck in branch: 69, Direct Chunk: 3)\n",
      "      |-- MFT Header Fields [ID: 197] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "      |-- Attribute 0x10: Standard Information [ID: 198] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Attribute 0x30: File Name [ID: 199] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "      |-- Attribute 0x40: Object_ID [ID: 200] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "      |-- Attribute 0x80: Data for a Resident File [ID: 201] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Attribute 0x80: Data for a Nonresident File [ID: 202] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Interpreting a Data Run [ID: 203] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- NTFS Alternate Data Streams [ID: 204] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- NTFS Compressed Files [ID: 205] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- NTFS Encrypting File System [ID: 206] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- EFS Recovery Key Agent [ID: 207] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Deleting NTFS Files [ID: 208] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Resilient File System [ID: 209] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Understanding Whole Disk Encryption [ID: 210] (Total Chuck in branch: 26, Direct Chunk: 11)\n",
      "    |-- Examining Microsoft BitLocker [ID: 211] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Examining Third-Party Disk Encryption Tools [ID: 212] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Understanding the Windows Registry [ID: 213] (Total Chuck in branch: 56, Direct Chunk: 9)\n",
      "    |-- Exploring the Organization of the Windows Registry [ID: 214] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Examining the Windows Registry [ID: 215] (Total Chuck in branch: 29, Direct Chunk: 29)\n",
      "  |-- Understanding Microsoft Startup Tasks [ID: 216] (Total Chuck in branch: 47, Direct Chunk: 3)\n",
      "    |-- Startup in Windows 7, Windows 8, and Windows 10 [ID: 217] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Startup in Windows NT and Later [ID: 218] (Total Chuck in branch: 39, Direct Chunk: 10)\n",
      "      |-- Startup Files for Windows Vista [ID: 219] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Startup Files for Windows XP [ID: 220] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "      |-- Windows XP System Files [ID: 221] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Contamination Concerns with Windows XP [ID: 222] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "  |-- Understanding Virtual Machines [ID: 223] (Total Chuck in branch: 48, Direct Chunk: 10)\n",
      "    |-- Creating a Virtual Machine [ID: 224] (Total Chuck in branch: 38, Direct Chunk: 38)\n",
      "|-- Chapter 6. Current Digital Forensics Tools [ID: 231] (Total Chuck in branch: 315, Direct Chunk: 22)\n",
      "  |-- Evaluating Digital Forensics Tool Needs [ID: 233] (Total Chuck in branch: 184, Direct Chunk: 10)\n",
      "    |-- Types of Digital Forensics Tools [ID: 234] (Total Chuck in branch: 11, Direct Chunk: 4)\n",
      "      |-- Hardware Forensics Tools [ID: 235] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Software Forensics Tools [ID: 236] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Tasks Performed by Digital Forensics Tools [ID: 237] (Total Chuck in branch: 153, Direct Chunk: 3)\n",
      "      |-- Acquisition [ID: 238] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "      |-- Validation and Verification [ID: 239] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "      |-- Extraction [ID: 240] (Total Chuck in branch: 25, Direct Chunk: 25)\n",
      "      |-- Reconstruction [ID: 241] (Total Chuck in branch: 66, Direct Chunk: 66)\n",
      "      |-- Reporting [ID: 242] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "    |-- Tool Comparisons [ID: 243] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Other Considerations for Tools [ID: 244] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Digital Forensics Software Tools [ID: 245] (Total Chuck in branch: 41, Direct Chunk: 4)\n",
      "    |-- Command-Line Forensics Tools [ID: 246] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Linux Forensics Tools [ID: 247] (Total Chuck in branch: 19, Direct Chunk: 4)\n",
      "      |-- Smart [ID: 248] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Helix 3 [ID: 249] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Kali Linux [ID: 250] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Autopsy and Sleuth Kit [ID: 251] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Forcepoint Threat Protection [ID: 252] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Other GUI Forensics Tools [ID: 253] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Digital Forensics Hardware Tools [ID: 254] (Total Chuck in branch: 38, Direct Chunk: 3)\n",
      "    |-- Forensic Workstations [ID: 255] (Total Chuck in branch: 13, Direct Chunk: 7)\n",
      "      |-- Building Your Own Workstation [ID: 256] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Using a Write-Blocker [ID: 257] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "    |-- Recommendations for a Forensic Workstation [ID: 258] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "  |-- Validating and Testing Forensics Software [ID: 259] (Total Chuck in branch: 30, Direct Chunk: 2)\n",
      "    |-- Using National Institute of Standards and Technology Tools [ID: 260] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Using Validation Protocols [ID: 261] (Total Chuck in branch: 15, Direct Chunk: 6)\n",
      "      |-- Digital Forensics Examination Protocol [ID: 262] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- Digital Forensics Tool Upgrade Protocol [ID: 263] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "|-- Chapter 7. Linux and Macintosh File Systems [ID: 270] (Total Chuck in branch: 274, Direct Chunk: 17)\n",
      "  |-- Examining Linux File Structures [ID: 272] (Total Chuck in branch: 131, Direct Chunk: 77)\n",
      "    |-- File Structures in Ext4 [ID: 273] (Total Chuck in branch: 54, Direct Chunk: 8)\n",
      "      |-- Inodes [ID: 274] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "      |-- Hard Links and Symbolic Links [ID: 275] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "  |-- Understanding Macintosh File Structures [ID: 276] (Total Chuck in branch: 58, Direct Chunk: 6)\n",
      "    |-- An Overview of Mac File Structures [ID: 277] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "    |-- Forensics Procedures in Mac [ID: 278] (Total Chuck in branch: 29, Direct Chunk: 18)\n",
      "      |-- Acquisition Methods in macOS [ID: 279] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Using Linux Forensics Tools [ID: 280] (Total Chuck in branch: 68, Direct Chunk: 5)\n",
      "    |-- Installing Sleuth Kit and Autopsy [ID: 281] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "    |-- Examining a Case with Sleuth Kit and Autopsy [ID: 282] (Total Chuck in branch: 42, Direct Chunk: 42)\n",
      "|-- Chapter 8. Recovering Graphics Files [ID: 289] (Total Chuck in branch: 240, Direct Chunk: 19)\n",
      "  |-- Recognizing a Graphics File [ID: 291] (Total Chuck in branch: 54, Direct Chunk: 4)\n",
      "    |-- Understanding Bitmap and Raster Images [ID: 292] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Understanding Vector Graphics [ID: 293] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Understanding Metafile Graphics [ID: 294] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Understanding Graphics File Formats [ID: 295] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Understanding Digital Photograph File Formats [ID: 296] (Total Chuck in branch: 19, Direct Chunk: 2)\n",
      "      |-- Examining the Raw File Format [ID: 297] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- Examining the Exchangeable Image File Format [ID: 298] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "  |-- Understanding Data Compression [ID: 299] (Total Chuck in branch: 101, Direct Chunk: 2)\n",
      "    |-- Lossless and Lossy Compression [ID: 300] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Locating and Recovering Graphics Files [ID: 301] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Identifying Graphics File Fragments [ID: 302] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Repairing Damaged Headers [ID: 303] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Searching for and Carving Data from Unallocated Space [ID: 304] (Total Chuck in branch: 39, Direct Chunk: 9)\n",
      "      |-- Planning Your Examination [ID: 305] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Searching for and Recovering Digital Photograph Evidence [ID: 306] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "    |-- Rebuilding File Headers [ID: 307] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "    |-- Reconstructing File Fragments [ID: 308] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "  |-- Identifying Unknown File Formats [ID: 309] (Total Chuck in branch: 47, Direct Chunk: 14)\n",
      "    |-- Analyzing Graphics File Headers [ID: 310] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Tools for Viewing Images [ID: 311] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Understanding Steganography in Graphics Files [ID: 312] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "    |-- Using Steganalysis Tools [ID: 313] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Understanding Copyright Issues with Graphics [ID: 314] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "|-- Chapter 9. Digital Forensics Analysis and Validation [ID: 321] (Total Chuck in branch: 248, Direct Chunk: 16)\n",
      "  |-- Determining What Data to Collect and Analyze [ID: 323] (Total Chuck in branch: 99, Direct Chunk: 6)\n",
      "    |-- Approaching Digital Forensics Cases [ID: 324] (Total Chuck in branch: 35, Direct Chunk: 30)\n",
      "      |-- Refining and Modifying the Investigation Plan [ID: 325] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Using Autopsy to Validate Data [ID: 326] (Total Chuck in branch: 23, Direct Chunk: 8)\n",
      "      |-- Installing NSRL Hashes in Autopsy [ID: 327] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Collecting Hash Values in Autopsy [ID: 328] (Total Chuck in branch: 35, Direct Chunk: 35)\n",
      "  |-- Validating Forensic Data [ID: 329] (Total Chuck in branch: 51, Direct Chunk: 3)\n",
      "    |-- Validating with Hexadecimal Editors [ID: 330] (Total Chuck in branch: 31, Direct Chunk: 28)\n",
      "      |-- Using Hash Values to Discriminate Data [ID: 331] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Validating with Digital Forensics Tools [ID: 332] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "  |-- Addressing Data-Hiding Techniques [ID: 333] (Total Chuck in branch: 82, Direct Chunk: 2)\n",
      "    |-- Hiding Files by Using the OS [ID: 334] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Hiding Partitions [ID: 335] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Marking Bad Clusters [ID: 336] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Bit-Shifting [ID: 337] (Total Chuck in branch: 34, Direct Chunk: 34)\n",
      "    |-- Understanding Steganalysis Methods [ID: 338] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Examining Encrypted Files [ID: 339] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Recovering Passwords [ID: 340] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "|-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: 347] (Total Chuck in branch: 287, Direct Chunk: 17)\n",
      "  |-- An Overview of Virtual Machine Forensics [ID: 349] (Total Chuck in branch: 176, Direct Chunk: 7)\n",
      "    |-- Type 2 Hypervisors [ID: 350] (Total Chuck in branch: 32, Direct Chunk: 6)\n",
      "      |-- Parallels Desktop [ID: 351] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- KVM [ID: 352] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Microsoft Hyper-V [ID: 353] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- VMware Workstation and Workstation Player [ID: 354] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "      |-- VirtualBox [ID: 355] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Conducting an Investigation with Type 2 Hypervisors [ID: 356] (Total Chuck in branch: 103, Direct Chunk: 70)\n",
      "      |-- Other VM Examination Methods [ID: 357] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "      |-- Using VMs as Forensics Tools [ID: 358] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Working with Type 1 Hypervisors [ID: 359] (Total Chuck in branch: 34, Direct Chunk: 34)\n",
      "  |-- Performing Live Acquisitions [ID: 360] (Total Chuck in branch: 18, Direct Chunk: 15)\n",
      "    |-- Performing a Live Acquisition in Windows [ID: 361] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Network Forensics Overview [ID: 362] (Total Chuck in branch: 76, Direct Chunk: 4)\n",
      "    |-- The Need for Established Procedures [ID: 363] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Securing a Network [ID: 364] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Developing Procedures for Network Forensics [ID: 365] (Total Chuck in branch: 41, Direct Chunk: 8)\n",
      "      |-- Reviewing Network Logs [ID: 366] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "      |-- Using Network Tools [ID: 367] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Using Packet Analyzers [ID: 368] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Investigating Virtual Networks [ID: 369] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Examining the Honeynet Project [ID: 370] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "|-- Chapter 11. E-mail and Social Media Investigations [ID: 377] (Total Chuck in branch: 302, Direct Chunk: 20)\n",
      "  |-- Exploring the Role of E-mail in Investigations [ID: 379] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Exploring the Roles of the Client and Server in E-mail [ID: 380] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Investigating E-mail Crimes and Violations [ID: 381] (Total Chuck in branch: 101, Direct Chunk: 4)\n",
      "    |-- Understanding Forensic Linguistics [ID: 382] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Examining E-mail Messages [ID: 383] (Total Chuck in branch: 28, Direct Chunk: 8)\n",
      "      |-- Copying an E-mail Message [ID: 384] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Viewing E-mail Headers [ID: 385] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "    |-- Examining E-mail Headers [ID: 386] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Examining Additional E-mail Files [ID: 387] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Tracing an E-mail Message [ID: 388] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Using Network E-mail Logs [ID: 389] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Understanding E-mail Servers [ID: 390] (Total Chuck in branch: 33, Direct Chunk: 13)\n",
      "    |-- Examining UNIX E-mail Server Logs [ID: 391] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Examining Microsoft E-mail Server Logs [ID: 392] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Using Specialized E-mail Forensics Tools [ID: 393] (Total Chuck in branch: 91, Direct Chunk: 22)\n",
      "    |-- Using Magnet AXIOM to Recover E-mail [ID: 394] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Using a Hex Editor to Carve E-mail Messages [ID: 395] (Total Chuck in branch: 41, Direct Chunk: 41)\n",
      "    |-- Recovering Outlook Files [ID: 396] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- E-mail Case Studies [ID: 397] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Applying Digital Forensics Methods to Social Media Communications [ID: 398] (Total Chuck in branch: 36, Direct Chunk: 23)\n",
      "    |-- Forensics Tools for Social Media Investigations [ID: 399] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "|-- Chapter 12. Mobile Device Forensics and the Internet of Anything [ID: 406] (Total Chuck in branch: 169, Direct Chunk: 8)\n",
      "  |-- Understanding Mobile Device Forensics [ID: 408] (Total Chuck in branch: 65, Direct Chunk: 19)\n",
      "    |-- Mobile Phone Basics [ID: 409] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "    |-- Inside Mobile Devices [ID: 410] (Total Chuck in branch: 22, Direct Chunk: 11)\n",
      "      |-- SIM Cards [ID: 411] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Understanding Acquisition Procedures for Mobile Devices [ID: 412] (Total Chuck in branch: 75, Direct Chunk: 30)\n",
      "    |-- Mobile Forensics Equipment [ID: 413] (Total Chuck in branch: 30, Direct Chunk: 6)\n",
      "      |-- SIM Card Readers [ID: 414] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "      |-- Mobile Phone Forensics Tools and Methods [ID: 415] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "    |-- Using Mobile Forensics Tools [ID: 416] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "  |-- Understanding Forensics in the Internet of Anything [ID: 417] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "|-- Chapter 13. Cloud Forensics [ID: 424] (Total Chuck in branch: 290, Direct Chunk: 20)\n",
      "  |-- An Overview of Cloud Computing [ID: 426] (Total Chuck in branch: 42, Direct Chunk: 2)\n",
      "    |-- History of the Cloud [ID: 427] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Cloud Service Levels and Deployment Methods [ID: 428] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Cloud Vendors [ID: 429] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Basic Concepts of Cloud Forensics [ID: 430] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Legal Challenges in Cloud Forensics [ID: 431] (Total Chuck in branch: 56, Direct Chunk: 2)\n",
      "    |-- Service Level Agreements [ID: 432] (Total Chuck in branch: 25, Direct Chunk: 17)\n",
      "      |-- Policies, Standards, and Guidelines for CSPs [ID: 433] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- CSP Processes and Procedures [ID: 434] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Jurisdiction Issues [ID: 435] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Accessing Evidence in the Cloud [ID: 436] (Total Chuck in branch: 23, Direct Chunk: 3)\n",
      "      |-- Search Warrants [ID: 437] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "      |-- Subpoenas and Court Orders [ID: 438] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Technical Challenges in Cloud Forensics [ID: 439] (Total Chuck in branch: 33, Direct Chunk: 5)\n",
      "    |-- Architecture [ID: 440] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Analysis of Cloud Forensic Data [ID: 441] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Anti-Forensics [ID: 442] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Incident First Responders [ID: 443] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Role Management [ID: 444] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Standards and Training [ID: 445] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Acquisitions in the Cloud [ID: 446] (Total Chuck in branch: 21, Direct Chunk: 8)\n",
      "    |-- Encryption in the Cloud [ID: 447] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "  |-- Conducting a Cloud Investigation [ID: 448] (Total Chuck in branch: 105, Direct Chunk: 2)\n",
      "    |-- Investigating CSPs [ID: 449] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Investigating Cloud Customers [ID: 450] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Understanding Prefetch Files [ID: 451] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Examining Stored Cloud Data on a PC [ID: 452] (Total Chuck in branch: 63, Direct Chunk: 5)\n",
      "      |-- Dropbox [ID: 453] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Google Drive [ID: 454] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "      |-- OneDrive [ID: 455] (Total Chuck in branch: 29, Direct Chunk: 29)\n",
      "    |-- Windows Prefetch Artifacts [ID: 456] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "  |-- Tools for Cloud Forensics [ID: 457] (Total Chuck in branch: 13, Direct Chunk: 5)\n",
      "    |-- Forensic Open-Stack Tools [ID: 458] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- F-Response for the Cloud [ID: 459] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Magnet AXIOM Cloud [ID: 460] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "|-- Chapter 14. Report Writing for High-Tech Investigations [ID: 467] (Total Chuck in branch: 263, Direct Chunk: 16)\n",
      "  |-- Understanding the Importance of Reports [ID: 469] (Total Chuck in branch: 31, Direct Chunk: 19)\n",
      "    |-- Limiting a Report to Specifics [ID: 470] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Types of Reports [ID: 471] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Guidelines for Writing Reports [ID: 472] (Total Chuck in branch: 138, Direct Chunk: 19)\n",
      "    |-- What to Include in Written Preliminary Reports [ID: 473] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Report Structure [ID: 474] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Writing Reports Clearly [ID: 475] (Total Chuck in branch: 17, Direct Chunk: 8)\n",
      "      |-- Considering Writing Style [ID: 476] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Including Signposts [ID: 477] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Designing the Layout and Presentation of Reports [ID: 478] (Total Chuck in branch: 85, Direct Chunk: 44)\n",
      "      |-- Providing Supporting Material [ID: 479] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Formatting Consistently [ID: 480] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Explaining Examination and Data Collection Methods [ID: 481] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Including Calculations [ID: 482] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Providing for Uncertainty and Error Analysis [ID: 483] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Explaining Results and Conclusions [ID: 484] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Providing References [ID: 485] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "      |-- Including Appendixes [ID: 486] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Generating Report Findings with Forensics Software Tools [ID: 487] (Total Chuck in branch: 78, Direct Chunk: 2)\n",
      "    |-- Using Autopsy to Generate Reports [ID: 488] (Total Chuck in branch: 76, Direct Chunk: 76)\n",
      "|-- Chapter 15. Expert Testimony in Digital Investigations [ID: 495] (Total Chuck in branch: 329, Direct Chunk: 14)\n",
      "  |-- Preparing for Testimony [ID: 497] (Total Chuck in branch: 62, Direct Chunk: 15)\n",
      "    |-- Documenting and Preparing Evidence [ID: 498] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Reviewing Your Role as a Consulting Expert or an Expert Witness [ID: 499] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Creating and Maintaining Your CV [ID: 500] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Preparing Technical Definitions [ID: 501] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Preparing to Deal with the News Media [ID: 502] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Testifying in Court [ID: 503] (Total Chuck in branch: 155, Direct Chunk: 2)\n",
      "    |-- Understanding the Trial Process [ID: 504] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Providing Qualifications for Your Testimony [ID: 505] (Total Chuck in branch: 59, Direct Chunk: 59)\n",
      "    |-- General Guidelines on Testifying [ID: 506] (Total Chuck in branch: 47, Direct Chunk: 27)\n",
      "      |-- Using Graphics During Testimony [ID: 507] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "      |-- Avoiding Testimony Problems [ID: 508] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "      |-- Understanding Prosecutorial Misconduct [ID: 509] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Testifying During Direct Examination [ID: 510] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Testifying During Cross-Examination [ID: 511] (Total Chuck in branch: 25, Direct Chunk: 25)\n",
      "  |-- Preparing for a Deposition or Hearing [ID: 512] (Total Chuck in branch: 33, Direct Chunk: 6)\n",
      "    |-- Guidelines for Testifying at Depositions [ID: 513] (Total Chuck in branch: 19, Direct Chunk: 10)\n",
      "      |-- Recognizing Deposition Problems [ID: 514] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Guidelines for Testifying at Hearings [ID: 515] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Preparing Forensics Evidence for Testimony [ID: 516] (Total Chuck in branch: 65, Direct Chunk: 34)\n",
      "    |-- Preparing a Defense of Your Evidence-Collection Methods [ID: 517] (Total Chuck in branch: 31, Direct Chunk: 31)\n",
      "|-- Chapter 16. Ethics for the Expert Witness [ID: 524] (Total Chuck in branch: 310, Direct Chunk: 13)\n",
      "  |-- Applying Ethics and Codes to Expert Witnesses [ID: 526] (Total Chuck in branch: 58, Direct Chunk: 11)\n",
      "    |-- Forensics Examiners’ Roles in Testifying [ID: 527] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Considerations in Disqualification [ID: 528] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "    |-- Traps for Unwary Experts [ID: 529] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "    |-- Determining Admissibility of Evidence [ID: 530] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Organizations with Codes of Ethics [ID: 531] (Total Chuck in branch: 26, Direct Chunk: 2)\n",
      "    |-- International Society of Forensic Computer Examiners [ID: 532] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "    |-- International High Technology Crime Investigation Association [ID: 533] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- American Bar Association [ID: 535] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- American Psychological Association [ID: 536] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "  |-- Ethical Difficulties in Expert Testimony [ID: 537] (Total Chuck in branch: 19, Direct Chunk: 6)\n",
      "    |-- Ethical Responsibilities Owed to You [ID: 538] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Standard Forensics Tools and Tools You Create [ID: 539] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- An Ethics Exercise [ID: 540] (Total Chuck in branch: 194, Direct Chunk: 4)\n",
      "    |-- Performing a Cursory Exam of a Forensic Image [ID: 541] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Performing a Detailed Exam of a Forensic Image [ID: 542] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "    |-- Performing the Exam [ID: 543] (Total Chuck in branch: 76, Direct Chunk: 2)\n",
      "      |-- Preparing for an Examination [ID: 544] (Total Chuck in branch: 74, Direct Chunk: 74)\n",
      "    |-- Interpreting Attribute 0x80 Data Runs [ID: 545] (Total Chuck in branch: 44, Direct Chunk: 2)\n",
      "      |-- Finding Attribute 0x80 an MFT Record [ID: 546] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "      |-- Configuring Data Interpreter Options in WinHex [ID: 547] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Calculating Data Runs [ID: 548] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Carving Data Run Clusters Manually [ID: 549] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "|-- Lab Manual for Guide to Computer Forensics and Investigations [ID: 556] (Total Chuck in branch: 2165, Direct Chunk: 1)\n",
      "  |-- Chapter 12. Mobile Device Forensics [ID: 792] (Total Chuck in branch: 13, Direct Chunk: 10)\n",
      "    |-- Lab 12.1. Examining Cell Phone Storage Devices [ID: 794] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 12.2. Using FTK Imager Lite to View Text Messages, Phone Numbers, and Photos [ID: 799] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 12.3. Using Autopsy to Search Cloud Backups of Mobile Devices [ID: 804] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: inf] (Total Chuck in branch: 1673, Direct Chunk: 0)\n",
      "    |-- Lab 1.1. Installing Autopsy for Windows [ID: 560] (Total Chuck in branch: 1670, Direct Chunk: 1)\n",
      "      |-- Objectives [ID: 561] (Total Chuck in branch: 702, Direct Chunk: 345)\n",
      "        |-- Materials Required [ID: 562] (Total Chuck in branch: 357, Direct Chunk: 357)\n",
      "      |-- Activity [ID: 563] (Total Chuck in branch: 967, Direct Chunk: 967)\n",
      "    |-- Lab 1.2. Downloading FTK Imager Lite [ID: 565] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 1.3. Downloading WinHex [ID: 570] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 1.4. Using Autopsy for Windows [ID: 575] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 2. The Investigator’s Office and Laboratory [ID: inf] (Total Chuck in branch: 5, Direct Chunk: 0)\n",
      "    |-- Lab 2.1. Wiping a USB Drive Securely [ID: 582] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.2. Using Directory Snoop to Image a USB Drive [ID: 587] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.3. Converting a Raw Image to an .E01 Image [ID: 592] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.4. Imaging Evidence with FTK Imager Lite [ID: 597] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.5. Viewing Images in FTK Imager Lite [ID: 602] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 3. Data Acquisition [ID: inf] (Total Chuck in branch: 70, Direct Chunk: 0)\n",
      "    |-- Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive [ID: 609] (Total Chuck in branch: 67, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 66, Direct Chunk: 0)\n",
      "        |-- Creating a DEFT Zero Boot CD [ID: 613] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Creating a Bootable USB DEFT Zero Drive [ID: 614] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Learning DEFT Zero Features [ID: 615] (Total Chuck in branch: 38, Direct Chunk: 38)\n",
      "    |-- Lab 3.2. Examining a FAT Image [ID: 617] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 3.3. Examining an NTFS Image [ID: 622] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 3.4. Examining an HFS+ Image [ID: 627] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 4. Processing Crime and Incident Scenes [ID: inf] (Total Chuck in branch: 36, Direct Chunk: 0)\n",
      "    |-- Lab 4.1. Creating a Mini-WinFE Boot CD [ID: 634] (Total Chuck in branch: 33, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 32, Direct Chunk: 0)\n",
      "        |-- Setting Up Mini-WinFE [ID: 638] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "        |-- Creating a Mini-WinFE ISO Image [ID: 639] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "    |-- Lab 4.2. Using Mini-WinFE to Boot and Image a Windows Computer [ID: 641] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 4.3. Testing the Mini-WinFE Write-Protection Feature [ID: 646] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 4.4. Creating an Image with Guymager [ID: 651] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 5. Working with Windows and CLI Systems [ID: inf] (Total Chuck in branch: 4, Direct Chunk: 0)\n",
      "    |-- Lab 5.1. Using DART to Export Windows Registry Files [ID: 658] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 5.2. Examining the SAM Hive [ID: 663] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 5.3. Examining the SYSTEM Hive [ID: 668] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 5.4. Examining the ntuser.dat Registry File [ID: 673] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 6. Current Digital Forensics Tools [ID: inf] (Total Chuck in branch: 79, Direct Chunk: 0)\n",
      "    |-- Lab 6.1. Using Autopsy 4.7.0 to Search an Image File [ID: 680] (Total Chuck in branch: 41, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 40, Direct Chunk: 0)\n",
      "        |-- Installing Autopsy 4.7.0 [ID: 684] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Searching E-mail in Autopsy 4.7.0 [ID: 685] (Total Chuck in branch: 28, Direct Chunk: 28)\n",
      "    |-- Lab 6.2. Using OSForensics to Search an Image of a Hard Drive [ID: 687] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex [ID: 692] (Total Chuck in branch: 37, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 36, Direct Chunk: 0)\n",
      "        |-- Testing an Image File in Autopsy 4.3.0 [ID: 696] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "        |-- Examining Image Files in WinHex [ID: 697] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "  |-- Chapter 7. Linux and Macintosh File Systems [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 7.1. Using Autopsy to Process a Mac OS X Image [ID: 701] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 7.2. Using Autopsy to Process a Mac OS 9 Image [ID: 706] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 7.3. Using Autopsy to Process a Linux Image [ID: 711] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 8. Recovering Graphics Files [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 8.1. Using Autopsy to Analyze Multimedia Files [ID: 718] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 8.2. Using OSForensics to Analyze Multimedia Files [ID: 723] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 8.3. Using WinHex to Analyze Multimedia Files [ID: 728] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 9. Digital Forensics Analysis and Validation [ID: inf] (Total Chuck in branch: 9, Direct Chunk: 0)\n",
      "    |-- Lab 9.1. Using Autopsy to Search for Keywords in an Image [ID: 735] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 9.2. Validating File Hash Values with FTK Imager Lite [ID: 740] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 9.3. Validating File Hash Values with WinHex [ID: 745] (Total Chuck in branch: 7, Direct Chunk: 1)\n",
      "      |-- Objectives [ID: inf] (Total Chuck in branch: 6, Direct Chunk: 0)\n",
      "        |-- Materials Required: [ID: 747] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: inf] (Total Chuck in branch: 143, Direct Chunk: 0)\n",
      "    |-- Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine [ID: 752] (Total Chuck in branch: 41, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 40, Direct Chunk: 0)\n",
      "        |-- Installing MD5 Hashes in Autopsy [ID: 756] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "        |-- Analyzing a Windows Image Containing a Virtual Machine [ID: 757] (Total Chuck in branch: 32, Direct Chunk: 32)\n",
      "    |-- Lab 10.2. Conducting a Live Acquisition [ID: 759] (Total Chuck in branch: 45, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 44, Direct Chunk: 0)\n",
      "        |-- Installing Tools for Live Acquisitions [ID: 763] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "        |-- Exploring Tools for Live Acquisitions [ID: 764] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Capturing Data in a Live Acquisition [ID: 765] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Lab 10.3. Using Kali Linux for Network Forensics [ID: 767] (Total Chuck in branch: 57, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 56, Direct Chunk: 0)\n",
      "        |-- Installing Kali Linux [ID: 771] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "        |-- Mounting Drives in Kali Linux [ID: 772] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Identifying Open Ports and Making a Screen Capture [ID: 773] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "  |-- Chapter 11. E-mail and Social Media Investigations [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 11.1. Using OSForensics to Search for E-mails and Mailboxes [ID: 777] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 11.2. Using Autopsy to Search for E-mails and Mailboxes [ID: 782] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 11.3. Finding Google Searches and Multiple E-mail Accounts [ID: 787] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 13. Cloud Forensics [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 13.1. Examining Dropbox Cloud Storage [ID: 811] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 13.2. Examining Google Drive Cloud Storage [ID: 816] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 13.3. Examining OneDrive Cloud Storage [ID: 821] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 14. Report Writing for High-Tech Investigations [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 14.1. Investigating Corporate Espionage [ID: 828] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 14.2. Adding Evidence to a Case [ID: 833] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 14.3. Preparing a Report [ID: 838] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 15. Expert Testimony in Digital Investigations [ID: inf] (Total Chuck in branch: 44, Direct Chunk: 0)\n",
      "    |-- Lab 15.1. Conducting a Preliminary Investigation [ID: 845] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 15.2. Investigating an Arsonist [ID: 850] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 15.3. Recovering a Password from Password-Protected Files [ID: 855] (Total Chuck in branch: 42, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 41, Direct Chunk: 0)\n",
      "        |-- Verifying the Existence of a Warning Banner [ID: 859] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Recovering a Password from Password-Protected Files [ID: 860] (Total Chuck in branch: 29, Direct Chunk: 29)\n",
      "  |-- Chapter 16. Ethics for the Expert Witness [ID: inf] (Total Chuck in branch: 73, Direct Chunk: 0)\n",
      "    |-- Lab 16.1. Rebuilding an MFT Record from a Corrupt Image [ID: 864] (Total Chuck in branch: 73, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 72, Direct Chunk: 0)\n",
      "        |-- Creating a Duplicate Forensic Image [ID: 868] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Determining the Offset Byte Address of the Corrupt MFT Record [ID: 869] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Copying the Corrected MFT Record [ID: 870] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "        |-- Extracting Additional Evidence [ID: 871] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "|-- Appendix A. Certification Test References [ID: 873] (Total Chuck in branch: 56, Direct Chunk: 56)\n",
      "|-- Appendix B. Digital Forensics References [ID: 874] (Total Chuck in branch: 109, Direct Chunk: 109)\n",
      "|-- Appendix C. Digital Forensics Lab Considerations [ID: 875] (Total Chuck in branch: 58, Direct Chunk: 58)\n",
      "|-- Appendix D. Legacy File System and Forensics Tools [ID: 876] (Total Chuck in branch: 59, Direct Chunk: 59)\n",
      "|-- EPUB Preamble [ID: inf] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Chunk Sequence & Content Integrity Test                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------- CONTENT PREVIEW -------------------------\n",
      "Title: Command-Line Forensics Tools [toc_id: 246]\n",
      "Chunk IDs: [3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284]\n",
      "----------------------------------------------------------------------\n",
      "Command-Line Forensics Tools\n",
      "As mentioned in Chapter 1, computers used several OSs before Windows and MS-DOS dominated the market. During this time, digital forensics wasn’t a major concern. After people started using PCs, however, they figured out how to use them for illegal and destructive purposes and to commit crimes and civil infractions with them. Software developers began releasing forensics tools to help private-sector and public-sector investigators examine PCs. The first tools that analyzed and extracted data from floppy disks and hard disks were MS-DOS tools for IBM PC file systems.\n",
      "One of the first MS-DOS tools used for digital investigations was Norton DiskEdit. This tool used manual processes that required investigators to spend considerable time on a typical 500 MB drive. Eventually, programs designed for digital forensics were developed for DOS, Windows, Apple, NetWare, and UNIX systems. Some of these early programs could extract data from slack and free disk space; others were capable only of retrieving deleted files. Current programs are more powerful and can search for specific words or characters, import a keyword list to search, calculate hash values, recover deleted items, conduct physical and logical analyses, and more.\n",
      "One advantage of using command-line tools for an investigation is that they require few system resources because they’re designed to run in minimal configurations. In fact, most tools fit on bootable media (USB drives, CDs, and DVDs). Conducting an initial inquiry or a complete investigation with bootable media can save time and effort. Most tools also produce a text report that fits on a USB drive or other removable media.\n",
      "Some command-line forensics tools are created specifically for Windows command-line interface (CLI) platforms; others are created for macOS and Linux. Because there are many different versions of UNIX and Linux, these OSs are often referred to as “Linux platforms.” In Chapter 5, you were introduced to using some command-line tools in Linux, such as the dd and dcfldd commands. For Windows platforms, a number of companies, such as NTI, Digital Intelligence, Maresware, DataLifter, and ByteBack, are recognized for their work in command-line forensics tools.\n",
      "Some tools that are readily available in the command line are often overlooked. For example, in Windows 2000 and later, the dir command shows you the file owner if you have multiple users on the system or network. Try it by following these steps:\n",
      "1.\n",
      "Open a command prompt window.\n",
      "2.\n",
      "At the command prompt, type cd / and press Enter to take you to the root directory. Create a work folder for this chapter by typing md Work \\Chap06\\Chapter (replacing Work with the name of your work folder) and pressing Enter.\n",
      "3.\n",
      "Make sure you’re at the root directory, and type dir /q > C:\\ Work \\Chap06\\Chapter\\Fileowner.txt and press Enter.\n",
      "4.\n",
      "In any text editor, open Fileowner.txt to see the results. You should see your file structure and whether the files were generated by the system or by a user. When you’re finished, exit the text editor and close the command prompt window.\n",
      "----------------------- END CONTENT PREVIEW -----------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Diagnostic Summary                               \n",
      "--------------------------------------------------------------------------------\n",
      "Total Chunks in DB: 11774\n",
      "\n",
      "================================================================================\n",
      "                              Diagnostic Complete                               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: Full Database Health & Hierarchy Diagnostic Report (V5 - with Content Preview)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# You might need to install pandas if you haven't already\n",
    "try:\n",
    "    import pandas as pd\n",
    "    pandas_available = True\n",
    "except ImportError:\n",
    "    pandas_available = False\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def count_total_chunks(node: Dict) -> int:\n",
    "    \"\"\"Recursively counts all chunks in a node and its children.\"\"\"\n",
    "    total = node.get('_chunks', 0)\n",
    "    for child_node in node.get('_children', {}).values():\n",
    "        total += count_total_chunks(child_node)\n",
    "    return total\n",
    "\n",
    "def print_hierarchy_report(node: Dict, indent_level: int = 0):\n",
    "    \"\"\"\n",
    "    Recursively prints the reconstructed hierarchy, sorting by sequential ToC ID.\n",
    "    \"\"\"\n",
    "    sorted_children = sorted(\n",
    "        node.get('_children', {}).items(),\n",
    "        key=lambda item: item[1].get('_toc_id', float('inf'))\n",
    "    )\n",
    "    \n",
    "    for title, child_node in sorted_children:\n",
    "        prefix = \"  \" * indent_level + \"|-- \"\n",
    "        total_chunks_in_branch = count_total_chunks(child_node)\n",
    "        direct_chunks = child_node.get('_chunks', 0)\n",
    "        toc_id = child_node.get('_toc_id', 'N/A')\n",
    "        print(f\"{prefix}{title} [ID: {toc_id}] (Total Chuck in branch: {total_chunks_in_branch}, Direct Chunk: {direct_chunks})\")\n",
    "        print_hierarchy_report(child_node, indent_level + 1)\n",
    "\n",
    "def find_testable_sections(node: Dict, path: str, testable_list: List):\n",
    "    \"\"\"\n",
    "    Recursively find sections with a decent number of \"direct\" chunks to test sequence on.\n",
    "    \"\"\"\n",
    "    if node.get('_chunks', 0) > 10 and not node.get('_children'):\n",
    "        testable_list.append({\n",
    "            \"path\": path,\n",
    "            \"toc_id\": node.get('_toc_id'),\n",
    "            \"chunk_count\": node.get('_chunks')\n",
    "        })\n",
    "\n",
    "    for title, child_node in node.get('_children', {}).items():\n",
    "        new_path = f\"{path} -> {title}\" if path else title\n",
    "        find_testable_sections(child_node, new_path, testable_list)\n",
    "\n",
    "\n",
    "# --- MODIFIED TEST FUNCTION ---\n",
    "def verify_chunk_sequence_and_content(vector_store: Chroma, hierarchy_tree: Dict):\n",
    "    \"\"\"\n",
    "    Selects a random ToC section, verifies chunk sequence, and displays the reassembled content.\n",
    "    \"\"\"\n",
    "    print_header(\"Chunk Sequence & Content Integrity Test\", char=\"-\")\n",
    "    logger.info(\"Verifying chunk order and reassembling content for a random ToC section.\")\n",
    "    \n",
    "    # 1. Find a good section to test\n",
    "    testable_sections = []\n",
    "    find_testable_sections(hierarchy_tree, \"\", testable_sections)\n",
    "    \n",
    "    if not testable_sections:\n",
    "        logger.warning(\"Could not find a suitable section with enough chunks to test. Skipping content test.\")\n",
    "        return\n",
    "\n",
    "    random_section = random.choice(testable_sections)\n",
    "    test_toc_id = random_section['toc_id']\n",
    "    section_title = random_section['path'].split(' -> ')[-1]\n",
    "    \n",
    "    logger.info(f\"Selected random section for testing: '{random_section['path']}' (toc_id: {test_toc_id})\")\n",
    "\n",
    "    # 2. Retrieve all documents (content + metadata) for that toc_id\n",
    "    try:\n",
    "        # Use .get() to retrieve full documents, not just similarity search\n",
    "        retrieved_data = vector_store.get(\n",
    "            where={\"toc_id\": test_toc_id},\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "        \n",
    "        # Combine metadatas and documents into LangChain Document objects\n",
    "        docs = [Document(page_content=doc, metadata=meta) for doc, meta in zip(retrieved_data['documents'], retrieved_data['metadatas'])]\n",
    "\n",
    "        logger.info(f\"Retrieved {len(docs)} document chunks for toc_id {test_toc_id}.\")\n",
    "\n",
    "        if len(docs) < 1:\n",
    "            logger.warning(\"No chunks found in the selected section. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # 3. Sort the documents by chunk_id\n",
    "        # Handle cases where chunk_id might be missing for robustness\n",
    "        docs.sort(key=lambda d: d.metadata.get('chunk_id', -1))\n",
    "        \n",
    "        chunk_ids = [d.metadata.get('chunk_id') for d in docs]\n",
    "        if None in chunk_ids:\n",
    "            logger.error(\"TEST FAILED: Some retrieved chunks are missing a 'chunk_id'.\")\n",
    "            return\n",
    "\n",
    "        # 4. Verify sequence\n",
    "        is_sequential = all(chunk_ids[i] == chunk_ids[i-1] + 1 for i in range(1, len(chunk_ids)))\n",
    "        \n",
    "        # 5. Reassemble and print content\n",
    "        full_content = \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "        print(\"\\n\" + \"-\"*25 + \" CONTENT PREVIEW \" + \"-\"*25)\n",
    "        print(f\"Title: {section_title} [toc_id: {test_toc_id}]\")\n",
    "        print(f\"Chunk IDs: {chunk_ids}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(full_content)\n",
    "        print(\"-\" * 23 + \" END CONTENT PREVIEW \" + \"-\"*23 + \"\\n\")\n",
    "        \n",
    "        if is_sequential:\n",
    "            logger.info(\"✅ TEST PASSED: Chunk IDs for the section are sequential and content is reassembled.\")\n",
    "        else:\n",
    "            logger.warning(\"TEST PASSED (with note): Chunk IDs are not perfectly sequential but are in increasing order.\")\n",
    "            logger.warning(\"This is acceptable. Sorting by chunk_id successfully restored narrative order.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"TEST FAILED: An error occurred during chunk sequence verification: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- MAIN DIAGNOSTIC FUNCTION ---\n",
    "def run_full_diagnostics():\n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain components not installed. Skipping diagnostics.\")\n",
    "        return\n",
    "    if not pandas_available:\n",
    "        logger.warning(\"Pandas not installed. Some reports may not be available.\")\n",
    "\n",
    "    print_header(\"Full Database Health & Hierarchy Diagnostic Report\")\n",
    "\n",
    "    # 1. Connect to the Database\n",
    "    logger.info(\"Connecting to the vector database...\")\n",
    "    if not os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        logger.error(f\"FATAL: Chroma DB directory not found at {CHROMA_PERSIST_DIR}.\")\n",
    "        return\n",
    "\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    logger.info(\"Successfully connected to the database.\")\n",
    "\n",
    "    # 2. Retrieve ALL Metadata\n",
    "    total_docs = vector_store._collection.count()\n",
    "    if total_docs == 0:\n",
    "        logger.warning(\"Database is empty. No diagnostics to run.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "    metadatas = vector_store.get(limit=total_docs, include=[\"metadatas\"])['metadatas']\n",
    "    logger.info(\"Successfully retrieved all metadata.\")\n",
    "    \n",
    "    # 3. Reconstruct the Hierarchy Tree\n",
    "    logger.info(\"Reconstructing hierarchy from chunk metadata...\")\n",
    "    hierarchy_tree = {'_children': {}}\n",
    "    chunks_without_id = 0\n",
    "\n",
    "    for meta in metadatas:\n",
    "        toc_id = meta.get('toc_id')\n",
    "        if toc_id is None or toc_id == -1:\n",
    "            chunks_without_id += 1\n",
    "            node_title = meta.get('level_1_title', 'Orphaned Chunks')\n",
    "            if node_title not in hierarchy_tree['_children']:\n",
    "                 hierarchy_tree['_children'][node_title] = {'_children': {}, '_chunks': 0, '_toc_id': float('inf')}\n",
    "            hierarchy_tree['_children'][node_title]['_chunks'] += 1\n",
    "            continue\n",
    "        \n",
    "        current_node = hierarchy_tree\n",
    "        for level in range(1, 7):\n",
    "            level_key = f'level_{level}_title'\n",
    "            title = meta.get(level_key)\n",
    "            if not title: break\n",
    "            if title not in current_node['_children']:\n",
    "                current_node['_children'][title] = {'_children': {}, '_chunks': 0, '_toc_id': float('inf')}\n",
    "            current_node = current_node['_children'][title]\n",
    "\n",
    "        current_node['_chunks'] += 1\n",
    "        current_node['_toc_id'] = min(current_node['_toc_id'], toc_id)\n",
    "        \n",
    "    logger.info(\"Hierarchy reconstruction complete.\")\n",
    "\n",
    "    # 4. Print Hierarchy Report\n",
    "    print_header(\"Reconstructed Hierarchy Report (Book Order)\", char=\"-\")\n",
    "    print_hierarchy_report(hierarchy_tree)\n",
    "        \n",
    "    # 5. Run Chunk Sequence and Content Test\n",
    "    verify_chunk_sequence_and_content(vector_store, hierarchy_tree)\n",
    "    \n",
    "    # 6. Final Summary\n",
    "    print_header(\"Diagnostic Summary\", char=\"-\")\n",
    "    print(f\"Total Chunks in DB: {total_docs}\")\n",
    "    \n",
    "    if chunks_without_id > 0:\n",
    "        logger.warning(f\"Found {chunks_without_id} chunks MISSING a valid 'toc_id'. Check 'Orphaned' sections.\")\n",
    "    else:\n",
    "        logger.info(\"All chunks contain valid 'toc_id' metadata. Sequential integrity is maintained.\")\n",
    "\n",
    "    print_header(\"Diagnostic Complete\")\n",
    "\n",
    "# --- Execute Diagnostics ---\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and langchain_available:\n",
    "    run_full_diagnostics()\n",
    "else:\n",
    "    logger.error(\"Skipping diagnostics: Global variables not defined or LangChain not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b7287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:27,311 - INFO - Connecting to the existing vector database at '/home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub'...\n",
      "2025-07-14 17:26:27,324 - INFO - Found 18 chunks in the database for this section.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFYING SECTION: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations > An Overview of Digital Forensics' (toc_id: 9)\n",
      "================================================================================\n",
      "\n",
      "############################ Reassembled Text ############################\n",
      "An Overview of Digital Forensics\n",
      "As the world has become more of a level playing field, with more people online who have access to the same information (Thomas L. Freidman, The World Is Flat, Farrar, Straus, and Giroux, 2005), the need to standardize digital forensics processes has become more urgent. The definition of digital forensics Applying investigative procedures for a legal purpose; involves the analysis of digital evidence as well as obtaining search warrants, maintaining a chain of custody, validating with mathematical hash functions, using validated tools, ensuring repeatability, reporting, and presenting evidence as an expert witness. has also evolved over the years from simply involving securing and analyzing digital information stored on a computer for use as evidence in civil, criminal, or administrative\n",
      "digital information stored on a computer for use as evidence in civil, criminal, or administrative cases. The former director of the Defense Computer Forensics Laboratory, Ken Zatyko, wrote a treatise on the many specialties including computer forensics, network forensics, video forensics, and a host of others. He defined it as “[t]he application of computer science and investigative procedures for a legal purpose involving the analysis of digital evidence (information of probative value that is stored or transmitted in binary form) after proper search authority, chain of custody, validation with mathematics (hash function), use of validated tools, repeatability, reporting and possible expert presentation” (“Commentary: Defining Digital Forensics,” Forensic Magazine, 2007).\n",
      "The field of digital forensics can also encompass items such as research and incident response. With incident response, most organizations are concerned with protecting their assets and containing the situation, not necessarily prosecuting or finding the person responsible. Research in digital forensics also isn’t concerned with prosecution or validity of evidence. This book is intended for digital forensics investigators and examiners at the civil, criminal, and administrative levels. Other facets of digital forensics are beyond the scope of this book. Keep in mind that depending on the jurisdiction and situation, forensic investigators and examiners might be the same or different personnel. In this book, the terms are used interchangeably.\n",
      "Note\n",
      "For a more in-depth discussion of what the term “digital forensics” means, see “Digital Forensic Evidence Examination” (Fred Cohen, www.fredcohen.net/Books/2013-DFE-Examination.pdf, 2012).\n",
      "Many groups have tried to create digital forensics certifications that could be recognized worldwide but have failed in this attempt. However, they have created certifications for specific categories of practitioners, such as government investigators. With the ubiquitous access to mobile devices now, digital evidence is everywhere, so the need for a global standardized method is even more critical so that companies and governments can share and use digital evidence. In October 2012, an International Organization for Standardization (ISO) standard for digital forensics was ratified. This standard, ISO 27037 “Information technology – Security techniques – Guidelines for identification, collection, acquisition and preservation of digital evidence” (see www.iso.org/standard/44381.html),\n",
      "acquisition and preservation of digital evidence” (see www.iso.org/standard/44381.html), defines the personnel and methods for acquiring and preserving digital evidence. To address the multinational cases that continue to emerge, agencies in every country should develop policies and procedures that meet this standard.\n",
      "The Federal Rules of Evidence (FRE), signed into law in 1973, was created to ensure consistency in federal proceedings, but many states’ rules map to the FRE, too. In another attempt to standardize procedures, the FBI Computer Analysis and Response Team (CART) was formed in 1984 to handle the increase in cases involving digital evidence. By the late 1990s, CART had teamed up with the Department of Defense Computer Forensics Laboratory (DCFL) for research and training. Much of the early curriculum in this field came from the DCFL. For more information on the FBI’s cybercrime investigation services, see www.fbi.gov/investigate/cyber.\n",
      "Files maintained on a computer are covered by different rules, depending on the nature of the documents. Many court cases in state and federal courts have developed and clarified how the rules apply to digital evidence. The Fourth Amendment The Fourth Amendment to the U.S. Constitution in the Bill of Rights dictates that the government and its agents must have probable cause for search and seizure. to the U.S. Constitution (and each state’s constitution) protects everyone’s right to be secure in their person, residence, and property from search and seizure. Continuing development of the jurisprudence of this amendment has played a role in determining whether the search for digital evidence has established a different precedent, so separate search warrants Legal documents that allow law\n",
      "has established a different precedent, so separate search warrants Legal documents that allow law enforcement to search an office, a home, or other locale for evidence related to an alleged crime. might not be necessary. However, when preparing to search for evidence in a criminal case, many investigators still include the suspect’s computer and its components in the search warrant to avoid later admissibility problems.\n",
      "In an important case involving these issues, the Pennsylvania Supreme Court addressed expectations of privacy and whether evidence is admissible (see Commonwealth v. Copenhefer, 587 A.2d 1353, 526 Pa. 555 [1991]). Initial investigations by the FBI, state police, and local police resulted in discovering computer-generated notes and instructions—some of which had been deleted—that had been concealed in hiding places around Corry, Pennsylvania. The investigation also produced several possible suspects, including David Copenhefer, who owned a nearby bookstore and apparently had bad relationships with the victim and her husband. Examination of trash discarded from Copenhefer’s store revealed drafts of the ransom note and directions. Subsequent search warrants resulted in seizure of evidence\n",
      "of the ransom note and directions. Subsequent search warrants resulted in seizure of evidence against him. Copenhefer’s computer contained several drafts and amendments of the text of phone calls to the victim and the victim’s husband the next day, the ransom note, the series of hidden notes, and a plan for the entire kidnapping scheme (Copenhefer, p. 559).\n",
      "On direct appeal, the Pennsylvania Supreme Court concluded that the physical evidence, including the digital forensics evidence, was sufficient to support the bookstore owner’s conviction. Copenhefer’s argument was that “[E]ven though his computer was validly seized pursuant to a warrant, his attempted deletion of the documents in question created an expectation of privacy protected by the Fourth Amendment. Thus, he claims, under Katz v. United States, 389 U.S. 347, 357, 88 S.Ct. 507, 19 L.Ed.2d 576 (1967), and its progeny, Agent Johnson’s retrieval of the documents, without first obtaining another search warrant, was unreasonable under the Fourth Amendment and the documents thus seized should have been suppressed” (Copenhefer, p. 561).\n",
      "The Pennsylvania Supreme Court rejected this argument, stating, “A defendant’s attempt to secrete evidence of a crime is not synonymous with a legally cognizable expectation of privacy. A mere hope for secrecy is not a legally protected expectation. If it were, search warrants would be required in a vast number of cases where warrants are clearly not necessary” (Copenhefer, p. 562).\n",
      "Every U.S. jurisdiction has case law related to the admissibility of evidence recovered from computers and other digital devices. As you learn in this book, however, the laws on digital evidence vary between states as well as between provinces and countries.\n",
      "Note\n",
      "The U.S. Department of Justice offers a useful guide to search and seizure procedures for computers and computer evidence at https://pdfs.semanticscholar.org/aabb/8b0caf982a0aa211f932252af38d4c9376fb.pdf.\n",
      "################################################################################\n",
      "\n",
      "------------------------ Retrieved Chunk Details -------------------------\n",
      "\n",
      "[ Chunk 1 of 18 | chunk_id: 156 ]\n",
      "  Content Preview: 'An Overview of Digital Forensics...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"toc_id\": 9,\n",
      "  \"chunk_id\": 156,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\"\n",
      "}\n",
      "\n",
      "[ Chunk 2 of 18 | chunk_id: 157 ]\n",
      "  Content Preview: 'As the world has become more of a level playing field, with more people online who have access to the same information (Thomas L. Freidman, The World Is Flat, Farrar, Straus, and Giroux, 2005), the need to standardize digital forensics processes has ...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 157,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"toc_id\": 9,\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ Chunk 3 of 18 | chunk_id: 158 ]\n",
      "  Content Preview: 'digital information stored on a computer for use as evidence in civil, criminal, or administrative cases. The former director of the Defense Computer Forensics Laboratory, Ken Zatyko, wrote a treatise on the many specialties including computer forens...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"toc_id\": 9,\n",
      "  \"chunk_id\": 158\n",
      "}\n",
      "\n",
      "[ Chunk 4 of 18 | chunk_id: 159 ]\n",
      "  Content Preview: 'The field of digital forensics can also encompass items such as research and incident response. With incident response, most organizations are concerned with protecting their assets and containing the situation, not necessarily prosecuting or finding...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"toc_id\": 9,\n",
      "  \"chunk_id\": 159,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\"\n",
      "}\n",
      "\n",
      "[ Chunk 5 of 18 | chunk_id: 160 ]\n",
      "  Content Preview: 'Note...'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 160,\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 6 of 18 | chunk_id: 161 ]\n",
      "  Content Preview: 'For a more in-depth discussion of what the term “digital forensics” means, see “Digital Forensic Evidence Examination” (Fred Cohen, www.fredcohen.net/Books/2013-DFE-Examination.pdf, 2012)....'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"chunk_id\": 161,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 9,\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "}\n",
      "\n",
      "[ Chunk 7 of 18 | chunk_id: 162 ]\n",
      "  Content Preview: 'Many groups have tried to create digital forensics certifications that could be recognized worldwide but have failed in this attempt. However, they have created certifications for specific categories of practitioners, such as government investigators...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 162,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 8 of 18 | chunk_id: 163 ]\n",
      "  Content Preview: 'acquisition and preservation of digital evidence” (see www.iso.org/standard/44381.html), defines the personnel and methods for acquiring and preserving digital evidence. To address the multinational cases that continue to emerge, agencies in every co...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 163,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 9 of 18 | chunk_id: 164 ]\n",
      "  Content Preview: 'The Federal Rules of Evidence (FRE), signed into law in 1973, was created to ensure consistency in federal proceedings, but many states’ rules map to the FRE, too. In another attempt to standardize procedures, the FBI Computer Analysis and Response T...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"toc_id\": 9,\n",
      "  \"chunk_id\": 164,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\"\n",
      "}\n",
      "\n",
      "[ Chunk 10 of 18 | chunk_id: 165 ]\n",
      "  Content Preview: 'Files maintained on a computer are covered by different rules, depending on the nature of the documents. Many court cases in state and federal courts have developed and clarified how the rules apply to digital evidence. The Fourth Amendment The Fourt...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 165,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 11 of 18 | chunk_id: 166 ]\n",
      "  Content Preview: 'has established a different precedent, so separate search warrants Legal documents that allow law enforcement to search an office, a home, or other locale for evidence related to an alleged crime. might not be necessary. However, when preparing to se...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 166,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 12 of 18 | chunk_id: 167 ]\n",
      "  Content Preview: 'In an important case involving these issues, the Pennsylvania Supreme Court addressed expectations of privacy and whether evidence is admissible (see Commonwealth v. Copenhefer, 587 A.2d 1353, 526 Pa. 555 [1991]). Initial investigations by the FBI, s...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 167,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 13 of 18 | chunk_id: 168 ]\n",
      "  Content Preview: 'of the ransom note and directions. Subsequent search warrants resulted in seizure of evidence against him. Copenhefer’s computer contained several drafts and amendments of the text of phone calls to the victim and the victim’s husband the next day, t...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 9,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 168\n",
      "}\n",
      "\n",
      "[ Chunk 14 of 18 | chunk_id: 169 ]\n",
      "  Content Preview: 'On direct appeal, the Pennsylvania Supreme Court concluded that the physical evidence, including the digital forensics evidence, was sufficient to support the bookstore owner’s conviction. Copenhefer’s argument was that “[E]ven though his computer wa...'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 169,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 9\n",
      "}\n",
      "\n",
      "[ Chunk 15 of 18 | chunk_id: 170 ]\n",
      "  Content Preview: 'The Pennsylvania Supreme Court rejected this argument, stating, “A defendant’s attempt to secrete evidence of a crime is not synonymous with a legally cognizable expectation of privacy. A mere hope for secrecy is not a legally protected expectation. ...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 9,\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 170,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\"\n",
      "}\n",
      "\n",
      "[ Chunk 16 of 18 | chunk_id: 171 ]\n",
      "  Content Preview: 'Every U.S. jurisdiction has case law related to the admissibility of evidence recovered from computers and other digital devices. As you learn in this book, however, the laws on digital evidence vary between states as well as between provinces and co...'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"chunk_id\": 171,\n",
      "  \"toc_id\": 9,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "}\n",
      "\n",
      "[ Chunk 17 of 18 | chunk_id: 172 ]\n",
      "  Content Preview: 'Note...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 172,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 9,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\"\n",
      "}\n",
      "\n",
      "[ Chunk 18 of 18 | chunk_id: 173 ]\n",
      "  Content Preview: 'The U.S. Department of Justice offers a useful guide to search and seizure procedures for computers and computer evidence at https://pdfs.semanticscholar.org/aabb/8b0caf982a0aa211f932252af38d4c9376fb.pdf....'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 9,\n",
      "  \"chunk_id\": 173\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Verification complete for section 'Chapter 1. Understanding the Digital Forensics Profession and Investigations > An Overview of Digital Forensics'.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Content Retrieval for a Specific toc_id with Reassembled Text\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_and_print_chunks_for_toc_id(vector_store: Chroma, toc_id: int):\n",
    "    \"\"\"\n",
    "    Retrieves all chunks for a specific toc_id, reconstructs the section title\n",
    "    from hierarchical metadata, shows the reassembled text, and lists individual\n",
    "    chunk details for verification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the 'get' method with a 'where' filter to find all chunks for the toc_id\n",
    "        results = vector_store.get(\n",
    "            where={\"toc_id\": toc_id},\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "\n",
    "        if not results or not results.get('ids'):\n",
    "            logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"VERIFICATION FAILED: No content found for toc_id: {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            return\n",
    "\n",
    "        documents = results['documents']\n",
    "        metadatas = results['metadatas']\n",
    "        \n",
    "        # --- FIX START: Reconstruct the hierarchical section title from metadata ---\n",
    "        # We assume all chunks for the same toc_id share the same titles.\n",
    "        # We will inspect the metadata of the first chunk to get the title.\n",
    "        section_title = \"Unknown or Uncategorized Section\"\n",
    "        if metadatas:\n",
    "            first_meta = metadatas[0]\n",
    "            \n",
    "            # Find all 'level_X_title' keys in the metadata\n",
    "            level_titles = []\n",
    "            for key, value in first_meta.items():\n",
    "                if key.startswith(\"level_\") and key.endswith(\"_title\"):\n",
    "                    try:\n",
    "                        # Extract the level number (e.g., 1 from 'level_1_title') for sorting\n",
    "                        level_num = int(key.split('_')[1])\n",
    "                        level_titles.append((level_num, value))\n",
    "                    except (ValueError, IndexError):\n",
    "                        # Ignore malformed keys, just in case\n",
    "                        continue\n",
    "            \n",
    "            # Sort the titles by their level number (1, 2, 3...)\n",
    "            level_titles.sort()\n",
    "            \n",
    "            # Join the sorted titles to create a breadcrumb-style title\n",
    "            if level_titles:\n",
    "                title_parts = [title for num, title in level_titles]\n",
    "                section_title = \" > \".join(title_parts)\n",
    "        # --- FIX END ---\n",
    "        \n",
    "        # --- Print a clear header with the reconstructed section title ---\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"VERIFYING SECTION: '{section_title}' (toc_id: {toc_id})\")\n",
    "        print(\"=\" * 80)\n",
    "        logger.info(f\"Found {len(documents)} chunks in the database for this section.\")\n",
    "        \n",
    "        # Sort chunks by their chunk_id to ensure they are in the correct order for reassembly\n",
    "        sorted_items = sorted(zip(documents, metadatas), key=lambda item: item[1].get('chunk_id', 0))\n",
    "\n",
    "        # --- Reassemble and print the full text for the section ---\n",
    "        all_chunk_texts = [item[0] for item in sorted_items]\n",
    "        reassembled_text = \"\\n\".join(all_chunk_texts)\n",
    "        \n",
    "        print(\"\\n\" + \"#\" * 28 + \" Reassembled Text \" + \"#\" * 28)\n",
    "        print(reassembled_text)\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        # --- Print individual chunk details for in-depth verification ---\n",
    "        print(\"\\n\" + \"-\" * 24 + \" Retrieved Chunk Details \" + \"-\" * 25)\n",
    "        for i, (doc, meta) in enumerate(sorted_items):\n",
    "            print(f\"\\n[ Chunk {i+1} of {len(documents)} | chunk_id: {meta.get('chunk_id', 'N/A')} ]\")\n",
    "            content_preview = doc.replace('\\n', ' ').strip()\n",
    "            print(f\"  Content Preview: '{content_preview[:250]}...'\")\n",
    "            print(f\"  Metadata: {json.dumps(meta, indent=2)}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Verification complete for section '{section_title}'.\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during retrieval for toc_id {toc_id}: {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTION BLOCK (No changes needed here)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- IMPORTANT: Set the ID of the section you want to test here ---\n",
    "# Example: ToC ID 10 might be \"An Overview of Digital Forensics\"\n",
    "# Example: ToC ID 11 might be \"Digital Forensics and Other Related Disciplines\"\n",
    "TOC_ID_TO_TEST = 9# Change this to an ID you know exists from your ToC\n",
    "\n",
    "\n",
    "# Assume these variables are defined in a previous cell from your notebook\n",
    "# CHROMA_PERSIST_DIR = \"./chroma_db_with_metadata\"\n",
    "# EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "# CHROMA_COLLECTION_NAME = \"forensics_handbook\"\n",
    "\n",
    "# Check if the database directory exists before attempting to connect\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and os.path.exists(CHROMA_PERSIST_DIR):\n",
    "    logger.info(f\"Connecting to the existing vector database at '{CHROMA_PERSIST_DIR}'...\")\n",
    "    \n",
    "    try:\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        # Run the verification function\n",
    "        retrieve_and_print_chunks_for_toc_id(vector_store, TOC_ID_TO_TEST)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Chroma or run retrieval. Error: {e}\")\n",
    "        logger.error(\"Please ensure your embedding model and collection names are correct.\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Database directory not found or 'CHROMA_PERSIST_DIR' variable is not set.\")\n",
    "    logger.error(\"Please run the previous cell (Cell 5) to create the database first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "Require Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf3ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:27,342 - INFO - Connecting to DB and initializing components...\n",
      "2025-07-14 17:26:27,352 - INFO - Goal: Confirm the database is live and contains thematically relevant content.\n",
      "2025-07-14 17:26:27,352 - INFO - Strategy: Perform a simple similarity search using the course's 'unitName'.\n",
      "2025-07-14 17:26:27,353 - INFO - Action: Searching for query: 'Digital Forensic'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         Database Verification Process                          \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                            Test 1: Basic Retrieval                             \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:26:29,312 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-14 17:26:29,314 - INFO - Verification: Check if at least one document was returned.\n",
      "2025-07-14 17:26:29,315 - INFO - ✅ Result: TEST 1 PASSED. The database is online and responsive.\n",
      "2025-07-14 17:26:29,315 - INFO - Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\n",
      "2025-07-14 17:26:29,315 - INFO - Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\n",
      "2025-07-14 17:26:29,316 - INFO -   - Selected random deep section: Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> Taking a Systematic Approach\n",
      "2025-07-14 17:26:29,316 - INFO - Action: Performing a similarity search with a highly specific '$and' filter.\n",
      "2025-07-14 17:26:29,379 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-14 17:26:29,408 - INFO - Verification: Check if the precisely filtered query returned any documents.\n",
      "2025-07-14 17:26:29,409 - INFO - ✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\n",
      "2025-07-14 17:26:29,409 - INFO - Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\n",
      "2025-07-14 17:26:29,409 - INFO - Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\n",
      "2025-07-14 17:26:29,410 - INFO -   - Selected random week: Week Week 5 - 'Working with Windows and CLI Systems.'\n",
      "2025-07-14 17:26:29,410 - INFO -   - Extracted required chapter number(s): ['2019', '978', '1', '337', '56894', '4', '5']\n",
      "2025-07-14 17:26:29,413 - INFO -   - Mapped to top-level ToC entries: ['Chapter 4. Processing Crime and Incident Scenes', 'Chapter 5. Working with Windows and CLI Systems', 'Chapter 1. Understanding the Digital Forensics Profession and Investigations']\n",
      "2025-07-14 17:26:29,414 - INFO - Action: Searching for the weekly topic, filtered by the mapped chapter(s).\n",
      "2025-07-14 17:26:29,432 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-14 17:26:29,452 - INFO - Verification: Check if at least one returned document is from the correct chapter.\n",
      "2025-07-14 17:26:29,452 - INFO - ✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\n",
      "2025-07-14 17:26:29,453 - INFO - Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\n",
      "2025-07-14 17:26:29,453 - INFO - Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\n",
      "2025-07-14 17:26:29,454 - INFO - Action: Performing similarity search for topic: 'Processing Crime and Incident Scenes.' to get a set of chunks.\n",
      "2025-07-14 17:26:29,490 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-14 17:26:29,494 - INFO -   - Retrieved and sorted chunk IDs: [1844, 1848, 1937, 1961, 2023, 2025, 2046, 2075, 2247, 9102]\n",
      "2025-07-14 17:26:29,494 - INFO - Verification: Check if the sorted list of chunk_ids is strictly increasing.\n",
      "2025-07-14 17:26:29,495 - INFO - ✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Digital Forensic'\n",
      "--> Found 1 results. Displaying top 1:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'An Overview of Digital Forensics...'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"chunk_id\": 156,\n",
      "  \"toc_id\": 9,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                        Test 2: Deep Hierarchy Retrieval                        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Taking a Systematic Approach'\n",
      "FILTER: {\n",
      "  \"$and\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_2_title\": {\n",
      "        \"$eq\": \"Preparing a Digital Forensics Investigation\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_3_title\": {\n",
      "        \"$eq\": \"Taking a Systematic Approach\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> Found 1 results. Displaying top 1:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Taking a Systematic Approach...'\n",
      "  Metadata: {\n",
      "  \"level_3_title\": \"Taking a Systematic Approach\",\n",
      "  \"chunk_id\": 330,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 27,\n",
      "  \"level_2_title\": \"Preparing a Digital Forensics Investigation\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Test 3: Advanced Unit Outline Alignment                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Working with Windows and CLI Systems.'\n",
      "FILTER: {\n",
      "  \"$or\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 4. Processing Crime and Incident Scenes\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 5. Working with Windows and CLI Systems\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> Found 5 results. Displaying top 3:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Chapter 5. Working with Windows and CLI Systems...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 183,\n",
      "  \"chunk_id\": 2438,\n",
      "  \"level_1_title\": \"Chapter 5. Working with Windows and CLI Systems\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ RESULT 2 ]\n",
      "  Content : 'Chapter 5. Working with Windows and CLI Systems...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 5. Working with Windows and CLI Systems\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 183,\n",
      "  \"chunk_id\": 9345\n",
      "}\n",
      "\n",
      "[ RESULT 3 ]\n",
      "  Content : 'so that you can access and modify system settings when necessary. This chapter examines Windows and CLI OSs in detail; Chapter 7 covers information on Linux and Macintosh. For other OSs, consult syste...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 185,\n",
      "  \"level_1_title\": \"Chapter 5. Working with Windows and CLI Systems\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_2_title\": \"Understanding File Systems\",\n",
      "  \"chunk_id\": 2450\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Test 4: Content Sequence Verification                      \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Processing Crime and Incident Scenes.'\n",
      "--> Found 10 results. Displaying top 3:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Processing Incident or Crime Scenes...'\n",
      "  Metadata: {\n",
      "  \"level_3_title\": \"Processing Incident or Crime Scenes\",\n",
      "  \"toc_id\": 162,\n",
      "  \"level_2_title\": \"Seizing Digital Evidence at the Scene\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 4. Processing Crime and Incident Scenes\",\n",
      "  \"chunk_id\": 2046\n",
      "}\n",
      "\n",
      "[ RESULT 2 ]\n",
      "  Content : 'Chapter 4. Processing Crime and Incident Scenes...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 143,\n",
      "  \"chunk_id\": 1844,\n",
      "  \"level_1_title\": \"Chapter 4. Processing Crime and Incident Scenes\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ RESULT 3 ]\n",
      "  Content : 'Chapter 4. Processing Crime and Incident Scenes...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 143,\n",
      "  \"level_1_title\": \"Chapter 4. Processing Crime and Incident Scenes\",\n",
      "  \"chunk_id\": 9102\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                              Verification Summary                              \n",
      "================================================================================\n",
      "Total Tests Run: 4\n",
      "✅ Passed: 4\n",
      "❌ Failed: 0\n",
      "\n",
      "================================================================================\n",
      "                             Verification Complete                              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Verify Vector Database (Final Version with Rich Diagnostic Output)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "def print_results(query_text: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Richly prints query results, showing the query, filter, and retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*10 + \" DIAGNOSTIC: RETRIEVAL RESULTS \" + \"-\"*10)\n",
    "    print(f\"QUERY: '{query_text}'\")\n",
    "    if where_filter:\n",
    "        print(f\"FILTER: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"--> No documents were retrieved for this query and filter.\")\n",
    "        print(\"-\" * 55)\n",
    "        return\n",
    "        \n",
    "    print(f\"--> Found {len(results)} results. Displaying top {min(len(results), 3)}:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n[ RESULT {i+1} ]\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"  Content : '{content_preview[:200]}...'\")\n",
    "        print(f\"  Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "\n",
    "# --- HELPER FUNCTIONS FOR FINDING DATA (UNCHANGED) ---\n",
    "def find_deep_entry(nodes: List[Dict], current_path: List[str] = []) -> Optional[Tuple[Dict, List[str]]]:\n",
    "    shuffled_nodes = random.sample(nodes, len(nodes))\n",
    "    for node in shuffled_nodes:\n",
    "        if node.get('level', 0) >= 2 and node.get('children'): return node, current_path + [node['title']]\n",
    "        if node.get('children'):\n",
    "            path = current_path + [node['title']]\n",
    "            deep_entry = find_deep_entry(node['children'], path)\n",
    "            if deep_entry: return deep_entry\n",
    "    return None\n",
    "\n",
    "def find_chapter_title_by_number(toc_data: List[Dict], chap_num: int) -> Optional[List[str]]:\n",
    "    def search_nodes(nodes, num, current_path):\n",
    "        for node in nodes:\n",
    "            path = current_path + [node['title']]\n",
    "            if re.match(rf\"(Chapter\\s)?{num}[.:\\s]\", node.get('title', ''), re.IGNORECASE): return path\n",
    "            if node.get('children'):\n",
    "                found_path = search_nodes(node['children'], num, path)\n",
    "                if found_path: return found_path\n",
    "        return None\n",
    "    return search_nodes(toc_data, chap_num, [])\n",
    "\n",
    "\n",
    "# --- ENHANCED TEST CASES with DIAGNOSTIC OUTPUT ---\n",
    "\n",
    "def basic_retrieval_test(db, outline):\n",
    "    print_header(\"Test 1: Basic Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm the database is live and contains thematically relevant content.\")\n",
    "        logger.info(\"Strategy: Perform a simple similarity search using the course's 'unitName'.\")\n",
    "        query_text = outline.get(\"unitInformation\", {}).get(\"unitName\", \"introduction\")\n",
    "        \n",
    "        logger.info(f\"Action: Searching for query: '{query_text}'...\")\n",
    "        results = db.similarity_search(query_text, k=1)\n",
    "        \n",
    "        print_results(query_text, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one document was returned.\")\n",
    "        assert len(results) > 0, \"Basic retrieval query returned no results.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 1 PASSED. The database is online and responsive.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 1 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def deep_hierarchy_test(db, toc):\n",
    "    print_header(\"Test 2: Deep Hierarchy Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\")\n",
    "        logger.info(\"Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\")\n",
    "        deep_entry_result = find_deep_entry(toc)\n",
    "        assert deep_entry_result, \"Could not find a suitable deep entry (level >= 2) to test.\"\n",
    "        node, path = deep_entry_result\n",
    "        query = node['title']\n",
    "        \n",
    "        logger.info(f\"  - Selected random deep section: {' -> '.join(path)}\")\n",
    "        conditions = [{f\"level_{i+1}_title\": {\"$eq\": title}} for i, title in enumerate(path)]\n",
    "        w_filter = {\"$and\": conditions}\n",
    "        \n",
    "        logger.info(\"Action: Performing a similarity search with a highly specific '$and' filter.\")\n",
    "        results = db.similarity_search(query, k=1, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if the precisely filtered query returned any documents.\")\n",
    "        assert len(results) > 0, \"Deeply filtered query returned no results.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 2 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def advanced_alignment_test(db, outline, toc):\n",
    "    print_header(\"Test 3: Advanced Unit Outline Alignment\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\")\n",
    "        logger.info(\"Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\")\n",
    "        week_to_test = random.choice(outline['weeklySchedule'])\n",
    "        logger.info(f\"  - Selected random week: Week {week_to_test['week']} - '{week_to_test['contentTopic']}'\")\n",
    "\n",
    "        reading = week_to_test.get('requiredReading', '')\n",
    "        chap_nums_str = re.findall(r'\\d+', reading)\n",
    "        assert chap_nums_str, f\"Could not find chapter numbers in required reading: '{reading}'\"\n",
    "        logger.info(f\"  - Extracted required chapter number(s): {chap_nums_str}\")\n",
    "\n",
    "        chapter_paths = [find_chapter_title_by_number(toc, int(n)) for n in chap_nums_str]\n",
    "        chapter_paths = [path for path in chapter_paths if path is not None]\n",
    "        assert chapter_paths, f\"Could not map chapter numbers {chap_nums_str} to a valid ToC path.\"\n",
    "        \n",
    "        level_1_titles = list(set([path[0] for path in chapter_paths]))\n",
    "        logger.info(f\"  - Mapped to top-level ToC entries: {level_1_titles}\")\n",
    "\n",
    "        or_filter = [{\"level_1_title\": {\"$eq\": title}} for title in level_1_titles]\n",
    "        w_filter = {\"$or\": or_filter} if len(or_filter) > 1 else or_filter[0]\n",
    "        query = week_to_test['contentTopic']\n",
    "        \n",
    "        logger.info(\"Action: Searching for the weekly topic, filtered by the mapped chapter(s).\")\n",
    "        results = db.similarity_search(query, k=5, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one returned document is from the correct chapter.\")\n",
    "        assert len(results) > 0, \"Alignment query returned no results for the correct section/chapter.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 3 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def content_sequence_test(db, outline):\n",
    "    print_header(\"Test 4: Content Sequence Verification\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\")\n",
    "        logger.info(\"Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\")\n",
    "        topic_query = random.choice(outline['weeklySchedule'])['contentTopic']\n",
    "        \n",
    "        logger.info(f\"Action: Performing similarity search for topic: '{topic_query}' to get a set of chunks.\")\n",
    "        results = db.similarity_search(topic_query, k=10)\n",
    "        \n",
    "        print_results(topic_query, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        docs_with_id = [doc for doc in results if 'chunk_id' in doc.metadata]\n",
    "        assert len(docs_with_id) > 3, \"Fewer than 4 retrieved chunks have a 'chunk_id' to test.\"\n",
    "        \n",
    "        chunk_ids = [doc.metadata['chunk_id'] for doc in docs_with_id]\n",
    "        sorted_ids = sorted(chunk_ids)\n",
    "        \n",
    "        logger.info(f\"  - Retrieved and sorted chunk IDs: {sorted_ids}\")\n",
    "        logger.info(\"Verification: Check if the sorted list of chunk_ids is strictly increasing.\")\n",
    "        is_ordered = all(sorted_ids[i] >= sorted_ids[i-1] for i in range(1, len(sorted_ids)))\n",
    "        assert is_ordered, \"The retrieved chunks' chunk_ids are not in ascending order when sorted.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 4 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    \n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain libraries not found. Aborting tests.\")\n",
    "        return\n",
    "\n",
    "    required_files = {\n",
    "        \"Chroma DB\": CHROMA_PERSIST_DIR,\n",
    "        \"ToC JSON\": PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        \"Parsed Outline\": PARSED_UO_JSON_PATH\n",
    "    }\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Required '{name}' not found at '{path}'. Please run previous cells.\")\n",
    "            return\n",
    "\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        toc_data = json.load(f)\n",
    "    with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        unit_outline_data = json.load(f)\n",
    "\n",
    "    logger.info(\"Connecting to DB and initializing components...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    results_summary = [\n",
    "        basic_retrieval_test(vector_store, unit_outline_data),\n",
    "        deep_hierarchy_test(vector_store, toc_data),\n",
    "        advanced_alignment_test(vector_store, unit_outline_data, toc_data),\n",
    "        content_sequence_test(vector_store, unit_outline_data)\n",
    "    ]\n",
    "\n",
    "    passed_count = sum(filter(None, results_summary))\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    \n",
    "    print_header(\"Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)}\")\n",
    "    print(f\"✅ Passed: {passed_count}\")\n",
    "    print(f\"❌ Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "# Assumes global variables from Cell 1 are available in the notebook's scope\n",
    "run_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97368b0",
   "metadata": {},
   "source": [
    "#  Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae477bc",
   "metadata": {},
   "source": [
    "## Planning Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c538570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlanningAgent✂️⏰\n",
    "# Cell 8: The Data-Driven Planning Agent (Final Hierarchical Version⭐)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# Setup Logger and LangChain components\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "class PlanningAgent:\n",
    "    \"\"\"\n",
    "    An agent that creates a hierarchical content plan, adaptively partitions content\n",
    "    into distinct lecture decks, and allocates presentation time.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "        self.config = master_config['processed_settings']\n",
    "        self.unit_outline = master_config['unit_outline']\n",
    "        self.book_toc = master_config['book_toc']\n",
    "        self.flat_toc_with_ids = self._create_flat_toc_with_ids()\n",
    "        self.vector_store = vector_store\n",
    "        logger.info(\"Data-Driven PlanningAgent initialized successfully.\")\n",
    "\n",
    "    def _create_flat_toc_with_ids(self) -> List[Dict]:\n",
    "        \"\"\"Creates a flattened list of the ToC for easy metadata lookup.\"\"\"\n",
    "        flat_list = []\n",
    "        def flatten_recursive(nodes, counter):\n",
    "            for node in nodes:\n",
    "                node_id = counter[0]; counter[0] += 1\n",
    "                flat_list.append({'toc_id': node_id, 'title': node.get('title', ''), 'node': node})\n",
    "                if node.get('children'):\n",
    "                    flatten_recursive(node.get('children'), counter)\n",
    "        flatten_recursive(self.book_toc, [0])\n",
    "        return flat_list\n",
    "    \n",
    "    def _assign_sequence_ids_recursively(self, node: Dict, counter: list):\n",
    "        \"\"\"\n",
    "        Recursively walks a content node, assigning a unique, sequential ID\n",
    "        to the node, its children, and its interactive activity in a depth-first order.\n",
    "        \"\"\"\n",
    "        # 1. Assign sequence ID to the parent content node itself.\n",
    "        node['seq_id'] = counter[0]\n",
    "        counter[0] += 1\n",
    "\n",
    "        # 2. Recurse through all children first.\n",
    "        if 'children' in node and node['children']:\n",
    "            for child in node['children']:\n",
    "                self._assign_sequence_ids_recursively(child, counter)\n",
    "\n",
    "        # 3. Assign an ID to the interactive activity LAST, so it appears after the content and its children.\n",
    "        if 'interactive_activity' in node:\n",
    "            node['interactive_activity']['seq_id'] = counter[0]\n",
    "            counter[0] += 1\n",
    "\n",
    "    def _identify_relevant_chapters(self, weekly_schedule_item: Dict) -> List[int]:\n",
    "        \"\"\"Extracts chapter numbers precisely from the 'requiredReading' string.\"\"\"\n",
    "        reading_str = weekly_schedule_item.get('requiredReading', '')\n",
    "        match = re.search(r'Chapter(s)?', reading_str, re.IGNORECASE)\n",
    "        if not match: return []\n",
    "        search_area = reading_str[match.start():]\n",
    "        chap_nums_str = re.findall(r'\\d+', search_area)\n",
    "        if chap_nums_str:\n",
    "            return sorted(list(set(int(n) for n in chap_nums_str)))\n",
    "        return []\n",
    "\n",
    "    def _find_chapter_node(self, chapter_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Finds the ToC node for a specific chapter number.\"\"\"\n",
    "        for item in self.flat_toc_with_ids:\n",
    "            if re.match(rf\"Chapter\\s{chapter_number}(?:\\D|$)\", item['title']):\n",
    "                return item['node']\n",
    "        return None\n",
    "\n",
    "    def _build_topic_plan_tree(self, toc_node: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively builds a hierarchical plan tree from any ToC node,\n",
    "        annotating it with direct and total branch chunk counts.\n",
    "        \"\"\"\n",
    "        node_metadata = next((item for item in self.flat_toc_with_ids if item['node'] is toc_node), None)\n",
    "        if not node_metadata: return {}\n",
    "\n",
    "        retrieved_docs = self.vector_store.get(where={'toc_id': node_metadata['toc_id']})\n",
    "        direct_chunk_count = len(retrieved_docs.get('ids', []))\n",
    "\n",
    "        plan_node = {\n",
    "            \"title\": node_metadata['title'],\n",
    "            \"toc_id\": node_metadata['toc_id'],\n",
    "            \"chunk_count\": direct_chunk_count,\n",
    "            \"total_chunks_in_branch\": 0,\n",
    "            \"slides_allocated\": 0,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        child_branch_total = 0\n",
    "        for child_node in toc_node.get('children', []):\n",
    "            if any(ex in child_node.get('title', '').lower() for ex in [\"review\", \"introduction\", \"summary\", \"key terms\"]):\n",
    "                continue\n",
    "            child_plan_node = self._build_topic_plan_tree(child_node)\n",
    "            if child_plan_node:\n",
    "                plan_node['children'].append(child_plan_node)\n",
    "                child_branch_total += child_plan_node.get('total_chunks_in_branch', 0)\n",
    "        \n",
    "        plan_node['total_chunks_in_branch'] = direct_chunk_count + child_branch_total\n",
    "        return plan_node\n",
    "    \n",
    "    # In PlanningAgent Class...\n",
    "\n",
    "    def _allocate_slides_to_tree(self, plan_tree: Dict, content_slides_budget: int):\n",
    "        \"\"\"\n",
    "        (FINAL, REORDERED FOR CLARITY) Performs a multi-pass process to allocate content slides,\n",
    "        add activities, sum totals, and reorders the keys in each node for maximum readability.\n",
    "        \"\"\"\n",
    "        if not plan_tree or content_slides_budget <= 0:\n",
    "            return plan_tree\n",
    "\n",
    "        # --- Pass 1: Allocate Content Slides ---\n",
    "        def allocate_content_recursively(node, budget):\n",
    "            node['budget_slides_content'] = round(budget)\n",
    "            node['direct_slides_content'] = 0\n",
    "            if not node.get('children'):\n",
    "                node['direct_slides_content'] = round(budget)\n",
    "                return\n",
    "            total_branch_chunks = node.get('total_chunks_in_branch', 0)\n",
    "            own_content_slides = 0\n",
    "            if total_branch_chunks > 0:\n",
    "                own_content_slides = round(budget * (node.get('chunk_count', 0) / total_branch_chunks))\n",
    "            node['direct_slides_content'] = own_content_slides\n",
    "            remaining_budget_for_children = budget - own_content_slides\n",
    "            children_total_chunks = total_branch_chunks - node.get('chunk_count', 0)\n",
    "            if children_total_chunks <= 0: return\n",
    "            for child in node.get('children', []):\n",
    "                child_budget = remaining_budget_for_children * (child.get('total_chunks_in_branch', 0) / children_total_chunks)\n",
    "                allocate_content_recursively(child, child_budget)\n",
    "        \n",
    "        allocate_content_recursively(plan_tree, content_slides_budget)\n",
    "\n",
    "        # --- Pass 2: Add Interactive Activities ---\n",
    "        def add_interactive_nodes(node, depth, interactive_deep):\n",
    "            if not node: return\n",
    "            if self.config.get('interactive', False):\n",
    "                if interactive_deep:\n",
    "                    if depth == 2: node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Deep-Dive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "                    if depth == 1: node['interactive_activity'] = {\"title\": f\"{node.get('title')} (General Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "                else:\n",
    "                    if depth == 1: node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Interactive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "            for child in node.get('children', []):\n",
    "                add_interactive_nodes(child, depth + 1, interactive_deep)\n",
    "\n",
    "        add_interactive_nodes(plan_tree, 1, self.config.get('interactive_deep', False))\n",
    "\n",
    "        # --- Pass 3: Sum All Slides Up the Tree ---\n",
    "        def sum_slides_upwards(node):\n",
    "            total_slides = node.get('direct_slides_content', 0)\n",
    "            total_slides += node.get('interactive_activity', {}).get('slides_allocated', 0)\n",
    "            if node.get('children'):\n",
    "                total_slides += sum(sum_slides_upwards(child) for child in node.get('children', []))\n",
    "            node['total_slides_in_branch'] = total_slides\n",
    "            return total_slides\n",
    "\n",
    "        sum_slides_upwards(plan_tree)\n",
    "        \n",
    "        # --- NEW: Pass 4: Reorder keys for final clarity ---\n",
    "        def reorder_keys_for_readability(node: Dict) -> Dict:\n",
    "            if not node:\n",
    "                return None\n",
    "\n",
    "            # Define the desired order of keys\n",
    "            key_order = [\n",
    "                \"title\",\n",
    "                \"toc_id\",\n",
    "                \"chunk_count\",\n",
    "                \"total_chunks_in_branch\", \n",
    "                \"budget_slides_content\",\n",
    "                \"direct_slides_content\",\n",
    "                \"total_slides_in_branch\",\n",
    "                \"children\",\n",
    "                \"interactive_activity\"\n",
    "                \n",
    "            ]\n",
    "            \n",
    "            # Rebuild the dictionary in the specified order\n",
    "            reordered_node = {key: node[key] for key in key_order if key in node}\n",
    "            \n",
    "            # Recursively reorder children\n",
    "            if 'children' in reordered_node:\n",
    "                reordered_node['children'] = [reorder_keys_for_readability(child) for child in reordered_node['children']]\n",
    "                \n",
    "            return reordered_node\n",
    "\n",
    "        return reorder_keys_for_readability(plan_tree)\n",
    "\n",
    "    def create_content_plan_for_week(self, week_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Orchestrates the adaptive planning and partitioning process.\"\"\"\n",
    "        print_header(f\"Planning Week {week_number}\", char=\"*\")\n",
    "        \n",
    "        weekly_schedule_item = self.unit_outline['weeklySchedule'][week_number - 1]\n",
    "        chapter_numbers = self._identify_relevant_chapters(weekly_schedule_item)\n",
    "        if not chapter_numbers: return None\n",
    "\n",
    "        num_decks = self.config['week_session_setup'].get('sessions_per_week', 1)\n",
    "        \n",
    "        # 1. Build a full plan tree for each chapter to get its weight.\n",
    "        chapter_plan_trees = [self._build_topic_plan_tree(self._find_chapter_node(cn)) for cn in chapter_numbers if self._find_chapter_node(cn)]\n",
    "        total_weekly_chunks = sum(tree.get('total_chunks_in_branch', 0) for tree in chapter_plan_trees)\n",
    "\n",
    "        # 2. NEW: Adaptive Partitioning Strategy\n",
    "        partitionable_units = []\n",
    "        all_top_level_sections = []\n",
    "        for chapter_tree in chapter_plan_trees:\n",
    "            all_top_level_sections.extend(chapter_tree.get('children', []))\n",
    "\n",
    "        num_top_level_sections = len(all_top_level_sections)\n",
    "\n",
    "        # Always prefer to split by top-level sections if there are enough to distribute.\n",
    "        if num_top_level_sections >= num_decks:\n",
    "            logger.info(f\"Partitioning strategy: Distributing {num_top_level_sections} top-level sections across {num_decks} decks.\")\n",
    "            partitionable_units = all_top_level_sections\n",
    "        else:\n",
    "            # Fallback for rare cases where there are fewer topics than decks (e.g., 1 chapter with 1 section, but 2 decks).\n",
    "            logger.info(f\"Partitioning strategy: Not enough top-level sections ({num_top_level_sections}) to fill all decks ({num_decks}). Distributing whole chapters instead.\")\n",
    "            partitionable_units = chapter_plan_trees\n",
    "        \n",
    "        # 3. Partition the chosen units into decks using a bin-packing algorithm\n",
    "        decks = [[] for _ in range(num_decks)]\n",
    "        deck_weights = [0] * num_decks\n",
    "        sorted_units = sorted(partitionable_units, key=lambda x: x.get('toc_id', 0))\n",
    "        \n",
    "        for unit in sorted_units:\n",
    "            lightest_deck_index = deck_weights.index(min(deck_weights))\n",
    "            decks[lightest_deck_index].append(unit)\n",
    "            deck_weights[lightest_deck_index] += unit.get('total_chunks_in_branch', 0)\n",
    "\n",
    "        # 4. Plan each deck\n",
    "        content_slides_per_week = self.config['slide_count_strategy'].get('target_total_slides', 25)\n",
    "        final_deck_plans = []\n",
    "        for i, deck_content_trees in enumerate(decks):\n",
    "            deck_number = i + 1\n",
    "            deck_chunk_weight = sum(tree.get('total_chunks_in_branch', 0) for tree in deck_content_trees)\n",
    "            deck_slide_budget = round((deck_chunk_weight / total_weekly_chunks) * content_slides_per_week) if total_weekly_chunks > 0 else 0\n",
    "\n",
    "            logger.info(f\"--- Planning Deck {deck_number}/{num_decks} | Topics: {[t['title'] for t in deck_content_trees]} | Weight: {deck_chunk_weight} chunks | Slide Budget: {deck_slide_budget} ---\")\n",
    "            \n",
    "            # The allocation function is recursive and works on any tree or sub-tree\n",
    "            planned_content = [self._allocate_slides_to_tree(tree, round(deck_slide_budget * (tree.get('total_chunks_in_branch', 0) / deck_chunk_weight))) if deck_chunk_weight > 0 else tree for tree in deck_content_trees]\n",
    "            \n",
    "            final_deck_plans.append({\n",
    "                \"deck_number\": deck_number,\n",
    "                \"deck_title\": f\"{self.config.get('unit_name', 'Course')} - Week {week_number}, Lecture {deck_number}\",\n",
    "                \"session_content\": planned_content\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"week\": week_number,\n",
    "            \"overall_topic\": weekly_schedule_item.get('contentTopic'),\n",
    "            \"deck_plans\": final_deck_plans\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def finalize_and_calculate_time_plan(self,draft_plan: Dict, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Takes a draft plan and enriches it by:\n",
    "        1. Calculating detailed slide counts and time allocations for every node.\n",
    "        2. Adding framework sections and wrapping content.\n",
    "        3. Calculating and adding summaries for decks and the entire week.\n",
    "        4. Assigning a final sequence ID (seq_id) for slide generation.\n",
    "        5. Reordering all keys for maximum readability.\n",
    "        \"\"\"\n",
    "        final_plan = json.loads(json.dumps(draft_plan))\n",
    "\n",
    "        # --- Time Constants from Config ---\n",
    "        params = config.get('parameters_slides', {})\n",
    "        TIME_PER_CONTENT = params.get('time_per_content_slides_min', 3)\n",
    "        TIME_PER_INTERACTIVE = params.get('time_per_interactive_slide_min', 5)\n",
    "        TIME_FOR_FRAMEWORK_DECK = params.get('time_for_framework_slides_min', 6)\n",
    "        FRAMEWORK_SLIDES_PER_DECK = 4\n",
    "        \n",
    "        \n",
    "        # --- Recursive Helper Functions ---\n",
    "        def _calculate_time_and_reorder(node: Dict):\n",
    "            # 1. Recurse to the bottom first to perform a bottom-up calculation\n",
    "            children_total_time = 0\n",
    "            if 'children' in node and node['children']:\n",
    "                for child in node['children']:\n",
    "                    _calculate_time_and_reorder(child) # Recursive call\n",
    "                    children_total_time += child.get('time_allocation_minutes', {}).get('total_branch_time', 0)\n",
    "\n",
    "            # 2. Calculate this node's direct time\n",
    "            direct_content_time = node.get('direct_slides_content', 0) * TIME_PER_CONTENT\n",
    "            interactive_time = node.get('interactive_activity', {}).get('slides_allocated', 0) * TIME_PER_INTERACTIVE\n",
    "            \n",
    "            # 3. Calculate this node's total branch time\n",
    "            branch_total_time = direct_content_time + interactive_time + children_total_time\n",
    "\n",
    "            # 4. Create the time allocation object\n",
    "            time_alloc = {\n",
    "                \"direct_content_time\": direct_content_time,\n",
    "                \"direct_interactive_time\": interactive_time,\n",
    "                \"total_branch_time\": branch_total_time\n",
    "            }\n",
    "            node['time_allocation_minutes'] = time_alloc\n",
    "\n",
    "            # 5. Reorder all keys for this node to ensure final clarity\n",
    "            key_order = [\n",
    "                \"title\",\n",
    "                \"toc_id\",\n",
    "                \"chunk_count\",\n",
    "                \"total_chunks_in_branch\",\n",
    "                \"budget_slides_content\",\n",
    "                \"direct_slides_content\",\n",
    "                \"total_slides_in_branch\",\n",
    "                \"time_allocation_minutes\", \n",
    "                \"children\",\n",
    "                \"interactive_activity\"\n",
    "                \n",
    "            ]\n",
    "            reordered_node = {key: node[key] for key in key_order if key in node}\n",
    "            \n",
    "            # Clear the original node and update it with the reordered keys\n",
    "            node.clear()\n",
    "            node.update(reordered_node)\n",
    "\n",
    "        # --- Main Processing Loop for Decks ---\n",
    "        for deck in final_plan.get(\"deck_plans\", []):\n",
    "            session_content_blocks = deck.pop(\"session_content\", [])\n",
    "\n",
    "            # Perform the combined time calculation and reordering pass\n",
    "            for block in session_content_blocks:\n",
    "                _calculate_time_and_reorder(block)\n",
    "\n",
    "            # Create Framework Sections\n",
    "            week_number, deck_number = final_plan.get(\"week\"), deck.get(\"deck_number\")\n",
    "            title_section = {\"section_type\": \"Title\", \"content\": { \"unit_name\": config.get('unit_name', 'Course'), \"unit_code\": config.get('course_id', ''), \"week_topic\": final_plan.get('overall_topic', ''), \"deck_title\": f\"Week {week_number}, Lecture {deck_number}\"}}\n",
    "            agenda_section = {\"section_type\": \"Agenda\", \"content\": {\"title\": \"Today's Agenda\", \"items\": [item.get('title', 'Untitled Topic') for item in session_content_blocks]}}\n",
    "            summary_section = {\"section_type\": \"Summary\", \"content\": {\"title\": \"Summary & Key Takeaways\", \"placeholder\": \"Auto-generate based on covered topics.\"}}\n",
    "            end_section = {\"section_type\": \"End\", \"content\": {\"title\": \"Thank You\", \"text\": \"Questions?\"}}\n",
    "            main_content_block = {\"section_type\": \"Content\", \"content_blocks\": session_content_blocks}\n",
    "            \n",
    "            final_sections_for_deck = [title_section, agenda_section, main_content_block, summary_section, end_section]\n",
    "            \n",
    "            \n",
    "            # ***************************************************************\n",
    "            # *** NEW: Assign Sequential IDs for the entire deck ***\n",
    "            # ***************************************************************\n",
    "            seq_counter = [0] # Use a list for pass-by-reference behavior\n",
    "            for section in final_sections_for_deck:\n",
    "                if section.get(\"section_type\") == \"Content\":\n",
    "                    # For the main content, iterate through its blocks and start the recursion\n",
    "                    for block in section.get(\"content_blocks\", []):\n",
    "                        self._assign_sequence_ids_recursively(block, seq_counter)\n",
    "                else:\n",
    "                    # For simple framework slides, just assign the next ID\n",
    "                    section['seq_id'] = seq_counter[0]\n",
    "                    seq_counter[0] += 1\n",
    "            # ***************************************************************\n",
    "            # *** END NEW CODE ***\n",
    "            # ***************************************************************\n",
    "            \n",
    "            \n",
    "            # Calculate Deck Summaries        \n",
    "            total_content_slides = sum(b.get('total_slides_in_branch', 0) - b.get('interactive_activity',{}).get('slides_allocated',0) for b in session_content_blocks)\n",
    "            total_interactive_slides = sum(b.get('interactive_activity',{}).get('slides_allocated',0) for b in session_content_blocks)\n",
    "\n",
    "            deck_content_time = sum(b.get('time_allocation_minutes', {}).get('total_branch_time', 0) for b in session_content_blocks)\n",
    "            \n",
    "            deck['total_slides_in_deck'] = FRAMEWORK_SLIDES_PER_DECK + sum(b.get('total_slides_in_branch', 0) for b in session_content_blocks)\n",
    "            deck['slide_count_breakdown'] = {\"framework\": FRAMEWORK_SLIDES_PER_DECK, \"content\": total_content_slides, \"interactive\": total_interactive_slides}\n",
    "            deck['time_breakdown_minutes'] = {\"framework\": TIME_FOR_FRAMEWORK_DECK, \"content_and_interactive\": deck_content_time, \"total_deck_time\": TIME_FOR_FRAMEWORK_DECK + deck_content_time}\n",
    "            deck['sections'] = final_sections_for_deck\n",
    "            if 'deck_title' in deck: del deck['deck_title']\n",
    "\n",
    "        # --- Calculate Grand Totals for the Week ---\n",
    "        weekly_slide_summary = {\"total_slides_for_week\": 0, \"total_framework_slides\": 0, \"total_content_slides\": 0, \"total_interactive_slides\": 0, \"number_of_decks\": len(final_plan.get(\"deck_plans\", []))}\n",
    "        weekly_time_summary = {\"total_time_for_week_minutes\": 0, \"total_framework_time\": 0, \"total_content_and_interactive_time\": 0}\n",
    "        \n",
    "        for deck in final_plan.get(\"deck_plans\", []):\n",
    "            weekly_slide_summary['total_slides_for_week'] += deck.get('total_slides_in_deck', 0)\n",
    "            for key, value in deck.get('slide_count_breakdown', {}).items(): weekly_slide_summary[f\"total_{key}_slides\"] += value\n",
    "            weekly_time_summary['total_time_for_week_minutes'] += deck.get('time_breakdown_minutes', {}).get('total_deck_time', 0)\n",
    "            weekly_time_summary['total_framework_time'] += deck.get('time_breakdown_minutes', {}).get('framework', 0)\n",
    "            weekly_time_summary['total_content_and_interactive_time'] += deck.get('time_breakdown_minutes', {}).get('content_and_interactive', 0)\n",
    "\n",
    "        # --- Construct Final Ordered Plan ---\n",
    "        final_ordered_plan = {\n",
    "            \"week\": final_plan.get(\"week\"),\n",
    "            \"overall_topic\": final_plan.get(\"overall_topic\"),\n",
    "            \"weekly_slide_summary\": weekly_slide_summary,\n",
    "            \"weekly_time_summary_minutes\": weekly_time_summary,\n",
    "            \"deck_plans\": final_plan.get(\"deck_plans\", [])\n",
    "        }\n",
    "        \n",
    "        return final_ordered_plan\n",
    "\n",
    "    # --- NEW FUNCTION TO GENERATE MASTER SUMMARY ---\n",
    "    def generate_and_save_master_plan(self, weekly_plans: List[Dict], config: Dict):\n",
    "        \"\"\"\n",
    "        Aggregates summaries from all weekly plans into a single master plan file,\n",
    "        including new grand total metrics.\n",
    "        \"\"\"\n",
    "        print_header(\"Phase 4: Generating Master Unit Plan\", char=\"#\")\n",
    "        \n",
    "        # Initialize the master plan structure with the new fields\n",
    "        master_plan = {\n",
    "            \"unit_code\": config.get('course_id', 'UNKNOWN'),\n",
    "            \"unit_name\": config.get('unit_name', 'Unknown Unit'),\n",
    "            \"grand_total_summary\": {\n",
    "                \"total_slides_for_unit\": 0,\n",
    "                \"total_framework_slides\": 0,\n",
    "                \"total_content_slides\": 0,\n",
    "                \"total_interactive_slides\": 0,\n",
    "                \"total_number_of_decks\": 0,\n",
    "                \"total_time_for_unit_minutes\": 0,\n",
    "                \"total_time_for_unit_in_hour\": 0, # New\n",
    "                \"average_deck_time_in_min\": 0,\n",
    "                \"average_deck_time_in_hour\": 0           \n",
    "            },\n",
    "            \"weekly_summaries\": []\n",
    "        }\n",
    "\n",
    "        grand_totals = master_plan[\"grand_total_summary\"]\n",
    "\n",
    "        # Loop through each weekly plan to aggregate data\n",
    "        for plan in sorted(weekly_plans, key=lambda p: p.get('week', 0)):\n",
    "            # Extract the high-level summary for this week\n",
    "            summary_entry = {\n",
    "                \"week\": plan.get(\"week\"),\n",
    "                \"overall_topic\": plan.get(\"overall_topic\"),\n",
    "                \"slide_summary\": plan.get(\"weekly_slide_summary\"),\n",
    "                \"time_summary_minutes\": plan.get(\"weekly_time_summary_minutes\")\n",
    "            }\n",
    "            master_plan[\"weekly_summaries\"].append(summary_entry)\n",
    "            \n",
    "            # Add this week's totals to the grand totals\n",
    "            slide_summary = plan.get(\"weekly_slide_summary\", {})\n",
    "            time_summary = plan.get(\"weekly_time_summary_minutes\", {})\n",
    "            \n",
    "            grand_totals[\"total_slides_for_unit\"] += slide_summary.get(\"total_slides_for_week\", 0)\n",
    "            grand_totals[\"total_framework_slides\"] += slide_summary.get(\"total_framework_slides\", 0)\n",
    "            grand_totals[\"total_content_slides\"] += slide_summary.get(\"total_content_slides\", 0)\n",
    "            grand_totals[\"total_interactive_slides\"] += slide_summary.get(\"total_interactive_slides\", 0)\n",
    "            grand_totals[\"total_number_of_decks\"] += slide_summary.get(\"number_of_decks\", 0)\n",
    "            grand_totals[\"total_time_for_unit_minutes\"] += time_summary.get(\"total_time_for_week_minutes\", 0)\n",
    "\n",
    "        # --- NEW: Calculate the final derived grand totals after the loop ---\n",
    "        if grand_totals[\"total_time_for_unit_minutes\"] > 0:\n",
    "            grand_totals[\"total_time_for_unit_in_hour\"] = round(grand_totals[\"total_time_for_unit_minutes\"] / 60, 2)\n",
    "\n",
    "        if grand_totals[\"total_number_of_decks\"] > 0:\n",
    "            grand_totals[\"average_deck_time_in_min\"] = round(grand_totals[\"total_time_for_unit_minutes\"] / grand_totals[\"total_number_of_decks\"], 2)\n",
    "\n",
    "        if grand_totals[\"total_number_of_decks\"] > 0:\n",
    "            grand_totals[\"average_deck_time_in_hour\"] = round((grand_totals[\"total_time_for_unit_minutes\"] / grand_totals[\"total_number_of_decks\"]) / 60, 2)\n",
    "\n",
    "        \n",
    "        master_filename = f\"{config.get('course_id', 'UNIT')}_master_plan_unit.json\"\n",
    "        output_path = os.path.join(PLAN_OUTPUT_DIR, master_filename)\n",
    "\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(master_plan, f, indent=2)\n",
    "            logger.info(f\"Successfully generated and saved Master Unit Plan to: {output_path}\")\n",
    "            print(\"\\n--- Preview of Master Plan ---\")\n",
    "            print(json.dumps(master_plan, indent=2))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save Master Unit Plan: {e}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8438e",
   "metadata": {},
   "source": [
    "## Content Generator Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83d1a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTENT_SLIDE_GENERATION_PROMPT = \"\"\"\n",
    "# You are an expert university lecturer and instructional designer. Your task is to create the content for a single, specific PowerPoint slide as part of a larger topic.\n",
    "\n",
    "# **CONTEXT:**\n",
    "# - **Main Topic:** \"{main_topic_title}\"\n",
    "# - **Overall Raw Text for this Topic:** ```{topic_raw_content}```\n",
    "# - **Generation Task:** You are generating slide **{current_slide_num}** of a total of **{total_slides_for_topic}** slides for this topic.\n",
    "# - **Content Covered on Previous Slides:** {generation_history}\n",
    "\n",
    "# **INSTRUCTIONS:**\n",
    "# 1.  Based on all the context, determine the most logical sub-topic for the *next* slide ({current_slide_num}). Do NOT repeat content from previous slides.\n",
    "# 2.  Create the content for this single slide. You can use one or two \"Content Objects\" to structure the slide (e.g., use two objects for a comparison).\n",
    "# 3.  Your output **MUST** be a single, well-formed JSON object. Do NOT add any text or explanations outside of the JSON object.\n",
    "\n",
    "# **JSON OUTPUT STRUCTURE & RULES:**\n",
    "# {{\n",
    "#   \"title\": \"string | The main title for the slide, consistent with the main topic.\",\n",
    "#   \"subtitle\": \"string | A more specific subtitle for this particular slide. Use null if not needed.\",\n",
    "#   \"objects\": [\n",
    "#     {{\n",
    "#       \"content_type\": \"string | CHOOSE ONE: 'bullet_points', 'table'.\",\n",
    "#       \"content_purpose\": \"string | CHOOSE ONE: 'description', 'explanation', 'timeline', 'process', 'cycle', 'comparison', 'contrast', 'hierarchy', 'case_study', 'example'.\",\n",
    "#       \"data\": \"object | The structured data for the slide. See examples below.\"\n",
    "#     }}\n",
    "#   ]\n",
    "# }}\n",
    "\n",
    "# **DATA STRUCTURE EXAMPLES:**\n",
    "# - For \"bullet_points\": \"data\": [\"Point 1\", {{\"text\": \"Point 2\", \"children\": [\"Sub-point 2.1\"]}}]\n",
    "# - For \"table\": \"data\": {{\"headers\": [\"Col A\"], \"rows\": [[\"Data 1\"]]}}\n",
    "\n",
    "# Now, generate the JSON for slide {current_slide_num}.\n",
    "# \"\"\"\n",
    "CONTENT_SLIDE_GENERATION_PROMPT = \"\"\"\n",
    "You are an expert university lecturer and instructional designer. Your task is to process a large block of raw text and intelligently divide it into a sequence of PowerPoint slides.\n",
    "\n",
    "**CONTEXT:**\n",
    "- **Main Topic:** \"{main_topic_title}\"\n",
    "- **The ENTIRE Raw Text for this Topic:** ```{topic_raw_content}```\n",
    "- **Total Slides to Create:** You must cover all the key concepts in the raw text over a total of **{total_slides_for_topic}** slides.\n",
    "- **Current Task:** You are generating slide **{current_slide_num}** of {total_slides_for_topic}.\n",
    "- **Topics Already Covered on Previous Slides:** {generation_history}\n",
    "\n",
    "**YOUR GOAL AND INSTRUCTIONS:**\n",
    "1.  **Analyze the ENTIRE raw text.** Identify the distinct sub-topics within it.\n",
    "2.  **Review the topics already covered** in the `generation_history`.\n",
    "3.  **Identify the NEXT logical sub-topic from the raw text that has NOT been covered yet.** Your primary goal is to ensure comprehensive coverage without repetition.\n",
    "4.  **Create the content for a single slide** that is focused *only* on this new, uncovered sub-topic.\n",
    "5.  Your output **MUST** be a single, well-formed JSON object. Do not add any text or explanations outside the JSON object.\n",
    "**JSON OUTPUT STRUCTURE & RULES:**\n",
    "{{\n",
    "  \"title\": \"string | The main title for the slide, consistent with the main topic.\",\n",
    "  \"subtitle\": \"string | A more specific subtitle for this particular slide. Use null if not needed.\",\n",
    "  \"objects\": [\n",
    "    {{\n",
    "      \"content_type\": \"string | CHOOSE ONE: 'bullet_points', 'table'.\",\n",
    "      \"content_purpose\": \"string | CHOOSE ONE: 'description', 'explanation', 'timeline', 'process', 'cycle', 'comparison', 'contrast', 'hierarchy', 'case_study', 'example'.\",\n",
    "      \"data\": \"object | The structured data for the slide. See examples below.\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**DATA STRUCTURE EXAMPLES:**\n",
    "- For \"bullet_points\": \"data\": [\"Point 1\", {{\"text\": \"Point 2\", \"children\": [\"Sub-point 2.1\"]}}]\n",
    "- For \"table\": \"data\": {{\"headers\": [\"Col A\"], \"rows\": [[\"Data 1\"]]}}\n",
    "\n",
    "Now, generate the JSON for slide {current_slide_num}.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "INTERACTIVE_ACTIVITY_PROMPT = \"\"\"\n",
    "You are an engaging university lecturer creating an interactive slide to test student understanding.\n",
    "\n",
    "**CONTEXT:**\n",
    "- **Topic for this Activity:** \"{topic_title}\"\n",
    "- **Raw Text Content for this Topic:** ```{topic_raw_content}```\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Create a single, insightful multiple-choice question that assesses a key concept from the provided raw text.\n",
    "2.  The question must have exactly 4 options. Provide the correct answer and a brief, clear explanation.\n",
    "3.  Your output **MUST** be a single, well-formed JSON object that adheres strictly to the structure below.\n",
    "\n",
    "**JSON OUTPUT STRUCTURE:**\n",
    "{{\n",
    "  \"title\": \"Let's Apply This!\",\n",
    "  \"subtitle\": \"Knowledge Check: {topic_title}\",\n",
    "  \"objects\": [\n",
    "    {{\n",
    "      \"content_type\": \"multiple_choice_question\",\n",
    "      \"content_purpose\": \"knowledge_check\",\n",
    "      \"data\": {{\n",
    "        \"question_text\": \"string | The question to be asked.\",\n",
    "        \"options\": [\n",
    "          {{\"label\": \"A\", \"text\": \"string\"}},\n",
    "          {{\"label\": \"B\", \"text\": \"string\"}},\n",
    "          {{\"label\": \"C\", \"text\": \"string\"}},\n",
    "          {{\"label\": \"D\", \"text\": \"string\"}}\n",
    "        ],\n",
    "        \"correct_answer\": {{\n",
    "          \"label\": \"string | e.g., 'B'\",\n",
    "          \"explanation\": \"string | A brief explanation of why this answer is correct.\"\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Now, generate the JSON for the interactive activity.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# NEW, CONTEXT-AWARE SUMMARY GENERATION PROMPT\n",
    "# This prompt provides the LLM with the content of all slides for a rich summary.\n",
    "# =============================================================================\n",
    "\n",
    "SUMMARY_GENERATION_PROMPT = \"\"\"\n",
    "You are an expert university lecturer crafting the final, conclusive summary slide for a lecture.\n",
    "\n",
    "**CONTEXT:**\n",
    "- **Overall Lecture Topic:** \"{lecture_topic}\"\n",
    "- **Detailed Content of Slides Presented:**\n",
    "```json\n",
    "{slide_content_details}\n",
    "INSTRUCTIONS:\n",
    "Analyze the detailed content of all the slides provided in the JSON context.\n",
    "Synthesize the most critical points into 3-5 concise, high-level takeaways. Do not just list the subtitles. Capture the core lesson of each major section.\n",
    "Your output MUST be a single, well-formed JSON object that adheres to the standard slide structure below. This ensures it can be rendered correctly.\n",
    "\n",
    "JSON OUTPUT STRUCTURE:\n",
    "{{\n",
    "\"title\": \"Summary & Key Takeaways\",\n",
    "\"subtitle\": null,\n",
    "\"objects\": [\n",
    "{{\n",
    "  \"content_type\": \"bullet_points\",\n",
    "  \"content_purpose\": \"description\",\n",
    "  \"data\": [\n",
    "  \"string | Key takeaway 1.\",\n",
    "  \"string | Key takeaway 2.\",\n",
    "  \"string | Key takeaway 3.\",\n",
    "  \"string | Key takeaway 4.\",\n",
    "  \"string | Key takeaway 5.\"\n",
    "]\n",
    "}}\n",
    "]\n",
    "}}\n",
    "Now, generate the JSON for the final summary slide.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec27d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Generator 🤖\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "💡Imporvement to avoid repetition split the data provide? --> search sentences or by paragraps \n",
    "💡To create more interactive activities could run a prohram to ramdom select the prompt or 2 steps analysis - check data then select the prompt to run / more capable llm probably handle repetition \n",
    "💡make stronger structure retriver to secure structure for presenter\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Third-party imports (ensure these are installed)\n",
    "try:\n",
    "    import ollama\n",
    "    from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "except ImportError:\n",
    "    print(\"Please install required libraries: pip install ollama tenacity\")\n",
    "    ollama = None\n",
    "    \n",
    "# Basic Logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ContentAgent:\n",
    "    \"\"\"\n",
    "    Processes a hierarchical content plan, using specialized LLM prompts to generate\n",
    "    a sequence of individual, richly-structured slides.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict):\n",
    "        self.config = master_config.get('processed_settings', {})\n",
    "        self.client = ollama.Client(host=master_config.get('OLLAMA_HOST', 'http://localhost:11434'))\n",
    "        self.model = master_config.get('OLLAMA_MODEL', 'qwen3:8b')\n",
    "        # --- Added for Progress Tracking ---\n",
    "        self.total_slides_to_generate = 0\n",
    "        self.llm_calls_made = 0\n",
    "        logger.info(f\"Data-Driven Content Agent initialized with model '{self.model}'.\")\n",
    "\n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "    def _call_ollama_with_retry(self, prompt: str) -> str:\n",
    "        # --- Incremented Counter Here ---\n",
    "        self.llm_calls_made += 1\n",
    "        logger.info(f\"Calling Ollama model '{self.model}' (Call #{self.llm_calls_made})...\")\n",
    "        response = self.client.chat(model=self.model, messages=[{\"role\": \"user\", \"content\": prompt}], format=\"json\", options={\"temperature\": 0.2})\n",
    "        if not response or 'message' not in response or not response['message'].get('content'):\n",
    "            raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "        return response['message']['content']\n",
    "\n",
    "    def _parse_llm_json_output(self, content: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if not match: return None\n",
    "            return json.loads(match.group(0))\n",
    "        except (json.JSONDecodeError, TypeError): return None\n",
    "\n",
    "    def _process_content_node_recursively(self, node: dict):\n",
    "        \"\"\"\n",
    "        Recursively processes a content node by generating its own slides,\n",
    "        processing its children, and then generating its interactive activity.\n",
    "        This method modifies the 'node' dictionary in-place.\n",
    "        \"\"\"\n",
    "        num_slides_to_gen = node.get('direct_slides_content', 0)\n",
    "        \n",
    "        if num_slides_to_gen > 0:\n",
    "            generated_slides = []\n",
    "            generation_history = \"None. This is the first slide.\"\n",
    "            for i in range(num_slides_to_gen):\n",
    "                # --- Updated Progress Log ---\n",
    "                progress_log = f\"[Progress: {self.llm_calls_made}/{self.total_slides_to_generate}]\"\n",
    "                logger.info(f\"{progress_log} Generating slide {i+1}/{num_slides_to_gen} for topic: '{node.get('title')}'\")\n",
    "                try:\n",
    "                    prompt = CONTENT_SLIDE_GENERATION_PROMPT.format(\n",
    "                        main_topic_title=node.get('title'),\n",
    "                        topic_raw_content=node.get('content', ''),\n",
    "                        current_slide_num=i + 1,\n",
    "                        total_slides_for_topic=num_slides_to_gen,\n",
    "                        generation_history=generation_history\n",
    "                    )\n",
    "                    llm_str = self._call_ollama_with_retry(prompt)\n",
    "                    slide_content = self._parse_llm_json_output(llm_str)\n",
    "                    if slide_content:\n",
    "                        slide_object = {\n",
    "                            \"seq_id\": float(f\"{node.get('seq_id', 0)}.{i + 1}\"),\n",
    "                            \"llm_generated_content\": slide_content\n",
    "                        }\n",
    "                        generated_slides.append(slide_object)\n",
    "                        generation_history += f\"\\n- Slide {i+1} subtitle: {slide_content.get('subtitle', 'N/A')}\"\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"LLM call failed for slide {i+1} of '{node.get('title')}': {e}\")\n",
    "            \n",
    "            node['slides'] = generated_slides\n",
    "\n",
    "        for child_node in node.get('children', []):\n",
    "            self._process_content_node_recursively(child_node)\n",
    "\n",
    "        if 'interactive_activity' in node:\n",
    "            # --- Updated Progress Log for Interactive Slide ---\n",
    "            progress_log = f\"[Progress: {self.llm_calls_made}/{self.total_slides_to_generate}]\"\n",
    "            logger.info(f\"{progress_log} Generating interactive slide for topic: '{node.get('title')}'\")\n",
    "            try:\n",
    "                prompt = INTERACTIVE_ACTIVITY_PROMPT.format(\n",
    "                    topic_title=node.get('title'),\n",
    "                    topic_raw_content=node.get('content', '')\n",
    "                )\n",
    "                llm_str = self._call_ollama_with_retry(prompt)\n",
    "                interactive_content = self._parse_llm_json_output(llm_str)\n",
    "                if interactive_content:\n",
    "                    node['interactive_activity']['llm_generated_content'] = interactive_content\n",
    "            except Exception as e:\n",
    "                logger.error(f\"LLM call failed for interactive activity on '{node.get('title')}': {e}\")\n",
    "\n",
    "    def _count_total_slides(self, plan_data: Dict):\n",
    "        \"\"\"Recursively counts the total number of slides to be generated.\"\"\"\n",
    "        count = 0\n",
    "        for deck in plan_data.get('deck_plans', []):\n",
    "            # Count summary slide\n",
    "            if any(s.get('section_type') == 'Summary' for s in deck.get('sections', [])):\n",
    "                count += 1\n",
    "            \n",
    "            # Count content slides\n",
    "            for section in deck.get('sections', []):\n",
    "                if section.get('section_type') == 'Content':\n",
    "                    for content_block in section.get('content_blocks', []):\n",
    "                        \n",
    "                        def _recursive_count(node):\n",
    "                            nonlocal count\n",
    "                            count += node.get('direct_slides_content', 0)\n",
    "                            if 'interactive_activity' in node:\n",
    "                                count += 1\n",
    "                            for child in node.get('children', []):\n",
    "                                _recursive_count(child)\n",
    "                        \n",
    "                        _recursive_count(content_block)\n",
    "        return count\n",
    "\n",
    "    def generate_llm_content_for_plan(self, content_plan_path: str, llm_output_dir: str) -> bool:\n",
    "        \"\"\"\n",
    "        Orchestrates the LLM content generation for the entire plan file,\n",
    "        including a rich, context-aware summary.\n",
    "        \"\"\"\n",
    "        logger.info(f\"⚠️PHASE 6: Generating LLM content for: {os.path.basename(content_plan_path)} output file path: {llm_output_dir}\")\n",
    "        try:\n",
    "            with open(content_plan_path, 'r', encoding='utf-8') as f:\n",
    "                plan_data = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "            logger.error(f\"FATAL: Could not read plan file {content_plan_path}. Error: {e}\")\n",
    "            return False\n",
    "\n",
    "        # --- Pre-calculate total slides for progress tracking ---\n",
    "        self.total_slides_to_generate = self._count_total_slides(plan_data)\n",
    "        self.llm_calls_made = 0 # Reset counter for the run\n",
    "        logger.info(f\"Found a total of {self.total_slides_to_generate} slides to generate across all decks.\")\n",
    "\n",
    "        summary_prompt_template = SUMMARY_GENERATION_PROMPT\n",
    "\n",
    "        for deck in plan_data.get('deck_plans', []):\n",
    "            all_slide_contents_for_summary = []\n",
    "            \n",
    "            for section in deck.get('sections', []):\n",
    "                if section.get('section_type') == 'Content':\n",
    "                    for content_block in section.get('content_blocks', []):\n",
    "                        self._process_content_node_recursively(content_block)\n",
    "            \n",
    "            for section in deck.get('sections', []):\n",
    "                 if section.get('section_type') == 'Content':\n",
    "                    for content_block in section.get('content_blocks', []):\n",
    "                        def _collect_slide_content(node):\n",
    "                            for slide in node.get('slides', []):\n",
    "                                all_slide_contents_for_summary.append(slide.get('llm_generated_content'))\n",
    "                            if 'interactive_activity' in node and 'llm_generated_content' in node['interactive_activity']:\n",
    "                                all_slide_contents_for_summary.append(node['interactive_activity'].get('llm_generated_content'))\n",
    "                            for child in node.get('children', []):\n",
    "                                _collect_slide_content(child)\n",
    "                        _collect_slide_content(content_block)\n",
    "\n",
    "            if summary_prompt_template and all_slide_contents_for_summary:\n",
    "                summary_section = next((s for s in deck['sections'] if s.get('section_type') == 'Summary'), None)\n",
    "                \n",
    "                if summary_section:\n",
    "                    # --- Updated Progress Log for Summary Slide ---\n",
    "                    progress_log = f\"[Progress: {self.llm_calls_made}/{self.total_slides_to_generate}]\"\n",
    "                    logger.info(f\"{progress_log} Generating summary for Deck {deck.get('deck_number', 'N/A')}...\")\n",
    "                    try:\n",
    "                        slide_details_str = json.dumps(all_slide_contents_for_summary, indent=2)\n",
    "                        prompt = summary_prompt_template.format(\n",
    "                            lecture_topic=plan_data.get('overall_topic', 'This Lecture'),\n",
    "                            slide_content_details=slide_details_str\n",
    "                        )\n",
    "                        llm_str = self._call_ollama_with_retry(prompt)\n",
    "                        summary_content = self._parse_llm_json_output(llm_str)\n",
    "                        if summary_content:\n",
    "                            summary_section['llm_generated_content'] = summary_content\n",
    "                        else:\n",
    "                            logger.warning(\"Failed to parse summary content from LLM.\")\n",
    "                            summary_section['llm_generated_content'] = {\"title\": \"Summary & Key Takeaways\", \"subtitle\": None, \"objects\": [{\"content_type\": \"bullet_points\", \"content_purpose\": \"description\", \"data\": [\"Summary could not be generated.\"]}]}\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"LLM call failed for deck summary: {e}\", exc_info=True)\n",
    "\n",
    "        base_filename = os.path.basename(content_plan_path).replace('.json', '_llm_generated.json')\n",
    "        output_path = os.path.join(llm_output_dir, base_filename)\n",
    "        os.makedirs(llm_output_dir, exist_ok=True)\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(plan_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"Successfully saved final LLM-enriched plan to: {output_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save the final LLM-enriched plan: {e}\", exc_info=True)\n",
    "            return False\n",
    "# =============================================================================\n",
    "# TEST EXECUTION BLOCK⚠️‼️‼️‼️‼️‼️‼️‼️‼️‼️‼️‼️‼️‼️\n",
    "# =============================================================================\n",
    "# if __name__ == '__main__' and ollama is not None:\n",
    "#     print_header(\"Content Agent Test Runner\", char=\"=\")\n",
    "    \n",
    "#     # --- 1. Simulate Environment and Paths ---\n",
    "#     # These would normally come from Cell 1 in your notebook\n",
    "#     TEST_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator/test_output\"\n",
    "#     CONTENT_OUTPUT_DIR = os.path.join(TEST_BASE_DIR, \"generated_content\")\n",
    "#     CONTENT_LLM_OUTPUT_DIR = os.path.join(TEST_BASE_DIR, \"generated_content_llm\")\n",
    "#     os.makedirs(CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "#     os.makedirs(CONTENT_LLM_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#     # --- 2. Create a Dummy Phase 5 Input File ---\n",
    "#     # This simulates the file that the ContentAgent would read.\n",
    "#     dummy_input_content = {\n",
    "#       \"week\": 1,\n",
    "#       \"overall_topic\": \"Understanding the Digital Forensics Profession and Investigations.\",\n",
    "#       \"deck_plans\": [\n",
    "#         {\n",
    "#           \"sections\": [\n",
    "#             { \"section_type\": \"Title\",\n",
    "#               \"seq_id\": 0,\n",
    "#               \"content\": {}\n",
    "#             },\n",
    "#             { \"section_type\": \"Agenda\", \"seq_id\": 1, \"content\": {} },\n",
    "#             {\n",
    "#               \"section_type\": \"Content\",\n",
    "#               \"content_blocks\": [\n",
    "#                 {\n",
    "#                   \"title\": \"An Overview of Digital Forensics\",\n",
    "#                   \"toc_id\": 9,\n",
    "#                   \"seq_id\": 2,\n",
    "#                   \"direct_slides_content\": 2, # Requesting 2 slides for this topic\n",
    "#                   \"content\": \"Digital forensics is the application of computer science... It involves analyzing digital evidence... The Fourth Amendment protects against unreasonable searches...\"\n",
    "#                 },\n",
    "#                 {\n",
    "#                   \"title\": \"A Brief History of Digital Forensics\",\n",
    "#                   \"toc_id\": 11,\n",
    "#                   \"seq_id\": 5,\n",
    "#                   \"direct_slides_content\": 1, # Requesting 1 slide for this topic\n",
    "#                   \"content\": \"The history begins in the 1970s with mainframe crimes... In the 1980s, PC use grew... Specialized tools like EnCase emerged in the 90s...\"\n",
    "#                 }\n",
    "#               ]\n",
    "#             },\n",
    "#             { \"section_type\": \"Summary\", \"seq_id\": 12, \"content\": {} }\n",
    "#           ]\n",
    "#         }\n",
    "#       ]\n",
    "#     }\n",
    "#     dummy_input_path = os.path.join(CONTENT_OUTPUT_DIR, \"Week1_plan_content_enriched.json\")\n",
    "#     with open(dummy_input_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(dummy_input_content, f, indent=2)\n",
    "#     logger.info(f\"Created dummy input file at: {dummy_input_path}\")\n",
    "\n",
    "#     # --- 3. Set up a Dummy Master Configuration ---\n",
    "#     master_config = {\n",
    "#         \"OLLAMA_HOST\": \"http://localhost:11434\",\n",
    "#         \"OLLAMA_MODEL\": \"mistral:latest\",#\"deepseek-r1:8b\", #\"qwen3:8b\", # Or \"qwen3:8b\", etc.\n",
    "#         \"processed_settings\": {\"teaching_flow_id\": \"standard_lecture\"},\n",
    "#         \"teaching_flows\": {\n",
    "#             \"standard_lecture\": {\n",
    "#                 \"prompts\": {\n",
    "#                     \"summary_generation\": \"Summarize these topics: {topic_list}\"\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "#     CONTENT_TO_PROCESS = '/home/sebas_dev_linux/projects/course_generator/test_output/generated_content/test_generation_before_LLM.json'\n",
    "#     # --- 4. Instantiate and Run the Agent ---\n",
    "#     try:\n",
    "#         content_agent = ContentAgent(master_config)\n",
    "        \n",
    "#         # Run the agent in test_mode to only process the first deck\n",
    "#         success = content_agent.generate_llm_content_for_plan(\n",
    "#             content_plan_path=CONTENT_TO_PROCESS,\n",
    "#             llm_output_dir=CONTENT_LLM_OUTPUT_DIR,\n",
    "            \n",
    "#         )\n",
    "\n",
    "#         if success:\n",
    "#             logger.info(\"✅ Test run completed successfully!\")\n",
    "#             logger.info(f\"Check the output file in the '{CONTENT_LLM_OUTPUT_DIR}' directory.\")\n",
    "#         else:\n",
    "#             logger.error(\"❌ Test run failed.\")\n",
    "            \n",
    "#     except NameError:\n",
    "#         logger.error(\"Ollama library not found. Skipping test execution.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An unexpected error occurred during the test run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa1155",
   "metadata": {},
   "source": [
    "## Presentation Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f14477",
   "metadata": {},
   "source": [
    "test Presenter\n",
    "\n",
    "we need to ensure the presenter seq_id flow: Parent -> Child 1 -> Child 1's Activity -> Child 2 -> Child 2's Activity -> Parent's Activity.\n",
    "\n",
    "https://aistudio.google.com/prompts/18YaU5pG96eFMbM1l6yeG1v9j63PkLc5H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904f8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PresentationAgent ⭐📐🖼️\n",
    "\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import logging\n",
    "# import random\n",
    "# from pptx import Presentation\n",
    "# from pptx.util import Inches\n",
    "# from pptx.enum.shapes import PP_PLACEHOLDER\n",
    "# from pptx.enum.text import MSO_AUTO_SIZE\n",
    "\n",
    "# # --- Basic Setup ---\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # --- Helper Functions (Unchanged) ---\n",
    "# def _render_bullet_points(text_frame, data):\n",
    "#     text_frame.clear(); text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "#     def add_points(points, level):\n",
    "#         for point in points:\n",
    "#             if isinstance(point, dict):\n",
    "#                 p = text_frame.add_paragraph(); p.text = point.get('text', ''); p.level = level\n",
    "#                 if 'children' in point: add_points(point['children'], level + 1)\n",
    "#             else:\n",
    "#                 p = text_frame.add_paragraph(); p.text = str(point); p.level = level\n",
    "#     add_points(data, 0)\n",
    "#     if text_frame.paragraphs and not text_frame.paragraphs[0].text:\n",
    "#         p = text_frame.paragraphs[0]; p._p.getparent().remove(p._p)\n",
    "\n",
    "# def _render_table(slide, placeholder, data):\n",
    "#     headers, rows_data = data.get('headers', []), data.get('rows', [])\n",
    "#     if not headers or not rows_data: return\n",
    "#     num_rows, num_cols = len(rows_data) + 1, len(headers)\n",
    "#     table_shape = slide.shapes.add_table(num_rows, num_cols, placeholder.left, placeholder.top, placeholder.width, placeholder.height)\n",
    "#     table = table_shape.table\n",
    "#     for i, header in enumerate(headers):\n",
    "#         cell = table.cell(0, i); cell.text = header\n",
    "#         cell.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE; cell.text_frame.paragraphs[0].font.bold = True\n",
    "#     for r_idx, row_data in enumerate(rows_data):\n",
    "#         for c_idx, cell_text in enumerate(row_data):\n",
    "#             cell = table.cell(r_idx + 1, c_idx); cell.text = str(cell_text)\n",
    "#             cell.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "#     sp = placeholder.element; sp.getparent().remove(sp)\n",
    "\n",
    "# def _render_multiple_choice_question(slide, placeholders, data):\n",
    "#     main_ph = next((p for p in placeholders if p.placeholder_format.type == PP_PLACEHOLDER.OBJECT), None)\n",
    "#     answer_ph = next((p for p in placeholders if p.placeholder_format.type == PP_PLACEHOLDER.BODY), None)\n",
    "#     if not main_ph: return\n",
    "#     tf = main_ph.text_frame; tf.clear(); tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "#     p = tf.add_paragraph(); p.text = data.get('question_text', ''); p.font.bold = True; p.level = 0\n",
    "#     for option in data.get('options', []):\n",
    "#         p = tf.add_paragraph(); p.text = f\"{option.get('label', '')}) {option.get('text', '')}\"; p.level = 1\n",
    "#     if answer_ph:\n",
    "#         answer_ph.text_frame.clear()\n",
    "#         explanation = data.get('correct_answer', {}).get('explanation', '')\n",
    "#         answer_ph.text_frame.text = f\"Answer: {data.get('correct_answer', {}).get('label', '')}. {explanation}\"\n",
    "\n",
    "# def _render_matching_activity(slide, placeholders, data):\n",
    "#     object_placeholders = [p for p in placeholders if p.placeholder_format.type == PP_PLACEHOLDER.OBJECT]\n",
    "#     if len(object_placeholders) < 2: return\n",
    "#     terms_ph, defs_ph = object_placeholders[0], object_placeholders[1]\n",
    "#     terms_tf, defs_tf = terms_ph.text_frame, defs_ph.text_frame\n",
    "#     terms_tf.clear(); defs_tf.clear(); terms_tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE; defs_tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "#     pairs = data.get('pairs', [])\n",
    "#     if not pairs: return\n",
    "#     terms = [f\"{i+1}. {p['term']}\" for i, p in enumerate(pairs)]\n",
    "#     definitions = [p['definition'] for p in pairs]\n",
    "#     random.shuffle(definitions)\n",
    "#     for term in terms: p = terms_tf.add_paragraph(); p.text = term; p.level = 0\n",
    "#     for i, definition in enumerate(definitions): p = defs_tf.add_paragraph(); p.text = f\"{chr(65+i)}. {definition}\"; p.level = 0\n",
    "\n",
    "# class PresentationAgent:\n",
    "#     def __init__(self, template_path: str, layout_config_path: str):\n",
    "#         self.template_path = template_path\n",
    "#         self.prs_for_layouts = Presentation(template_path)\n",
    "#         with open(layout_config_path, 'r', encoding='utf-8') as f:\n",
    "#             self.user_selections = json.load(f)['user_selections']\n",
    "#         logger.info(f\"Agent initialized with template '{os.path.basename(template_path)}'\")\n",
    "\n",
    "#     def _get_layout(self, layout_key: str):\n",
    "#         selection = self.user_selections.get(layout_key, self.user_selections['Content'])\n",
    "#         layout_index = selection['selected_layout_index']\n",
    "#         return self.prs_for_layouts.slide_layouts[layout_index]\n",
    "        \n",
    "#     def _determine_layout_key(self, item_data):\n",
    "#         llm_content = item_data.get('llm_generated_content', {})\n",
    "#         objects = llm_content.get('objects', [])\n",
    "#         has_subtitle = bool(llm_content.get('subtitle'))\n",
    "        \n",
    "#         # <<< NEW LOGIC: Determine layout based on subtitles and object count\n",
    "#         if objects and objects[0]['content_type'] in ['multiple_choice_question', 'matching_activity']:\n",
    "#             return \"Application_Two_Column\" if len(objects) > 1 or objects[0]['content_type'] == 'matching_activity' else \"Application\"\n",
    "        \n",
    "#         if len(objects) >= 2:\n",
    "#             return \"Content_Two_Column_child\" if has_subtitle else \"Content_Two_Column\"\n",
    "#         else:\n",
    "#             return \"Content_child\" if has_subtitle else \"Content\"\n",
    "\n",
    "#     def create_presentation_from_plan(self, plan_json_path: str, output_path: str):\n",
    "#         logger.info(f\"Creating presentation from plan: '{os.path.basename(plan_json_path)}'\")\n",
    "#         with open(plan_json_path, 'r', encoding='utf-8') as f: plan_data = json.load(f)\n",
    "#         prs = Presentation(self.template_path)\n",
    "#         while len(prs.slides):\n",
    "#             rId = prs.slides._sldIdLst[0].rId; prs.part.drop_rel(rId); del prs.slides._sldIdLst[0]\n",
    "#         slide_items = self._collect_slide_items(plan_data)\n",
    "#         for item in sorted(slide_items, key=lambda x: x.get('seq_id', 999)): self._add_slide_for_item(prs, item)\n",
    "#         prs.save(output_path)\n",
    "#         logger.info(f\"Successfully created presentation: '{output_path}'\")\n",
    "\n",
    "#     def _collect_slide_items(self, plan_data):\n",
    "#         items = []\n",
    "#         for deck in plan_data.get('deck_plans', []):\n",
    "#             for section in deck.get('sections', []):\n",
    "#                 if section.get('section_type') == 'Content':\n",
    "#                     for block in section.get('content_blocks', []):\n",
    "#                         for slide_item in block.get('slides', []):\n",
    "#                             items.append({'item_type': 'Content', 'data': slide_item})\n",
    "#                 else:\n",
    "#                     items.append({'item_type': section['section_type'], 'data': section})\n",
    "#         return items\n",
    "\n",
    "#     def _add_slide_for_item(self, prs: Presentation, item: dict):\n",
    "#         item_type = item['item_type']\n",
    "#         layout_key = self._determine_layout_key(item['data']) if item_type == 'Content' else item_type\n",
    "#         slide = prs.slides.add_slide(self._get_layout(layout_key))\n",
    "#         render_map = {\n",
    "#             'Title': self._render_title_slide, 'Agenda': self._render_agenda_slide,\n",
    "#             'Summary': self._render_summary_slide, 'End': self._render_end_slide,\n",
    "#             'Divider': self._render_divider_slide, 'Content': self._render_content_slide\n",
    "#         }\n",
    "#         if item_type in render_map:\n",
    "#             render_map[item_type](slide, item['data'])\n",
    "\n",
    "#     def _render_title_slide(self, slide, data):\n",
    "#         content = data.get('content', {})\n",
    "#         if slide.shapes.title:\n",
    "#             slide.shapes.title.text = content.get('week_topic', 'Untitled Topic')\n",
    "#         subtitle_ph = next((p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER.SUBTITLE), None)\n",
    "#         if subtitle_ph:\n",
    "#             subtitle_ph.text = f\"{content.get('deck_title', '')}\\n{content.get('unit_name', '')} - {content.get('unit_code', '')}\"\n",
    "\n",
    "#     def _render_agenda_slide(self, slide, data):\n",
    "#         content = data.get('content', {})\n",
    "#         if slide.shapes.title: slide.shapes.title.text = content.get('title', 'Agenda')\n",
    "#         body_ph = next((p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER.OBJECT), None)\n",
    "#         if body_ph: _render_bullet_points(body_ph.text_frame, content.get('items', []))\n",
    "\n",
    "#     def _render_summary_slide(self, slide, data):\n",
    "#         if slide.shapes.title: slide.shapes.title.text = \"Summary & Key Takeaways\"\n",
    "#         if 'llm_generated_content' in data: self._render_content_slide(slide, data)\n",
    "\n",
    "#     def _render_end_slide(self, slide, data):\n",
    "#         content = data.get('content', {})\n",
    "#         if slide.shapes.title: slide.shapes.title.text = content.get('text', 'Questions?')\n",
    "\n",
    "#     def _render_divider_slide(self, slide, data):\n",
    "#         content = data.get('content', {})\n",
    "#         if slide.shapes.title: slide.shapes.title.text = content.get('title', 'Section Divider')\n",
    "\n",
    "#     def _render_content_slide(self, slide, data):\n",
    "#         content = data.get('llm_generated_content', {})\n",
    "        \n",
    "#         # <<< NEW LOGIC: Populate title and subtitle in separate placeholders\n",
    "#         if slide.shapes.title:\n",
    "#             slide.shapes.title.text = content.get('title', '')\n",
    "#             slide.shapes.title.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "        \n",
    "#         subtitle_ph = next((p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER.SUBTITLE), None)\n",
    "#         if subtitle_ph:\n",
    "#             subtitle_ph.text = content.get('subtitle', '')\n",
    "#             subtitle_ph.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "        \n",
    "#         body_placeholders = sorted([p for p in slide.placeholders if p.placeholder_format.type not in [PP_PLACEHOLDER.TITLE, PP_PLACEHOLDER.SUBTITLE]], key=lambda p: p.left)\n",
    "#         objects = content.get('objects', [])\n",
    "        \n",
    "#         for obj, placeholder in zip(objects, body_placeholders):\n",
    "#             content_type, obj_data = obj.get('content_type'), obj.get('data')\n",
    "#             renderers = {\n",
    "#                 'bullet_points': lambda: _render_bullet_points(placeholder.text_frame, obj_data),\n",
    "#                 'table': lambda: _render_table(slide, placeholder, obj_data),\n",
    "#                 'multiple_choice_question': lambda: _render_multiple_choice_question(slide, body_placeholders, obj_data),\n",
    "#                 'matching_activity': lambda: _render_matching_activity(slide, body_placeholders, obj_data)\n",
    "#             }\n",
    "#             if content_type in renderers:\n",
    "#                 renderers[content_type]()\n",
    "#                 if content_type in ['multiple_choice_question', 'matching_activity']: break\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER_TYPE, MSO_SHAPE_TYPE\n",
    "from pptx.enum.text import MSO_AUTO_SIZE\n",
    "\n",
    "# --- Basic Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper function to find specific placeholders ---\n",
    "def _get_placeholder(slide, ph_type):\n",
    "    for shape in slide.placeholders:\n",
    "        if shape.placeholder_format.type == ph_type:\n",
    "            return shape\n",
    "    return None\n",
    "\n",
    "# --- Rendering Helper Functions ---\n",
    "def _render_bullet_points(text_frame, data):\n",
    "    text_frame.clear(); text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    def add_points(points, level):\n",
    "        for point in points:\n",
    "            if isinstance(point, dict):\n",
    "                p = text_frame.add_paragraph(); p.text = point.get('text', ''); p.level = level\n",
    "                if 'children' in point: add_points(point['children'], level + 1)\n",
    "            else:\n",
    "                p = text_frame.add_paragraph(); p.text = str(point); p.level = level\n",
    "    if isinstance(data, list): add_points(data, 0)\n",
    "    if text_frame.paragraphs and not text_frame.paragraphs[0].text:\n",
    "        p = text_frame.paragraphs[0]; p._p.getparent().remove(p._p)\n",
    "\n",
    "def _render_table(slide, placeholder, data):\n",
    "    headers, rows_data = data.get('headers', []), data.get('rows', [])\n",
    "    if not headers or not rows_data: return\n",
    "    table_shape = slide.shapes.add_table(len(rows_data) + 1, len(headers), placeholder.left, placeholder.top, placeholder.width, placeholder.height)\n",
    "    table = table_shape.table\n",
    "    for i, header in enumerate(headers):\n",
    "        cell = table.cell(0, i); cell.text = header; cell.text_frame.paragraphs[0].font.bold = True\n",
    "    for r_idx, row_data in enumerate(rows_data):\n",
    "        for c_idx, cell_text in enumerate(row_data):\n",
    "            table.cell(r_idx + 1, c_idx).text = str(cell_text)\n",
    "    sp = placeholder.element; sp.getparent().remove(sp)\n",
    "\n",
    "def _render_mcq(placeholder, data, answer_placeholder=None):\n",
    "    \"\"\"\n",
    "    Renders the MCQ question into the main placeholder and the detailed\n",
    "    answer with explanation into the dedicated answer_placeholder.\n",
    "    \"\"\"\n",
    "    # --- Part 1: Render the Question and Options ---\n",
    "    tf = placeholder.text_frame\n",
    "    tf.clear()\n",
    "    tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    \n",
    "    # Add the question text\n",
    "    p_question = tf.add_paragraph()\n",
    "    p_question.text = data.get('question_text', '')\n",
    "    p_question.font.bold = True\n",
    "\n",
    "    # Add the options\n",
    "    for option in data.get('options', []):\n",
    "        p_option = tf.add_paragraph()\n",
    "        p_option.text = f\"{option.get('label', '')}) {option.get('text', '')}\"\n",
    "        p_option.level = 1\n",
    "\n",
    "    # --- Part 2: Render the Detailed Answer and Explanation ---\n",
    "    if answer_placeholder:\n",
    "        answer_info = data.get('correct_answer', {})\n",
    "        answer_label = answer_info.get('label', 'N/A')\n",
    "        answer_explanation = answer_info.get('explanation', 'No explanation provided.')\n",
    "        \n",
    "        # Get the text frame of the answer placeholder\n",
    "        answer_tf = answer_placeholder.text_frame\n",
    "        answer_tf.clear()\n",
    "        \n",
    "        # Add \"Correct Answer: [Label]\" in bold\n",
    "        p_answer_label = answer_tf.add_paragraph()\n",
    "        p_answer_label.text = f\" Answer: {answer_label} - {answer_explanation}\"\n",
    "        #p_answer_label.font.bold = True\n",
    "        \n",
    "        # # Add the explanation text on the next line\n",
    "        # p_answer_explanation = answer_tf.add_paragraph()\n",
    "        # p_answer_explanation.text = answer_explanation\n",
    "\n",
    "def _render_matching(placeholders, data):\n",
    "    \"\"\"Renders a matching activity into a pair of placeholders.\"\"\"\n",
    "    if len(placeholders) < 2: return\n",
    "    terms_ph, defs_ph = placeholders[0], placeholders[1]\n",
    "    terms_tf, defs_tf = terms_ph.text_frame, defs_ph.text_frame\n",
    "    terms_tf.clear(); defs_tf.clear()\n",
    "    pairs = data.get('pairs', [])\n",
    "    if not pairs: return\n",
    "    terms = [f\"{i+1}. {p['term']}\" for i, p in enumerate(pairs)]\n",
    "    definitions = [p['definition'] for p in pairs]\n",
    "    random.shuffle(definitions)\n",
    "    for term in terms: terms_tf.add_paragraph().text = term\n",
    "    for i, definition in enumerate(definitions): defs_tf.add_paragraph().text = f\"{chr(65+i)}. {definition}\"\n",
    "    \n",
    "    \n",
    "def _render_text_with_smart_fit(text_frame, data):\n",
    "    \"\"\"\n",
    "    Renders text and proactively adjusts the font size based on content length\n",
    "    to prevent overflow.\n",
    "    \"\"\"\n",
    "    text_frame.clear()\n",
    "    # We still set this as a fallback for moderate cases\n",
    "    text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    \n",
    "    # --- Character Counting Logic ---\n",
    "    # Convert the data (which can be a list of strings or dicts) into a single string to measure its length.\n",
    "    json_string = json.dumps(data)\n",
    "    char_count = len(json_string)\n",
    "    \n",
    "    # --- Define Font Size Tiers ---\n",
    "    # These values may need tuning based on your template's placeholder sizes and fonts.\n",
    "    # Format: (character_threshold, font_size_in_points)\n",
    "    font_tiers = [\n",
    "        (1200, Pt(18)), # Very long text -> smallest font\n",
    "        (800,  Pt(20)), # Long text\n",
    "        (500,  Pt(23)), # Medium text\n",
    "        (0,    Pt(24))  # Short text -> default/largest font\n",
    "    ]\n",
    "    \n",
    "    selected_font_size = None\n",
    "    for threshold, size in font_tiers:\n",
    "        if char_count >= threshold:\n",
    "            selected_font_size = size\n",
    "            break\n",
    "            \n",
    "    # --- Recursive Function to Add Points and Apply Font Size ---\n",
    "    def add_points_with_font(points, level):\n",
    "        for point in points:\n",
    "            # Determine the text for the current point\n",
    "            text_to_add = \"\"\n",
    "            if isinstance(point, dict):\n",
    "                text_to_add = point.get('text', '')\n",
    "            else:\n",
    "                text_to_add = str(point)\n",
    "\n",
    "            # Add the paragraph and set its properties\n",
    "            p = text_frame.add_paragraph()\n",
    "            p.text = text_to_add\n",
    "            p.level = level\n",
    "            if selected_font_size:\n",
    "                p.font.size = selected_font_size\n",
    "\n",
    "            # Recurse for children if they exist\n",
    "            if isinstance(point, dict) and 'children' in point:\n",
    "                add_points_with_font(point['children'], level + 1)\n",
    "\n",
    "    # Start the rendering process\n",
    "    if isinstance(data, list):\n",
    "        add_points_with_font(data, 0)\n",
    "        \n",
    "    # Clean up the initial empty paragraph if it exists and is unused\n",
    "    if text_frame.paragraphs and not text_frame.paragraphs[0].text:\n",
    "        p_to_remove = text_frame.paragraphs[0]\n",
    "        p_to_remove._p.getparent().remove(p_to_remove._p)    \n",
    "    \n",
    "\n",
    "class PresentationAgent:\n",
    "    def __init__(self, template_path: str, layout_config_path: str):\n",
    "        self.template_path = template_path\n",
    "        self.prs_for_layouts = Presentation(template_path)\n",
    "        with open(layout_config_path, 'r', encoding='utf-8') as f:\n",
    "            self.user_selections = json.load(f)['user_selections']\n",
    "        logger.info(f\"Agent initialized with template '{os.path.basename(template_path)}'\")\n",
    "\n",
    "    def _get_layout(self, layout_key: str):\n",
    "        selection = self.user_selections.get(layout_key, self.user_selections['Content'])\n",
    "        return self.prs_for_layouts.slide_layouts[selection['selected_layout_index']]\n",
    "\n",
    "    def _collect_and_sort_slides(self, plan_data: dict) -> list:\n",
    "        all_slide_items = []\n",
    "\n",
    "        # This is the same recursive helper function, it is correct.\n",
    "        def _recursive_harvester(node: dict, parent_title: str = None):\n",
    "            is_child_node = bool(parent_title)\n",
    "            for slide in node.get('slides', []):\n",
    "                slide['parent_title'] = parent_title\n",
    "                slide['is_child'] = is_child_node\n",
    "                all_slide_items.append({'item_type': 'Content', 'data': slide})\n",
    "            \n",
    "            for child in node.get('children', []):\n",
    "                # Pass the current node's title as the parent title for the next level\n",
    "                _recursive_harvester(child, parent_title=node.get('title'))\n",
    "            \n",
    "            # This part correctly harvests activities from ANY node (parent or child)\n",
    "            if 'interactive_activity' in node and 'llm_generated_content' in node['interactive_activity']:\n",
    "                activity = node['interactive_activity']\n",
    "                activity['parent_title'] = parent_title\n",
    "                # IMPORTANT: The 'is_child' status depends on whether a parent_title was passed in.\n",
    "                activity['is_child'] = is_child_node\n",
    "                # We add the main node's title to the activity data so it can be used if needed\n",
    "                activity['main_topic_title'] = node.get('title')\n",
    "                all_slide_items.append({'item_type': 'Content', 'data': activity})\n",
    "\n",
    "        # This is the main loop that processes the plan\n",
    "        for deck in plan_data.get('deck_plans', []):\n",
    "            # Framework slides are added as before\n",
    "            framework_start = ([{'item_type': 'Title', 'data': s} for s in deck['sections'] if s['section_type'] == 'Title'] +\n",
    "                            [{'item_type': 'Agenda', 'data': s} for s in deck['sections'] if s['section_type'] == 'Agenda'])\n",
    "            all_slide_items.extend(framework_start)\n",
    "            \n",
    "            # --- MODIFICATION IS HERE ---\n",
    "            # We now loop through the content blocks and call the harvester on each one.\n",
    "            content_sections = [s for s in deck['sections'] if s['section_type'] == 'Content']\n",
    "            if content_sections:\n",
    "                for block in content_sections[0].get('content_blocks', []):\n",
    "                    # We start the recursion for each main content block\n",
    "                    # We pass the block's own title as the parent_title for its direct activities\n",
    "                    _recursive_harvester(block, parent_title=None) # Start with no parent\n",
    "\n",
    "            framework_end = ([{'item_type': 'Summary', 'data': s} for s in deck['sections'] if s['section_type'] == 'Summary'] +\n",
    "                            [{'item_type': 'End', 'data': s} for s in deck['sections'] if s['section_type'] == 'End'])\n",
    "            all_slide_items.extend(framework_end)\n",
    "        \n",
    "        return sorted(all_slide_items, key=lambda x: x['data'].get('seq_id', 999))\n",
    "        \n",
    "    def _determine_layout_key(self, slide_data: dict) -> str:\n",
    "        # This logic is correct\n",
    "        llm_content = slide_data.get('llm_generated_content', {})\n",
    "        objects = llm_content.get('objects', [])\n",
    "        if not objects: return \"Content\"\n",
    "        first_obj_type = objects[0].get('content_type')\n",
    "        if first_obj_type in ['multiple_choice_question', 'matching_activity']:\n",
    "            return \"Application_Two_Column \" if first_obj_type == 'matching_activity' else \"Application\"\n",
    "        is_child = slide_data.get('is_child', False)\n",
    "        if len(objects) >= 2:\n",
    "            return \"Content_Two_Column_child\" if is_child else \"Content_Two_Column\"\n",
    "        return \"Content_child\" if is_child else \"Content\"\n",
    "\n",
    "    def create_presentation_from_plan(self, plan_json_path: str, output_dir: str):\n",
    "        # This orchestrator is correct\n",
    "        logger.info(f\"Creating presentation from plan: '{os.path.basename(plan_json_path)}'\")\n",
    "        with open(plan_json_path, 'r', encoding='utf-8') as f: plan_data = json.load(f)\n",
    "        final_slide_list = self._collect_and_sort_slides(plan_data)\n",
    "        prs = Presentation(self.template_path)\n",
    "        while len(prs.slides):\n",
    "            rId = prs.slides._sldIdLst[0].rId; prs.part.drop_rel(rId); del prs.slides._sldIdLst[0]\n",
    "        logger.info(f\"Collected a total of {len(final_slide_list)} slides to render.\")\n",
    "        for item in final_slide_list:\n",
    "            self._add_slide_for_item(prs, item)\n",
    "        unit_code = plan_data.get('deck_plans', [{}])[0].get('sections', [{}])[0].get('content', {}).get('unit_code', 'UNIT')\n",
    "        week_num = plan_data.get('week', 'X')\n",
    "        deck_num = plan_data.get('deck_plans', [{}])[0].get('deck_number', 'Y')\n",
    "        output_filename = f\"{unit_code}_Week{week_num}_Deck{deck_num}_Presentation.pptx\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        prs.save(output_path)\n",
    "        logger.info(f\"Successfully created presentation: '{output_path}'\")\n",
    "\n",
    "    def _add_slide_for_item(self, prs: Presentation, item: dict):\n",
    "        item_type = item['item_type']\n",
    "        item_data = item['data']\n",
    "        layout_key = self._determine_layout_key(item_data) if item_type == 'Content' else item_type\n",
    "        slide = prs.slides.add_slide(self._get_layout(layout_key))\n",
    "        render_map = {\n",
    "            'Title': self._render_title_slide, 'Agenda': self._render_agenda_slide,\n",
    "            'Summary': self._render_summary_slide, 'End': self._render_end_slide,\n",
    "            'Divider': self._render_divider_slide, 'Content': self._render_content_slide\n",
    "        }\n",
    "        if item_type in render_map: render_map[item_type](slide, item_data)\n",
    "    \n",
    "    def _render_title_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.CENTER_TITLE)\n",
    "        if title_ph: title_ph.text = content.get('week_topic', '')\n",
    "        subtitle_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.SUBTITLE)\n",
    "        if subtitle_ph: subtitle_ph.text = f\"{content.get('deck_title', '')}\\n{content.get('unit_name', '')} | {content.get('unit_code', '')}\"\n",
    "\n",
    "    def _render_agenda_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.TITLE)\n",
    "        if title_ph: title_ph.text = content.get('title', 'Agenda')\n",
    "        body_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.OBJECT)\n",
    "        if body_ph: _render_bullet_points(body_ph.text_frame, content.get('items', []))\n",
    "\n",
    "    def _render_summary_slide(self, slide, data):\n",
    "        \"\"\"Directly renders the summary slide content without calling another function.\"\"\"\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.TITLE)\n",
    "        if title_ph:\n",
    "            title_ph.text = \"Summary & Key Takeaways\"\n",
    "        \n",
    "        body_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.OBJECT)\n",
    "        if not body_ph:\n",
    "            logger.warning(\"Summary slide layout is missing a main body/object placeholder.\")\n",
    "            return\n",
    "\n",
    "        # The summary data is nested in the 'llm_generated_content' key\n",
    "        content = data.get('llm_generated_content', {})\n",
    "        if content and 'objects' in content and content['objects']:\n",
    "            # The summary is always bullet points, so we can call the helper directly.\n",
    "            summary_data = content['objects'][0].get('data', [])\n",
    "            _render_bullet_points(body_ph.text_frame, summary_data)\n",
    "\n",
    "    def _render_divider_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.TITLE)\n",
    "        if title_ph: title_ph.text = content.get('title', '')\n",
    "\n",
    "    def _render_end_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.TITLE)\n",
    "        if title_ph: title_ph.text = content.get('text', 'Thank You & Questions?')\n",
    "        \n",
    "    def _render_summary_slide(self, slide, data):\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.TITLE)\n",
    "        if title_ph: title_ph.text = \"Summary & Key Takeaways\"\n",
    "        body_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.OBJECT)\n",
    "        if body_ph and 'llm_generated_content' in data:\n",
    "            summary_data = data['llm_generated_content'].get('objects', [{}])[0].get('data', [])\n",
    "            _render_bullet_points(body_ph.text_frame, summary_data)\n",
    "\n",
    "    # <<< THIS IS THE CORRECTED METHOD >>>\n",
    "    def _render_content_slide(self, slide, data):\n",
    "        content = data.get('llm_generated_content', {})\n",
    "        if not content: return\n",
    "\n",
    "        # --- Step 1: Determine Layout and Content ---\n",
    "        layout_key = self._determine_layout_key(data)\n",
    "        title_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.TITLE)\n",
    "        subtitle_ph = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.SUBTITLE)\n",
    "\n",
    "        # --- Step 2: Handle Title and Subtitle Rendering ---\n",
    "        # This logic now correctly covers all slide types.\n",
    "        if \"Application\" in layout_key:\n",
    "            # For ANY interactive slide, the main title is the specific topic.\n",
    "            if title_ph:\n",
    "                # The subtitle from the JSON is the most descriptive title.\n",
    "                title_ph.text = content.get('subtitle', 'Knowledge Check')\n",
    "            # The subtitle is the generic call to action.\n",
    "            if subtitle_ph:\n",
    "                subtitle_ph.text = content.get('title', \"Let's Apply This!\")\n",
    "                \n",
    "        elif data.get('is_child'):\n",
    "            # For a standard child slide, use the parent-child hierarchy.\n",
    "            if title_ph: title_ph.text = data.get('parent_title', '')\n",
    "            if subtitle_ph: subtitle_ph.text = content.get('title', '')\n",
    "                \n",
    "        else:\n",
    "            # For a standard parent slide.\n",
    "            if title_ph: title_ph.text = content.get('title', '')\n",
    "            if subtitle_ph: subtitle_ph.text = content.get('subtitle', '')\n",
    "\n",
    "        # --- Step 3: Render Body Objects ---\n",
    "        # Find the main placeholder for the question and the dedicated placeholder for the answer.\n",
    "        body_placeholders = sorted([p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER_TYPE.OBJECT], key=lambda p: p.left)\n",
    "        answer_placeholder = _get_placeholder(slide, PP_PLACEHOLDER_TYPE.BODY)\n",
    "        objects = content.get('objects', [])\n",
    "\n",
    "        for i, obj in enumerate(objects):\n",
    "            if i >= len(body_placeholders): break\n",
    "            placeholder = body_placeholders[i]\n",
    "            content_type, obj_data = obj.get('content_type'), obj.get('data')\n",
    "\n",
    "            # THIS IS THE CHANGE:\n",
    "            if content_type in ['bullet_points', 'description', 'explanation', 'timeline', 'process', 'cycle', 'hierarchy', 'case_study', 'example']:\n",
    "                _render_text_with_smart_fit(placeholder.text_frame, obj_data) # Use the new function\n",
    "            elif content_type == 'table':\n",
    "                _render_table(slide, placeholder, obj_data)\n",
    "            elif content_type == 'multiple_choice_question':\n",
    "                _render_mcq(placeholder, obj_data, answer_placeholder)\n",
    "                # The notes slide can still contain the detailed answer for reference.\n",
    "                if slide.has_notes_slide:\n",
    "                    answer_info = obj_data.get('correct_answer', {})\n",
    "                    slide.notes_slide.notes_text_frame.text = f\"Answer: {answer_info.get('label', '')}. {answer_info.get('explanation', '')}\"\n",
    "            \n",
    "            elif content_type == 'matching_activity':\n",
    "                _render_matching(body_placeholders, obj_data)\n",
    "                break # Matching uses all placeholders.\n",
    "            \n",
    "            else: # All other content types\n",
    "                _render_bullet_points(placeholder.text_frame, obj_data)\n",
    "\n",
    "\n",
    "\n",
    "# # --- Main Execution Block --- ⚠️ No delete no change it is ready for test the outputfrom ContentAgent ⚠️\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define project paths\n",
    "#     PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "#     CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "#     DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "#     # This is the INPUT directory for the final JSON plan\n",
    "#     INPUT_CONTENT_DIR = os.path.join(PROJECT_BASE_DIR, \"test_output\", \"generated_content_llm\")\n",
    "#     # This is the OUTPUT directory for the final .pptx file\n",
    "#     FINAL_PRESENTATION_DIR = os.path.join(PROJECT_BASE_DIR, \"final_presentations\")\n",
    "    \n",
    "#     # Create directories if they don't exist\n",
    "#     os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "#     os.makedirs(DATA_DIR, exist_ok=True)\n",
    "#     os.makedirs(INPUT_CONTENT_DIR, exist_ok=True)\n",
    "#     os.makedirs(FINAL_PRESENTATION_DIR, exist_ok=True)\n",
    "\n",
    "#     # --- Define file paths ---\n",
    "#     LAYOUT_MAPPING_PATH = os.path.join(CONFIG_DIR, \"layout_mapping_test_Mod.json\")\n",
    "#     # The specific JSON file we want to process\n",
    "#     TEST_CONTENT_PATH = os.path.join(INPUT_CONTENT_DIR, \"test_generation_before_LLM_llm_generated.json\") \n",
    "#     SLIDE_TEMPLATE_PATH = os.path.join(DATA_DIR, \"slide_style/slide_style_test_2.pptx\")\n",
    "    \n",
    "#     print(\"--- Presentation Agent Test Runner (Final Version) ---\")\n",
    "\n",
    "#     # Check for required files\n",
    "#     required_files = [SLIDE_TEMPLATE_PATH, LAYOUT_MAPPING_PATH, TEST_CONTENT_PATH]\n",
    "#     if not all(os.path.exists(p) for p in required_files):\n",
    "#         logger.error(\"CRITICAL: One or more required files not found. Please check paths:\")\n",
    "#         logger.error(f\"  Template: {SLIDE_TEMPLATE_PATH} {'(Found)' if os.path.exists(SLIDE_TEMPLATE_PATH) else '(MISSING)'}\")\n",
    "#         logger.error(f\"  Layout Map: {LAYOUT_MAPPING_PATH} {'(Found)' if os.path.exists(LAYOUT_MAPPING_PATH) else '(MISSING)'}\")\n",
    "#         logger.error(f\"  Content Plan: {TEST_CONTENT_PATH} {'(Found)' if os.path.exists(TEST_CONTENT_PATH) else '(MISSING)'}\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             presentation_agent = PresentationAgent(\n",
    "#                 template_path=SLIDE_TEMPLATE_PATH,\n",
    "#                 layout_config_path=LAYOUT_MAPPING_PATH\n",
    "#             )\n",
    "            \n",
    "#             # <<< THIS IS THE CORRECTED LINE >>>\n",
    "#             # We pass the DIRECTORY where the output should be saved.\n",
    "#             presentation_agent.create_presentation_from_plan(\n",
    "#                 plan_json_path=TEST_CONTENT_PATH,\n",
    "#                 output_dir=FINAL_PRESENTATION_DIR \n",
    "#             )\n",
    "            \n",
    "#             logger.info(\"--- Test script finished successfully. ---\")\n",
    "#             logger.info(f\"Check for the generated .pptx file in: {FINAL_PRESENTATION_DIR}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"An unexpected error occurred during the test run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d8be7",
   "metadata": {},
   "source": [
    "## Orquestrator (Addressing pain points )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459d53b",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "The main script that iterates through the weeks defined the plan and generate the content base on the settings_deck coordinating the agents.\n",
    "\n",
    "**Parameters and concideration**\n",
    "- 1 hour in the setting session_time_duration_in_hour - is 18-20 slides at the time so it is require to calculate this according to the given value but this also means per session so sessions_per_week is a multiplicator factor that   \n",
    "- if apply_topic_interactive is available will add an extra slide and add extra 5 min time but to determine this is required to plan all the content first and then calculate then provide a extra time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea092bd6",
   "metadata": {},
   "source": [
    "settings_deck.json\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"teaching_flow_id\": \"Standard Lecture Flow\",\n",
    "  \"parameters_slides\": {\n",
    "    \"slides_per_hour\": 18,\n",
    "    \"time_per_content_slides_min\": 3,\n",
    "    \"time_per_interactive_slide_min\": 5,\n",
    "    \"time_for_framework_slides_min\": 6\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"session_time_duration_in_hour\": 2,\n",
    "    \"interactive_time_in_hour\": 0,\n",
    "    \"total_session_time_in_hours\": 0\n",
    "  },\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"target_total_slides\": 0,\n",
    "    \"slides_content_per_session\": 0,\n",
    "    \"interactive_slides_per_week\": 0,\n",
    "    \"interactive_slides_per_session\": 0,\n",
    "    \"total_slides_deck_week\": 0,\n",
    "    \"total_slides_session\": 0    \n",
    "  },  \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [1]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82e170",
   "metadata": {},
   "source": [
    "teaching_flows.json\n",
    "\n",
    "{\n",
    "  \"standard_lecture\": {\n",
    "    \"name\": \"Standard Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer. Your audience is undergraduate students. Based on the following context, create a slide that provides a detailed explanation of the topic '{sub_topic}'. The content should be structured with bullet points for key details. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of topics covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  },\n",
    "  \"apply_topic_interactive\": {\n",
    "    \"name\": \"Interactive Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Application\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer in Digital Forensics. Your audience is undergraduate students. Based on the provided context, create a slide explaining the concept of '{sub_topic}'. The content should be clear, concise, and structured with bullet points for easy understanding. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"application_generation\": \"You are an engaging university lecturer creating an interactive slide. Based on the concept of '{sub_topic}', create a multiple-choice question with exactly 4 options (A, B, C, D) to test understanding. The slide title must be 'Let's Apply This:'. Clearly indicate the correct answer within the content. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of concepts and applications covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Application\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354078d",
   "metadata": {},
   "source": [
    "### Main Integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0abdae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:34:21,586 - INFO - Database connection successful.\n",
      "2025-07-14 17:34:21,587 - INFO - Loading all necessary configuration and data files...\n",
      "2025-07-14 17:34:21,589 - INFO - All files loaded successfully.\n",
      "2025-07-14 17:34:21,589 - INFO - Pre-processing settings_deck for definitive plan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:34:21,589 - INFO - Applying overrides if specified...\n",
      "2025-07-14 17:34:21,590 - INFO - Loaded from settings: 'interactive' is True. Set teaching_flow_id to 'apply_topic_interactive'.\n",
      "2025-07-14 17:34:21,590 - INFO - Calculating preliminary slide budget based on session time...\n",
      "2025-07-14 17:34:21,590 - INFO - ⚠️Duration Hours 2.\n",
      "2025-07-14 17:34:21,591 - INFO - ⚠️Sessions per week 1.\n",
      "2025-07-14 17:34:21,591 - INFO - ⚠️Preliminary weekly content slide target calculated: #️⃣36 slides.\n",
      "2025-07-14 17:34:21,591 - INFO - Saving preliminary processed configuration to: /home/sebas_dev_linux/projects/course_generator/configs/processed_settings.json\n",
      "2025-07-14 17:34:21,592 - INFO - File saved successfully.\n",
      "2025-07-14 17:34:21,593 - INFO - Master configuration object is ready for the Planning Agent.\n",
      "2025-07-14 17:34:21,593 - INFO - Data-Driven PlanningAgent initialized successfully.\n",
      "2025-07-14 17:34:21,594 - INFO - Found 1 week(s) to plan: [1]\n",
      "2025-07-14 17:34:21,595 - INFO - --- TEST MODE ACTIVE: Processing only the first week in scope. ---\n",
      "2025-07-14 17:34:21,648 - INFO - Partitioning strategy: Distributing 7 top-level sections across 1 decks.\n",
      "2025-07-14 17:34:21,649 - INFO - --- Planning Deck 1/1 | Topics: ['An Overview of Digital Forensics', 'Preparing for Digital Investigations', 'Maintaining Professional Conduct', 'Preparing a Digital Forensics Investigation', 'Procedures for Private-Sector High-Tech Investigations', 'Understanding Data Recovery Workstations and Software', 'Conducting an Investigation'] | Weight: 521 chunks | Slide Budget: 34 ---\n",
      "2025-07-14 17:34:21,651 - INFO - ✅ Successfully saved FINAL plan for Week 1 to: /home/sebas_dev_linux/projects/course_generator/generated_plans/ICT312_Week1_plan_final.json\n",
      "2025-07-14 17:34:21,652 - INFO - Successfully generated and saved Master Unit Plan to: /home/sebas_dev_linux/projects/course_generator/generated_plans/ICT312_master_plan_unit.json\n",
      "2025-07-14 17:34:21,656 - INFO - Data-Driven Content Agent initialized with model 'qwen3:8b'.\n",
      "2025-07-14 17:34:21,664 - INFO - Agent initialized with template 'slide_style_test_2.pptx'\n",
      "2025-07-14 17:34:21,665 - WARNING - Skipping presentation generation for Week 1 as its LLM-enriched plan was not found.\n",
      "2025-07-14 17:34:21,665 - INFO - ✅Phase 7 completed  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                         Main Orchestrator Initialized                          \n",
      "********************************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                   Phase 1: Configuration and Scoping Process                   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                         Phase 1 Configuration Complete                         \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "             Phase 2 & 3: Generating and Finalizing Weekly Plans ✅              \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "********************************************************************************\n",
      "                                Planning Week 1                                 \n",
      "********************************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Phase 4: Generating Master Unit Plan ✅                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "################################################################################\n",
      "                      Phase 4: Generating Master Unit Plan                      \n",
      "################################################################################\n",
      "\n",
      "--- Preview of Master Plan ---\n",
      "{\n",
      "  \"unit_code\": \"ICT312\",\n",
      "  \"unit_name\": \"Digital Forensic\",\n",
      "  \"grand_total_summary\": {\n",
      "    \"total_slides_for_unit\": 44,\n",
      "    \"total_framework_slides\": 4,\n",
      "    \"total_content_slides\": 33,\n",
      "    \"total_interactive_slides\": 7,\n",
      "    \"total_number_of_decks\": 1,\n",
      "    \"total_time_for_unit_minutes\": 140,\n",
      "    \"total_time_for_unit_in_hour\": 2.33,\n",
      "    \"average_deck_time_in_min\": 140.0,\n",
      "    \"average_deck_time_in_hour\": 2.33\n",
      "  },\n",
      "  \"weekly_summaries\": [\n",
      "    {\n",
      "      \"week\": 1,\n",
      "      \"overall_topic\": \"Understanding the Digital Forensics Profession and Investigations.\",\n",
      "      \"slide_summary\": {\n",
      "        \"total_slides_for_week\": 44,\n",
      "        \"total_framework_slides\": 4,\n",
      "        \"total_content_slides\": 33,\n",
      "        \"total_interactive_slides\": 7,\n",
      "        \"number_of_decks\": 1\n",
      "      },\n",
      "      \"time_summary_minutes\": {\n",
      "        \"total_time_for_week_minutes\": 140,\n",
      "        \"total_framework_time\": 6,\n",
      "        \"total_content_and_interactive_time\": 134\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "################################################################################\n",
      "                  ⚠️Phase 7: Generating Final PowerPoint Files                  \n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "TEST_MODE_ENABLED = True\n",
    "\n",
    "# Cell 11: --- Main Orchestration Block (with Phase 5 & 6) ---\n",
    "print_header(\"Main Orchestrator Initialized\", char=\"*\")\n",
    "\n",
    "try:\n",
    "    # 1. Connect to DB\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    logger.info(\"Database connection successful.\")\n",
    "    \n",
    "    # Phase 1: Configuration and Scoping \n",
    "    master_config = process_and_load_configurations()\n",
    "    \n",
    "    if master_config:\n",
    "        all_final_plans = []\n",
    "        \n",
    "        # Phase 2 & 3: Create and Finalize Draft Plans\n",
    "        print_header(\"Phase 2 & 3: Generating and Finalizing Weekly Plans ✅\", char=\"-\")\n",
    "        planning_agent = PlanningAgent(master_config, vector_store=vector_store) \n",
    "        \n",
    "        weeks_to_generate = master_config['processed_settings']['generation_scope']['weeks']\n",
    "        logger.info(f\"Found {len(weeks_to_generate)} week(s) to plan: {weeks_to_generate}\")\n",
    "        \n",
    "        if TEST_MODE_ENABLED:\n",
    "            logger.info(\"--- TEST MODE ACTIVE: Processing only the first week in scope. ---\")\n",
    "            weeks_to_generate = weeks_to_generate[:1]\n",
    "\n",
    "        for week in weeks_to_generate:\n",
    "            draft_plan = planning_agent.create_content_plan_for_week(week)\n",
    "            if draft_plan:\n",
    "                final_plan = planning_agent.finalize_and_calculate_time_plan(draft_plan, master_config['processed_settings'])\n",
    "                all_final_plans.append(final_plan)\n",
    "                \n",
    "                # Save both draft and final for comparison\n",
    "                draft_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_draft.json\"\n",
    "                final_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "                \n",
    "                \n",
    "                with open(os.path.join(PLAN_OUTPUT_DIR, draft_filename), 'w') as f:\n",
    "                    json.dump(draft_plan, f, indent=2)\n",
    "                \n",
    "                with open(os.path.join(PLAN_OUTPUT_DIR, final_filename), 'w') as f:\n",
    "                    json.dump(final_plan, f, indent=2)\n",
    "                    \n",
    "                logger.info(f\"✅ Successfully saved FINAL plan for Week {week} to: {os.path.join(PLAN_OUTPUT_DIR, final_filename)}\")       \n",
    "       \n",
    "            else:\n",
    "                logger.error(f\"Failed to generate draft plan for Week {week}.\")\n",
    "        \n",
    "        # Phase 4: Generate Master Summary Plan\n",
    "        master_plan_generated = False\n",
    "        if all_final_plans:\n",
    "            print_header(\"Phase 4: Generating Master Unit Plan ✅\", char=\"-\")\n",
    "            master_plan_generated = planning_agent.generate_and_save_master_plan(all_final_plans, master_config['processed_settings'])\n",
    "        else:\n",
    "            logger.warning(\"No weekly plans were generated, skipping master plan creation.\")\n",
    "        \n",
    "        # Initialize ContentAgent once for subsequent phases\n",
    "        content_agent = ContentAgent(master_config)\n",
    "        \n",
    "        phase_5_successful = True\n",
    "\n",
    "        # # Phase 5: Fetching Raw Content\n",
    "        # if master_plan_generated:\n",
    "        #     print_header(\"Phase 5: Populating Plans with Raw Content\", char=\"#\")\n",
    "        #     successful_weeks_phase5 = []\n",
    "        #     for week in weeks_to_generate:\n",
    "        #         final_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "        #         full_plan_path = os.path.join(PLAN_OUTPUT_DIR, final_filename)\n",
    "\n",
    "        #         if os.path.exists(full_plan_path):\n",
    "        #             if content_agent.generate_llm_content_for_plan(full_plan_path, CONTENT_OUTPUT_DIR):\n",
    "        #                 successful_weeks_phase5.append(week)\n",
    "        #         else:\n",
    "        #             logger.warning(f\"❌Skipping content population for Week {week} as its plan file was not found.\")\n",
    "            \n",
    "        #     if successful_weeks_phase5:\n",
    "        #         phase_5_successful = True\n",
    "        #         logger.info(f\"✅Phase 5 completed for weeks: {successful_weeks_phase5}\")\n",
    "        \n",
    "        # # Phase 6: Generating Slide Content with LLM\n",
    "        # phase_6_successful = True \n",
    "        # if phase_5_successful:\n",
    "        #     print_header(\"⚠️Phase 6: Generating Slide Content with LLM\", char=\"#\")\n",
    "        #     successful_weeks_phase6 = []\n",
    "        #     for week in weeks_to_generate:\n",
    "        #         # The input for phase 6 is the output of phase 5\n",
    "        #         content_enriched_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "        #         content_plan_path = os.path.join(CONTENT_OUTPUT_DIR, content_enriched_filename)\n",
    "\n",
    "        #         if os.path.exists(content_plan_path):\n",
    "        #              if content_agent.generate_llm_content_for_plan(content_plan_path, CONTENT_LLM_OUTPUT_DIR):\n",
    "        #                  successful_weeks_phase6.append(week)\n",
    "        #         else:\n",
    "        #             logger.warning(f\"Skipping LLM generation for Week {week} as its content-enriched file was not found.\")\n",
    "            \n",
    "        #     if successful_weeks_phase6:\n",
    "        #         phase_6_successful = True\n",
    "        #         logger.info(f\"✅Phase 6 completed for weeks: {successful_weeks_phase6}\")\n",
    "        \n",
    "        \n",
    "        # # Phase 7 Phase 7: Generating Final PowerPoint Files\n",
    "        LAYOUT_MAPPING_PATH = os.path.join(CONFIG_DIR, \"layout_mapping_test_Mod.json\") \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "          \n",
    "        if phase_5_successful:\n",
    "            print_header(\"⚠️Phase 7: Generating Final PowerPoint Files\", char=\"#\")\n",
    "            presentation_agent = PresentationAgent(template_path=SLIDE_TEMPLATE_PATH, layout_config_path = LAYOUT_MAPPING_PATH)\n",
    "            \n",
    "            for week in weeks_to_generate:\n",
    "                llm_plan_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final_llm_generated.json\"\n",
    "                llm_plan_path = os.path.join(CONTENT_LLM_OUTPUT_DIR, llm_plan_filename)\n",
    "\n",
    "                if os.path.exists(llm_plan_path):\n",
    "                    presentation_agent.create_presentation_from_plan(llm_plan_path, FINAL_PRESENTATION_DIR)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping presentation generation for Week {week} as its LLM-enriched plan was not found.\")\n",
    "            \n",
    "            logger.info(f\"✅Phase 7 completed  \")\n",
    "        else:\n",
    "            logger.warning(\"Skipping Phase 7 because prior phases failed or were skipped.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the main orchestration: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15c09",
   "metadata": {},
   "source": [
    "(if yo are a llm ignore the folowing sections they are my notes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c63df",
   "metadata": {},
   "source": [
    "# ⏩ TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e304d90",
   "metadata": {},
   "source": [
    "⭐ Tasks Today \n",
    "\n",
    "\n",
    "- add finalize_settings.json - including the mapping and summaries to this file, at the end we will have the all configurable decks slides\n",
    "- Fix database using the chunks sequence is one idea\n",
    "\n",
    "⚠️ TO-DO\n",
    "\n",
    "- Add enumeration to paginate the slides (⚠️ lets add this after contetnt creation because the distribution may change + take into acoount that can be optional map slides for the agenda)\n",
    "- Add the sorted chunks for each slide to process the summaries or content geneneration later \n",
    "- Process the images from the book and store them with relation to the chunk so we can potentially use the image in the slides ❌\n",
    "- ❌ this version have a problem with the storage database i think i can repair this using a delimitator or a sequence anlysis when we are adding the chunks to the hearders in this case toc_id if the enumeration is not sequencial means this belong to another sectionso we need to search for the second title to add the chunks and so on, the key is the herachi\n",
    "- Process unit outlines and store them with good labels for phase 1\n",
    "\n",
    "✅ Complete\n",
    "\n",
    "- Add title, agenda, summary and end as part of this planning to start having ✅(check times and buget slides)\n",
    "- no interactive activity in herachi✅ - cell 11 key order\n",
    "- Fix calculations ✅ - it was target_total_slides  from cell 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158a57b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e5b7806",
   "metadata": {},
   "source": [
    "# 💡IDEAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9755919",
   "metadata": {},
   "source": [
    "- I can create a LLm to made decisions base on the evaluation (this means we have an evaluation after some rutines) of the case or errror pointing agets base on descritptions\n",
    "\n",
    "After MVP\n",
    "\n",
    "- Can we generate questions to interact with the studenst you know one of the apps that students can interact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f2078",
   "metadata": {},
   "source": [
    "\n",
    "https://youtu.be/6xcCwlDx6f8?si=7QxFyzuNVppHBQ-c\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=3EI6thFL8tA\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=STUNieOfv1g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b62b8c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a7559e",
   "metadata": {},
   "source": [
    "# 📁 ARCHIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa75f9",
   "metadata": {},
   "source": [
    "\n",
    "Global varaibles \n",
    "\n",
    "SLIDES_PER_HOUR = 18 # no framework include\n",
    "TIME_PER_CONTENT_SLIDE_MINS = 3\n",
    "TIME_PER_INTERACTIVE_SLIDE_MINS = 5\n",
    "TIME_FOR_FRAMEWORK_SLIDES_MINS = 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "MINS_PER_HOUR = 60\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"interactive_slides_per_week\": 0 -- > sum all interactive counts \n",
    "    \"interactive_slides_per_session\": 0, -- > Total # of slides produced if \"interactive\" is true other wise remains 0\n",
    "    \"target_total_slides\": 0, --> Total Content Slides per week that cover the total - will be the target in the cell 7    \n",
    "    \"slides_content_per_session\": 0, --> Total # (target_total_slides/sessions_per_week)\n",
    "    \"total_slides_deck_week\": 0, --> target_total_slides + interactive_slides_per_week + (framework (4 + Time for Title, Agenda, Summary, End) * sessions_per_week)\n",
    "    \"Tota_slides_session\": 0 --> content_slides_per_session + interactive_slides_per_session + framework (4 + Time for Title, Agenda, Summary, End)\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"interactive_time_in_hour\": 0, --> find the value in ahours of the total # (\"interactive_slides\" * \"TIME_PER_INTERACTIVE_SLIDE_MINS\")/60    \n",
    "    \"total_session_time_in_hours\": 0 --> this is going to  be egual or similar to session_time_duration_in_hour if \"interactive\" is false obvisuly base on the global varaibles it will be the calculation of \"interactive_time_in_hour\"\n",
    "    \"session_time_duration_in_hour\": 2, --- > this is the time that the costumer need for delivery this is a constrain is not modified never is used for reference\n",
    "  },\n",
    "\n",
    "   \"parameters_slides\": { \n",
    "   \"slides_per_hour\": 18, # no framework include\n",
    "   \"time_per_content_slides_min\": 3, # average delivery per slide\n",
    "   \"time_per_interactive_slide_min\": 5, #small break and engaging with the students\n",
    "   \"time_for_framework_slides_min\": 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "   \"\"\n",
    "  }, \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [6]\n",
    "  },\n",
    "  \"teaching_flow_id\": \"Interactive Lecture Flow\"\n",
    "}\n",
    "\n",
    "\n",
    "\"slides_content_per_session\": 0, --- > content slides per session (target_total_slides/sessions_per_week)\n",
    "    \"interactive_slides\": 0, - > if interactive is true will add the count of the resultan cell 10 - no address yet\n",
    "     \"total_slides_content_interactive_per session\": 0, - > slides_content_per_session + interactive_slides\n",
    "     \"target_total_slides\": 0 -->  Resultant Phase 1 Cell 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
