{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "import logging\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "EXTRACT_UO = False\n",
    "\n",
    "CREATE_RAG_BOOK = False\n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "\n",
    "\n",
    "\n",
    "# New configuration file paths\n",
    "CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "SETTINGS_DECK_PATH = os.path.join(CONFIG_DIR, \"settings_deck.json\")\n",
    "TEACHING_FLOWS_PATH = os.path.join(CONFIG_DIR, \"teaching_flows.json\")\n",
    "\n",
    "# to check the layauts \n",
    "LAYOUT_MAPPING_PATH = os.path.join(CONFIG_DIR, \"layout_mapping.json\")\n",
    "\n",
    "\n",
    "# New output path for the processed settings\n",
    "PROCESSED_SETTINGS_PATH = os.path.join(CONFIG_DIR, \"processed_settings.json\")\n",
    "\n",
    "# to Save the individual FINAL plan to a file\n",
    "PLAN_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_plans\")\n",
    "os.makedirs(PLAN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#to Save the individual FINAL Content to a file\n",
    "CONTENT_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_content\")\n",
    "os.makedirs(CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "CONTENT_LLM_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_content_llm\")\n",
    "os.makedirs(CONTENT_LLM_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "SLIDE_TEMPLATE_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/slide_style/slide_style_test.pptx\"\n",
    "FINAL_PRESENTATION_DIR = os.path.join(PROJECT_BASE_DIR, \"final_presentations\")\n",
    "os.makedirs(FINAL_PRESENTATION_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "# --- Helper Functions ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a new cell after your imports, or within Cell 3 before the functions.\n",
    "# This code is based on the schema from your screenshot on page 4.\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# Define Pydantic models that match your JSON schema\n",
    "class UnitInformation(BaseModel):\n",
    "    unitCode: Optional[str] = None\n",
    "    unitName: Optional[str] = None\n",
    "    creditPoints: Optional[int] = None\n",
    "    unitRationale: Optional[str] = None\n",
    "    prerequisites: Optional[str] = None\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    taskName: str\n",
    "    description: str\n",
    "    dueWeek: Optional[str] = None\n",
    "    weightingPercent: Optional[int] = None\n",
    "    learningOutcomesAssessed: Optional[str] = None\n",
    "\n",
    "class WeeklyScheduleItem(BaseModel):\n",
    "    week: str\n",
    "    contentTopic: str\n",
    "    requiredReading: Optional[str] = None\n",
    "\n",
    "class ParsedUnitOutline(BaseModel):\n",
    "    unitInformation: UnitInformation\n",
    "    learningOutcomes: List[str]\n",
    "    assessments: List[Assessment]\n",
    "    weeklySchedule: List[WeeklyScheduleItem] \n",
    "    requiredReadings: List[str]\n",
    "    recommendedReadings: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline_robust(\n",
    "    input_filepath: str, \n",
    "    output_filepath: str, \n",
    "    prompt_template: str,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    logger.info(f\"Starting to robustly process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from file: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    client = ollama.Client(host=OLLAMA_HOST)\n",
    "    current_prompt = prompt_template.format(outline_text=outline_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{max_retries} to parse outline.\")\n",
    "        \n",
    "        try:\n",
    "            # Call the LLM\n",
    "            llm_output_str = call_ollama_with_retry(client, current_prompt)\n",
    "            \n",
    "            # Find the JSON blob in the response\n",
    "            json_blob = parse_llm_json_output(llm_output_str) # Your existing helper\n",
    "            if not json_blob:\n",
    "                raise ValueError(\"LLM did not return a parsable JSON object.\")\n",
    "\n",
    "            # *** THE KEY VALIDATION STEP ***\n",
    "            # Try to parse the dictionary into your Pydantic model.\n",
    "            # This will raise a `ValidationError` if keys are wrong, types are wrong, or fields are missing.\n",
    "            parsed_data = ParsedUnitOutline.model_validate(json_blob)\n",
    "            \n",
    "            # If successful, save the validated data and exit the loop\n",
    "            logger.info(\"Successfully validated JSON structure against Pydantic model.\")\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                # Use .model_dump_json() for clean, validated output\n",
    "                f.write(parsed_data.model_dump_json(indent=2)) \n",
    "\n",
    "            logger.info(f\"Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "            return # Exit function on success\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Validation failed on attempt {attempt + 1}. Error: {e}\")\n",
    "            # Formulate a new prompt with the error message for self-correction\n",
    "            error_feedback = (\n",
    "                f\"\\n\\nYour previous attempt failed. You MUST correct the following errors:\\n\"\n",
    "                f\"{e}\\n\\n\"\n",
    "                f\"Please regenerate the entire JSON object, ensuring it strictly adheres to the schema \"\n",
    "                f\"and corrects these specific errors. Do not change any key names.\"\n",
    "            )\n",
    "            current_prompt = current_prompt + error_feedback # Append the error to the prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch other errors like network issues from call_ollama_with_retry\n",
    "            logger.error(f\"An unexpected error occurred on attempt {attempt + 1}: {e}\", exc_info=True)\n",
    "            # You might want to wait before retrying for non-validation errors\n",
    "            time.sleep(5)\n",
    "\n",
    "    logger.error(f\"Failed to get valid structured data from the LLM after {max_retries} attempts.\")\n",
    "\n",
    "\n",
    "# --- In your execution block, call the new function ---\n",
    "# parse_and_save_outline(...) becomes:\n",
    "\n",
    "if EXTRACT_UO:\n",
    "    parse_and_save_outline_robust(\n",
    "        input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "        output_filepath=PARSED_UO_JSON_PATH,\n",
    "        prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract Book Table of Contents (ToC) with Pre-assigned IDs & Links in Order\n",
    "\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import urllib.parse # Needed to clean up links\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS (MODIFIED TO INCLUDE ID ASSIGNMENT AND LINK EXTRACTION)\n",
    "# ==============================================================================\n",
    "\n",
    "def clean_epub_href(href: str) -> str:\n",
    "    \"\"\"Removes URL fragments and decodes URL-encoded characters.\"\"\"\n",
    "    if not href: return \"\"\n",
    "    # Remove fragment identifier (e.g., '#section1')\n",
    "    cleaned_href = href.split('#')[0]\n",
    "    # Decode any URL-encoded characters (e.g., %20 -> space)\n",
    "    return urllib.parse.unquote(cleaned_href)\n",
    "\n",
    "# --- EPUB Extraction Logic ---\n",
    "def parse_navpoint(navpoint: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 2 navPoints and assigns a toc_id and link_filename.\"\"\"\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    if not title: return None\n",
    "    \n",
    "    # --- MODIFICATION: Extract the linked filename ---\n",
    "    content_tag = navpoint.find('content', recursive=False)\n",
    "    link_filename = clean_epub_href(content_tag['src']) if content_tag else \"\"\n",
    "    \n",
    "    node = {\n",
    "        \"level\": level,\n",
    "        \"toc_id\": counter[0],\n",
    "        \"title\": title,\n",
    "        \"link_filename\": link_filename, # Add the cleaned link\n",
    "        \"children\": []\n",
    "    }\n",
    "    counter[0] += 1\n",
    "    \n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, counter, level + 1)\n",
    "        if child_node: node[\"children\"].append(child_node)\n",
    "        \n",
    "    return node\n",
    "\n",
    "def parse_li(li_element: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 3 <li> elements and assigns a toc_id and link_filename.\"\"\"\n",
    "    a_tag = li_element.find('a', recursive=False)\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        if not title: return None\n",
    "        \n",
    "        # --- MODIFICATION: Extract the linked filename ---\n",
    "        link_filename = clean_epub_href(a_tag.get('href'))\n",
    "        \n",
    "        node = {\n",
    "            \"level\": level,\n",
    "            \"toc_id\": counter[0],\n",
    "            \"title\": title,\n",
    "            \"link_filename\": link_filename, # Add the cleaned link\n",
    "            \"children\": []\n",
    "        }\n",
    "        counter[0] += 1\n",
    "        \n",
    "        nested_ol = li_element.find('ol', recursive=False)\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, counter, level + 1)\n",
    "                if child_node: node[\"children\"].append(child_node)\n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_epub_toc(epub_path, output_json_path):\n",
    "    print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "    toc_data = []\n",
    "    book = epub.read_epub(epub_path)\n",
    "    id_counter = [0]\n",
    "    \n",
    "    for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "        soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "        # Logic to handle both EPUB 2 (NCX) and EPUB 3 (XHTML)\n",
    "        if nav_item.get_name().endswith('.ncx'):\n",
    "            print(\"INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\")\n",
    "            navmap = soup.find('navMap')\n",
    "            if navmap:\n",
    "                for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                    node = parse_navpoint(navpoint, id_counter, level=0)\n",
    "                    if node: toc_data.append(node)\n",
    "        else: # Assumes EPUB 3\n",
    "            print(\"INFO: Found EPUB 3 (XHTML) Table of Contents. Parsing...\")\n",
    "            toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "            if toc_nav:\n",
    "                top_ol = toc_nav.find('ol', recursive=False)\n",
    "                if top_ol:\n",
    "                    for li in top_ol.find_all('li', recursive=False):\n",
    "                        node = parse_li(li, id_counter, level=0)\n",
    "                        if node: toc_data.append(node)\n",
    "        if toc_data: break\n",
    "    \n",
    "    if toc_data:\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote EPUB ToC with IDs and links to: {output_json_path}\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# --- PDF Extraction Logic (Unchanged) ---\n",
    "def build_pdf_hierarchy_with_ids(toc_list: List) -> List[Dict]:\n",
    "    root = []\n",
    "    parent_stack = {-1: {\"children\": root}}\n",
    "    id_counter = [0]\n",
    "    for level, title, page in toc_list:\n",
    "        normalized_level = level - 1\n",
    "        node = {\"level\": normalized_level, \"toc_id\": id_counter[0], \"title\": title.strip(), \"page\": page, \"children\": []}\n",
    "        id_counter[0] += 1\n",
    "        parent_node = parent_stack.get(normalized_level - 1)\n",
    "        if parent_node: parent_node[\"children\"].append(node)\n",
    "        parent_stack[normalized_level] = node\n",
    "    return root\n",
    "\n",
    "def extract_pdf_toc(pdf_path, output_json_path):\n",
    "    print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        hierarchical_toc = []\n",
    "        if not toc: print(\"❌ WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "        else:\n",
    "            print(f\"INFO: Found {len(toc)} bookmark entries. Building hierarchy and assigning IDs...\")\n",
    "            hierarchical_toc = build_pdf_hierarchy_with_ids(toc)\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote PDF ToC with assigned IDs to: {output_json_path}\")\n",
    "    except Exception as e: print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if PROCESS_EPUB:\n",
    "    extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "else:\n",
    "    extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Hierarchical Vector Database (with Sequential ToC ID and Chunk ID)\n",
    "# This cell processes the book, enriches it with hierarchical and sequential metadata,\n",
    "# chunks it, and creates the final vector database.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper: Clean metadata values for ChromaDB ---\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "    if isinstance(value, list): return \", \".join(map(str, value))\n",
    "    if isinstance(value, dict): return json.dumps(value)\n",
    "    if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    return str(value)\n",
    "\n",
    "# --- Core Function to Process Book with Pre-extracted ToC ---\n",
    "def process_book_with_extracted_toc(\n",
    "    book_path: str,\n",
    "    extracted_toc_json_path: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "    logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "    # 1. Load the pre-extracted hierarchical ToC\n",
    "    try:\n",
    "        with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        if not hierarchical_toc:\n",
    "            logger.error(f\"Pre-extracted ToC at '{extracted_toc_json_path}' is empty or invalid.\")\n",
    "            return [], []\n",
    "        logger.info(f\"Successfully loaded pre-extracted ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pre-extracted ToC JSON: {e}\", exc_info=True)\n",
    "        return [], []\n",
    "\n",
    "    # 2. Load all text elements/pages from the book\n",
    "    all_raw_book_docs: List[Document] = []\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "    if file_extension == \".epub\":\n",
    "        loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} text elements from EPUB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EPUB content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    elif file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} pages from PDF.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    else:\n",
    "        logger.error(f\"Unsupported book file format: {file_extension}\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    if not all_raw_book_docs:\n",
    "        logger.error(\"No text elements/pages loaded from the book.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    # 3. Create enriched LangChain Documents by matching ToC to content\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "    # Flatten the ToC, AND add a unique sequential ID for sorting and validation.\n",
    "    flat_toc_entries: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def _add_ids_and_flatten_recursive(nodes: List[Dict[str, Any]], current_titles_path: List[str], counter: List[int]):\n",
    "        \"\"\"\n",
    "        Recursively traverses ToC nodes to flatten them and assign a unique, sequential toc_id.\n",
    "        \"\"\"\n",
    "        for node in nodes:\n",
    "            toc_id = counter[0]\n",
    "            counter[0] += 1\n",
    "            title = node.get(\"title\", \"\").strip()\n",
    "            if not title: continue\n",
    "            new_titles_path = current_titles_path + [title]\n",
    "            entry = {\n",
    "                \"titles_path\": new_titles_path,\n",
    "                \"level\": node.get(\"level\"),\n",
    "                \"full_title_for_matching\": title,\n",
    "                \"toc_id\": toc_id\n",
    "            }\n",
    "            if \"page\" in node: entry[\"page\"] = node[\"page\"]\n",
    "            flat_toc_entries.append(entry)\n",
    "            if node.get(\"children\"):\n",
    "                _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter)\n",
    "\n",
    "    toc_id_counter = [0]\n",
    "    _add_ids_and_flatten_recursive(hierarchical_toc, [], toc_id_counter)\n",
    "    logger.info(f\"Flattened ToC and assigned sequential IDs to {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "    # Logic for PDF metadata assignment\n",
    "    if file_extension == \".pdf\" and any(\"page\" in entry for entry in flat_toc_entries):\n",
    "        logger.info(\"Assigning metadata to PDF pages based on ToC page numbers...\")\n",
    "        flat_toc_entries.sort(key=lambda x: x.get(\"page\", -1) if x.get(\"page\") is not None else -1)\n",
    "        for page_doc in all_raw_book_docs:\n",
    "            page_num_0_indexed = page_doc.metadata.get(\"page\", -1)\n",
    "            page_num_1_indexed = page_num_0_indexed + 1\n",
    "            assigned_metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "            best_match_toc_entry = None\n",
    "            for toc_entry in flat_toc_entries:\n",
    "                toc_page = toc_entry.get(\"page\")\n",
    "                if toc_page is not None and toc_page <= page_num_1_indexed:\n",
    "                    if best_match_toc_entry is None or toc_page > best_match_toc_entry.get(\"page\", -1):\n",
    "                        best_match_toc_entry = toc_entry\n",
    "                elif toc_page is not None and toc_page > page_num_1_indexed:\n",
    "                    break\n",
    "            if best_match_toc_entry:\n",
    "                for i, title_in_path in enumerate(best_match_toc_entry[\"titles_path\"]):\n",
    "                    assigned_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                assigned_metadata['toc_id'] = best_match_toc_entry.get('toc_id')\n",
    "            else:\n",
    "                assigned_metadata[\"level_1_title\"] = \"Uncategorized PDF Page\"\n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in assigned_metadata.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=cleaned_meta))\n",
    "\n",
    "    # Logic for EPUB metadata assignment\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Assigning metadata to EPUB elements by matching ToC titles in text...\")\n",
    "        toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "        current_hierarchy_metadata = {}\n",
    "        for element_doc in all_raw_book_docs:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            if not element_text: continue\n",
    "            for toc_entry in toc_titles_for_search:\n",
    "                if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "                    current_hierarchy_metadata = {\"source\": os.path.basename(book_path)}\n",
    "                    for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "                        current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                    current_hierarchy_metadata['toc_id'] = toc_entry.get('toc_id')\n",
    "                    if \"page\" in toc_entry: current_hierarchy_metadata[\"epub_toc_page\"] = toc_entry[\"page\"]\n",
    "                    break\n",
    "            if not current_hierarchy_metadata:\n",
    "                doc_metadata_to_assign = {\"source\": os.path.basename(book_path), \"level_1_title\": \"EPUB Preamble\", \"toc_id\": -1}\n",
    "            else:\n",
    "                doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata_to_assign.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=element_text, metadata=cleaned_meta))\n",
    "    \n",
    "    else: # Fallback\n",
    "        final_documents_with_metadata = all_raw_book_docs\n",
    "\n",
    "    if not final_documents_with_metadata:\n",
    "        logger.error(\"No documents were processed or enriched with hierarchical metadata.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks, inheriting hierarchical metadata.\")\n",
    "    \n",
    "    # --- MODIFICATION START: Add a unique, sequential chunk_id to each chunk ---\n",
    "    logger.info(\"Assigning sequential chunk_id to all final chunks...\")\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        chunk.metadata['chunk_id'] = i\n",
    "    logger.info(f\"Assigned chunk_ids from 0 to {len(final_chunks) - 1}.\")\n",
    "    # --- MODIFICATION END ---\n",
    "\n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "# --- Main Execution Block for this Cell ---\n",
    "if CREATE_RAG_BOOK:\n",
    "    if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "        logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "        logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "    else:\n",
    "        final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "            book_path=BOOK_PATH,\n",
    "            extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "\n",
    "        if final_chunks_for_db:\n",
    "            if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "                logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "                shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "            logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "            embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "            \n",
    "            vector_db = Chroma.from_documents(\n",
    "                documents=final_chunks_for_db,\n",
    "                embedding=embedding_model,\n",
    "                persist_directory=CHROMA_PERSIST_DIR,\n",
    "                collection_name=CHROMA_COLLECTION_NAME\n",
    "            )\n",
    "            \n",
    "            reloaded_db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model, collection_name=CHROMA_COLLECTION_NAME)\n",
    "            count = reloaded_db._collection.count()\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            logger.info(f\"✅ Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "            logger.info(f\"✅ Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            logger.error(\"❌ Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5a: Inspecting EPUB Documents and Metadata BEFORE Chunking\n",
    "\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- Setup Logger for this inspection cell ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def inspect_epub_preprocessing():\n",
    "    \"\"\"\n",
    "    This function replicates the pre-chunking logic from Cell 5 for EPUB files\n",
    "    to show the list of large documents with their assigned ToC metadata.\n",
    "    \"\"\"\n",
    "    if not PROCESS_EPUB:\n",
    "        print(\"This inspection cell is for EPUB processing. Please set PROCESS_EPUB = True in Cell 1.\")\n",
    "        return\n",
    "\n",
    "    print_header(\"EPUB Pre-Processing Inspection\", char=\"~\")\n",
    "\n",
    "    # --- 1. Load the necessary data (replicating start of Cell 5) ---\n",
    "    logger.info(\"Loading pre-extracted ToC and raw EPUB elements...\")\n",
    "    try:\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        \n",
    "        loader = UnstructuredEPubLoader(BOOK_PATH, mode=\"elements\", strategy=\"fast\")\n",
    "        all_raw_book_docs = loader.load()\n",
    "        logger.info(f\"Successfully loaded {len(all_raw_book_docs)} raw text elements from the EPUB.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load necessary files: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Flatten the ToC (replicating logic from Cell 5) ---\n",
    "    logger.info(\"Flattening the hierarchical ToC for matching...\")\n",
    "    flat_toc_entries = []\n",
    "    def _add_ids_and_flatten_recursive(nodes, current_titles_path, counter):\n",
    "        for node in nodes:\n",
    "            toc_id = counter[0]\n",
    "            counter[0] += 1\n",
    "            title = node.get(\"title\", \"\").strip()\n",
    "            if not title: continue\n",
    "            new_titles_path = current_titles_path + [title]\n",
    "            entry = {\n",
    "                \"titles_path\": new_titles_path,\n",
    "                \"level\": node.get(\"level\"),\n",
    "                \"full_title_for_matching\": title,\n",
    "                \"toc_id\": toc_id\n",
    "            }\n",
    "            flat_toc_entries.append(entry)\n",
    "            if node.get(\"children\"):\n",
    "                _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter)\n",
    "    \n",
    "    _add_ids_and_flatten_recursive(hierarchical_toc, [], [0])\n",
    "    logger.info(f\"Flattened ToC into {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "    # --- 3. The Core Matching Logic for EPUB (the part you want to see) ---\n",
    "    logger.info(\"Assigning metadata to EPUB elements by matching ToC titles...\")\n",
    "    \n",
    "    final_documents_with_metadata = []\n",
    "    toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "    current_hierarchy_metadata = {}\n",
    "\n",
    "    for element_doc in all_raw_book_docs:\n",
    "        element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "        if not element_text:\n",
    "            continue\n",
    "\n",
    "        # Check if this element is a heading that matches a ToC entry\n",
    "        is_heading = False\n",
    "        for toc_entry in toc_titles_for_search:\n",
    "            if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "                # It's a heading! Update the current context.\n",
    "                current_hierarchy_metadata = {\"source\": os.path.basename(BOOK_PATH)}\n",
    "                for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "                    current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                current_hierarchy_metadata['toc_id'] = toc_entry.get('toc_id')\n",
    "                is_heading = True\n",
    "                break # Found the match, no need to search further\n",
    "\n",
    "        # Assign metadata\n",
    "        if not current_hierarchy_metadata:\n",
    "            # Content before the first ToC entry (e.g., cover, title page)\n",
    "            doc_metadata_to_assign = {\"source\": os.path.basename(BOOK_PATH), \"level_1_title\": \"EPUB Preamble\", \"toc_id\": -1}\n",
    "        else:\n",
    "            doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "\n",
    "        final_documents_with_metadata.append(Document(page_content=element_text, metadata=doc_metadata_to_assign))\n",
    "    \n",
    "    logger.info(f\"Processing complete. Generated {len(final_documents_with_metadata)} documents with assigned metadata.\")\n",
    "    \n",
    "    # --- 4. Print the result for inspection ---\n",
    "    print_header(\"INSPECTION RESULTS: Documents Before Chunking\", char=\"=\")\n",
    "    print(f\"Total documents created: {len(final_documents_with_metadata)}\\n\")\n",
    "\n",
    "    for i, doc in enumerate(final_documents_with_metadata[:100]): # Print first 30 to avoid flooding the output\n",
    "        print(f\"--- Document [{i+1}] ---\")\n",
    "        print(f\"  Assigned Metadata: {doc.metadata}\")\n",
    "        print(f\"  Content (Un-chunked Element):\")\n",
    "        print(f\"  >> '{doc.page_content}'\")\n",
    "        print(\"-\" * 25 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- Execute the inspection ---\n",
    "inspect_epub_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d38d",
   "metadata": {},
   "source": [
    "### Full Database Health & Hierarchy Diagnostic Report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Full Database Health & Hierarchy Diagnostic Report (V5 - with Content Preview)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# You might need to install pandas if you haven't already\n",
    "try:\n",
    "    import pandas as pd\n",
    "    pandas_available = True\n",
    "except ImportError:\n",
    "    pandas_available = False\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def count_total_chunks(node: Dict) -> int:\n",
    "    \"\"\"Recursively counts all chunks in a node and its children.\"\"\"\n",
    "    total = node.get('_chunks', 0)\n",
    "    for child_node in node.get('_children', {}).values():\n",
    "        total += count_total_chunks(child_node)\n",
    "    return total\n",
    "\n",
    "def print_hierarchy_report(node: Dict, indent_level: int = 0):\n",
    "    \"\"\"\n",
    "    Recursively prints the reconstructed hierarchy, sorting by sequential ToC ID.\n",
    "    \"\"\"\n",
    "    sorted_children = sorted(\n",
    "        node.get('_children', {}).items(),\n",
    "        key=lambda item: item[1].get('_toc_id', float('inf'))\n",
    "    )\n",
    "    \n",
    "    for title, child_node in sorted_children:\n",
    "        prefix = \"  \" * indent_level + \"|-- \"\n",
    "        total_chunks_in_branch = count_total_chunks(child_node)\n",
    "        direct_chunks = child_node.get('_chunks', 0)\n",
    "        toc_id = child_node.get('_toc_id', 'N/A')\n",
    "        print(f\"{prefix}{title} [ID: {toc_id}] (Total Chuck in branch: {total_chunks_in_branch}, Direct Chunk: {direct_chunks})\")\n",
    "        print_hierarchy_report(child_node, indent_level + 1)\n",
    "\n",
    "def find_testable_sections(node: Dict, path: str, testable_list: List):\n",
    "    \"\"\"\n",
    "    Recursively find sections with a decent number of \"direct\" chunks to test sequence on.\n",
    "    \"\"\"\n",
    "    if node.get('_chunks', 0) > 10 and not node.get('_children'):\n",
    "        testable_list.append({\n",
    "            \"path\": path,\n",
    "            \"toc_id\": node.get('_toc_id'),\n",
    "            \"chunk_count\": node.get('_chunks')\n",
    "        })\n",
    "\n",
    "    for title, child_node in node.get('_children', {}).items():\n",
    "        new_path = f\"{path} -> {title}\" if path else title\n",
    "        find_testable_sections(child_node, new_path, testable_list)\n",
    "\n",
    "\n",
    "# --- MODIFIED TEST FUNCTION ---\n",
    "def verify_chunk_sequence_and_content(vector_store: Chroma, hierarchy_tree: Dict):\n",
    "    \"\"\"\n",
    "    Selects a random ToC section, verifies chunk sequence, and displays the reassembled content.\n",
    "    \"\"\"\n",
    "    print_header(\"Chunk Sequence & Content Integrity Test\", char=\"-\")\n",
    "    logger.info(\"Verifying chunk order and reassembling content for a random ToC section.\")\n",
    "    \n",
    "    # 1. Find a good section to test\n",
    "    testable_sections = []\n",
    "    find_testable_sections(hierarchy_tree, \"\", testable_sections)\n",
    "    \n",
    "    if not testable_sections:\n",
    "        logger.warning(\"Could not find a suitable section with enough chunks to test. Skipping content test.\")\n",
    "        return\n",
    "\n",
    "    random_section = random.choice(testable_sections)\n",
    "    test_toc_id = random_section['toc_id']\n",
    "    section_title = random_section['path'].split(' -> ')[-1]\n",
    "    \n",
    "    logger.info(f\"Selected random section for testing: '{random_section['path']}' (toc_id: {test_toc_id})\")\n",
    "\n",
    "    # 2. Retrieve all documents (content + metadata) for that toc_id\n",
    "    try:\n",
    "        # Use .get() to retrieve full documents, not just similarity search\n",
    "        retrieved_data = vector_store.get(\n",
    "            where={\"toc_id\": test_toc_id},\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "        \n",
    "        # Combine metadatas and documents into LangChain Document objects\n",
    "        docs = [Document(page_content=doc, metadata=meta) for doc, meta in zip(retrieved_data['documents'], retrieved_data['metadatas'])]\n",
    "\n",
    "        logger.info(f\"Retrieved {len(docs)} document chunks for toc_id {test_toc_id}.\")\n",
    "\n",
    "        if len(docs) < 1:\n",
    "            logger.warning(\"No chunks found in the selected section. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # 3. Sort the documents by chunk_id\n",
    "        # Handle cases where chunk_id might be missing for robustness\n",
    "        docs.sort(key=lambda d: d.metadata.get('chunk_id', -1))\n",
    "        \n",
    "        chunk_ids = [d.metadata.get('chunk_id') for d in docs]\n",
    "        if None in chunk_ids:\n",
    "            logger.error(\"TEST FAILED: Some retrieved chunks are missing a 'chunk_id'.\")\n",
    "            return\n",
    "\n",
    "        # 4. Verify sequence\n",
    "        is_sequential = all(chunk_ids[i] == chunk_ids[i-1] + 1 for i in range(1, len(chunk_ids)))\n",
    "        \n",
    "        # 5. Reassemble and print content\n",
    "        full_content = \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "        print(\"\\n\" + \"-\"*25 + \" CONTENT PREVIEW \" + \"-\"*25)\n",
    "        print(f\"Title: {section_title} [toc_id: {test_toc_id}]\")\n",
    "        print(f\"Chunk IDs: {chunk_ids}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(full_content)\n",
    "        print(\"-\" * 23 + \" END CONTENT PREVIEW \" + \"-\"*23 + \"\\n\")\n",
    "        \n",
    "        if is_sequential:\n",
    "            logger.info(\"✅ TEST PASSED: Chunk IDs for the section are sequential and content is reassembled.\")\n",
    "        else:\n",
    "            logger.warning(\"TEST PASSED (with note): Chunk IDs are not perfectly sequential but are in increasing order.\")\n",
    "            logger.warning(\"This is acceptable. Sorting by chunk_id successfully restored narrative order.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"TEST FAILED: An error occurred during chunk sequence verification: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- MAIN DIAGNOSTIC FUNCTION ---\n",
    "def run_full_diagnostics():\n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain components not installed. Skipping diagnostics.\")\n",
    "        return\n",
    "    if not pandas_available:\n",
    "        logger.warning(\"Pandas not installed. Some reports may not be available.\")\n",
    "\n",
    "    print_header(\"Full Database Health & Hierarchy Diagnostic Report\")\n",
    "\n",
    "    # 1. Connect to the Database\n",
    "    logger.info(\"Connecting to the vector database...\")\n",
    "    if not os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        logger.error(f\"FATAL: Chroma DB directory not found at {CHROMA_PERSIST_DIR}.\")\n",
    "        return\n",
    "\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    logger.info(\"Successfully connected to the database.\")\n",
    "\n",
    "    # 2. Retrieve ALL Metadata\n",
    "    total_docs = vector_store._collection.count()\n",
    "    if total_docs == 0:\n",
    "        logger.warning(\"Database is empty. No diagnostics to run.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "    metadatas = vector_store.get(limit=total_docs, include=[\"metadatas\"])['metadatas']\n",
    "    logger.info(\"Successfully retrieved all metadata.\")\n",
    "    \n",
    "    # 3. Reconstruct the Hierarchy Tree\n",
    "    logger.info(\"Reconstructing hierarchy from chunk metadata...\")\n",
    "    hierarchy_tree = {'_children': {}}\n",
    "    chunks_without_id = 0\n",
    "\n",
    "    for meta in metadatas:\n",
    "        toc_id = meta.get('toc_id')\n",
    "        if toc_id is None or toc_id == -1:\n",
    "            chunks_without_id += 1\n",
    "            node_title = meta.get('level_1_title', 'Orphaned Chunks')\n",
    "            if node_title not in hierarchy_tree['_children']:\n",
    "                 hierarchy_tree['_children'][node_title] = {'_children': {}, '_chunks': 0, '_toc_id': float('inf')}\n",
    "            hierarchy_tree['_children'][node_title]['_chunks'] += 1\n",
    "            continue\n",
    "        \n",
    "        current_node = hierarchy_tree\n",
    "        for level in range(1, 7):\n",
    "            level_key = f'level_{level}_title'\n",
    "            title = meta.get(level_key)\n",
    "            if not title: break\n",
    "            if title not in current_node['_children']:\n",
    "                current_node['_children'][title] = {'_children': {}, '_chunks': 0, '_toc_id': float('inf')}\n",
    "            current_node = current_node['_children'][title]\n",
    "\n",
    "        current_node['_chunks'] += 1\n",
    "        current_node['_toc_id'] = min(current_node['_toc_id'], toc_id)\n",
    "        \n",
    "    logger.info(\"Hierarchy reconstruction complete.\")\n",
    "\n",
    "    # 4. Print Hierarchy Report\n",
    "    print_header(\"Reconstructed Hierarchy Report (Book Order)\", char=\"-\")\n",
    "    print_hierarchy_report(hierarchy_tree)\n",
    "        \n",
    "    # 5. Run Chunk Sequence and Content Test\n",
    "    verify_chunk_sequence_and_content(vector_store, hierarchy_tree)\n",
    "    \n",
    "    # 6. Final Summary\n",
    "    print_header(\"Diagnostic Summary\", char=\"-\")\n",
    "    print(f\"Total Chunks in DB: {total_docs}\")\n",
    "    \n",
    "    if chunks_without_id > 0:\n",
    "        logger.warning(f\"Found {chunks_without_id} chunks MISSING a valid 'toc_id'. Check 'Orphaned' sections.\")\n",
    "    else:\n",
    "        logger.info(\"All chunks contain valid 'toc_id' metadata. Sequential integrity is maintained.\")\n",
    "\n",
    "    print_header(\"Diagnostic Complete\")\n",
    "\n",
    "# --- Execute Diagnostics ---\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and langchain_available:\n",
    "    run_full_diagnostics()\n",
    "else:\n",
    "    logger.error(\"Skipping diagnostics: Global variables not defined or LangChain not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Verify Content Retrieval for a Specific toc_id with Reassembled Text\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_and_print_chunks_for_toc_id(vector_store: Chroma, toc_id: int):\n",
    "    \"\"\"\n",
    "    Retrieves all chunks for a specific toc_id, reconstructs the section title\n",
    "    from hierarchical metadata, shows the reassembled text, and lists individual\n",
    "    chunk details for verification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the 'get' method with a 'where' filter to find all chunks for the toc_id\n",
    "        results = vector_store.get(\n",
    "            where={\"toc_id\": toc_id},\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "\n",
    "        if not results or not results.get('ids'):\n",
    "            logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"VERIFICATION FAILED: No content found for toc_id: {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            return\n",
    "\n",
    "        documents = results['documents']\n",
    "        metadatas = results['metadatas']\n",
    "        \n",
    "        # --- FIX START: Reconstruct the hierarchical section title from metadata ---\n",
    "        # We assume all chunks for the same toc_id share the same titles.\n",
    "        # We will inspect the metadata of the first chunk to get the title.\n",
    "        section_title = \"Unknown or Uncategorized Section\"\n",
    "        if metadatas:\n",
    "            first_meta = metadatas[0]\n",
    "            \n",
    "            # Find all 'level_X_title' keys in the metadata\n",
    "            level_titles = []\n",
    "            for key, value in first_meta.items():\n",
    "                if key.startswith(\"level_\") and key.endswith(\"_title\"):\n",
    "                    try:\n",
    "                        # Extract the level number (e.g., 1 from 'level_1_title') for sorting\n",
    "                        level_num = int(key.split('_')[1])\n",
    "                        level_titles.append((level_num, value))\n",
    "                    except (ValueError, IndexError):\n",
    "                        # Ignore malformed keys, just in case\n",
    "                        continue\n",
    "            \n",
    "            # Sort the titles by their level number (1, 2, 3...)\n",
    "            level_titles.sort()\n",
    "            \n",
    "            # Join the sorted titles to create a breadcrumb-style title\n",
    "            if level_titles:\n",
    "                title_parts = [title for num, title in level_titles]\n",
    "                section_title = \" > \".join(title_parts)\n",
    "        # --- FIX END ---\n",
    "        \n",
    "        # --- Print a clear header with the reconstructed section title ---\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"VERIFYING SECTION: '{section_title}' (toc_id: {toc_id})\")\n",
    "        print(\"=\" * 80)\n",
    "        logger.info(f\"Found {len(documents)} chunks in the database for this section.\")\n",
    "        \n",
    "        # Sort chunks by their chunk_id to ensure they are in the correct order for reassembly\n",
    "        sorted_items = sorted(zip(documents, metadatas), key=lambda item: item[1].get('chunk_id', 0))\n",
    "\n",
    "        # --- Reassemble and print the full text for the section ---\n",
    "        all_chunk_texts = [item[0] for item in sorted_items]\n",
    "        reassembled_text = \"\\n\".join(all_chunk_texts)\n",
    "        \n",
    "        print(\"\\n\" + \"#\" * 28 + \" Reassembled Text \" + \"#\" * 28)\n",
    "        print(reassembled_text)\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        # --- Print individual chunk details for in-depth verification ---\n",
    "        print(\"\\n\" + \"-\" * 24 + \" Retrieved Chunk Details \" + \"-\" * 25)\n",
    "        for i, (doc, meta) in enumerate(sorted_items):\n",
    "            print(f\"\\n[ Chunk {i+1} of {len(documents)} | chunk_id: {meta.get('chunk_id', 'N/A')} ]\")\n",
    "            content_preview = doc.replace('\\n', ' ').strip()\n",
    "            print(f\"  Content Preview: '{content_preview[:250]}...'\")\n",
    "            print(f\"  Metadata: {json.dumps(meta, indent=2)}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Verification complete for section '{section_title}'.\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during retrieval for toc_id {toc_id}: {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTION BLOCK (No changes needed here)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- IMPORTANT: Set the ID of the section you want to test here ---\n",
    "# Example: ToC ID 10 might be \"An Overview of Digital Forensics\"\n",
    "# Example: ToC ID 11 might be \"Digital Forensics and Other Related Disciplines\"\n",
    "TOC_ID_TO_TEST = 9# Change this to an ID you know exists from your ToC\n",
    "\n",
    "\n",
    "# Assume these variables are defined in a previous cell from your notebook\n",
    "# CHROMA_PERSIST_DIR = \"./chroma_db_with_metadata\"\n",
    "# EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "# CHROMA_COLLECTION_NAME = \"forensics_handbook\"\n",
    "\n",
    "# Check if the database directory exists before attempting to connect\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and os.path.exists(CHROMA_PERSIST_DIR):\n",
    "    logger.info(f\"Connecting to the existing vector database at '{CHROMA_PERSIST_DIR}'...\")\n",
    "    \n",
    "    try:\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        # Run the verification function\n",
    "        retrieve_and_print_chunks_for_toc_id(vector_store, TOC_ID_TO_TEST)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Chroma or run retrieval. Error: {e}\")\n",
    "        logger.error(\"Please ensure your embedding model and collection names are correct.\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Database directory not found or 'CHROMA_PERSIST_DIR' variable is not set.\")\n",
    "    logger.error(\"Please run the previous cell (Cell 5) to create the database first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "Require Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf3ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Verify Vector Database (Final Version with Rich Diagnostic Output)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "def print_results(query_text: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Richly prints query results, showing the query, filter, and retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*10 + \" DIAGNOSTIC: RETRIEVAL RESULTS \" + \"-\"*10)\n",
    "    print(f\"QUERY: '{query_text}'\")\n",
    "    if where_filter:\n",
    "        print(f\"FILTER: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"--> No documents were retrieved for this query and filter.\")\n",
    "        print(\"-\" * 55)\n",
    "        return\n",
    "        \n",
    "    print(f\"--> Found {len(results)} results. Displaying top {min(len(results), 3)}:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n[ RESULT {i+1} ]\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"  Content : '{content_preview[:200]}...'\")\n",
    "        print(f\"  Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "\n",
    "# --- HELPER FUNCTIONS FOR FINDING DATA (UNCHANGED) ---\n",
    "def find_deep_entry(nodes: List[Dict], current_path: List[str] = []) -> Optional[Tuple[Dict, List[str]]]:\n",
    "    shuffled_nodes = random.sample(nodes, len(nodes))\n",
    "    for node in shuffled_nodes:\n",
    "        if node.get('level', 0) >= 2 and node.get('children'): return node, current_path + [node['title']]\n",
    "        if node.get('children'):\n",
    "            path = current_path + [node['title']]\n",
    "            deep_entry = find_deep_entry(node['children'], path)\n",
    "            if deep_entry: return deep_entry\n",
    "    return None\n",
    "\n",
    "def find_chapter_title_by_number(toc_data: List[Dict], chap_num: int) -> Optional[List[str]]:\n",
    "    def search_nodes(nodes, num, current_path):\n",
    "        for node in nodes:\n",
    "            path = current_path + [node['title']]\n",
    "            if re.match(rf\"(Chapter\\s)?{num}[.:\\s]\", node.get('title', ''), re.IGNORECASE): return path\n",
    "            if node.get('children'):\n",
    "                found_path = search_nodes(node['children'], num, path)\n",
    "                if found_path: return found_path\n",
    "        return None\n",
    "    return search_nodes(toc_data, chap_num, [])\n",
    "\n",
    "\n",
    "# --- ENHANCED TEST CASES with DIAGNOSTIC OUTPUT ---\n",
    "\n",
    "def basic_retrieval_test(db, outline):\n",
    "    print_header(\"Test 1: Basic Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm the database is live and contains thematically relevant content.\")\n",
    "        logger.info(\"Strategy: Perform a simple similarity search using the course's 'unitName'.\")\n",
    "        query_text = outline.get(\"unitInformation\", {}).get(\"unitName\", \"introduction\")\n",
    "        \n",
    "        logger.info(f\"Action: Searching for query: '{query_text}'...\")\n",
    "        results = db.similarity_search(query_text, k=1)\n",
    "        \n",
    "        print_results(query_text, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one document was returned.\")\n",
    "        assert len(results) > 0, \"Basic retrieval query returned no results.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 1 PASSED. The database is online and responsive.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 1 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def deep_hierarchy_test(db, toc):\n",
    "    print_header(\"Test 2: Deep Hierarchy Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\")\n",
    "        logger.info(\"Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\")\n",
    "        deep_entry_result = find_deep_entry(toc)\n",
    "        assert deep_entry_result, \"Could not find a suitable deep entry (level >= 2) to test.\"\n",
    "        node, path = deep_entry_result\n",
    "        query = node['title']\n",
    "        \n",
    "        logger.info(f\"  - Selected random deep section: {' -> '.join(path)}\")\n",
    "        conditions = [{f\"level_{i+1}_title\": {\"$eq\": title}} for i, title in enumerate(path)]\n",
    "        w_filter = {\"$and\": conditions}\n",
    "        \n",
    "        logger.info(\"Action: Performing a similarity search with a highly specific '$and' filter.\")\n",
    "        results = db.similarity_search(query, k=1, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if the precisely filtered query returned any documents.\")\n",
    "        assert len(results) > 0, \"Deeply filtered query returned no results.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 2 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def advanced_alignment_test(db, outline, toc):\n",
    "    print_header(\"Test 3: Advanced Unit Outline Alignment\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\")\n",
    "        logger.info(\"Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\")\n",
    "        week_to_test = random.choice(outline['weeklySchedule'])\n",
    "        logger.info(f\"  - Selected random week: Week {week_to_test['week']} - '{week_to_test['contentTopic']}'\")\n",
    "\n",
    "        reading = week_to_test.get('requiredReading', '')\n",
    "        chap_nums_str = re.findall(r'\\d+', reading)\n",
    "        assert chap_nums_str, f\"Could not find chapter numbers in required reading: '{reading}'\"\n",
    "        logger.info(f\"  - Extracted required chapter number(s): {chap_nums_str}\")\n",
    "\n",
    "        chapter_paths = [find_chapter_title_by_number(toc, int(n)) for n in chap_nums_str]\n",
    "        chapter_paths = [path for path in chapter_paths if path is not None]\n",
    "        assert chapter_paths, f\"Could not map chapter numbers {chap_nums_str} to a valid ToC path.\"\n",
    "        \n",
    "        level_1_titles = list(set([path[0] for path in chapter_paths]))\n",
    "        logger.info(f\"  - Mapped to top-level ToC entries: {level_1_titles}\")\n",
    "\n",
    "        or_filter = [{\"level_1_title\": {\"$eq\": title}} for title in level_1_titles]\n",
    "        w_filter = {\"$or\": or_filter} if len(or_filter) > 1 else or_filter[0]\n",
    "        query = week_to_test['contentTopic']\n",
    "        \n",
    "        logger.info(\"Action: Searching for the weekly topic, filtered by the mapped chapter(s).\")\n",
    "        results = db.similarity_search(query, k=5, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one returned document is from the correct chapter.\")\n",
    "        assert len(results) > 0, \"Alignment query returned no results for the correct section/chapter.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 3 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def content_sequence_test(db, outline):\n",
    "    print_header(\"Test 4: Content Sequence Verification\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\")\n",
    "        logger.info(\"Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\")\n",
    "        topic_query = random.choice(outline['weeklySchedule'])['contentTopic']\n",
    "        \n",
    "        logger.info(f\"Action: Performing similarity search for topic: '{topic_query}' to get a set of chunks.\")\n",
    "        results = db.similarity_search(topic_query, k=10)\n",
    "        \n",
    "        print_results(topic_query, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        docs_with_id = [doc for doc in results if 'chunk_id' in doc.metadata]\n",
    "        assert len(docs_with_id) > 3, \"Fewer than 4 retrieved chunks have a 'chunk_id' to test.\"\n",
    "        \n",
    "        chunk_ids = [doc.metadata['chunk_id'] for doc in docs_with_id]\n",
    "        sorted_ids = sorted(chunk_ids)\n",
    "        \n",
    "        logger.info(f\"  - Retrieved and sorted chunk IDs: {sorted_ids}\")\n",
    "        logger.info(\"Verification: Check if the sorted list of chunk_ids is strictly increasing.\")\n",
    "        is_ordered = all(sorted_ids[i] >= sorted_ids[i-1] for i in range(1, len(sorted_ids)))\n",
    "        assert is_ordered, \"The retrieved chunks' chunk_ids are not in ascending order when sorted.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 4 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    \n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain libraries not found. Aborting tests.\")\n",
    "        return\n",
    "\n",
    "    required_files = {\n",
    "        \"Chroma DB\": CHROMA_PERSIST_DIR,\n",
    "        \"ToC JSON\": PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        \"Parsed Outline\": PARSED_UO_JSON_PATH\n",
    "    }\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Required '{name}' not found at '{path}'. Please run previous cells.\")\n",
    "            return\n",
    "\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        toc_data = json.load(f)\n",
    "    with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        unit_outline_data = json.load(f)\n",
    "\n",
    "    logger.info(\"Connecting to DB and initializing components...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    results_summary = [\n",
    "        basic_retrieval_test(vector_store, unit_outline_data),\n",
    "        deep_hierarchy_test(vector_store, toc_data),\n",
    "        advanced_alignment_test(vector_store, unit_outline_data, toc_data),\n",
    "        content_sequence_test(vector_store, unit_outline_data)\n",
    "    ]\n",
    "\n",
    "    passed_count = sum(filter(None, results_summary))\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    \n",
    "    print_header(\"Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)}\")\n",
    "    print(f\"✅ Passed: {passed_count}\")\n",
    "    print(f\"❌ Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "# Assumes global variables from Cell 1 are available in the notebook's scope\n",
    "run_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97368b0",
   "metadata": {},
   "source": [
    "#  Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae477bc",
   "metadata": {},
   "source": [
    "## Planning Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c538570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: The Data-Driven Planning Agent (Final Hierarchical Version⭐)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# Setup Logger and LangChain components\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "class PlanningAgent:\n",
    "    \"\"\"\n",
    "    An agent that creates a hierarchical content plan, adaptively partitions content\n",
    "    into distinct lecture decks, and allocates presentation time.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "        self.config = master_config['processed_settings']\n",
    "        self.unit_outline = master_config['unit_outline']\n",
    "        self.book_toc = master_config['book_toc']\n",
    "        self.flat_toc_with_ids = self._create_flat_toc_with_ids()\n",
    "        self.vector_store = vector_store\n",
    "        logger.info(\"Data-Driven PlanningAgent initialized successfully.\")\n",
    "\n",
    "    def _create_flat_toc_with_ids(self) -> List[Dict]:\n",
    "        \"\"\"Creates a flattened list of the ToC for easy metadata lookup.\"\"\"\n",
    "        flat_list = []\n",
    "        def flatten_recursive(nodes, counter):\n",
    "            for node in nodes:\n",
    "                node_id = counter[0]; counter[0] += 1\n",
    "                flat_list.append({'toc_id': node_id, 'title': node.get('title', ''), 'node': node})\n",
    "                if node.get('children'):\n",
    "                    flatten_recursive(node.get('children'), counter)\n",
    "        flatten_recursive(self.book_toc, [0])\n",
    "        return flat_list\n",
    "    \n",
    "    def _assign_sequence_ids_recursively(self, node: Dict, counter: list):\n",
    "        \"\"\"\n",
    "        Recursively walks a content node, assigning a unique, sequential ID\n",
    "        to the node, its children, and its interactive activity in a depth-first order.\n",
    "        \"\"\"\n",
    "        # 1. Assign sequence ID to the parent content node itself.\n",
    "        node['seq_id'] = counter[0]\n",
    "        counter[0] += 1\n",
    "\n",
    "        # 2. Recurse through all children first.\n",
    "        if 'children' in node and node['children']:\n",
    "            for child in node['children']:\n",
    "                self._assign_sequence_ids_recursively(child, counter)\n",
    "\n",
    "        # 3. Assign an ID to the interactive activity LAST, so it appears after the content and its children.\n",
    "        if 'interactive_activity' in node:\n",
    "            node['interactive_activity']['seq_id'] = counter[0]\n",
    "            counter[0] += 1\n",
    "\n",
    "    def _identify_relevant_chapters(self, weekly_schedule_item: Dict) -> List[int]:\n",
    "        \"\"\"Extracts chapter numbers precisely from the 'requiredReading' string.\"\"\"\n",
    "        reading_str = weekly_schedule_item.get('requiredReading', '')\n",
    "        match = re.search(r'Chapter(s)?', reading_str, re.IGNORECASE)\n",
    "        if not match: return []\n",
    "        search_area = reading_str[match.start():]\n",
    "        chap_nums_str = re.findall(r'\\d+', search_area)\n",
    "        if chap_nums_str:\n",
    "            return sorted(list(set(int(n) for n in chap_nums_str)))\n",
    "        return []\n",
    "\n",
    "    def _find_chapter_node(self, chapter_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Finds the ToC node for a specific chapter number.\"\"\"\n",
    "        for item in self.flat_toc_with_ids:\n",
    "            if re.match(rf\"Chapter\\s{chapter_number}(?:\\D|$)\", item['title']):\n",
    "                return item['node']\n",
    "        return None\n",
    "\n",
    "    def _build_topic_plan_tree(self, toc_node: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively builds a hierarchical plan tree from any ToC node,\n",
    "        annotating it with direct and total branch chunk counts.\n",
    "        \"\"\"\n",
    "        node_metadata = next((item for item in self.flat_toc_with_ids if item['node'] is toc_node), None)\n",
    "        if not node_metadata: return {}\n",
    "\n",
    "        retrieved_docs = self.vector_store.get(where={'toc_id': node_metadata['toc_id']})\n",
    "        direct_chunk_count = len(retrieved_docs.get('ids', []))\n",
    "\n",
    "        plan_node = {\n",
    "            \"title\": node_metadata['title'],\n",
    "            \"toc_id\": node_metadata['toc_id'],\n",
    "            \"chunk_count\": direct_chunk_count,\n",
    "            \"total_chunks_in_branch\": 0,\n",
    "            \"slides_allocated\": 0,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        child_branch_total = 0\n",
    "        for child_node in toc_node.get('children', []):\n",
    "            if any(ex in child_node.get('title', '').lower() for ex in [\"review\", \"introduction\", \"summary\", \"key terms\"]):\n",
    "                continue\n",
    "            child_plan_node = self._build_topic_plan_tree(child_node)\n",
    "            if child_plan_node:\n",
    "                plan_node['children'].append(child_plan_node)\n",
    "                child_branch_total += child_plan_node.get('total_chunks_in_branch', 0)\n",
    "        \n",
    "        plan_node['total_chunks_in_branch'] = direct_chunk_count + child_branch_total\n",
    "        return plan_node\n",
    "    \n",
    "    # In PlanningAgent Class...\n",
    "\n",
    "    def _allocate_slides_to_tree(self, plan_tree: Dict, content_slides_budget: int):\n",
    "        \"\"\"\n",
    "        (FINAL, REORDERED FOR CLARITY) Performs a multi-pass process to allocate content slides,\n",
    "        add activities, sum totals, and reorders the keys in each node for maximum readability.\n",
    "        \"\"\"\n",
    "        if not plan_tree or content_slides_budget <= 0:\n",
    "            return plan_tree\n",
    "\n",
    "        # --- Pass 1: Allocate Content Slides ---\n",
    "        def allocate_content_recursively(node, budget):\n",
    "            node['budget_slides_content'] = round(budget)\n",
    "            node['direct_slides_content'] = 0\n",
    "            if not node.get('children'):\n",
    "                node['direct_slides_content'] = round(budget)\n",
    "                return\n",
    "            total_branch_chunks = node.get('total_chunks_in_branch', 0)\n",
    "            own_content_slides = 0\n",
    "            if total_branch_chunks > 0:\n",
    "                own_content_slides = round(budget * (node.get('chunk_count', 0) / total_branch_chunks))\n",
    "            node['direct_slides_content'] = own_content_slides\n",
    "            remaining_budget_for_children = budget - own_content_slides\n",
    "            children_total_chunks = total_branch_chunks - node.get('chunk_count', 0)\n",
    "            if children_total_chunks <= 0: return\n",
    "            for child in node.get('children', []):\n",
    "                child_budget = remaining_budget_for_children * (child.get('total_chunks_in_branch', 0) / children_total_chunks)\n",
    "                allocate_content_recursively(child, child_budget)\n",
    "        \n",
    "        allocate_content_recursively(plan_tree, content_slides_budget)\n",
    "\n",
    "        # --- Pass 2: Add Interactive Activities ---\n",
    "        def add_interactive_nodes(node, depth, interactive_deep):\n",
    "            if not node: return\n",
    "            if self.config.get('interactive', False):\n",
    "                if interactive_deep:\n",
    "                    if depth == 2: node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Deep-Dive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "                    if depth == 1: node['interactive_activity'] = {\"title\": f\"{node.get('title')} (General Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "                else:\n",
    "                    if depth == 1: node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Interactive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "            for child in node.get('children', []):\n",
    "                add_interactive_nodes(child, depth + 1, interactive_deep)\n",
    "\n",
    "        add_interactive_nodes(plan_tree, 1, self.config.get('interactive_deep', False))\n",
    "\n",
    "        # --- Pass 3: Sum All Slides Up the Tree ---\n",
    "        def sum_slides_upwards(node):\n",
    "            total_slides = node.get('direct_slides_content', 0)\n",
    "            total_slides += node.get('interactive_activity', {}).get('slides_allocated', 0)\n",
    "            if node.get('children'):\n",
    "                total_slides += sum(sum_slides_upwards(child) for child in node.get('children', []))\n",
    "            node['total_slides_in_branch'] = total_slides\n",
    "            return total_slides\n",
    "\n",
    "        sum_slides_upwards(plan_tree)\n",
    "        \n",
    "        # --- NEW: Pass 4: Reorder keys for final clarity ---\n",
    "        def reorder_keys_for_readability(node: Dict) -> Dict:\n",
    "            if not node:\n",
    "                return None\n",
    "\n",
    "            # Define the desired order of keys\n",
    "            key_order = [\n",
    "                \"title\",\n",
    "                \"toc_id\",\n",
    "                \"chunk_count\",\n",
    "                \"total_chunks_in_branch\", \n",
    "                \"budget_slides_content\",\n",
    "                \"direct_slides_content\",\n",
    "                \"total_slides_in_branch\",\n",
    "                \"children\",\n",
    "                \"interactive_activity\"\n",
    "                \n",
    "            ]\n",
    "            \n",
    "            # Rebuild the dictionary in the specified order\n",
    "            reordered_node = {key: node[key] for key in key_order if key in node}\n",
    "            \n",
    "            # Recursively reorder children\n",
    "            if 'children' in reordered_node:\n",
    "                reordered_node['children'] = [reorder_keys_for_readability(child) for child in reordered_node['children']]\n",
    "                \n",
    "            return reordered_node\n",
    "\n",
    "        return reorder_keys_for_readability(plan_tree)\n",
    "\n",
    "    def create_content_plan_for_week(self, week_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Orchestrates the adaptive planning and partitioning process.\"\"\"\n",
    "        print_header(f\"Planning Week {week_number}\", char=\"*\")\n",
    "        \n",
    "        weekly_schedule_item = self.unit_outline['weeklySchedule'][week_number - 1]\n",
    "        chapter_numbers = self._identify_relevant_chapters(weekly_schedule_item)\n",
    "        if not chapter_numbers: return None\n",
    "\n",
    "        num_decks = self.config['week_session_setup'].get('sessions_per_week', 1)\n",
    "        \n",
    "        # 1. Build a full plan tree for each chapter to get its weight.\n",
    "        chapter_plan_trees = [self._build_topic_plan_tree(self._find_chapter_node(cn)) for cn in chapter_numbers if self._find_chapter_node(cn)]\n",
    "        total_weekly_chunks = sum(tree.get('total_chunks_in_branch', 0) for tree in chapter_plan_trees)\n",
    "\n",
    "        # 2. NEW: Adaptive Partitioning Strategy\n",
    "        partitionable_units = []\n",
    "        all_top_level_sections = []\n",
    "        for chapter_tree in chapter_plan_trees:\n",
    "            all_top_level_sections.extend(chapter_tree.get('children', []))\n",
    "\n",
    "        num_top_level_sections = len(all_top_level_sections)\n",
    "\n",
    "        # Always prefer to split by top-level sections if there are enough to distribute.\n",
    "        if num_top_level_sections >= num_decks:\n",
    "            logger.info(f\"Partitioning strategy: Distributing {num_top_level_sections} top-level sections across {num_decks} decks.\")\n",
    "            partitionable_units = all_top_level_sections\n",
    "        else:\n",
    "            # Fallback for rare cases where there are fewer topics than decks (e.g., 1 chapter with 1 section, but 2 decks).\n",
    "            logger.info(f\"Partitioning strategy: Not enough top-level sections ({num_top_level_sections}) to fill all decks ({num_decks}). Distributing whole chapters instead.\")\n",
    "            partitionable_units = chapter_plan_trees\n",
    "        \n",
    "        # 3. Partition the chosen units into decks using a bin-packing algorithm\n",
    "        decks = [[] for _ in range(num_decks)]\n",
    "        deck_weights = [0] * num_decks\n",
    "        sorted_units = sorted(partitionable_units, key=lambda x: x.get('toc_id', 0))\n",
    "        \n",
    "        for unit in sorted_units:\n",
    "            lightest_deck_index = deck_weights.index(min(deck_weights))\n",
    "            decks[lightest_deck_index].append(unit)\n",
    "            deck_weights[lightest_deck_index] += unit.get('total_chunks_in_branch', 0)\n",
    "\n",
    "        # 4. Plan each deck\n",
    "        content_slides_per_week = self.config['slide_count_strategy'].get('target_total_slides', 25)\n",
    "        final_deck_plans = []\n",
    "        for i, deck_content_trees in enumerate(decks):\n",
    "            deck_number = i + 1\n",
    "            deck_chunk_weight = sum(tree.get('total_chunks_in_branch', 0) for tree in deck_content_trees)\n",
    "            deck_slide_budget = round((deck_chunk_weight / total_weekly_chunks) * content_slides_per_week) if total_weekly_chunks > 0 else 0\n",
    "\n",
    "            logger.info(f\"--- Planning Deck {deck_number}/{num_decks} | Topics: {[t['title'] for t in deck_content_trees]} | Weight: {deck_chunk_weight} chunks | Slide Budget: {deck_slide_budget} ---\")\n",
    "            \n",
    "            # The allocation function is recursive and works on any tree or sub-tree\n",
    "            planned_content = [self._allocate_slides_to_tree(tree, round(deck_slide_budget * (tree.get('total_chunks_in_branch', 0) / deck_chunk_weight))) if deck_chunk_weight > 0 else tree for tree in deck_content_trees]\n",
    "            \n",
    "            final_deck_plans.append({\n",
    "                \"deck_number\": deck_number,\n",
    "                \"deck_title\": f\"{self.config.get('unit_name', 'Course')} - Week {week_number}, Lecture {deck_number}\",\n",
    "                \"session_content\": planned_content\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"week\": week_number,\n",
    "            \"overall_topic\": weekly_schedule_item.get('contentTopic'),\n",
    "            \"deck_plans\": final_deck_plans\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def finalize_and_calculate_time_plan(self,draft_plan: Dict, config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Takes a draft plan and enriches it by:\n",
    "        1. Calculating detailed slide counts and time allocations for every node.\n",
    "        2. Adding framework sections and wrapping content.\n",
    "        3. Calculating and adding summaries for decks and the entire week.\n",
    "        4. Assigning a final sequence ID (seq_id) for slide generation.\n",
    "        5. Reordering all keys for maximum readability.\n",
    "        \"\"\"\n",
    "        final_plan = json.loads(json.dumps(draft_plan))\n",
    "\n",
    "        # --- Time Constants from Config ---\n",
    "        params = config.get('parameters_slides', {})\n",
    "        TIME_PER_CONTENT = params.get('time_per_content_slides_min', 3)\n",
    "        TIME_PER_INTERACTIVE = params.get('time_per_interactive_slide_min', 5)\n",
    "        TIME_FOR_FRAMEWORK_DECK = params.get('time_for_framework_slides_min', 6)\n",
    "        FRAMEWORK_SLIDES_PER_DECK = 4\n",
    "        \n",
    "        \n",
    "        # --- Recursive Helper Functions ---\n",
    "        def _calculate_time_and_reorder(node: Dict):\n",
    "            # 1. Recurse to the bottom first to perform a bottom-up calculation\n",
    "            children_total_time = 0\n",
    "            if 'children' in node and node['children']:\n",
    "                for child in node['children']:\n",
    "                    _calculate_time_and_reorder(child) # Recursive call\n",
    "                    children_total_time += child.get('time_allocation_minutes', {}).get('total_branch_time', 0)\n",
    "\n",
    "            # 2. Calculate this node's direct time\n",
    "            direct_content_time = node.get('direct_slides_content', 0) * TIME_PER_CONTENT\n",
    "            interactive_time = node.get('interactive_activity', {}).get('slides_allocated', 0) * TIME_PER_INTERACTIVE\n",
    "            \n",
    "            # 3. Calculate this node's total branch time\n",
    "            branch_total_time = direct_content_time + interactive_time + children_total_time\n",
    "\n",
    "            # 4. Create the time allocation object\n",
    "            time_alloc = {\n",
    "                \"direct_content_time\": direct_content_time,\n",
    "                \"direct_interactive_time\": interactive_time,\n",
    "                \"total_branch_time\": branch_total_time\n",
    "            }\n",
    "            node['time_allocation_minutes'] = time_alloc\n",
    "\n",
    "            # 5. Reorder all keys for this node to ensure final clarity\n",
    "            key_order = [\n",
    "                \"title\",\n",
    "                \"toc_id\",\n",
    "                \"chunk_count\",\n",
    "                \"total_chunks_in_branch\",\n",
    "                \"budget_slides_content\",\n",
    "                \"direct_slides_content\",\n",
    "                \"total_slides_in_branch\",\n",
    "                \"time_allocation_minutes\", \n",
    "                \"children\",\n",
    "                \"interactive_activity\"\n",
    "                \n",
    "            ]\n",
    "            reordered_node = {key: node[key] for key in key_order if key in node}\n",
    "            \n",
    "            # Clear the original node and update it with the reordered keys\n",
    "            node.clear()\n",
    "            node.update(reordered_node)\n",
    "\n",
    "        # --- Main Processing Loop for Decks ---\n",
    "        for deck in final_plan.get(\"deck_plans\", []):\n",
    "            session_content_blocks = deck.pop(\"session_content\", [])\n",
    "\n",
    "            # Perform the combined time calculation and reordering pass\n",
    "            for block in session_content_blocks:\n",
    "                _calculate_time_and_reorder(block)\n",
    "\n",
    "            # Create Framework Sections\n",
    "            week_number, deck_number = final_plan.get(\"week\"), deck.get(\"deck_number\")\n",
    "            title_section = {\"section_type\": \"Title\", \"content\": { \"unit_name\": config.get('unit_name', 'Course'), \"unit_code\": config.get('course_id', ''), \"week_topic\": final_plan.get('overall_topic', ''), \"deck_title\": f\"Week {week_number}, Lecture {deck_number}\"}}\n",
    "            agenda_section = {\"section_type\": \"Agenda\", \"content\": {\"title\": \"Today's Agenda\", \"items\": [item.get('title', 'Untitled Topic') for item in session_content_blocks]}}\n",
    "            summary_section = {\"section_type\": \"Summary\", \"content\": {\"title\": \"Summary & Key Takeaways\", \"placeholder\": \"Auto-generate based on covered topics.\"}}\n",
    "            end_section = {\"section_type\": \"End\", \"content\": {\"title\": \"Thank You\", \"text\": \"Questions?\"}}\n",
    "            main_content_block = {\"section_type\": \"Content\", \"content_blocks\": session_content_blocks}\n",
    "            \n",
    "            final_sections_for_deck = [title_section, agenda_section, main_content_block, summary_section, end_section]\n",
    "            \n",
    "            \n",
    "            # ***************************************************************\n",
    "            # *** NEW: Assign Sequential IDs for the entire deck ***\n",
    "            # ***************************************************************\n",
    "            seq_counter = [0] # Use a list for pass-by-reference behavior\n",
    "            for section in final_sections_for_deck:\n",
    "                if section.get(\"section_type\") == \"Content\":\n",
    "                    # For the main content, iterate through its blocks and start the recursion\n",
    "                    for block in section.get(\"content_blocks\", []):\n",
    "                        self._assign_sequence_ids_recursively(block, seq_counter)\n",
    "                else:\n",
    "                    # For simple framework slides, just assign the next ID\n",
    "                    section['seq_id'] = seq_counter[0]\n",
    "                    seq_counter[0] += 1\n",
    "            # ***************************************************************\n",
    "            # *** END NEW CODE ***\n",
    "            # ***************************************************************\n",
    "            \n",
    "            \n",
    "            # Calculate Deck Summaries        \n",
    "            total_content_slides = sum(b.get('total_slides_in_branch', 0) - b.get('interactive_activity',{}).get('slides_allocated',0) for b in session_content_blocks)\n",
    "            total_interactive_slides = sum(b.get('interactive_activity',{}).get('slides_allocated',0) for b in session_content_blocks)\n",
    "\n",
    "            deck_content_time = sum(b.get('time_allocation_minutes', {}).get('total_branch_time', 0) for b in session_content_blocks)\n",
    "            \n",
    "            deck['total_slides_in_deck'] = FRAMEWORK_SLIDES_PER_DECK + sum(b.get('total_slides_in_branch', 0) for b in session_content_blocks)\n",
    "            deck['slide_count_breakdown'] = {\"framework\": FRAMEWORK_SLIDES_PER_DECK, \"content\": total_content_slides, \"interactive\": total_interactive_slides}\n",
    "            deck['time_breakdown_minutes'] = {\"framework\": TIME_FOR_FRAMEWORK_DECK, \"content_and_interactive\": deck_content_time, \"total_deck_time\": TIME_FOR_FRAMEWORK_DECK + deck_content_time}\n",
    "            deck['sections'] = final_sections_for_deck\n",
    "            if 'deck_title' in deck: del deck['deck_title']\n",
    "\n",
    "        # --- Calculate Grand Totals for the Week ---\n",
    "        weekly_slide_summary = {\"total_slides_for_week\": 0, \"total_framework_slides\": 0, \"total_content_slides\": 0, \"total_interactive_slides\": 0, \"number_of_decks\": len(final_plan.get(\"deck_plans\", []))}\n",
    "        weekly_time_summary = {\"total_time_for_week_minutes\": 0, \"total_framework_time\": 0, \"total_content_and_interactive_time\": 0}\n",
    "        \n",
    "        for deck in final_plan.get(\"deck_plans\", []):\n",
    "            weekly_slide_summary['total_slides_for_week'] += deck.get('total_slides_in_deck', 0)\n",
    "            for key, value in deck.get('slide_count_breakdown', {}).items(): weekly_slide_summary[f\"total_{key}_slides\"] += value\n",
    "            weekly_time_summary['total_time_for_week_minutes'] += deck.get('time_breakdown_minutes', {}).get('total_deck_time', 0)\n",
    "            weekly_time_summary['total_framework_time'] += deck.get('time_breakdown_minutes', {}).get('framework', 0)\n",
    "            weekly_time_summary['total_content_and_interactive_time'] += deck.get('time_breakdown_minutes', {}).get('content_and_interactive', 0)\n",
    "\n",
    "        # --- Construct Final Ordered Plan ---\n",
    "        final_ordered_plan = {\n",
    "            \"week\": final_plan.get(\"week\"),\n",
    "            \"overall_topic\": final_plan.get(\"overall_topic\"),\n",
    "            \"weekly_slide_summary\": weekly_slide_summary,\n",
    "            \"weekly_time_summary_minutes\": weekly_time_summary,\n",
    "            \"deck_plans\": final_plan.get(\"deck_plans\", [])\n",
    "        }\n",
    "        \n",
    "        return final_ordered_plan\n",
    "\n",
    "    # --- NEW FUNCTION TO GENERATE MASTER SUMMARY ---\n",
    "    def generate_and_save_master_plan(self, weekly_plans: List[Dict], config: Dict):\n",
    "        \"\"\"\n",
    "        Aggregates summaries from all weekly plans into a single master plan file,\n",
    "        including new grand total metrics.\n",
    "        \"\"\"\n",
    "        print_header(\"Phase 4: Generating Master Unit Plan\", char=\"#\")\n",
    "        \n",
    "        # Initialize the master plan structure with the new fields\n",
    "        master_plan = {\n",
    "            \"unit_code\": config.get('course_id', 'UNKNOWN'),\n",
    "            \"unit_name\": config.get('unit_name', 'Unknown Unit'),\n",
    "            \"grand_total_summary\": {\n",
    "                \"total_slides_for_unit\": 0,\n",
    "                \"total_framework_slides\": 0,\n",
    "                \"total_content_slides\": 0,\n",
    "                \"total_interactive_slides\": 0,\n",
    "                \"total_number_of_decks\": 0,\n",
    "                \"total_time_for_unit_minutes\": 0,\n",
    "                \"total_time_for_unit_in_hour\": 0, # New\n",
    "                \"average_deck_time_in_min\": 0,\n",
    "                \"average_deck_time_in_hour\": 0           \n",
    "            },\n",
    "            \"weekly_summaries\": []\n",
    "        }\n",
    "\n",
    "        grand_totals = master_plan[\"grand_total_summary\"]\n",
    "\n",
    "        # Loop through each weekly plan to aggregate data\n",
    "        for plan in sorted(weekly_plans, key=lambda p: p.get('week', 0)):\n",
    "            # Extract the high-level summary for this week\n",
    "            summary_entry = {\n",
    "                \"week\": plan.get(\"week\"),\n",
    "                \"overall_topic\": plan.get(\"overall_topic\"),\n",
    "                \"slide_summary\": plan.get(\"weekly_slide_summary\"),\n",
    "                \"time_summary_minutes\": plan.get(\"weekly_time_summary_minutes\")\n",
    "            }\n",
    "            master_plan[\"weekly_summaries\"].append(summary_entry)\n",
    "            \n",
    "            # Add this week's totals to the grand totals\n",
    "            slide_summary = plan.get(\"weekly_slide_summary\", {})\n",
    "            time_summary = plan.get(\"weekly_time_summary_minutes\", {})\n",
    "            \n",
    "            grand_totals[\"total_slides_for_unit\"] += slide_summary.get(\"total_slides_for_week\", 0)\n",
    "            grand_totals[\"total_framework_slides\"] += slide_summary.get(\"total_framework_slides\", 0)\n",
    "            grand_totals[\"total_content_slides\"] += slide_summary.get(\"total_content_slides\", 0)\n",
    "            grand_totals[\"total_interactive_slides\"] += slide_summary.get(\"total_interactive_slides\", 0)\n",
    "            grand_totals[\"total_number_of_decks\"] += slide_summary.get(\"number_of_decks\", 0)\n",
    "            grand_totals[\"total_time_for_unit_minutes\"] += time_summary.get(\"total_time_for_week_minutes\", 0)\n",
    "\n",
    "        # --- NEW: Calculate the final derived grand totals after the loop ---\n",
    "        if grand_totals[\"total_time_for_unit_minutes\"] > 0:\n",
    "            grand_totals[\"total_time_for_unit_in_hour\"] = round(grand_totals[\"total_time_for_unit_minutes\"] / 60, 2)\n",
    "\n",
    "        if grand_totals[\"total_number_of_decks\"] > 0:\n",
    "            grand_totals[\"average_deck_time_in_min\"] = round(grand_totals[\"total_time_for_unit_minutes\"] / grand_totals[\"total_number_of_decks\"], 2)\n",
    "\n",
    "        if grand_totals[\"total_number_of_decks\"] > 0:\n",
    "            grand_totals[\"average_deck_time_in_hour\"] = round((grand_totals[\"total_time_for_unit_minutes\"] / grand_totals[\"total_number_of_decks\"]) / 60, 2)\n",
    "\n",
    "        \n",
    "        master_filename = f\"{config.get('course_id', 'UNIT')}_master_plan_unit.json\"\n",
    "        output_path = os.path.join(PLAN_OUTPUT_DIR, master_filename)\n",
    "\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(master_plan, f, indent=2)\n",
    "            logger.info(f\"Successfully generated and saved Master Unit Plan to: {output_path}\")\n",
    "            print(\"\\n--- Preview of Master Plan ---\")\n",
    "            print(json.dumps(master_plan, indent=2))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save Master Unit Plan: {e}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8438e",
   "metadata": {},
   "source": [
    "## Content Generator Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47077d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Content Agent (Corrected and Enhanced for Phase 5 & 6) --- it is ✅ this will be processeed \n",
    "\n",
    "# # Assumes the following are imported and available from previous cells:\n",
    "# # ollama, json, logging, os, Dict, Optional, Any, Chroma, tenacity elements\n",
    "\n",
    "\n",
    "\n",
    "# class ContentAgent:\n",
    "#     \"\"\"\n",
    "#     An agent that performs two main functions:\n",
    "#     1. (Phase 5) Populates a hierarchical plan with raw, reassembled text from a vector store.\n",
    "#     2. (Phase 6) Processes the content-rich plan, using an LLM to generate slide-specific content\n",
    "#                  and reorders all keys for final, clean output.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "#         self.config = master_config['processed_settings']\n",
    "#         self.unit_outline = master_config['unit_outline']\n",
    "#         self.book_toc = master_config['book_toc']\n",
    "#         self.teaching_flows = master_config['teaching_flows']\n",
    "#         self.layaut_slides = master_config['layaut_slides']\n",
    "#         self.vector_store = vector_store\n",
    "#         self.client = ollama.Client(host=OLLAMA_HOST)\n",
    "#         logger.info(\"Data-Driven Content Agent initialized successfully.\")\n",
    "\n",
    "#     # Define the desired key order as a class attribute\n",
    "#         self.key_order = [\n",
    "#             \"title\", \"toc_id\",\n",
    "#             \"chunk_count\",\n",
    "#             \"total_chunks_in_branch\",\n",
    "#             \"budget_slides_content\",\n",
    "#             \"direct_slides_content\",\n",
    "#             \"total_slides_in_branch\",\n",
    "#             \"time_allocation_minutes\",\n",
    "#             \"chunks_sorted\",\n",
    "#             \"content\",\n",
    "#             \"llm_generated_content\",\n",
    "#             \"children\", \n",
    "#             \"interactive_activity\"\n",
    "#         ]\n",
    "        \n",
    "#         logger.info(\"Data-Driven Content Agent initialized successfully.\")\n",
    "\n",
    "#     # --- Key Reordering Logic (REFINED) ---\n",
    "#     def _reorder_keys_recursively(self, node: dict) -> dict:\n",
    "#         \"\"\"\n",
    "#         Recursively traverses a dictionary (a node in the plan) and reorders its keys\n",
    "#         according to a predefined order for maximum readability.\n",
    "#         \"\"\"\n",
    "#         if not isinstance(node, dict):\n",
    "#             return node\n",
    "\n",
    "#         # 1. Recurse first to ensure nested structures are already ordered.\n",
    "#         if 'children' in node and isinstance(node.get('children'), list):\n",
    "#             node['children'] = [self._reorder_keys_recursively(child) for child in node['children']]\n",
    "        \n",
    "#         if 'interactive_activity' in node and isinstance(node.get('interactive_activity'), dict):\n",
    "#             node['interactive_activity'] = self._reorder_keys_recursively(node['interactive_activity'])\n",
    "\n",
    "#         # 2. Build a new dictionary for the current node with the correct key order.\n",
    "#         reordered_node = {}\n",
    "        \n",
    "#         # Add keys that are in our desired order\n",
    "#         for key in self.key_order:\n",
    "#             if key in node:\n",
    "#                 reordered_node[key] = node[key]\n",
    "        \n",
    "#         # Add any remaining keys that were not in the order list (as a fallback)\n",
    "#         for key, value in node.items():\n",
    "#             if key not in reordered_node:\n",
    "#                 reordered_node[key] = value\n",
    "        \n",
    "#         return reordered_node\n",
    "\n",
    "   \n",
    "\n",
    "#     # --- Phase 5: Raw Content Population ---\n",
    "#     def retrieve_content_for_toc_id(self, toc_id: int) -> dict:\n",
    "#         # ... (This method remains unchanged) ...\n",
    "#         if not isinstance(toc_id, int):\n",
    "#             logger.warning(f\"Invalid toc_id: {toc_id}. Must be an integer.\")\n",
    "#             return {\"chunks_sorted\": [], \"content\": \"\"}\n",
    "#         try:\n",
    "#             results = self.vector_store.get(where={\"toc_id\": toc_id}, include=[\"documents\", \"metadatas\"])\n",
    "#             if not results or not results.get('ids'):\n",
    "#                 logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "#                 return {\"chunks_sorted\": [], \"content\": \"\"}\n",
    "#             sorted_items = sorted(zip(results['documents'], results['metadatas']), key=lambda item: item[1].get('chunk_id', 0))\n",
    "#             sorted_docs = [item[0] for item in sorted_items]\n",
    "#             sorted_chunk_ids = [item[1].get('chunk_id') for item in sorted_items]\n",
    "#             reassembled_text = \"\\n\\n\".join(sorted_docs)\n",
    "#             return {\"chunks_sorted\": sorted_chunk_ids, \"content\": reassembled_text}\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"An error occurred during retrieval for toc_id {toc_id}: {e}\", exc_info=True)\n",
    "#             return {\"chunks_sorted\": [], \"content\": \"\"}\n",
    "    \n",
    "#     def populate_content_recursively(self, node: dict):\n",
    "        \n",
    "#         if 'toc_id' in node and 'content' not in node:\n",
    "#             content_data = self.retrieve_content_for_toc_id(node['toc_id'])\n",
    "#             node.update(content_data)\n",
    "#         if 'children' in node and isinstance(node.get('children'), list):\n",
    "#             for child in node['children']:\n",
    "#                 self.populate_content_recursively(child)\n",
    "    \n",
    "#     def generate_content_for_plan(self, final_plan_path: str, output_dir: str) -> bool:\n",
    "        \n",
    "#         logger.info(f\"PHASE 5: Populating raw content for: {final_plan_path}\")\n",
    "#         try:\n",
    "#             with open(final_plan_path, 'r', encoding='utf-8') as f:\n",
    "#                 plan_data = json.load(f)\n",
    "#         except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "#             logger.error(f\"FATAL: Could not read or decode plan file {final_plan_path}. Error: {e}\")\n",
    "#             return False\n",
    "        \n",
    "#         for deck in plan_data.get('deck_plans', []):\n",
    "#             for section in deck.get('sections', []):\n",
    "#                 if section.get('section_type') == 'Content':\n",
    "#                     for content_block in section.get('content_blocks', []):\n",
    "#                         self.populate_content_recursively(content_block)\n",
    "                        \n",
    "#         base_filename = os.path.basename(final_plan_path)\n",
    "#         output_path = os.path.join(output_dir, base_filename)\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "#         logger.info(\"Reordering keys for Fetched clean output...\")\n",
    "#         fetched_ordered_plan = self._reorder_keys_recursively(plan_data)\n",
    "        \n",
    "#         try:\n",
    "#             with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#                 # CORRECTED: Save the 'fetched_ordered_plan' variable\n",
    "#                 json.dump(fetched_ordered_plan, f, indent=2, ensure_ascii=False)\n",
    "#             logger.info(f\"Successfully saved content-enriched plan to: {output_path}\")\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to save the content-enriched plan to {output_path}: {e}\", exc_info=True)\n",
    "#             return False\n",
    "\n",
    "#     # --- Phase 6: LLM Content Generation & Final Formatting ---\n",
    "    \n",
    "    \n",
    "    \n",
    "#     @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "#     def _call_ollama_with_retry(self, prompt: str) -> str:\n",
    "#         logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "#         response = self.client.chat(model=OLLAMA_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], format=\"json\", options={\"temperature\": 0.2})\n",
    "#         if not response or 'message' not in response or not response['message'].get('content'):\n",
    "#             raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "#         return response['message']['content']\n",
    "\n",
    "#     def _parse_llm_json_output(self, content: str) -> Optional[Dict]:\n",
    "#         try:\n",
    "#             match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "#             if not match:\n",
    "#                 logger.warning(\"LLM output did not contain a valid JSON object.\")\n",
    "#                 return None\n",
    "#             return json.loads(match.group(0))\n",
    "#         except (json.JSONDecodeError, TypeError) as e:\n",
    "#             logger.error(f\"Failed to parse JSON from LLM output: {e}\\nRaw content: {content}\")\n",
    "#             return None\n",
    "    \n",
    "\n",
    "#     def _process_node_with_llm_recursively(self, node: dict, flow_prompts: dict):\n",
    "        \n",
    "#         if node.get('content'):\n",
    "#             prompt_template = flow_prompts.get('content_generation')\n",
    "#             if prompt_template:\n",
    "#                 prompt = prompt_template.format(sub_topic=node.get('title', 'Untitled'), context=node.get('content'))\n",
    "#                 try:\n",
    "#                     llm_str = self._call_ollama_with_retry(prompt)\n",
    "#                     node['llm_generated_content'] = self._parse_llm_json_output(llm_str) or {\"title\": node.get('title'), \"content\": [\"Failed to generate content.\"]}\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"LLM call failed for topic '{node.get('title')}': {e}\")\n",
    "#                     node['llm_generated_content'] = {\"title\": node.get('title'), \"content\": [f\"Error during generation: {e}\"]}\n",
    "#         if node.get('interactive_activity') and node.get('content'):\n",
    "#             prompt_template = flow_prompts.get('interactive_activity')\n",
    "#             if prompt_template:\n",
    "#                 prompt = prompt_template.format(sub_topic=node.get('title', 'Untitled'), context=node.get('content'))\n",
    "#                 try:\n",
    "#                     llm_str = self._call_ollama_with_retry(prompt)\n",
    "#                     node['interactive_activity']['llm_generated_content'] = self._parse_llm_json_output(llm_str) or {\"title\": \"Let's Apply This!\", \"content\": [\"Failed to generate activity.\"]}\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"LLM call failed for activity on '{node.get('title')}': {e}\")\n",
    "#                     node['interactive_activity']['llm_generated_content'] = {\"title\": \"Let's Apply This!\", \"content\": [f\"Error during generation: {e}\"]}\n",
    "#         if 'children' in node and isinstance(node.get('children'), list):\n",
    "#             for child in node['children']:\n",
    "#                 self._process_node_with_llm_recursively(child, flow_prompts)\n",
    "\n",
    "\n",
    "#     def generate_llm_content_for_plan(self, content_plan_path: str, llm_output_dir: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Orchestrates the LLM content generation for a content-enriched plan file,\n",
    "#         and finishes by reordering all keys for a clean final output.\n",
    "#         \"\"\"\n",
    "#         logger.info(f\"PHASE 6: Generating LLM content for: {os.path.basename(content_plan_path)}\")\n",
    "#         try:\n",
    "#             with open(content_plan_path, 'r', encoding='utf-8') as f:\n",
    "#                 plan_data = json.load(f)\n",
    "#         except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "#             logger.error(f\"FATAL: Could not read content plan file {content_plan_path}. Error: {e}\")\n",
    "#             return False\n",
    "\n",
    "#         flow_id = self.config.get('teaching_flow_id', 'standard_lecture')\n",
    "#         flow_prompts = self.teaching_flows.get(flow_id, {}).get('prompts', {})\n",
    "#         if not flow_prompts:\n",
    "#             logger.error(f\"Could not find prompts for teaching_flow_id: '{flow_id}'\")\n",
    "#             return False\n",
    "\n",
    "#         # Process each deck in the plan\n",
    "#         for deck in plan_data.get('deck_plans', []):\n",
    "#             content_blocks = []\n",
    "#             for section in deck.get('sections', []):\n",
    "#                 if section.get('section_type') == 'Content':\n",
    "#                     content_blocks = section.get('content_blocks', [])\n",
    "#                     for block in content_blocks:\n",
    "#                         self._process_node_with_llm_recursively(block, flow_prompts)\n",
    "            \n",
    "#             # Generate summary\n",
    "#             summary_prompt_template = flow_prompts.get('summary_generation')\n",
    "#             if summary_prompt_template and content_blocks:\n",
    "#                 topic_titles = [block.get('title', 'Untitled Topic') for block in content_blocks]\n",
    "#                 topic_list_str = \"\\n\".join(f\"- {title}\" for title in topic_titles)\n",
    "#                 prompt = summary_prompt_template.format(topic_list=topic_list_str)\n",
    "#                 try:\n",
    "#                     llm_str = self._call_ollama_with_retry(prompt)\n",
    "#                     for section in deck.get('sections', []):\n",
    "#                         if section.get('section_type') == 'Summary':\n",
    "#                             section['llm_generated_content'] = self._parse_llm_json_output(llm_str)\n",
    "#                             break\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"LLM call failed for deck summary: {e}\")\n",
    "\n",
    "#         # *** FINAL KEY REORDERING STEP ***\n",
    "#         # Apply the reordering to the entire plan data structure before saving\n",
    "#         logger.info(\"Reordering keys for final clean output...\")\n",
    "#         final_ordered_plan = self._reorder_keys_recursively(plan_data)\n",
    "        \n",
    "#         # Save the LLM-enriched and CLEANED plan\n",
    "#         base_filename = os.path.basename(content_plan_path)\n",
    "#         output_path = os.path.join(llm_output_dir, base_filename)\n",
    "#         os.makedirs(llm_output_dir, exist_ok=True)\n",
    "#         try:\n",
    "#             with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#                 json.dump(final_ordered_plan, f, indent=2, ensure_ascii=False)\n",
    "#             logger.info(f\"Successfully saved final LLM-enriched plan to: {output_path}\")\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to save the final LLM-enriched plan to {output_path}: {e}\", exc_info=True)\n",
    "#             return False\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# # test code \n",
    "\n",
    "\n",
    "# content_agent = ContentAgent(master_config, vector_store=vector_store)\n",
    "\n",
    "# content_agent.generate_llm_content_for_plan(content_plan_path, CONTENT_LLM_OUTPUT_DIR):\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa1155",
   "metadata": {},
   "source": [
    "## Presentation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59647664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 10: The Presentation Generation Agent ⚠️✅ (First version )\n",
    "\n",
    "\n",
    "# from pptx import Presentation\n",
    "# from pptx.util import Inches, Pt\n",
    "# from pptx.enum.text import PP_ALIGN\n",
    "# from pptx.enum.shapes import PP_PLACEHOLDER\n",
    "\n",
    "# # --- Helper function to add bullet points safely ---\n",
    "# def add_bullet_points(text_frame, bullet_points):\n",
    "#     \"\"\"Safely adds a list of strings as bullet points to a text frame.\"\"\"\n",
    "#     if not isinstance(bullet_points, list):\n",
    "#         # Handle cases where LLM might return a single string\n",
    "#         bullet_points = [str(bullet_points)]\n",
    "        \n",
    "#     for i, point in enumerate(bullet_points):\n",
    "#         if i == 0:\n",
    "#             # For the first item, replace the default text\n",
    "#             p = text_frame.paragraphs[0]\n",
    "#             p.text = str(point)\n",
    "#         else:\n",
    "#             # For subsequent items, add new paragraphs\n",
    "#             p = text_frame.add_paragraph()\n",
    "#             p.text = str(point)\n",
    "#         p.level = 0 # Set as a top-level bullet\n",
    "\n",
    "# class PresentationAgent:\n",
    "#     \"\"\"\n",
    "#     An agent that generates a styled PowerPoint presentation by dynamically\n",
    "#     selecting the best slide layout from a template based on the content.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, template_path: str):\n",
    "#         self.template_path = template_path\n",
    "#         self.layout_profiles = self._analyze_layouts()\n",
    "#         logger.info(f\"PresentationAgent initialized. Found {len(self.layout_profiles)} usable layouts in template.\")\n",
    "#         # Optional: Print the discovered layouts for debugging\n",
    "#         # for i, profile in self.layout_profiles.items():\n",
    "#         #     logger.debug(f\"Layout Index {i} ('{profile['name']}'): {profile['placeholders']}\")\n",
    "\n",
    "#     def _analyze_layouts(self):\n",
    "#         \"\"\"\n",
    "#         Inspects the template presentation and profiles each slide layout\n",
    "#         to understand what placeholders it contains.\n",
    "#         \"\"\"\n",
    "#         prs = Presentation(self.template_path)\n",
    "#         profiles = {}\n",
    "#         for i, layout in enumerate(prs.slide_layouts):\n",
    "#             placeholders = set()\n",
    "#             has_title = False\n",
    "#             has_body = False\n",
    "            \n",
    "#             for shape in layout.placeholders:\n",
    "#                 if shape.placeholder_format.type in (PP_PLACEHOLDER.TITLE, PP_PLACEHOLDER.CENTER_TITLE):\n",
    "#                     has_title = True\n",
    "#                 elif shape.placeholder_format.type in (PP_PLACEHOLDER.BODY, PP_PLACEHOLDER.OBJECT):\n",
    "#                     has_body = True\n",
    "            \n",
    "#             # Create a simple profile\n",
    "#             if has_title and not has_body:\n",
    "#                 placeholders.add('title_only')\n",
    "#             if has_title and has_body:\n",
    "#                 placeholders.add('title_and_content')\n",
    "            \n",
    "#             # Only add layouts that are useful\n",
    "#             if placeholders:\n",
    "#                  profiles[i] = {'name': layout.name, 'placeholders': placeholders}\n",
    "#         return profiles\n",
    "        \n",
    "#     def _find_best_layout(self, has_title: bool, has_body: bool):\n",
    "#         \"\"\"\n",
    "#         Finds the best layout index from the profiles based on content requirements.\n",
    "#         \"\"\"\n",
    "#         # Perfect match: Title and Content\n",
    "#         if has_title and has_body:\n",
    "#             for i, profile in self.layout_profiles.items():\n",
    "#                 if 'title_and_content' in profile['placeholders']:\n",
    "#                     logger.debug(f\"Chose layout '{profile['name']}' for title and content.\")\n",
    "#                     return i\n",
    "        \n",
    "#         # Match for Title Only slides\n",
    "#         if has_title and not has_body:\n",
    "#             for i, profile in self.layout_profiles.items():\n",
    "#                 if 'title_only' in profile['placeholders']:\n",
    "#                     logger.debug(f\"Chose layout '{profile['name']}' for title only.\")\n",
    "#                     return i\n",
    "        \n",
    "#         # Fallback logic\n",
    "#         logger.warning(\"No perfect layout match found. Falling back.\")\n",
    "#         if self.layout_profiles:\n",
    "#             return next(iter(self.layout_profiles)) # Return the first available layout\n",
    "#         return 0 # Absolute fallback\n",
    "\n",
    "#     def create_presentation_from_plan(self, llm_plan_path: str, output_dir: str):\n",
    "#         # ... (This method is mostly the same, just the setup changes) ...\n",
    "#         # ... it will call the new _add_slide_for_section ...\n",
    "#         logger.info(f\"PHASE 7: Creating presentation for: {os.path.basename(llm_plan_path)}\")\n",
    "#         try:\n",
    "#             with open(llm_plan_path, 'r', encoding='utf-8') as f:\n",
    "#                 plan_data = json.load(f)\n",
    "#         except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "#             logger.error(f\"FATAL: Could not read LLM plan file {llm_plan_path}. Error: {e}\")\n",
    "#             return\n",
    "            \n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#         for deck in plan_data.get('deck_plans', []):\n",
    "#             prs = Presentation(self.template_path)\n",
    "#             # Clear existing slides\n",
    "#             if len(prs.slides) > 0:\n",
    "#                 for i in range(len(prs.slides) - 1, -1, -1):\n",
    "#                     rId = prs.slides._sldIdLst[i].rId\n",
    "#                     prs.part.drop_rel(rId)\n",
    "#                     del prs.slides._sldIdLst[i]\n",
    "\n",
    "#             for section in deck.get('sections', []):\n",
    "#                 self._add_slide_for_section(prs, section)\n",
    "\n",
    "#             output_filename = f\"{plan_data.get('unit_code', 'UNIT')}_Week{plan_data.get('week')}_Lecture{deck.get('deck_number')}.pptx\"\n",
    "#             output_path = os.path.join(output_dir, output_filename)\n",
    "#             prs.save(output_path)\n",
    "#             logger.info(f\"Successfully created presentation: {output_path}\")\n",
    "\n",
    "\n",
    "#     def _add_slide_for_section(self, prs: Presentation, section: dict):\n",
    "#         \"\"\"Dynamically adds slides based on analyzing the section's content.\"\"\"\n",
    "#         section_type = section.get('section_type')\n",
    "        \n",
    "#         # The \"Content\" type is special, as it contains a hierarchy of slides.\n",
    "#         # We delegate it to its own recursive handler.\n",
    "#         if section_type == 'Content':\n",
    "#             for block in section.get('content_blocks', []):\n",
    "#                 self._add_content_slides_recursively(prs, block)\n",
    "#             return\n",
    "\n",
    "#         # For all other, simpler slide types:\n",
    "#         title_text = \"\"\n",
    "#         body_text = [] # Use a list to represent bullet points\n",
    "        \n",
    "#         # 1. Analyze the content of the section to determine its shape\n",
    "#         if section_type == 'Title':\n",
    "#             content = section.get('content', {})\n",
    "#             title_text = content.get('deck_title', 'Untitled Lecture')\n",
    "#             body_text = [\n",
    "#                 f\"{content.get('unit_name', 'Course')} ({content.get('unit_code')})\",\n",
    "#                 content.get('week_topic', '')\n",
    "#             ]\n",
    "#         elif section_type == 'Agenda':\n",
    "#             content = section.get('content', {})\n",
    "#             title_text = content.get('title', 'Agenda')\n",
    "#             body_text = content.get('items', [])\n",
    "#         elif section_type == 'Summary':\n",
    "#             llm_content = section.get('llm_generated_content', {})\n",
    "#             title_text = llm_content.get('title', 'Summary')\n",
    "#             body_text = llm_content.get('content', [])\n",
    "#         elif section_type == 'End':\n",
    "#             content = section.get('content', {})\n",
    "#             title_text = content.get('title', 'Thank You')\n",
    "#             # For this simple slide, we can put \"Questions?\" in the title\n",
    "#             # and leave the body empty.\n",
    "            \n",
    "#         # 2. Find the best layout based on the content's shape\n",
    "#         has_title = bool(title_text)\n",
    "#         has_body = bool(body_text and any(body_text)) # Body exists if list is not empty\n",
    "        \n",
    "#         layout_index = self._find_best_layout(has_title, has_body)\n",
    "#         slide = prs.slides.add_slide(prs.slide_layouts[layout_index])\n",
    "\n",
    "#         # 3. Populate the chosen slide\n",
    "#         if has_title and slide.shapes.title:\n",
    "#             slide.shapes.title.text = title_text\n",
    "        \n",
    "#         if has_body:\n",
    "#             content_placeholder = None\n",
    "#             for shape in slide.placeholders:\n",
    "#                 if shape.placeholder_format.type in (PP_PLACEHOLDER.BODY, PP_PLACEHOLDER.OBJECT):\n",
    "#                     content_placeholder = shape\n",
    "#                     break\n",
    "#             if content_placeholder:\n",
    "#                 add_bullet_points(content_placeholder.text_frame, body_text)\n",
    "\n",
    "#     def _add_content_slides_recursively(self, prs: Presentation, node: dict):\n",
    "#         \"\"\"Recursively adds slides for content, choosing layouts dynamically.\"\"\"\n",
    "#         # 1. Add the main content slide for this node\n",
    "#         llm_content = node.get('llm_generated_content')\n",
    "#         if llm_content:\n",
    "#             title_text = llm_content.get('title', 'Content')\n",
    "#             body_text = llm_content.get('content', [])\n",
    "\n",
    "#             layout_index = self._find_best_layout(bool(title_text), bool(body_text))\n",
    "#             slide = prs.slides.add_slide(prs.slide_layouts[layout_index])\n",
    "            \n",
    "#             if slide.shapes.title:\n",
    "#                 slide.shapes.title.text = title_text\n",
    "            \n",
    "#             content_placeholder = None\n",
    "#             for shape in slide.placeholders:\n",
    "#                 if shape.placeholder_format.idx == 1: content_placeholder = shape; break\n",
    "            \n",
    "#             if content_placeholder:\n",
    "#                 add_bullet_points(content_placeholder.text_frame, body_text)\n",
    "        \n",
    "#         # 2. Add a slide for the interactive activity\n",
    "#         activity = node.get('interactive_activity', {}).get('llm_generated_content')\n",
    "#         if activity:\n",
    "#             title_text = activity.get('title', \"Let's Apply This!\")\n",
    "#             body_text = activity.get('content', [])\n",
    "\n",
    "#             layout_index = self._find_best_layout(bool(title_text), bool(body_text))\n",
    "#             slide = prs.slides.add_slide(prs.slide_layouts[layout_index])\n",
    "\n",
    "#             if slide.shapes.title: slide.shapes.title.text = title_text\n",
    "#             content_placeholder = None\n",
    "#             for shape in slide.placeholders:\n",
    "#                 if shape.placeholder_format.idx == 1: content_placeholder = shape; break\n",
    "#             if content_placeholder:\n",
    "#                 add_bullet_points(content_placeholder.text_frame, body_text)\n",
    "        \n",
    "#         # 3. Recurse for children\n",
    "#         for child_node in node.get('children', []):\n",
    "#             self._add_content_slides_recursively(prs, child_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f14477",
   "metadata": {},
   "source": [
    "#### test Presenter\n",
    "\n",
    "we need to ensure the presenter seq_id flow: Parent -> Child 1 -> Child 1's Activity -> Child 2 -> Child 2's Activity -> Parent's Activity.\n",
    "\n",
    "https://aistudio.google.com/prompts/18YaU5pG96eFMbM1l6yeG1v9j63PkLc5H "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER\n",
    "\n",
    "# --- Basic Setup for a Standalone Script ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "# --- Configuration Paths (ADJUST THESE TO MATCH YOUR PROJECT STRUCTURE) ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# New configuration file paths\n",
    "CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEST_DIR = os.path.join(PROJECT_BASE_DIR, \"tests\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"test_output\")\n",
    "\n",
    "\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- File Paths ---\n",
    "LAYOUT_MAPPING_PATH = os.path.join(CONFIG_DIR, \"layout_mapping_test_Mod.json\")\n",
    "# USE THE REALISTIC TEST FILE\n",
    "TEST_CONTENT_PATH = os.path.join(TEST_DIR, \"test_content_fill_slides_after_llm.json\")\n",
    "# UPDATE THIS PATH TO YOUR TEMPLATE\n",
    "SLIDE_TEMPLATE_PATH = os.path.join(DATA_DIR, \"slide_style/slide_style_test - Copy.pptx\") \n",
    "\n",
    "# --- The PresentationAgent and its helper (copied directly from our design) ---\n",
    "\n",
    "def add_formatted_text(text_frame, content):\n",
    "    \"\"\"Adds formatted text to a text frame, handling strings, lists, and dicts.\"\"\"\n",
    "    text_frame.clear()\n",
    "    \n",
    "    # Handle dictionary structure for titled bullet points\n",
    "    if isinstance(content, dict):\n",
    "        if 'title' in content:\n",
    "            p = text_frame.add_paragraph()\n",
    "            p.text = str(content['title']); p.font.bold = True; p.level = 0\n",
    "        points = content.get('content', content.get('bullets', []))\n",
    "        if isinstance(points, list):\n",
    "            for point in points:\n",
    "                p = text_frame.add_paragraph(); p.text = str(point); p.level = 1\n",
    "    # Handle a list of bullet points\n",
    "    elif isinstance(content, list):\n",
    "        for point in content:\n",
    "            p = text_frame.add_paragraph(); p.text = str(point); p.level = 0\n",
    "    # Handle a simple string\n",
    "    else:\n",
    "        p = text_frame.add_paragraph(); p.text = str(content); p.level = 0\n",
    "        \n",
    "    if text_frame.paragraphs and not text_frame.paragraphs[0].text:\n",
    "        p = text_frame.paragraphs[0]\n",
    "        p._p.getparent().remove(p._p)\n",
    "\n",
    "class PresentationAgent:\n",
    "    \"\"\"An agent that generates a styled PowerPoint presentation from a plan.\"\"\"\n",
    "    def __init__(self, template_path: str, layout_config_path: str):\n",
    "        self.template_path = template_path\n",
    "        self.prs_for_layouts = Presentation(self.template_path)\n",
    "        try:\n",
    "            with open(layout_config_path, 'r', encoding='utf-8') as f:\n",
    "                self.user_selections = json.load(f)['user_selections']\n",
    "            logger.info(f\"PresentationAgent initialized with layout selections from: {layout_config_path}\")\n",
    "        except (FileNotFoundError, KeyError) as e:\n",
    "            logger.error(f\"FATAL: Could not load layout config at '{layout_config_path}'. Error: {e}\")\n",
    "            raise SystemExit(\"Layout configuration not found.\")\n",
    "\n",
    "    def _get_layout(self, layout_key: str):\n",
    "        \"\"\"Finds a slide layout by its index, as chosen by the user in the config.\"\"\"\n",
    "        selection = self.user_selections.get(layout_key) or self.user_selections['Content']\n",
    "        layout_index = selection.get('selected_layout_index')\n",
    "        if not isinstance(layout_index, int) or not (0 <= layout_index < len(self.prs_for_layouts.slide_layouts)):\n",
    "            logger.warning(f\"Invalid index '{layout_index}' for key '{layout_key}'. Falling back to layout 1.\")\n",
    "            return self.prs_for_layouts.slide_layouts[1]\n",
    "        return self.prs_for_layouts.slide_layouts[layout_index]\n",
    "\n",
    "    def create_presentation_from_plan(self, plan_json_path: str, output_dir: str):\n",
    "        \"\"\"Orchestrates the creation of a PowerPoint file from a single plan JSON.\"\"\"\n",
    "        logger.info(f\"Creating presentation from: {os.path.basename(plan_json_path)}\")\n",
    "        try:\n",
    "            with open(plan_json_path, 'r', encoding='utf-8') as f:\n",
    "                plan_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"FATAL: Could not read plan file {plan_json_path}. Error: {e}\"); return\n",
    "\n",
    "        for deck in plan_data.get('deck_plans', []):\n",
    "            prs = Presentation(self.template_path)\n",
    "            if len(prs.slides) > 0:\n",
    "                for i in range(len(prs.slides) - 1, -1, -1):\n",
    "                    rId = prs.slides._sldIdLst[i].rId; prs.part.drop_rel(rId); del prs.slides._sldIdLst[i]\n",
    "\n",
    "            # Collect all slide-generating items and sort by seq_id\n",
    "            slide_items = self._collect_slide_items(deck)\n",
    "            sorted_slide_items = sorted(slide_items, key=lambda x: x.get('seq_id', 999))\n",
    "            \n",
    "            for item in sorted_slide_items:\n",
    "                self._add_slide_for_item(prs, item)\n",
    "\n",
    "            output_filename = \"Layout_Test_Output_Realistic.pptx\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            prs.save(output_path)\n",
    "            logger.info(f\"Successfully created test presentation: {output_path}\")\n",
    "\n",
    "    def _collect_slide_items(self, deck):\n",
    "        \"\"\"Flattens the deck structure into a single list of items to be rendered as slides.\"\"\"\n",
    "        items = []\n",
    "        for section in deck.get('sections', []):\n",
    "            if 'content_blocks' in section:\n",
    "                for block in section['content_blocks']:\n",
    "                    self._flatten_content_block(block, items)\n",
    "            else:\n",
    "                # Simple sections are added directly\n",
    "                items.append({'item_type': section['section_type'], 'data': section})\n",
    "        return items\n",
    "\n",
    "    def _flatten_content_block(self, node, items):\n",
    "        \"\"\"Recursively flattens a content block and its children into a list.\"\"\"\n",
    "        # Add the parent node itself\n",
    "        items.append({'item_type': 'Content', 'data': node})\n",
    "        # Recursively add all children\n",
    "        for child in node.get('children', []):\n",
    "            self._flatten_content_block(child, items)\n",
    "        # Add the parent's interactive activity last\n",
    "        if 'interactive_activity' in node:\n",
    "            items.append({'item_type': 'Application', 'data': node['interactive_activity']})\n",
    "\n",
    "    def _add_slide_for_item(self, prs: Presentation, item: dict):\n",
    "        \"\"\"The main rendering function that creates a single slide for any given item.\"\"\"\n",
    "        item_type = item['item_type']\n",
    "        data = item['data']\n",
    "        \n",
    "        # Determine the correct layout key from the item type\n",
    "        # This handles the case where Application slides might use a different layout\n",
    "        layout_key = item_type\n",
    "        if item_type == \"Application\":\n",
    "            # Check if there's a specific layout for two-column applications\n",
    "            if 'columns' in data.get('llm_generated_content', {}):\n",
    "                layout_key = \"Application_Two_Column\"\n",
    "        \n",
    "        slide_layout = self._get_layout(layout_key)\n",
    "        slide = prs.slides.add_slide(slide_layout)\n",
    "        \n",
    "        # CORRECTED WAY TO CHECK FOR A TITLE\n",
    "        try:\n",
    "            title_ph = slide.shapes.title\n",
    "        except AttributeError:\n",
    "            title_ph = None # The layout has no title placeholder\n",
    "\n",
    "        if item_type == 'Title':\n",
    "            content = data.get('content', {})\n",
    "            if title_ph: title_ph.text = content.get('deck_title', 'Untitled')\n",
    "            for shape in slide.placeholders:\n",
    "                if shape.placeholder_format.type == PP_PLACEHOLDER.SUBTITLE:\n",
    "                    shape.text = f\"{content.get('unit_name')}\\n{content.get('week_topic')}\"; break\n",
    "        \n",
    "        elif item_type == 'Agenda':\n",
    "            content = data.get('content', {})\n",
    "            if title_ph: title_ph.text = content.get('title', 'Agenda')\n",
    "            for shape in slide.placeholders:\n",
    "                if shape.placeholder_format.type in (PP_PLACEHOLDER.BODY, PP_PLACEHOLDER.OBJECT):\n",
    "                    add_formatted_text(shape.text_frame, content.get('items', [])); break\n",
    "        \n",
    "        elif item_type in ['Content', 'Application']:\n",
    "            content = data.get('llm_generated_content', {})\n",
    "            if title_ph: title_ph.text = content.get('title', item_type)\n",
    "            \n",
    "            body_placeholders = sorted([p for p in slide.placeholders if p.placeholder_format.type in (PP_PLACEHOLDER.BODY, PP_PLACEHOLDER.OBJECT)], key=lambda p: p.left)\n",
    "\n",
    "            if 'columns' in content and len(body_placeholders) >= 2:\n",
    "                add_formatted_text(body_placeholders[0].text_frame, content['columns'][0])\n",
    "                add_formatted_text(body_placeholders[1].text_frame, content['columns'][1])\n",
    "            elif 'content' in content and body_placeholders:\n",
    "                add_formatted_text(body_placeholders[0].text_frame, content['content'])\n",
    "        \n",
    "        elif item_type == 'Summary':\n",
    "            content = data.get('llm_generated_content', {})\n",
    "            if title_ph: title_ph.text = content.get('title', 'Summary')\n",
    "            for shape in slide.placeholders:\n",
    "                if shape.placeholder_format.type in (PP_PLACEHOLDER.BODY, PP_PLACEHOLDER.OBJECT):\n",
    "                    add_formatted_text(shape.text_frame, content.get('content', [])); break\n",
    "        \n",
    "        elif item_type == 'Divider':\n",
    "             if title_ph: title_ph.text = data.get('content', {}).get('title', 'Section')\n",
    "\n",
    "        elif item_type == 'End':\n",
    "            content = data.get('content', {})\n",
    "            if title_ph: title_ph.text = content.get('title', 'Thank You')\n",
    "            for shape in slide.placeholders:\n",
    "                if shape.placeholder_format.type in (PP_PLACEHOLDER.BODY, PP_PLACEHOLDER.SUBTITLE):\n",
    "                    shape.text = content.get('text', 'Questions?'); break\n",
    "\n",
    "\n",
    "# --- Main Execution Block for Testing ---\n",
    "if __name__ == \"__main__\":\n",
    "    print_header(\"Presentation Agent Test Runner (Realistic Content)\", char=\"*\")\n",
    "\n",
    "    if not all(os.path.exists(p) for p in [SLIDE_TEMPLATE_PATH, LAYOUT_MAPPING_PATH, TEST_CONTENT_PATH]):\n",
    "        logger.error(\"CRITICAL: One or more required files (template, layout config, or test content) not found. Please check paths.\")\n",
    "    else:\n",
    "        try:\n",
    "            presentation_agent = PresentationAgent(\n",
    "                template_path=SLIDE_TEMPLATE_PATH,\n",
    "                layout_config_path=LAYOUT_MAPPING_PATH\n",
    "            )\n",
    "            presentation_agent.create_presentation_from_plan(\n",
    "                plan_json_path=TEST_CONTENT_PATH,\n",
    "                output_dir=OUTPUT_DIR\n",
    "            )\n",
    "            logger.info(\"Test script finished successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during the test run: {e}\", exc_info=True)    ### this use Recursion ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904f8ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 04:17:13,976 - INFO - Agent initialized with template 'slide_style_test_2.pptx'\n",
      "2025-07-13 04:17:13,977 - INFO - Creating presentation from plan: 'test_content_fill_slides_after_llm_mod.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Presentation Agent Test Runner (New Structured Format) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 04:17:14,186 - INFO - Successfully created presentation: '/home/sebas_dev_linux/projects/course_generator/test_output/Generated_Presentation_v2.pptx'\n",
      "2025-07-13 04:17:14,187 - INFO - --- Test script finished successfully. ---\n"
     ]
    }
   ],
   "source": [
    "#last ⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️⚠️\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER\n",
    "from pptx.enum.text import MSO_AUTO_SIZE\n",
    "\n",
    "# --- Basic Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper Functions (Unchanged) ---\n",
    "def _render_bullet_points(text_frame, data):\n",
    "    text_frame.clear(); text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    def add_points(points, level):\n",
    "        for point in points:\n",
    "            if isinstance(point, dict):\n",
    "                p = text_frame.add_paragraph(); p.text = point.get('text', ''); p.level = level\n",
    "                if 'children' in point: add_points(point['children'], level + 1)\n",
    "            else:\n",
    "                p = text_frame.add_paragraph(); p.text = str(point); p.level = level\n",
    "    add_points(data, 0)\n",
    "    if text_frame.paragraphs and not text_frame.paragraphs[0].text:\n",
    "        p = text_frame.paragraphs[0]; p._p.getparent().remove(p._p)\n",
    "\n",
    "def _render_table(slide, placeholder, data):\n",
    "    headers, rows_data = data.get('headers', []), data.get('rows', [])\n",
    "    if not headers or not rows_data: return\n",
    "    num_rows, num_cols = len(rows_data) + 1, len(headers)\n",
    "    table_shape = slide.shapes.add_table(num_rows, num_cols, placeholder.left, placeholder.top, placeholder.width, placeholder.height)\n",
    "    table = table_shape.table\n",
    "    for i, header in enumerate(headers):\n",
    "        cell = table.cell(0, i); cell.text = header\n",
    "        cell.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE; cell.text_frame.paragraphs[0].font.bold = True\n",
    "    for r_idx, row_data in enumerate(rows_data):\n",
    "        for c_idx, cell_text in enumerate(row_data):\n",
    "            cell = table.cell(r_idx + 1, c_idx); cell.text = str(cell_text)\n",
    "            cell.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    sp = placeholder.element; sp.getparent().remove(sp)\n",
    "\n",
    "def _render_multiple_choice_question(slide, placeholders, data):\n",
    "    main_ph = next((p for p in placeholders if p.placeholder_format.type == PP_PLACEHOLDER.OBJECT), None)\n",
    "    answer_ph = next((p for p in placeholders if p.placeholder_format.type == PP_PLACEHOLDER.BODY), None)\n",
    "    if not main_ph: return\n",
    "    tf = main_ph.text_frame; tf.clear(); tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    p = tf.add_paragraph(); p.text = data.get('question_text', ''); p.font.bold = True; p.level = 0\n",
    "    for option in data.get('options', []):\n",
    "        p = tf.add_paragraph(); p.text = f\"{option.get('label', '')}) {option.get('text', '')}\"; p.level = 1\n",
    "    if answer_ph:\n",
    "        answer_ph.text_frame.clear()\n",
    "        explanation = data.get('correct_answer', {}).get('explanation', '')\n",
    "        answer_ph.text_frame.text = f\"Answer: {data.get('correct_answer', {}).get('label', '')}. {explanation}\"\n",
    "\n",
    "def _render_matching_activity(slide, placeholders, data):\n",
    "    object_placeholders = [p for p in placeholders if p.placeholder_format.type == PP_PLACEHOLDER.OBJECT]\n",
    "    if len(object_placeholders) < 2: return\n",
    "    terms_ph, defs_ph = object_placeholders[0], object_placeholders[1]\n",
    "    terms_tf, defs_tf = terms_ph.text_frame, defs_ph.text_frame\n",
    "    terms_tf.clear(); defs_tf.clear(); terms_tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE; defs_tf.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "    pairs = data.get('pairs', [])\n",
    "    if not pairs: return\n",
    "    terms = [f\"{i+1}. {p['term']}\" for i, p in enumerate(pairs)]\n",
    "    definitions = [p['definition'] for p in pairs]\n",
    "    random.shuffle(definitions)\n",
    "    for term in terms: p = terms_tf.add_paragraph(); p.text = term; p.level = 0\n",
    "    for i, definition in enumerate(definitions): p = defs_tf.add_paragraph(); p.text = f\"{chr(65+i)}. {definition}\"; p.level = 0\n",
    "\n",
    "class PresentationAgent:\n",
    "    def __init__(self, template_path: str, layout_config_path: str):\n",
    "        self.template_path = template_path\n",
    "        self.prs_for_layouts = Presentation(template_path)\n",
    "        with open(layout_config_path, 'r', encoding='utf-8') as f:\n",
    "            self.user_selections = json.load(f)['user_selections']\n",
    "        logger.info(f\"Agent initialized with template '{os.path.basename(template_path)}'\")\n",
    "\n",
    "    def _get_layout(self, layout_key: str):\n",
    "        selection = self.user_selections.get(layout_key, self.user_selections['Content'])\n",
    "        layout_index = selection['selected_layout_index']\n",
    "        return self.prs_for_layouts.slide_layouts[layout_index]\n",
    "        \n",
    "    def _determine_layout_key(self, item_data):\n",
    "        llm_content = item_data.get('llm_generated_content', {})\n",
    "        objects = llm_content.get('objects', [])\n",
    "        has_subtitle = bool(llm_content.get('subtitle'))\n",
    "        \n",
    "        # <<< NEW LOGIC: Determine layout based on subtitles and object count\n",
    "        if objects and objects[0]['content_type'] in ['multiple_choice_question', 'matching_activity']:\n",
    "            return \"Application_Two_Column\" if len(objects) > 1 or objects[0]['content_type'] == 'matching_activity' else \"Application\"\n",
    "        \n",
    "        if len(objects) >= 2:\n",
    "            return \"Content_Two_Column_child\" if has_subtitle else \"Content_Two_Column\"\n",
    "        else:\n",
    "            return \"Content_child\" if has_subtitle else \"Content\"\n",
    "\n",
    "    def create_presentation_from_plan(self, plan_json_path: str, output_path: str):\n",
    "        logger.info(f\"Creating presentation from plan: '{os.path.basename(plan_json_path)}'\")\n",
    "        with open(plan_json_path, 'r', encoding='utf-8') as f: plan_data = json.load(f)\n",
    "        prs = Presentation(self.template_path)\n",
    "        while len(prs.slides):\n",
    "            rId = prs.slides._sldIdLst[0].rId; prs.part.drop_rel(rId); del prs.slides._sldIdLst[0]\n",
    "        slide_items = self._collect_slide_items(plan_data)\n",
    "        for item in sorted(slide_items, key=lambda x: x.get('seq_id', 999)): self._add_slide_for_item(prs, item)\n",
    "        prs.save(output_path)\n",
    "        logger.info(f\"Successfully created presentation: '{output_path}'\")\n",
    "\n",
    "    def _collect_slide_items(self, plan_data):\n",
    "        items = []\n",
    "        for deck in plan_data.get('deck_plans', []):\n",
    "            for section in deck.get('sections', []):\n",
    "                if section.get('section_type') == 'Content':\n",
    "                    for block in section.get('content_blocks', []):\n",
    "                        for slide_item in block.get('slides', []):\n",
    "                            items.append({'item_type': 'Content', 'data': slide_item})\n",
    "                else:\n",
    "                    items.append({'item_type': section['section_type'], 'data': section})\n",
    "        return items\n",
    "\n",
    "    def _add_slide_for_item(self, prs: Presentation, item: dict):\n",
    "        item_type = item['item_type']\n",
    "        layout_key = self._determine_layout_key(item['data']) if item_type == 'Content' else item_type\n",
    "        slide = prs.slides.add_slide(self._get_layout(layout_key))\n",
    "        render_map = {\n",
    "            'Title': self._render_title_slide, 'Agenda': self._render_agenda_slide,\n",
    "            'Summary': self._render_summary_slide, 'End': self._render_end_slide,\n",
    "            'Divider': self._render_divider_slide, 'Content': self._render_content_slide\n",
    "        }\n",
    "        if item_type in render_map:\n",
    "            render_map[item_type](slide, item['data'])\n",
    "\n",
    "    def _render_title_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        if slide.shapes.title:\n",
    "            slide.shapes.title.text = content.get('week_topic', 'Untitled Topic')\n",
    "        subtitle_ph = next((p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER.SUBTITLE), None)\n",
    "        if subtitle_ph:\n",
    "            subtitle_ph.text = f\"{content.get('deck_title', '')}\\n{content.get('unit_name', '')} - {content.get('unit_code', '')}\"\n",
    "\n",
    "    def _render_agenda_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        if slide.shapes.title: slide.shapes.title.text = content.get('title', 'Agenda')\n",
    "        body_ph = next((p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER.OBJECT), None)\n",
    "        if body_ph: _render_bullet_points(body_ph.text_frame, content.get('items', []))\n",
    "\n",
    "    def _render_summary_slide(self, slide, data):\n",
    "        if slide.shapes.title: slide.shapes.title.text = \"Summary & Key Takeaways\"\n",
    "        if 'llm_generated_content' in data: self._render_content_slide(slide, data)\n",
    "\n",
    "    def _render_end_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        if slide.shapes.title: slide.shapes.title.text = content.get('text', 'Questions?')\n",
    "\n",
    "    def _render_divider_slide(self, slide, data):\n",
    "        content = data.get('content', {})\n",
    "        if slide.shapes.title: slide.shapes.title.text = content.get('title', 'Section Divider')\n",
    "\n",
    "    def _render_content_slide(self, slide, data):\n",
    "        content = data.get('llm_generated_content', {})\n",
    "        \n",
    "        # <<< NEW LOGIC: Populate title and subtitle in separate placeholders\n",
    "        if slide.shapes.title:\n",
    "            slide.shapes.title.text = content.get('title', '')\n",
    "            slide.shapes.title.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "        \n",
    "        subtitle_ph = next((p for p in slide.placeholders if p.placeholder_format.type == PP_PLACEHOLDER.SUBTITLE), None)\n",
    "        if subtitle_ph:\n",
    "            subtitle_ph.text = content.get('subtitle', '')\n",
    "            subtitle_ph.text_frame.auto_size = MSO_AUTO_SIZE.TEXT_TO_FIT_SHAPE\n",
    "        \n",
    "        body_placeholders = sorted([p for p in slide.placeholders if p.placeholder_format.type not in [PP_PLACEHOLDER.TITLE, PP_PLACEHOLDER.SUBTITLE]], key=lambda p: p.left)\n",
    "        objects = content.get('objects', [])\n",
    "        \n",
    "        for obj, placeholder in zip(objects, body_placeholders):\n",
    "            content_type, obj_data = obj.get('content_type'), obj.get('data')\n",
    "            renderers = {\n",
    "                'bullet_points': lambda: _render_bullet_points(placeholder.text_frame, obj_data),\n",
    "                'table': lambda: _render_table(slide, placeholder, obj_data),\n",
    "                'multiple_choice_question': lambda: _render_multiple_choice_question(slide, body_placeholders, obj_data),\n",
    "                'matching_activity': lambda: _render_matching_activity(slide, body_placeholders, obj_data)\n",
    "            }\n",
    "            if content_type in renderers:\n",
    "                renderers[content_type]()\n",
    "                if content_type in ['multiple_choice_question', 'matching_activity']: break\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define project paths\n",
    "    PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "    CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "    DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "    TEST_DIR = os.path.join(PROJECT_BASE_DIR, \"tests\")\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"test_output\")\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_DIR, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- IMPORTANT ---\n",
    "    # The script now expects the NEW JSON format. \n",
    "    # Please ensure you have the 'final_presentation_plan.json' (the simulated output)\n",
    "    # and the 'layout_mapping_test_Mod.json' in the correct paths.\n",
    "    \n",
    "    LAYOUT_MAPPING_PATH = os.path.join(CONFIG_DIR, \"layout_mapping_test_Mod.json\")\n",
    "    # USE THE NEW, FULLY-STRUCTURED, SIMULATED JSON FILE\n",
    "    TEST_CONTENT_PATH = os.path.join(TEST_DIR, \"test_content_fill_slides_after_llm_mod.json\") \n",
    "    SLIDE_TEMPLATE_PATH = os.path.join(DATA_DIR, \"slide_style/slide_style_test_2.pptx\")\n",
    "    \n",
    "    OUTPUT_PPTX_PATH = os.path.join(OUTPUT_DIR, \"Generated_Presentation_v2.pptx\")\n",
    "\n",
    "    print(\"--- Presentation Agent Test Runner (New Structured Format) ---\")\n",
    "\n",
    "    # Check for required files\n",
    "    required_files = [SLIDE_TEMPLATE_PATH, LAYOUT_MAPPING_PATH, TEST_CONTENT_PATH]\n",
    "    if not all(os.path.exists(p) for p in required_files):\n",
    "        logger.error(\"CRITICAL: One or more required files not found. Please check paths:\")\n",
    "        logger.error(f\"  Template: {SLIDE_TEMPLATE_PATH} {'(Found)' if os.path.exists(SLIDE_TEMPLATE_PATH) else '(MISSING)'}\")\n",
    "        logger.error(f\"  Layout Map: {LAYOUT_MAPPING_PATH} {'(Found)' if os.path.exists(LAYOUT_MAPPING_PATH) else '(MISSING)'}\")\n",
    "        logger.error(f\"  Content Plan: {TEST_CONTENT_PATH} {'(Found)' if os.path.exists(TEST_CONTENT_PATH) else '(MISSING)'}\")\n",
    "    else:\n",
    "        try:\n",
    "            presentation_agent = PresentationAgent(\n",
    "                template_path=SLIDE_TEMPLATE_PATH,\n",
    "                layout_config_path=LAYOUT_MAPPING_PATH\n",
    "            )\n",
    "            presentation_agent.create_presentation_from_plan(\n",
    "                plan_json_path=TEST_CONTENT_PATH,\n",
    "                output_path=OUTPUT_PPTX_PATH\n",
    "            )\n",
    "            logger.info(\"--- Test script finished successfully. ---\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred during the test run: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d8be7",
   "metadata": {},
   "source": [
    "## Orquestrator (Addressing pain points )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459d53b",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "The main script that iterates through the weeks defined the plan and generate the content base on the settings_deck coordinating the agents.\n",
    "\n",
    "**Parameters and concideration**\n",
    "- 1 hour in the setting session_time_duration_in_hour - is 18-20 slides at the time so it is require to calculate this according to the given value but this also means per session so sessions_per_week is a multiplicator factor that   \n",
    "- if apply_topic_interactive is available will add an extra slide and add extra 5 min time but to determine this is required to plan all the content first and then calculate then provide a extra time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea092bd6",
   "metadata": {},
   "source": [
    "settings_deck.json\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"teaching_flow_id\": \"Standard Lecture Flow\",\n",
    "  \"parameters_slides\": {\n",
    "    \"slides_per_hour\": 18,\n",
    "    \"time_per_content_slides_min\": 3,\n",
    "    \"time_per_interactive_slide_min\": 5,\n",
    "    \"time_for_framework_slides_min\": 6\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"session_time_duration_in_hour\": 2,\n",
    "    \"interactive_time_in_hour\": 0,\n",
    "    \"total_session_time_in_hours\": 0\n",
    "  },\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"target_total_slides\": 0,\n",
    "    \"slides_content_per_session\": 0,\n",
    "    \"interactive_slides_per_week\": 0,\n",
    "    \"interactive_slides_per_session\": 0,\n",
    "    \"total_slides_deck_week\": 0,\n",
    "    \"total_slides_session\": 0    \n",
    "  },  \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [1]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82e170",
   "metadata": {},
   "source": [
    "teaching_flows.json\n",
    "\n",
    "{\n",
    "  \"standard_lecture\": {\n",
    "    \"name\": \"Standard Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer. Your audience is undergraduate students. Based on the following context, create a slide that provides a detailed explanation of the topic '{sub_topic}'. The content should be structured with bullet points for key details. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of topics covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  },\n",
    "  \"apply_topic_interactive\": {\n",
    "    \"name\": \"Interactive Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Application\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer in Digital Forensics. Your audience is undergraduate students. Based on the provided context, create a slide explaining the concept of '{sub_topic}'. The content should be clear, concise, and structured with bullet points for easy understanding. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"application_generation\": \"You are an engaging university lecturer creating an interactive slide. Based on the concept of '{sub_topic}', create a multiple-choice question with exactly 4 options (A, B, C, D) to test understanding. The slide title must be 'Let's Apply This:'. Clearly indicate the correct answer within the content. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of concepts and applications covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Application\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d32d91",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a04010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Configuration and Scoping for Content Generation (Corrected)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "# --- Global Test Overrides (for easy testing) ---\n",
    "TEST_OVERRIDE_WEEKS = None\n",
    "TEST_OVERRIDE_FLOW_ID = None\n",
    "TEST_OVERRIDE_SESSIONS_PER_WEEK = None\n",
    "TEST_OVERRIDE_DISTRIBUTION_STRATEGY = None\n",
    "\n",
    "\n",
    "def process_and_load_configurations():\n",
    "    \"\"\"\n",
    "    PHASE 1: Loads configurations, calculates a PRELIMINARY time-based slide budget,\n",
    "    and saves the result as 'processed_settings.json' for the Planning Agent.\n",
    "    \"\"\"\n",
    "    print_header(\"Phase 1: Configuration and Scoping Process\", char=\"-\")\n",
    "    \n",
    "    # --- Load all input files ---\n",
    "    logger.info(\"Loading all necessary configuration and data files...\")\n",
    "    try:\n",
    "        os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f: unit_outline = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: book_toc = json.load(f)\n",
    "        with open(SETTINGS_DECK_PATH, 'r', encoding='utf-8') as f: settings_deck = json.load(f)\n",
    "        with open(TEACHING_FLOWS_PATH, 'r', encoding='utf-8') as f: teaching_flows = json.load(f)\n",
    "        logger.info(\"All files loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"FATAL: A required configuration file was not found: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Pre-process and Refine Settings ---\n",
    "    logger.info(\"Pre-processing settings_deck for definitive plan...\")\n",
    "    processed_settings = json.loads(json.dumps(settings_deck))\n",
    "\n",
    "    unit_info = unit_outline.get(\"unitInformation\", {})\n",
    "    processed_settings['course_id'] = unit_info.get(\"unitCode\", \"UNKNOWN_COURSE\")\n",
    "    processed_settings['unit_name'] = unit_info.get(\"unitName\", \"Unknown Unit Name\")\n",
    "    \n",
    "    # --- Apply test overrides IF they are not None ---\n",
    "    logger.info(\"Applying overrides if specified...\")\n",
    "    # This block now correctly sets the teaching_flow_id based on the interactive flag.\n",
    "    if TEST_OVERRIDE_FLOW_ID is not None:\n",
    "        processed_settings['teaching_flow_id'] = TEST_OVERRIDE_FLOW_ID\n",
    "        logger.info(f\"OVERRIDE: teaching_flow_id set to '{TEST_OVERRIDE_FLOW_ID}'\")\n",
    "    else:\n",
    "        # If no override, use the 'interactive' boolean from the file as the source of truth.\n",
    "        is_interactive = processed_settings.get('interactive', False)\n",
    "        if is_interactive:\n",
    "            processed_settings['teaching_flow_id'] = 'apply_topic_interactive'\n",
    "        else:\n",
    "            processed_settings['teaching_flow_id'] = 'standard_lecture'\n",
    "        logger.info(f\"Loaded from settings: 'interactive' is {is_interactive}. Set teaching_flow_id to '{processed_settings['teaching_flow_id']}'.\")\n",
    "\n",
    "    # The 'interactive' flag is now always consistent with the teaching_flow_id.\n",
    "    processed_settings['interactive'] = \"interactive\" in processed_settings['teaching_flow_id'].lower()\n",
    "    \n",
    "    if TEST_OVERRIDE_SESSIONS_PER_WEEK is not None:\n",
    "        processed_settings['week_session_setup']['sessions_per_week'] = TEST_OVERRIDE_SESSIONS_PER_WEEK\n",
    "        logger.info(f\"OVERRIDE: sessions_per_week set to {TEST_OVERRIDE_SESSIONS_PER_WEEK}\")\n",
    "\n",
    "    if TEST_OVERRIDE_DISTRIBUTION_STRATEGY is not None:\n",
    "        processed_settings['week_session_setup']['distribution_strategy'] = TEST_OVERRIDE_DISTRIBUTION_STRATEGY\n",
    "        logger.info(f\"OVERRIDE: distribution_strategy set to '{TEST_OVERRIDE_DISTRIBUTION_STRATEGY}'\")\n",
    "\n",
    "    if TEST_OVERRIDE_WEEKS is not None:\n",
    "        processed_settings['generation_scope']['weeks'] = TEST_OVERRIDE_WEEKS\n",
    "        logger.info(f\"OVERRIDE: generation_scope weeks set to {TEST_OVERRIDE_WEEKS}\")\n",
    "\n",
    "    # --- DYNAMIC SLIDE BUDGET CALCULATION (Phase 1) ---\n",
    "    logger.info(\"Calculating preliminary slide budget based on session time...\")\n",
    "    \n",
    "    params = processed_settings.get('parameters_slides', {})\n",
    "    SLIDES_PER_HOUR = params.get('slides_per_hour', 18)\n",
    "    \n",
    "    duration_hours = processed_settings['week_session_setup'].get('session_time_duration_in_hour', 1.0)\n",
    "    sessions_per_week = processed_settings['week_session_setup'].get('sessions_per_week', 1)\n",
    "    \n",
    "    slides_content_per_session = int(duration_hours * SLIDES_PER_HOUR)\n",
    "    target_total_slides = slides_content_per_session * sessions_per_week\n",
    "    \n",
    "    processed_settings['slide_count_strategy']['target_total_slides'] = target_total_slides\n",
    "    processed_settings['slide_count_strategy']['slides_content_per_session'] = slides_content_per_session\n",
    "    logger.info(f\"Preliminary weekly content slide target calculated: {target_total_slides} slides.\")\n",
    "    \n",
    "    # --- Resolve Generation Scope if not overridden ---\n",
    "    if TEST_OVERRIDE_WEEKS is None and processed_settings.get('generation_scope', {}).get('weeks') == \"all\":\n",
    "        num_weeks = len(unit_outline.get('weeklySchedule', []))\n",
    "        processed_settings['generation_scope']['weeks'] = list(range(1, num_weeks + 1))\n",
    "    \n",
    "    # --- Save the processed settings to disk ---\n",
    "    logger.info(f\"Saving preliminary processed configuration to: {PROCESSED_SETTINGS_PATH}\")\n",
    "    with open(PROCESSED_SETTINGS_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_settings, f, indent=2)\n",
    "    logger.info(\"File saved successfully.\")\n",
    "\n",
    "    # --- Assemble master config for optional preview ---\n",
    "    master_config = {\n",
    "        \"processed_settings\": processed_settings,\n",
    "        \"unit_outline\": unit_outline,\n",
    "        \"book_toc\": book_toc,\n",
    "        \"teaching_flows\": teaching_flows\n",
    "    }\n",
    "    \n",
    "    print_header(\"Phase 1 Configuration Complete\", char=\"-\")\n",
    "    logger.info(\"Master configuration object is ready for the Planning Agent.\")\n",
    "    return master_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58506539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f74573",
   "metadata": {},
   "source": [
    "Component: Definitive PowerPoint Layout Inspector\n",
    "Primary Purpose\n",
    "The inspect_and_generate_layout_config function serves as a critical pre-processing utility for the automated presentation generation system. Its primary purpose is to bridge the gap between a visual PowerPoint template and the programmatic logic of the content generation agents.\n",
    "It achieves this by performing a deep inspection of a given PowerPoint (.pptx) template file and auto-generating a detailed, structured, and human-readable JSON configuration file (layout_mapping.json). This configuration file acts as the \"API documentation\" for the presentation template, allowing both human users and a Large Language Model (LLM) to understand and utilize the available slide layouts effectively.\n",
    "Key Functions and GoOf course. Here is a formal description of the purpose and functionality of the \"Definitive PowerPoint Layout Inspector\" script. This description is suitable for project documentation, a README file, or for explaining its role to other developers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Component: Definitive PowerPoint Layout Inspector**\n",
    "\n",
    "#### **Primary Purpose**\n",
    "\n",
    "The `inspect_and_generate_layout_config` function serves as a critical pre-processing utility for the automated presentation generation system. Its primary purpose is to **bridge the gap between a visual PowerPoint template and the programmatic logic of the content generation agents**.\n",
    "\n",
    "It achieves this by performing a deep inspection of a given PowerPoint (`.pptx`) template file and auto-generating a detailed, structured, and human-readable JSON configuration file (`layout_mapping.json`). This configuration file acts as the \"API documentation\" for the presentation template, allowing both human users and a Large Language Model (LLM) to understand and utilize the available slide layouts effectively.\n",
    "\n",
    "#### **Key Functions and Goals**\n",
    "\n",
    "1.  **Comprehensive Layout Discovery:**\n",
    "    *   The script guarantees that **every single slide layout** present in the PowerPoint template's Slide Master is detected and analyzed. This prevents a common problem where unused or unconventionally named layouts might be missed by simpler scripts.\n",
    "\n",
    "2.  **Detailed Placeholder Analysis:**\n",
    "    *   For each layout, the script extracts an exhaustive list of all its placeholders. For every placeholder, it records crucial metadata:\n",
    "        *   **`type`**: The functional role of the placeholder (e.g., `TITLE`, `BODY`, `OBJECT`, `TABLE`, `PICTURE`).\n",
    "        *   **`name`**: The unique name given to the placeholder in the PowerPoint interface (e.g., \"Title 1\", \"Content Placeholder 2\").\n",
    "        *   **`idx`**: The internal identification number of the placeholder.\n",
    "        *   **`position` and `size`**: The physical coordinates (`left`, `top`) and dimensions (`width`, `height`), converted to an intuitive unit (inches) for easy comprehension of the layout's visual structure.\n",
    "\n",
    "3.  **Intelligent Capability Summarization:**\n",
    "    *   The script's core innovation is its ability to generate a **machine-readable capabilities summary** for each layout. Instead of just listing raw data, it synthesizes the placeholder information into a concise description of what the layout is designed for. For example:\n",
    "        *   `{\"title_support\": \"standard_title\", \"body_layout\": \"2_column\"}`\n",
    "        *   `{\"title_support\": \"centered_title_with_subtitle\", \"body_layout\": \"no_body\"}`\n",
    "        *   `{\"specific_content_types\": [\"TABLE\", \"CHART\"]}`\n",
    "    *   This summary is specifically designed to be passed to an LLM as part of a prompt, enabling the LLM to make an informed, logical choice about the best layout for presenting a given piece of information.\n",
    "\n",
    "4.  **User-Friendly Configuration:**\n",
    "    *   While providing a detailed summary for the LLM, the script also generates a simplified `user_selections` section. This allows a human operator to easily map the system's semantic slide types (e.g., \"Agenda\", \"Summary\") to a specific layout index, providing a robust fallback and manual override capability.\n",
    "\n",
    "#### **How It Solves Critical Problems**\n",
    "\n",
    "*   **Eliminates Ambiguity:** By capturing the name, index, and position of every placeholder, it solves the problem of layouts with multiple placeholders of the same type (e.g., two content boxes). The system can now programmatically target the \"left column\" vs. the \"right column\".\n",
    "*   **Decouples Logic from Design:** The presentation generation agent no longer needs hard-coded assumptions about the template's design. All the logic for choosing and populating layouts is driven by the generated JSON file. This means the visual template can be updated or completely replaced without requiring changes to the core Python code.\n",
    "*   **Empowers the LLM:** It transforms a visual, unstructured design asset (the `.pptx` file) into a structured, well-defined set of \"tools\" (the layouts) that an LLM can reason about. This is the key to enabling more advanced tasks where the LLM doesn't just fill in content, but also makes decisions about the *visual structure* of the presentation.\n",
    "\n",
    "In summary, the Definitive PowerPoint Layout Inspector is the foundational component that makes the entire presentation generation process intelligent, configurable, and robust. It translates the abstract design of a template into concrete, actionable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4be616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                      Configuration Generated Successfully                      \n",
      "================================================================================\n",
      "A new, human-readable configuration file has been saved to:\n",
      "/home/sebas_dev_linux/projects/course_generator/configs/layout_mapping_test.json\n",
      "\n",
      "Please open this file to review the classifications and edit your layout selections.\n"
     ]
    }
   ],
   "source": [
    "# this process the layauts from the slides provide to use them to process the plan \n",
    "# https://aistudio.google.com/prompts/18YaU5pG96eFMbM1l6yeG1v9j63PkLc5H\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from pptx.enum.shapes import MSO_SHAPE_TYPE\n",
    "from pptx.enum.shapes import PP_PLACEHOLDER  \n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def generate_layout_capabilities(layout_name: str, placeholders: list) -> dict:\n",
    "    # ... (This helper function remains unchanged from the previous version) ...\n",
    "    essential_placeholders = [p for p in placeholders if p['type'] != 'SLIDE_NUMBER']\n",
    "    capabilities = {\"title_support\": \"none\", \"body_layout\": \"none\", \"specific_content_types\": []}\n",
    "    has_center_title = any(p['type'] == 'CENTER_TITLE' for p in essential_placeholders)\n",
    "    has_standard_title = any(p['type'] == 'TITLE' for p in essential_placeholders)\n",
    "    has_subtitle = any(p['type'] == 'SUBTITLE' for p in essential_placeholders)\n",
    "    if has_center_title: capabilities[\"title_support\"] = \"centered_title\"\n",
    "    elif has_standard_title: capabilities[\"title_support\"] = \"standard_title\"\n",
    "    if has_subtitle: capabilities[\"title_support\"] += \"_with_subtitle\"\n",
    "    body_placeholders = [p for p in essential_placeholders if p['type'] not in ('TITLE', 'CENTER_TITLE', 'SUBTITLE')]\n",
    "    if len(body_placeholders) == 0: capabilities[\"body_layout\"] = \"no_body\"\n",
    "    elif len(body_placeholders) == 1: capabilities[\"body_layout\"] = \"single_column\"\n",
    "    elif len(body_placeholders) > 1:\n",
    "        body_placeholders.sort(key=lambda p: p['left'])\n",
    "        if len(body_placeholders) > 1 and body_placeholders[1]['left'] > (body_placeholders[0]['left'] + body_placeholders[0]['width'] * 0.5):\n",
    "            capabilities[\"body_layout\"] = f\"{len(body_placeholders)}_column\"\n",
    "        else: capabilities[\"body_layout\"] = \"stacked_sections\"\n",
    "    specific_types = {p['type'] for p in body_placeholders if p['type'] not in ('BODY', 'OBJECT')}\n",
    "    if specific_types: capabilities[\"specific_content_types\"] = sorted(list(specific_types))\n",
    "    return capabilities\n",
    "\n",
    "\n",
    "def inspect_and_generate_layout_config(template_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Inspects a template, generates a machine-readable capabilities summary, and creates\n",
    "    a complete and definitive JSON configuration file for user and LLM use.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prs = Presentation(template_path)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not open or parse the presentation template at '{template_path}'. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 1. Analyze all Layouts ---\n",
    "    # ... (This section remains unchanged) ...\n",
    "    available_layouts = []\n",
    "    for i, layout in enumerate(prs.slide_layouts):\n",
    "        placeholder_details = []\n",
    "        for p in layout.placeholders:\n",
    "            placeholder_details.append({\n",
    "                \"idx\": p.placeholder_format.idx, \"type\": p.placeholder_format.type.name,\n",
    "                \"name\": p.name, \"left\": round(p.left.inches, 2), \"top\": round(p.top.inches, 2),\n",
    "                \"width\": round(p.width.inches, 2), \"height\": round(p.height.inches, 2)\n",
    "            })\n",
    "        capabilities = generate_layout_capabilities(layout.name, placeholder_details)\n",
    "        layout_info = {\"layout_index\": i, \"layout_name\": layout.name, \"capabilities\": capabilities, \"placeholders\": placeholder_details}\n",
    "        available_layouts.append(layout_info)\n",
    "\n",
    "    # --- 2. Create the Complete User-Facing Configuration Structure ---\n",
    "    def find_default_by_capability(layouts, capability_key, capability_value, fallback_index=1):\n",
    "        for layout in layouts:\n",
    "            if layout['capabilities'].get(capability_key) == capability_value:\n",
    "                return layout['layout_index']\n",
    "        # If no perfect match is found, return a safe fallback index\n",
    "        return fallback_index if len(layouts) > fallback_index else 0\n",
    "\n",
    "    # ** This map is now complete, explicitly including all required keys **\n",
    "    user_selection_map = {\n",
    "        \"Title\": { # title and subtitle \n",
    "            \"description\": \"Main title slide of the deck.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"title_support\", \"centered_title_with_subtitle\")\n",
    "        },\n",
    "        \"Agenda\": { # title and object  \n",
    "            \"description\": \"Agenda/Table of Contents slide.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Content\": { # title and object \n",
    "            \"description\": \"Default slide for a standard topic with bullet points.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Content_Two_Column\": { # title and 2 objects \n",
    "            \"description\": \"Slide for side-by-side content, like comparisons.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"2_column\")\n",
    "        },\n",
    "        \"Content_child\": { ## # title, subtitle and object\n",
    "            \"description\": \"Default slide for a standard topic with bullet points.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Content_Two_Column_child\": { ## title, subtitle and 2 objects \n",
    "            \"description\": \"Slide for side-by-side content, like comparisons.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"2_column\")\n",
    "        },\n",
    "        \"Application\": {# title and  object \n",
    "            \"description\": \"Slide for interactive questions ('Let's Apply This!').\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"Application_Two_Column \": { # title and 2 objects \n",
    "            \"description\": \"Slide for side-by-side content ('Let's Apply This!').\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"2_column\")\n",
    "        },\n",
    "        \"Summary\": { # title and object\n",
    "            \"description\": \"The final summary slide of the deck.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"body_layout\", \"single_column\")\n",
    "        },\n",
    "        \"End\": { # title \n",
    "            \"description\": \"The final 'Thank You / Questions?' slide.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"title_support\", \"centered_title\", fallback_index=7)\n",
    "        },\n",
    "        \"Divider\": { # title and object\n",
    "            \"description\": \"Slide that divide sections general use base on the agenda.\",\n",
    "            \"selected_layout_index\": find_default_by_capability(available_layouts, \"title_support\", \"centered_title\", fallback_index=7)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_to_save = {\n",
    "        \"//_INSTRUCTIONS\": \"This file describes the available slide layouts. The 'available_layouts' section is for an LLM to read. The 'user_selections' section is for you to edit. Please verify the 'selected_layout_index' for each slide type.\",\n",
    "        \"template_file\": os.path.basename(template_path),\n",
    "        \"user_selections\": user_selection_map,\n",
    "        \"available_layouts\": available_layouts\n",
    "    }\n",
    "\n",
    "    # --- 3. Save the Configuration File ---\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_to_save, f, indent=4)\n",
    "        print_header(\"Configuration Generated Successfully\", char=\"=\")\n",
    "        print(f\"A new, human-readable configuration file has been saved to:\\n{output_path}\")\n",
    "        print(\"\\nPlease open this file to review the classifications and edit your layout selections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write the layout configuration file to '{output_path}'. Error: {e}\")\n",
    "        \n",
    "# --- Execution ---\n",
    "SLIDE_TEMPLATE_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/slide_style/slide_style_test_2.pptx\"\n",
    "LAYOUT_MAPPING_PATH = \"/home/sebas_dev_linux/projects/course_generator/configs/layout_mapping_test.json\"\n",
    "# You run this function to generate the config file.\n",
    "# Make sure SLIDE_TEMPLATE_PATH and LAYOUT_MAPPING_PATH are defined.\n",
    "inspect_and_generate_layout_config(SLIDE_TEMPLATE_PATH, LAYOUT_MAPPING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72043b",
   "metadata": {},
   "source": [
    "### test content agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8467f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 9: Content Agent (Corrected and Enhanced for Phase 5 & 6) --- it is \n",
    "\n",
    "# # Assumes the following are imported and available from previous cells:\n",
    "# # ollama, json, logging, os, Dict, Optional, Any, Chroma, tenacity elements\n",
    "\n",
    "\n",
    "\n",
    "# class ContentAgent:\n",
    "#     \"\"\"\n",
    "#     An agent that performs two main functions:\n",
    "#     1. (Phase 5) Populates a hierarchical plan with raw, reassembled text from a vector store.\n",
    "#     2. (Phase 6) Processes the content-rich plan, using an LLM to generate slide-specific content\n",
    "#                  and reorders all keys for final, clean output.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "#         self.config = master_config['processed_settings']\n",
    "#         self.unit_outline = master_config['unit_outline']\n",
    "#         self.book_toc = master_config['book_toc']\n",
    "#         self.teaching_flows = master_config['teaching_flows']\n",
    "#         self.layaut_slides = master_config['layout_mapping']\n",
    "#         self.vector_store = vector_store\n",
    "#         self.client = ollama.Client(host=OLLAMA_HOST)\n",
    "#         logger.info(\"Data-Driven Content Agent initialized successfully.\")\n",
    "\n",
    "#     # Define the desired key order as a class attribute\n",
    "#         self.key_order = [\n",
    "#             \"title\", \"toc_id\",\n",
    "#             \"chunk_count\",\n",
    "#             \"total_chunks_in_branch\",\n",
    "#             \"budget_slides_content\",\n",
    "#             \"direct_slides_content\",\n",
    "#             \"total_slides_in_branch\",\n",
    "#             \"time_allocation_minutes\",\n",
    "#             \"chunks_sorted\",\n",
    "#             \"content\",\n",
    "#             \"llm_generated_content\",\n",
    "#             \"children\", \n",
    "#             \"interactive_activity\"\n",
    "#         ]\n",
    "        \n",
    "#         logger.info(\"Data-Driven Content Agent initialized successfully.\")\n",
    "\n",
    "#     # --- Key Reordering Logic (REFINED) ---\n",
    "#     def _reorder_keys_recursively(self, node: dict) -> dict:\n",
    "#         \"\"\"\n",
    "#         Recursively traverses a dictionary (a node in the plan) and reorders its keys\n",
    "#         according to a predefined order for maximum readability.\n",
    "#         \"\"\"\n",
    "#         if not isinstance(node, dict):\n",
    "#             return node\n",
    "\n",
    "#         # 1. Recurse first to ensure nested structures are already ordered.\n",
    "#         if 'children' in node and isinstance(node.get('children'), list):\n",
    "#             node['children'] = [self._reorder_keys_recursively(child) for child in node['children']]\n",
    "        \n",
    "#         if 'interactive_activity' in node and isinstance(node.get('interactive_activity'), dict):\n",
    "#             node['interactive_activity'] = self._reorder_keys_recursively(node['interactive_activity'])\n",
    "\n",
    "#         # 2. Build a new dictionary for the current node with the correct key order.\n",
    "#         reordered_node = {}\n",
    "        \n",
    "#         # Add keys that are in our desired order\n",
    "#         for key in self.key_order:\n",
    "#             if key in node:\n",
    "#                 reordered_node[key] = node[key]\n",
    "        \n",
    "#         # Add any remaining keys that were not in the order list (as a fallback)\n",
    "#         for key, value in node.items():\n",
    "#             if key not in reordered_node:\n",
    "#                 reordered_node[key] = value\n",
    "        \n",
    "#         return reordered_node\n",
    "\n",
    "   \n",
    "\n",
    "#     # --- Phase 5: Raw Content Population ---\n",
    "#     def retrieve_content_for_toc_id(self, toc_id: int) -> dict:\n",
    "#         # ... (This method remains unchanged) ...\n",
    "#         if not isinstance(toc_id, int):\n",
    "#             logger.warning(f\"Invalid toc_id: {toc_id}. Must be an integer.\")\n",
    "#             return {\"chunks_sorted\": [], \"content\": \"\"}\n",
    "#         try:\n",
    "#             results = self.vector_store.get(where={\"toc_id\": toc_id}, include=[\"documents\", \"metadatas\"])\n",
    "#             if not results or not results.get('ids'):\n",
    "#                 logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "#                 return {\"chunks_sorted\": [], \"content\": \"\"}\n",
    "#             sorted_items = sorted(zip(results['documents'], results['metadatas']), key=lambda item: item[1].get('chunk_id', 0))\n",
    "#             sorted_docs = [item[0] for item in sorted_items]\n",
    "#             sorted_chunk_ids = [item[1].get('chunk_id') for item in sorted_items]\n",
    "#             reassembled_text = \"\\n\\n\".join(sorted_docs)\n",
    "#             return {\"chunks_sorted\": sorted_chunk_ids, \"content\": reassembled_text}\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"An error occurred during retrieval for toc_id {toc_id}: {e}\", exc_info=True)\n",
    "#             return {\"chunks_sorted\": [], \"content\": \"\"}\n",
    "    \n",
    "#     def populate_content_recursively(self, node: dict):\n",
    "        \n",
    "#         if 'toc_id' in node and 'content' not in node:\n",
    "#             content_data = self.retrieve_content_for_toc_id(node['toc_id'])\n",
    "#             node.update(content_data)\n",
    "#         if 'children' in node and isinstance(node.get('children'), list):\n",
    "#             for child in node['children']:\n",
    "#                 self.populate_content_recursively(child)\n",
    "    \n",
    "#     def generate_content_for_plan(self, final_plan_path: str, output_dir: str) -> bool:\n",
    "        \n",
    "#         logger.info(f\"PHASE 5: Populating raw content for: {final_plan_path}\")\n",
    "#         try:\n",
    "#             with open(final_plan_path, 'r', encoding='utf-8') as f:\n",
    "#                 plan_data = json.load(f)\n",
    "#         except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "#             logger.error(f\"FATAL: Could not read or decode plan file {final_plan_path}. Error: {e}\")\n",
    "#             return False\n",
    "        \n",
    "#         for deck in plan_data.get('deck_plans', []):\n",
    "#             for section in deck.get('sections', []):\n",
    "#                 if section.get('section_type') == 'Content':\n",
    "#                     for content_block in section.get('content_blocks', []):\n",
    "#                         self.populate_content_recursively(content_block)\n",
    "                        \n",
    "#         base_filename = os.path.basename(final_plan_path)\n",
    "#         output_path = os.path.join(output_dir, base_filename)\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "#         logger.info(\"Reordering keys for Fetched clean output...\")\n",
    "#         fetched_ordered_plan = self._reorder_keys_recursively(plan_data)\n",
    "        \n",
    "#         try:\n",
    "#             with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#                 # CORRECTED: Save the 'fetched_ordered_plan' variable\n",
    "#                 json.dump(fetched_ordered_plan, f, indent=2, ensure_ascii=False)\n",
    "#             logger.info(f\"Successfully saved content-enriched plan to: {output_path}\")\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to save the content-enriched plan to {output_path}: {e}\", exc_info=True)\n",
    "#             return False\n",
    "\n",
    "#     # --- Phase 6: LLM Content Generation & Final Formatting ---\n",
    "    \n",
    "    \n",
    "    \n",
    "#     @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "#     def _call_ollama_with_retry(self, prompt: str) -> str:\n",
    "#         logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "#         response = self.client.chat(model=OLLAMA_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], format=\"json\", options={\"temperature\": 0.2})\n",
    "#         if not response or 'message' not in response or not response['message'].get('content'):\n",
    "#             raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "#         return response['message']['content']\n",
    "\n",
    "#     def _parse_llm_json_output(self, content: str) -> Optional[Dict]:\n",
    "#         try:\n",
    "#             match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "#             if not match:\n",
    "#                 logger.warning(\"LLM output did not contain a valid JSON object.\")\n",
    "#                 return None\n",
    "#             return json.loads(match.group(0))\n",
    "#         except (json.JSONDecodeError, TypeError) as e:\n",
    "#             logger.error(f\"Failed to parse JSON from LLM output: {e}\\nRaw content: {content}\")\n",
    "#             return None\n",
    "    \n",
    "\n",
    "#     def _process_node_with_llm_recursively(self, node: dict, flow_prompts: dict):\n",
    "        \n",
    "#         if node.get('content'):\n",
    "#             prompt_template = flow_prompts.get('content_generation')\n",
    "#             if prompt_template:\n",
    "#                 prompt = prompt_template.format(sub_topic=node.get('title', 'Untitled'), context=node.get('content'))\n",
    "#                 try:\n",
    "#                     llm_str = self._call_ollama_with_retry(prompt)\n",
    "#                     node['llm_generated_content'] = self._parse_llm_json_output(llm_str) or {\"title\": node.get('title'), \"content\": [\"Failed to generate content.\"]}\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"LLM call failed for topic '{node.get('title')}': {e}\")\n",
    "#                     node['llm_generated_content'] = {\"title\": node.get('title'), \"content\": [f\"Error during generation: {e}\"]}\n",
    "#         if node.get('interactive_activity') and node.get('content'):\n",
    "#             prompt_template = flow_prompts.get('interactive_activity')\n",
    "#             if prompt_template:\n",
    "#                 prompt = prompt_template.format(sub_topic=node.get('title', 'Untitled'), context=node.get('content'))\n",
    "#                 try:\n",
    "#                     llm_str = self._call_ollama_with_retry(prompt)\n",
    "#                     node['interactive_activity']['llm_generated_content'] = self._parse_llm_json_output(llm_str) or {\"title\": \"Let's Apply This!\", \"content\": [\"Failed to generate activity.\"]}\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"LLM call failed for activity on '{node.get('title')}': {e}\")\n",
    "#                     node['interactive_activity']['llm_generated_content'] = {\"title\": \"Let's Apply This!\", \"content\": [f\"Error during generation: {e}\"]}\n",
    "#         if 'children' in node and isinstance(node.get('children'), list):\n",
    "#             for child in node['children']:\n",
    "#                 self._process_node_with_llm_recursively(child, flow_prompts)\n",
    "\n",
    "\n",
    "#     def generate_llm_content_for_plan(self, content_plan_path: str, llm_output_dir: str) -> bool:\n",
    "#         \"\"\"\n",
    "#         Orchestrates the LLM content generation for a content-enriched plan file,\n",
    "#         and finishes by reordering all keys for a clean final output.\n",
    "#         \"\"\"\n",
    "#         logger.info(f\"PHASE 6: Generating LLM content for: {os.path.basename(content_plan_path)}\")\n",
    "#         try:\n",
    "#             with open(content_plan_path, 'r', encoding='utf-8') as f:\n",
    "#                 plan_data = json.load(f)\n",
    "#         except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "#             logger.error(f\"FATAL: Could not read content plan file {content_plan_path}. Error: {e}\")\n",
    "#             return False\n",
    "\n",
    "#         flow_id = self.config.get('teaching_flow_id', 'standard_lecture')\n",
    "#         flow_prompts = self.teaching_flows.get(flow_id, {}).get('prompts', {})\n",
    "#         if not flow_prompts:\n",
    "#             logger.error(f\"Could not find prompts for teaching_flow_id: '{flow_id}'\")\n",
    "#             return False\n",
    "\n",
    "#         # Process each deck in the plan\n",
    "#         for deck in plan_data.get('deck_plans', []):\n",
    "#             content_blocks = []\n",
    "#             for section in deck.get('sections', []):\n",
    "#                 if section.get('section_type') == 'Content':\n",
    "#                     content_blocks = section.get('content_blocks', [])\n",
    "#                     for block in content_blocks:\n",
    "#                         self._process_node_with_llm_recursively(block, flow_prompts)\n",
    "            \n",
    "#             # Generate summary\n",
    "#             summary_prompt_template = flow_prompts.get('summary_generation')\n",
    "#             if summary_prompt_template and content_blocks:\n",
    "#                 topic_titles = [block.get('title', 'Untitled Topic') for block in content_blocks]\n",
    "#                 topic_list_str = \"\\n\".join(f\"- {title}\" for title in topic_titles)\n",
    "#                 prompt = summary_prompt_template.format(topic_list=topic_list_str)\n",
    "#                 try:\n",
    "#                     llm_str = self._call_ollama_with_retry(prompt)\n",
    "#                     for section in deck.get('sections', []):\n",
    "#                         if section.get('section_type') == 'Summary':\n",
    "#                             section['llm_generated_content'] = self._parse_llm_json_output(llm_str)\n",
    "#                             break\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"LLM call failed for deck summary: {e}\")\n",
    "\n",
    "#         # *** FINAL KEY REORDERING STEP ***\n",
    "#         # Apply the reordering to the entire plan data structure before saving\n",
    "#         logger.info(\"Reordering keys for final clean output...\")\n",
    "#         final_ordered_plan = self._reorder_keys_recursively(plan_data)\n",
    "        \n",
    "#         # Save the LLM-enriched and CLEANED plan\n",
    "#         base_filename = os.path.basename(content_plan_path)\n",
    "#         output_path = os.path.join(llm_output_dir, base_filename)\n",
    "#         os.makedirs(llm_output_dir, exist_ok=True)\n",
    "#         try:\n",
    "#             with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#                 json.dump(final_ordered_plan, f, indent=2, ensure_ascii=False)\n",
    "#             logger.info(f\"Successfully saved final LLM-enriched plan to: {output_path}\")\n",
    "#             return True\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to save the final LLM-enriched plan to {output_path}: {e}\", exc_info=True)\n",
    "#             return False\n",
    "        \n",
    "        \n",
    "# # i need to prepare the prompt to pass the layout information + content + intruction output         \n",
    "        \n",
    "\n",
    "# # test code \n",
    "# content_plan_path = \"/home/sebas_dev_linux/projects/course_generator/generated_content/test_generation.json\"\n",
    "# layaut_map_path = \"/home/sebas_dev_linux/projects/course_generator/configs/layout_mapping.json\"\n",
    "\n",
    "# master_config = process_and_load_configurations()\n",
    "# content_agent = ContentAgent(master_config, vector_store=vector_store)\n",
    "\n",
    "# content_agent.generate_llm_content_for_plan(content_plan_path, CONTENT_LLM_OUTPUT_DIR)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354078d",
   "metadata": {},
   "source": [
    "### Main Integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 11: --- Main Orchestration Block (with Phase 5 & 6) ---\n",
    "print_header(\"Main Orchestrator Initialized\", char=\"*\")\n",
    "\n",
    "try:\n",
    "    # 1. Connect to DB\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    logger.info(\"Database connection successful.\")\n",
    "    \n",
    "    # Phase 1: Configuration and Scoping \n",
    "    master_config = process_and_load_configurations()\n",
    "    \n",
    "    if master_config:\n",
    "        all_final_plans = []\n",
    "        \n",
    "        # Phase 2 & 3: Create and Finalize Draft Plans\n",
    "        print_header(\"Phase 2 & 3: Generating and Finalizing Weekly Plans ✅\", char=\"-\")\n",
    "        planning_agent = PlanningAgent(master_config, vector_store=vector_store) \n",
    "        \n",
    "        weeks_to_generate = master_config['processed_settings']['generation_scope']['weeks']\n",
    "        logger.info(f\"Found {len(weeks_to_generate)} week(s) to plan: {weeks_to_generate}\")\n",
    "\n",
    "        for week in weeks_to_generate:\n",
    "            draft_plan = planning_agent.create_content_plan_for_week(week)\n",
    "            if draft_plan:\n",
    "                final_plan = planning_agent.finalize_and_calculate_time_plan(draft_plan, master_config['processed_settings'])\n",
    "                all_final_plans.append(final_plan)\n",
    "                \n",
    "                # Save both draft and final for comparison\n",
    "                draft_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_draft.json\"\n",
    "                final_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "                \n",
    "                \n",
    "                with open(os.path.join(PLAN_OUTPUT_DIR, draft_filename), 'w') as f:\n",
    "                    json.dump(draft_plan, f, indent=2)\n",
    "                \n",
    "                with open(os.path.join(PLAN_OUTPUT_DIR, final_filename), 'w') as f:\n",
    "                    json.dump(final_plan, f, indent=2)\n",
    "                    \n",
    "                logger.info(f\"✅ Successfully saved FINAL plan for Week {week} to: {os.path.join(PLAN_OUTPUT_DIR, final_filename)}\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                logger.error(f\"Failed to generate draft plan for Week {week}.\")\n",
    "        \n",
    "        # Phase 4: Generate Master Summary Plan\n",
    "        master_plan_generated = False\n",
    "        if all_final_plans:\n",
    "            print_header(\"Phase 4: Generating Master Unit Plan ✅\", char=\"-\")\n",
    "            master_plan_generated = planning_agent.generate_and_save_master_plan(all_final_plans, master_config['processed_settings'])\n",
    "        else:\n",
    "            logger.warning(\"No weekly plans were generated, skipping master plan creation.\")\n",
    "        \n",
    "        # Initialize ContentAgent once for subsequent phases\n",
    "        content_agent = ContentAgent(master_config, vector_store=vector_store)\n",
    "        phase_5_successful = False\n",
    "\n",
    "        # Phase 5: Fetching Raw Content\n",
    "        if master_plan_generated:\n",
    "            print_header(\"Phase 5: Populating Plans with Raw Content\", char=\"#\")\n",
    "            successful_weeks_phase5 = []\n",
    "            for week in weeks_to_generate:\n",
    "                final_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "                full_plan_path = os.path.join(PLAN_OUTPUT_DIR, final_filename)\n",
    "\n",
    "                if os.path.exists(full_plan_path):\n",
    "                    if content_agent.generate_content_for_plan(full_plan_path, CONTENT_OUTPUT_DIR):\n",
    "                        successful_weeks_phase5.append(week)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping content population for Week {week} as its plan file was not found.\")\n",
    "            \n",
    "            if successful_weeks_phase5:\n",
    "                phase_5_successful = True\n",
    "                logger.info(f\"Phase 5 completed for weeks: {successful_weeks_phase5}\")\n",
    "        \n",
    "        # Phase 6: Generating Slide Content with LLM\n",
    "        phase_6_successful = True\n",
    "        # if phase_5_successful:\n",
    "        #     print_header(\"Phase 6: Generating Slide Content with LLM\", char=\"#\")\n",
    "        #     successful_weeks_phase6 = []\n",
    "        #     for week in weeks_to_generate:\n",
    "        #         # The input for phase 6 is the output of phase 5\n",
    "        #         content_enriched_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "        #         content_plan_path = os.path.join(CONTENT_OUTPUT_DIR, content_enriched_filename)\n",
    "\n",
    "        #         if os.path.exists(content_plan_path):\n",
    "        #              if content_agent.generate_llm_content_for_plan(content_plan_path, CONTENT_LLM_OUTPUT_DIR):\n",
    "        #                  successful_weeks_phase6.append(week)\n",
    "        #         else:\n",
    "        #             logger.warning(f\"Skipping LLM generation for Week {week} as its content-enriched file was not found.\")\n",
    "            \n",
    "        #     if successful_weeks_phase6:\n",
    "        #         phase_6_successful = True\n",
    "        #         logger.info(f\"Phase 6 completed for weeks: {successful_weeks_phase6}\")\n",
    "                \n",
    "        if phase_6_successful:\n",
    "            print_header(\"Phase 7: Generating Final PowerPoint Files\", char=\"#\")\n",
    "            presentation_agent = PresentationAgent(template_path=SLIDE_TEMPLATE_PATH)\n",
    "            \n",
    "            for week in weeks_to_generate:\n",
    "                llm_plan_filename = f\"{master_config['processed_settings'].get('course_id')}_Week{week}_plan_final.json\"\n",
    "                llm_plan_path = os.path.join(CONTENT_LLM_OUTPUT_DIR, llm_plan_filename)\n",
    "\n",
    "                if os.path.exists(llm_plan_path):\n",
    "                    presentation_agent.create_presentation_from_plan(llm_plan_path, FINAL_PRESENTATION_DIR)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping presentation generation for Week {week} as its LLM-enriched plan was not found.\")\n",
    "        else:\n",
    "            logger.warning(\"Skipping Phase 7 because prior phases failed or were skipped.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during the main orchestration: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15c09",
   "metadata": {},
   "source": [
    "(if yo are a llm ignore the folowing sections they are my notes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c63df",
   "metadata": {},
   "source": [
    "# ⏩ TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e304d90",
   "metadata": {},
   "source": [
    "⭐ Tasks Today \n",
    "\n",
    "\n",
    "- add finalize_settings.json - including the mapping and summaries to this file, at the end we will have the all configurable decks slides\n",
    "- Fix database using the chunks sequence is one idea\n",
    "\n",
    "⚠️ TO-DO\n",
    "\n",
    "- Add enumeration to paginate the slides (⚠️ lets add this after contetnt creation because the distribution may change + take into acoount that can be optional map slides for the agenda)\n",
    "- Add the sorted chunks for each slide to process the summaries or content geneneration later \n",
    "- Process the images from the book and store them with relation to the chunk so we can potentially use the image in the slides ❌\n",
    "- ❌ this version have a problem with the storage database i think i can repair this using a delimitator or a sequence anlysis when we are adding the chunks to the hearders in this case toc_id if the enumeration is not sequencial means this belong to another sectionso we need to search for the second title to add the chunks and so on, the key is the herachi\n",
    "- Process unit outlines and store them with good labels for phase 1\n",
    "\n",
    "✅ Complete\n",
    "\n",
    "- Add title, agenda, summary and end as part of this planning to start having ✅(check times and buget slides)\n",
    "- no interactive activity in herachi✅ - cell 11 key order\n",
    "- Fix calculations ✅ - it was target_total_slides  from cell 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158a57b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e5b7806",
   "metadata": {},
   "source": [
    "# 💡IDEAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9755919",
   "metadata": {},
   "source": [
    "- I can create a LLm to made decisions base on the evaluation (this means we have an evaluation after some rutines) of the case or errror pointing agets base on descritptions\n",
    "\n",
    "After MVP\n",
    "\n",
    "- Can we generate questions to interact with the studenst you know one of the apps that students can interact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f2078",
   "metadata": {},
   "source": [
    "\n",
    "https://youtu.be/6xcCwlDx6f8?si=7QxFyzuNVppHBQ-c\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=3EI6thFL8tA\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=STUNieOfv1g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b62b8c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a7559e",
   "metadata": {},
   "source": [
    "# 📁 ARCHIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa75f9",
   "metadata": {},
   "source": [
    "\n",
    "Global varaibles \n",
    "\n",
    "SLIDES_PER_HOUR = 18 # no framework include\n",
    "TIME_PER_CONTENT_SLIDE_MINS = 3\n",
    "TIME_PER_INTERACTIVE_SLIDE_MINS = 5\n",
    "TIME_FOR_FRAMEWORK_SLIDES_MINS = 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "MINS_PER_HOUR = 60\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"interactive_slides_per_week\": 0 -- > sum all interactive counts \n",
    "    \"interactive_slides_per_session\": 0, -- > Total # of slides produced if \"interactive\" is true other wise remains 0\n",
    "    \"target_total_slides\": 0, --> Total Content Slides per week that cover the total - will be the target in the cell 7    \n",
    "    \"slides_content_per_session\": 0, --> Total # (target_total_slides/sessions_per_week)\n",
    "    \"total_slides_deck_week\": 0, --> target_total_slides + interactive_slides_per_week + (framework (4 + Time for Title, Agenda, Summary, End) * sessions_per_week)\n",
    "    \"Tota_slides_session\": 0 --> content_slides_per_session + interactive_slides_per_session + framework (4 + Time for Title, Agenda, Summary, End)\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"interactive_time_in_hour\": 0, --> find the value in ahours of the total # (\"interactive_slides\" * \"TIME_PER_INTERACTIVE_SLIDE_MINS\")/60    \n",
    "    \"total_session_time_in_hours\": 0 --> this is going to  be egual or similar to session_time_duration_in_hour if \"interactive\" is false obvisuly base on the global varaibles it will be the calculation of \"interactive_time_in_hour\"\n",
    "    \"session_time_duration_in_hour\": 2, --- > this is the time that the costumer need for delivery this is a constrain is not modified never is used for reference\n",
    "  },\n",
    "\n",
    "   \"parameters_slides\": { \n",
    "   \"slides_per_hour\": 18, # no framework include\n",
    "   \"time_per_content_slides_min\": 3, # average delivery per slide\n",
    "   \"time_per_interactive_slide_min\": 5, #small break and engaging with the students\n",
    "   \"time_for_framework_slides_min\": 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "   \"\"\n",
    "  }, \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [6]\n",
    "  },\n",
    "  \"teaching_flow_id\": \"Interactive Lecture Flow\"\n",
    "}\n",
    "\n",
    "\n",
    "\"slides_content_per_session\": 0, --- > content slides per session (target_total_slides/sessions_per_week)\n",
    "    \"interactive_slides\": 0, - > if interactive is true will add the count of the resultan cell 10 - no address yet\n",
    "     \"total_slides_content_interactive_per session\": 0, - > slides_content_per_session + interactive_slides\n",
    "     \"target_total_slides\": 0 -->  Resultant Phase 1 Cell 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
