{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9771e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION SUMMARY ---\n",
      "Processing Mode: EPUB\n",
      "Unit ID: ICT312\n",
      "Unit Outline Path: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "Book Path: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "Parsed UO Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "Parsed ToC Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n",
      "Vector DB Path: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub_v2\n",
      "Vector DB Collection: book_toc_guided_chunks_epub_v2\n",
      "--- SETUP COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}_v2\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0852ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a new cell after your imports, or within Cell 3 before the functions.\n",
    "# This code is based on the schema from your screenshot on page 4.\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# Define Pydantic models that match your JSON schema\n",
    "class UnitInformation(BaseModel):\n",
    "    unitCode: Optional[str] = None\n",
    "    unitName: Optional[str] = None\n",
    "    creditPoints: Optional[int] = None\n",
    "    unitRationale: Optional[str] = None\n",
    "    prerequisites: Optional[str] = None\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    taskName: str\n",
    "    description: str\n",
    "    dueWeek: Optional[str] = None\n",
    "    weightingPercent: Optional[int] = None\n",
    "    learningOutcomesAssessed: Optional[str] = None\n",
    "\n",
    "class WeeklyScheduleItem(BaseModel):\n",
    "    week: str\n",
    "    contentTopic: str\n",
    "    requiredReading: Optional[str] = None\n",
    "\n",
    "class ParsedUnitOutline(BaseModel):\n",
    "    unitInformation: UnitInformation\n",
    "    learningOutcomes: List[str]\n",
    "    assessments: List[Assessment]\n",
    "    weeklySchedule: List[WeeklyScheduleItem] \n",
    "    requiredReadings: List[str]\n",
    "    recommendedReadings: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200383d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 115\u001b[0m\n\u001b[1;32m    110\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get valid structured data from the LLM after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attempts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# --- In your execution block, call the new function ---\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# parse_and_save_outline(...) becomes:\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[43mparse_and_save_outline_robust\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFULL_PATH_UNIT_OUTLINE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPARSED_UO_JSON_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 71\u001b[0m, in \u001b[0;36mparse_and_save_outline_robust\u001b[0;34m(input_filepath, output_filepath, prompt_template, max_retries)\u001b[0m\n\u001b[1;32m     67\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to parse outline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Call the LLM\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     llm_output_str \u001b[38;5;241m=\u001b[39m \u001b[43mcall_ollama_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Find the JSON blob in the response\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     json_blob \u001b[38;5;241m=\u001b[39m parse_llm_json_output(llm_output_str) \u001b[38;5;66;03m# Your existing helper\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/tenacity/__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mcall_ollama_with_retry\u001b[0;34m(client, prompt)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@retry\u001b[39m(stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(\u001b[38;5;241m3\u001b[39m), wait\u001b[38;5;241m=\u001b[39mwait_exponential(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_ollama_with_retry\u001b[39m(client, prompt):\n\u001b[1;32m     30\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling Ollama model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOLLAMA_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOLLAMA_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama returned an empty or invalid response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py:342\u001b[0m, in \u001b[0;36mClient.chat\u001b[0;34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[1;32m    298\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    299\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[1;32m    178\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py:120\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensor2/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline_robust(\n",
    "    input_filepath: str, \n",
    "    output_filepath: str, \n",
    "    prompt_template: str,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    logger.info(f\"Starting to robustly process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from file: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    client = ollama.Client(host=OLLAMA_HOST)\n",
    "    current_prompt = prompt_template.format(outline_text=outline_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{max_retries} to parse outline.\")\n",
    "        \n",
    "        try:\n",
    "            # Call the LLM\n",
    "            llm_output_str = call_ollama_with_retry(client, current_prompt)\n",
    "            \n",
    "            # Find the JSON blob in the response\n",
    "            json_blob = parse_llm_json_output(llm_output_str) # Your existing helper\n",
    "            if not json_blob:\n",
    "                raise ValueError(\"LLM did not return a parsable JSON object.\")\n",
    "\n",
    "            # *** THE KEY VALIDATION STEP ***\n",
    "            # Try to parse the dictionary into your Pydantic model.\n",
    "            # This will raise a `ValidationError` if keys are wrong, types are wrong, or fields are missing.\n",
    "            parsed_data = ParsedUnitOutline.model_validate(json_blob)\n",
    "            \n",
    "            # If successful, save the validated data and exit the loop\n",
    "            logger.info(\"Successfully validated JSON structure against Pydantic model.\")\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                # Use .model_dump_json() for clean, validated output\n",
    "                f.write(parsed_data.model_dump_json(indent=2)) \n",
    "\n",
    "            logger.info(f\"Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "            return # Exit function on success\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Validation failed on attempt {attempt + 1}. Error: {e}\")\n",
    "            # Formulate a new prompt with the error message for self-correction\n",
    "            error_feedback = (\n",
    "                f\"\\n\\nYour previous attempt failed. You MUST correct the following errors:\\n\"\n",
    "                f\"{e}\\n\\n\"\n",
    "                f\"Please regenerate the entire JSON object, ensuring it strictly adheres to the schema \"\n",
    "                f\"and corrects these specific errors. Do not change any key names.\"\n",
    "            )\n",
    "            current_prompt = current_prompt + error_feedback # Append the error to the prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch other errors like network issues from call_ollama_with_retry\n",
    "            logger.error(f\"An unexpected error occurred on attempt {attempt + 1}: {e}\", exc_info=True)\n",
    "            # You might want to wait before retrying for non-validation errors\n",
    "            time.sleep(5)\n",
    "\n",
    "    logger.error(f\"Failed to get valid structured data from the LLM after {max_retries} attempts.\")\n",
    "\n",
    "\n",
    "# --- In your execution block, call the new function ---\n",
    "# parse_and_save_outline(...) becomes:\n",
    "parse_and_save_outline_robust(\n",
    "    input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "    output_filepath=PARSED_UO_JSON_PATH,\n",
    "    prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or epub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad22c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 4: Extract Book Table of Contents (ToC)\n",
    "# # This cell extracts the ToC from the specified book (EPUB or PDF)\n",
    "# # and saves it to the path defined in Cell 1.\n",
    "\n",
    "# from ebooklib import epub, ITEM_NAVIGATION\n",
    "# from bs4 import BeautifulSoup\n",
    "# import fitz  # PyMuPDF\n",
    "# import json\n",
    "\n",
    "# # --- EPUB Extraction Logic ---\n",
    "# def parse_navpoint(navpoint, level=0):\n",
    "#     # (Your existing parse_navpoint function)\n",
    "#     title = navpoint.navLabel.text.strip()\n",
    "#     content_tag = navpoint.content\n",
    "#     href = content_tag['src'] if content_tag else None\n",
    "\n",
    "#     # Add filtering logic here if needed\n",
    "#     node = {\"level\": level, \"title\": title, \"href\": href, \"children\": []}\n",
    "#     for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "#         child_node = parse_navpoint(child_navpoint, level + 1)\n",
    "#         if child_node: node[\"children\"].append(child_node)\n",
    "#     return node\n",
    "\n",
    "# def parse_li(li_element, level=0):\n",
    "#     # (Your existing parse_li function)\n",
    "#     a_tag = li_element.find('a')\n",
    "#     if a_tag:\n",
    "#         title = a_tag.get_text(strip=True)\n",
    "#         href = a_tag.get('href', None)\n",
    "#         # Add filtering logic here if needed\n",
    "#         node = {\"level\": level, \"title\": title, \"href\": href, \"children\": []}\n",
    "#         nested_ol = li_element.find('ol')\n",
    "#         if nested_ol:\n",
    "#             for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "#                 child_node = parse_li(sub_li, level + 1)\n",
    "#                 if child_node: node[\"children\"].append(child_node)\n",
    "#         return node\n",
    "#     return None\n",
    "\n",
    "# def extract_epub_toc(epub_path, output_json_path):\n",
    "#     print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "#     toc_data = []\n",
    "#     book = epub.read_epub(epub_path)\n",
    "#     for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "#         soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "#         if nav_item.get_name().endswith('.ncx'):\n",
    "#             print(\"INFO: Found EPUB 2 (NCX) Table of Contents.\")\n",
    "#             navmap = soup.find('navMap')\n",
    "#             if navmap:\n",
    "#                 for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "#                     node = parse_navpoint(navpoint, level=0)\n",
    "#                     if node: toc_data.append(node)\n",
    "#         else:\n",
    "#             print(\"INFO: Found EPUB 3 (XHTML) Table of Contents.\")\n",
    "#             toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "#             if toc_nav:\n",
    "#                 top_ol = toc_nav.find('ol')\n",
    "#                 if top_ol:\n",
    "#                     for li in top_ol.find_all('li', recursive=False):\n",
    "#                         node = parse_li(li, level=0)\n",
    "#                         if node: toc_data.append(node)\n",
    "#         if toc_data: break\n",
    "    \n",
    "#     if toc_data:\n",
    "#         os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "#         with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "#         print(f\" Successfully wrote EPUB ToC to: {output_json_path}\")\n",
    "#     else:\n",
    "#         print(\" WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# # --- PDF Extraction Logic ---\n",
    "# def build_pdf_hierarchy(toc_list):\n",
    "#     \"\"\"\n",
    "#     Builds a hierarchical structure from a flat ToC list from PyMuPDF.\n",
    "#     MODIFIED: Normalizes levels to start at 0 for consistency with EPUB.\n",
    "#     \"\"\"\n",
    "#     root = []\n",
    "#     # The parent_stack keys are now level-based, starting from -1 for the root's parent.\n",
    "#     parent_stack = {-1: {\"children\": root}}\n",
    "\n",
    "#     for level, title, page in toc_list:\n",
    "#         # --- FIX: NORMALIZE LEVEL TO START AT 0 ---\n",
    "#         # fitz/PyMuPDF ToC levels start at 1, so we subtract 1.\n",
    "#         normalized_level = level - 1\n",
    "\n",
    "#         node = {\n",
    "#             \"level\": normalized_level,\n",
    "#             \"title\": title.strip(),\n",
    "#             \"page\": page,\n",
    "#             \"children\": []\n",
    "#         }\n",
    "\n",
    "#         # Find the correct parent in the stack. The parent's level is one less than the current node's.\n",
    "#         # This logic correctly places the node under its parent in the hierarchy.\n",
    "#         parent_node = parent_stack[normalized_level - 1]\n",
    "#         parent_node[\"children\"].append(node)\n",
    "\n",
    "#         # Add the current node to the stack so it can be a parent for subsequent nodes.\n",
    "#         parent_stack[normalized_level] = node\n",
    "\n",
    "#     return root\n",
    "\n",
    "# def extract_pdf_toc(pdf_path, output_json_path):\n",
    "#     print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "#     try:\n",
    "#         doc = fitz.open(pdf_path)\n",
    "#         toc = doc.get_toc()\n",
    "#         if not toc:\n",
    "#             print(\" WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "#             hierarchical_toc = []\n",
    "#         else:\n",
    "#             print(f\"INFO: Found {len(toc)} bookmark entries.\")\n",
    "#             hierarchical_toc = build_pdf_hierarchy(toc)\n",
    "        \n",
    "#         os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "#         with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "#         print(f\" Successfully wrote PDF ToC to: {output_json_path}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# # --- Execute ToC Extraction ---\n",
    "# if PROCESS_EPUB:\n",
    "#     extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "# else:\n",
    "#     extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b489050",
   "metadata": {},
   "source": [
    "this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EPUB ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "INFO: Found EPUB 2 (NCX) Table of Contents.\n",
      " Successfully wrote EPUB ToC to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "# # Cell 4: Extract Book Table of Contents (ToC)\n",
    "# # This cell extracts the ToC from the specified book (EPUB or PDF)\n",
    "# # and saves it to the path defined in Cell 1.\n",
    "\n",
    "# from ebooklib import epub, ITEM_NAVIGATION\n",
    "# from bs4 import BeautifulSoup\n",
    "# import fitz  # PyMuPDF\n",
    "# import json\n",
    "\n",
    "# # --- EPUB Extraction Logic ---\n",
    "# def parse_navpoint(navpoint, level=0):\n",
    "#     # (Your existing parse_navpoint function)\n",
    "#     title = navpoint.navLabel.text.strip()\n",
    "#     # Add filtering logic here if needed\n",
    "#     node = {\"level\": level, \"title\": title, \"children\": []}\n",
    "#     for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "#         child_node = parse_navpoint(child_navpoint, level + 1)\n",
    "#         if child_node: node[\"children\"].append(child_node)\n",
    "#     return node\n",
    "\n",
    "# def parse_li(li_element, level=0):\n",
    "#     # (Your existing parse_li function)\n",
    "#     a_tag = li_element.find('a')\n",
    "#     if a_tag:\n",
    "#         title = a_tag.get_text(strip=True)\n",
    "#         # Add filtering logic here if needed\n",
    "#         node = {\"level\": level, \"title\": title, \"children\": []}\n",
    "#         nested_ol = li_element.find('ol')\n",
    "#         if nested_ol:\n",
    "#             for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "#                 child_node = parse_li(sub_li, level + 1)\n",
    "#                 if child_node: node[\"children\"].append(child_node)\n",
    "#         return node\n",
    "#     return None\n",
    "\n",
    "# def extract_epub_toc(epub_path, output_json_path):\n",
    "#     print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "#     toc_data = []\n",
    "#     book = epub.read_epub(epub_path)\n",
    "#     for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "#         soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "#         if nav_item.get_name().endswith('.ncx'):\n",
    "#             print(\"INFO: Found EPUB 2 (NCX) Table of Contents.\")\n",
    "#             navmap = soup.find('navMap')\n",
    "#             if navmap:\n",
    "#                 for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "#                     node = parse_navpoint(navpoint, level=0)\n",
    "#                     if node: toc_data.append(node)\n",
    "#         else:\n",
    "#             print(\"INFO: Found EPUB 3 (XHTML) Table of Contents.\")\n",
    "#             toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "#             if toc_nav:\n",
    "#                 top_ol = toc_nav.find('ol')\n",
    "#                 if top_ol:\n",
    "#                     for li in top_ol.find_all('li', recursive=False):\n",
    "#                         node = parse_li(li, level=0)\n",
    "#                         if node: toc_data.append(node)\n",
    "#         if toc_data: break\n",
    "    \n",
    "#     if toc_data:\n",
    "#         os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "#         with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "#         print(f\" Successfully wrote EPUB ToC to: {output_json_path}\")\n",
    "#     else:\n",
    "#         print(\" WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# # --- PDF Extraction Logic ---\n",
    "# def build_pdf_hierarchy(toc_list):\n",
    "#     \"\"\"\n",
    "#     Builds a hierarchical structure from a flat ToC list from PyMuPDF.\n",
    "#     MODIFIED: Normalizes levels to start at 0 for consistency with EPUB.\n",
    "#     \"\"\"\n",
    "#     root = []\n",
    "#     # The parent_stack keys are now level-based, starting from -1 for the root's parent.\n",
    "#     parent_stack = {-1: {\"children\": root}}\n",
    "\n",
    "#     for level, title, page in toc_list:\n",
    "#         # --- FIX: NORMALIZE LEVEL TO START AT 0 ---\n",
    "#         # fitz/PyMuPDF ToC levels start at 1, so we subtract 1.\n",
    "#         normalized_level = level - 1\n",
    "\n",
    "#         node = {\n",
    "#             \"level\": normalized_level,\n",
    "#             \"title\": title.strip(),\n",
    "#             \"page\": page,\n",
    "#             \"children\": []\n",
    "#         }\n",
    "\n",
    "#         # Find the correct parent in the stack. The parent's level is one less than the current node's.\n",
    "#         # This logic correctly places the node under its parent in the hierarchy.\n",
    "#         parent_node = parent_stack[normalized_level - 1]\n",
    "#         parent_node[\"children\"].append(node)\n",
    "\n",
    "#         # Add the current node to the stack so it can be a parent for subsequent nodes.\n",
    "#         parent_stack[normalized_level] = node\n",
    "\n",
    "#     return root\n",
    "\n",
    "# def extract_pdf_toc(pdf_path, output_json_path):\n",
    "#     print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "#     try:\n",
    "#         doc = fitz.open(pdf_path)\n",
    "#         toc = doc.get_toc()\n",
    "#         if not toc:\n",
    "#             print(\" WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "#             hierarchical_toc = []\n",
    "#         else:\n",
    "#             print(f\"INFO: Found {len(toc)} bookmark entries.\")\n",
    "#             hierarchical_toc = build_pdf_hierarchy(toc)\n",
    "        \n",
    "#         os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "#         with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "#         print(f\" Successfully wrote PDF ToC to: {output_json_path}\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# # --- Execute ToC Extraction ---\n",
    "# if PROCESS_EPUB:\n",
    "#     extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "# else:\n",
    "#     extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effd9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 05:10:12,833 - INFO - Processing book 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub'...\n",
      "2025-06-26 05:10:14,768 - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-06-26 05:10:14,768 - INFO - NumExpr defaulting to 16 threads.\n",
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-06-26 05:10:21,588 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-26 05:10:21,589 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-26 05:10:24,902 - INFO - Enriching documents with robust hierarchical metadata...\n",
      "2025-06-26 05:10:29,337 - INFO - Split documents into 12498 final chunks.\n",
      "2025-06-26 05:10:29,337 - INFO - Assigning sequence IDs and canonical paths...\n",
      "2025-06-26 05:10:29,415 - INFO - Sanitizing all chunk metadata for ChromaDB compatibility...\n",
      "2025-06-26 05:10:29,447 - INFO - Initializing embedding model 'nomic-embed-text' and creating new vector database...\n",
      "2025-06-26 05:10:29,497 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-26 05:11:40,679 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 05:12:54,167 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 05:13:19,488 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 05:13:20,617 - INFO -  Vector DB created successfully. Collection contains 12498 documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Hierarchical Vector Database (Definitive Final Version)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Converts text to a canonical form for matching: lowercase, no punctuation, single spaces.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "    if isinstance(value, list): return \", \".join(map(str, value))\n",
    "    if isinstance(value, dict): return json.dumps(value)\n",
    "    if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    return str(value)\n",
    "\n",
    "# --- CORE PROCESSING FUNCTION ---\n",
    "# In Cell 5, replace the entire function with this one.\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "def process_book_with_extracted_toc(book_path: str, extracted_toc_json_path: str,\n",
    "                                    chunk_size: int, chunk_overlap: int) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    logger.info(f\"Processing book '{os.path.basename(book_path)}'...\")\n",
    "    try:\n",
    "        with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FATAL: Error loading ToC JSON: {e}\"); return ([], [])\n",
    "\n",
    "    # --- [PATCH A - PART 1] ---\n",
    "    # Build the href -> nav_path mapping from the full ToC\n",
    "    href_to_navpath = {}\n",
    "    def _walk(nodes, trail):\n",
    "        for n in nodes:\n",
    "            # Add the current node's title to the trail for its children\n",
    "            current_trail = trail + [n.get(\"title\", \"\")]\n",
    "            href = n.get(\"href\")\n",
    "            if href:\n",
    "                # Store the full trail using the href as the key\n",
    "                href_to_navpath[href.split(\"#\")[0]] = current_trail\n",
    "            \n",
    "            if n.get(\"children\"):\n",
    "                _walk(n[\"children\"], current_trail)\n",
    "    \n",
    "    _walk(hierarchical_toc, [])\n",
    "    # --- [END PATCH A - PART 1] ---\n",
    "\n",
    "    loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "    all_raw_book_docs = loader.load()\n",
    "    \n",
    "    logger.info(\"Enriching documents with robust hierarchical metadata...\")\n",
    "    # (The following section combines the original heading-based enrichment with the new full_path enrichment)\n",
    "    flat_toc_entries = []\n",
    "    def _flatten_toc_recursive(nodes: List[Dict[str, Any]], path: List[str]):\n",
    "        for node in nodes:\n",
    "            if title := node.get(\"title\", \"\").strip():\n",
    "                new_path = path + [title]\n",
    "                flat_toc_entries.append({\"full_title_for_matching\": title, \"titles_path\": new_path})\n",
    "                if node.get(\"children\"):\n",
    "                    _flatten_toc_recursive(node[\"children\"], new_path)\n",
    "    _flatten_toc_recursive(hierarchical_toc, [])\n",
    "    \n",
    "    toc_title_set = {normalize_text(entry[\"full_title_for_matching\"]) for entry in flat_toc_entries}\n",
    "    normalized_title_to_path_map = {normalize_text(entry[\"full_title_for_matching\"]): entry[\"titles_path\"] for entry in flat_toc_entries}\n",
    "\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    current_hierarchy = {}\n",
    "    for doc in all_raw_book_docs:\n",
    "        normalized_text = normalize_text(doc.page_content)\n",
    "        if not normalized_text: continue\n",
    "\n",
    "        match, score, _ = process.extractOne(normalized_text, toc_title_set, scorer=fuzz.token_set_ratio)\n",
    "        if score > 95:\n",
    "            current_hierarchy = {}\n",
    "            path = normalized_title_to_path_map[match]\n",
    "            for i, title in enumerate(path):\n",
    "                current_hierarchy[f\"level_{i}_title\"] = title\n",
    "        \n",
    "        if not current_hierarchy: continue\n",
    "            \n",
    "        new_metadata = doc.metadata.copy()\n",
    "        new_metadata.update(current_hierarchy)\n",
    "        final_documents_with_metadata.append(Document(page_content=doc.page_content, metadata=new_metadata))\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    \n",
    "    logger.info(f\"Split documents into {len(final_chunks)} final chunks.\")\n",
    "    logger.info(\"Assigning sequence IDs and canonical paths...\")\n",
    "    \n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        chunk.metadata['global_chunk_sequence_id'] = i\n",
    "        path_parts = [chunk.metadata.get(f\"level_{j}_title\", \"\") for j in range(6)]\n",
    "        raw_path = \" > \".join(filter(None, path_parts))\n",
    "        chunk.metadata['toc_path'] = raw_path\n",
    "        chunk.metadata['toc_path_norm'] = normalize_text(raw_path)\n",
    "        \n",
    "        # --- [PATCH A - PART 2] ---\n",
    "        # Attach the canonical navMap path to every chunk.\n",
    "        source_file = chunk.metadata.get(\"source\", \"\").split(\"#\")[0]\n",
    "        nav_path = href_to_navpath.get(source_file, [])\n",
    "        full_nav = \" > \".join(filter(None, nav_path))\n",
    "        \n",
    "        chunk.metadata[\"toc_path_full\"] = full_nav\n",
    "        chunk.metadata[\"toc_path_full_norm\"] = normalize_text(full_nav)\n",
    "        # --- [END PATCH A - PART 2] ---\n",
    "\n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "    logger.error(\"CRITICAL: Pre-extracted ToC file not found. Run Cell 4 first.\")\n",
    "else:\n",
    "    final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "        book_path=BOOK_PATH, extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH, \n",
    "        chunk_size=500, chunk_overlap=50\n",
    "    )\n",
    "    if final_chunks_for_db:\n",
    "        logger.info(\"Sanitizing all chunk metadata for ChromaDB compatibility...\")\n",
    "        for chunk in final_chunks_for_db:\n",
    "            chunk.metadata = {k: clean_metadata_for_chroma(v) for k, v in chunk.metadata.items()}\n",
    "        \n",
    "        if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "            logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "            shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "        logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=final_chunks_for_db, embedding=embedding_model, \n",
    "            persist_directory=CHROMA_PERSIST_DIR, collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        count = vector_db._collection.count()\n",
    "        print(\"-\" * 50); logger.info(f\" Vector DB created successfully. Collection contains {count} documents.\"); print(\"-\" * 50)\n",
    "    else:\n",
    "        logger.error(\" Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d160e",
   "metadata": {},
   "source": [
    "### Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e559480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5.1: Smoke Test & Sanity Check\n",
    "\n",
    "# print(\"Smoke Test & Sanity Check\")\n",
    "\n",
    "# if 'final_chunks_for_db' in locals() and final_chunks_for_db:\n",
    "#     logger.info(\" `final_chunks_for_db` object exists. Running sanity checks...\")\n",
    "\n",
    "#     # 1. Peek at one sample chunk's metadata\n",
    "#     print(\"\\n PEEKING at metadata of the first chunk:\")\n",
    "#     sample_metadata = final_chunks_for_db[0].metadata\n",
    "#     print(json.dumps(sample_metadata, indent=2))\n",
    "#     assert \"toc_path\" in sample_metadata, \"FATAL: 'toc_path' field is missing!\"\n",
    "#     assert \"toc_path_norm\" in sample_metadata, \"FATAL: 'toc_path_norm' field is missing!\"\n",
    "#     logger.info(\" Sample chunk contains the required 'toc_path' and 'toc_path_norm' fields.\")\n",
    "\n",
    "#     # 2. Confirm every chunk has the new field\n",
    "#     all_chunks_have_norm_path = all(\"toc_path_norm\" in c.metadata for c in final_chunks_for_db)\n",
    "#     assert all_chunks_have_norm_path, \"FATAL: Not all chunks have the 'toc_path_norm' metadata field!\"\n",
    "#     logger.info(f\" Verified that all {len(final_chunks_for_db)} chunks have the 'toc_path_norm' field.\")\n",
    "\n",
    "#     # 3. Sanity-count distinct paths\n",
    "#     unique_paths = {c.metadata[\"toc_path_norm\"] for c in final_chunks_for_db if \"toc_path_norm\" in c.metadata}\n",
    "#     logger.info(f\" Found {len(unique_paths)} unique normalized paths in the dataset.\")\n",
    "    \n",
    "#     print(\"\\n\" + \"*\"*80)\n",
    "#     print(\"SMOKE TEST PASSED. The data is correctly structured. You can now proceed to build the database.\".center(80))\n",
    "#     print(\"*\"*80)\n",
    "\n",
    "# else:\n",
    "#     logger.error(\" `final_chunks_for_db` not found or is empty. Run the main part of Cell 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "### Verification Test Strategy\n",
    "The script automatically validates the vector database by performing four dynamic tests that increase in complexity, moving from a general health check to specific application-level requirements.\n",
    "\n",
    "Basic Retrieval Test:\n",
    "- Goal: Confirm the database is live and its content is broadly relevant to the course subject.\n",
    "- Method: It performs a simple search using the course's unitName (e.g., \"Digital Forensic\") extracted from the unit outline.\n",
    "- Success means: The database is online, and the ingested content is thematically correct.\n",
    "\n",
    "Deep Hierarchy Test:\n",
    "- Goal: Verify the structural integrity of the metadata, ensuring text is correctly tagged with its full, multi-level context (e.g., Part -> Chapter -> Section).\n",
    "- Method: It randomly picks a deeply nested sub-section from the Table of Contents and performs a search that is filtered to match that exact hierarchical path.\n",
    "- Success means: The data ingestion process is correctly assigning detailed, nested parentage to all text chunks.\n",
    "\n",
    "Advanced Unit Outline Alignment Test:\n",
    "- Goal: Ensure the system can correctly map a weekly syllabus topic to the right chapter(s) in the book, adapting to different ToC structures (e.g., flat chapters vs. chapters inside \"Parts\").\n",
    "- Method: It randomly selects a week, finds all required chapter numbers from the reading list, and dynamically determines the correct metadata level to check. It then verifies that a search for the weekly topic retrieves chunks belonging to the correct chapters.\n",
    "- Success means: The database is directly useful for its primary purpose: linking the course structure to the source textbook reliably.\n",
    "\n",
    "Content Sequence Test (PDF-only):\n",
    "- Goal: Check if retrieved content can be re-ordered chronologically to form a coherent narrative.\n",
    "- Method: It retrieves multiple chunks for a random topic, sorts them using the page_number metadata, and verifies the page numbers are in ascending order.\n",
    "- Success means: The database contains the necessary metadata to reconstruct the original flow of the book's content, which is crucial for generating logical summaries or lecture material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602f941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 6: Verify Vector Database (Definitive Final Suite with Diagnostics)\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import re\n",
    "# import random\n",
    "# import logging\n",
    "# from typing import List, Dict, Any, Tuple, Optional\n",
    "# import pandas as pd\n",
    "\n",
    "# try:\n",
    "#     from langchain_chroma import Chroma\n",
    "#     from langchain_core.documents import Document\n",
    "#     from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "#     langchain_available = True\n",
    "# except ImportError:\n",
    "#     langchain_available = False\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # --- HELPER FUNCTIONS ---\n",
    "# def normalize_text(text: str) -> str:\n",
    "#     \"\"\"Converts text to a canonical form for matching: lowercase, no punctuation, single spaces.\"\"\"\n",
    "#     if not text: return \"\"\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#     return text\n",
    "\n",
    "# def print_header(text: str, char: str = \"=\"):\n",
    "#     \"\"\"Prints a centered header to the console.\"\"\"\n",
    "#     print(\"\\n\" + char * 80)\n",
    "#     print(text.center(80))\n",
    "#     print(char * 80)\n",
    "\n",
    "# def print_results(operation_name: str, results: list, where_filter: Optional[Dict] = None):\n",
    "#     \"\"\"Prints the results of a vector store operation in a clear, readable format.\"\"\"\n",
    "#     print(\"\\n\" + \"-\"*50)\n",
    "#     print(f\" Operation: '{operation_name}'\")\n",
    "#     if where_filter:\n",
    "#         print(f\" Filter: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "#     if not results:\n",
    "#         print(\"\\n RESULTS: No documents found for this operation.\")\n",
    "#         print(\"-\" * 50)\n",
    "#         return\n",
    "        \n",
    "#     print(f\"\\n RESULTS: Found {len(results)} documents. Displaying details for top 3:\")\n",
    "#     for i, doc in enumerate(results[:3]):\n",
    "#         print(f\"\\n--- Result {i+1} ---\")\n",
    "#         print(f\"Content: '{doc.page_content.replace('', ' ').strip()[:150]}...'\")\n",
    "#         print(f\"Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# def find_leaf_section(nodes: List[Dict]) -> Optional[List[str]]:\n",
    "#     \"\"\"Finds a random, deep, leaf-node section from the ToC.json.\"\"\"\n",
    "#     leaf_paths = []\n",
    "#     def _traverse(sub_nodes, current_path):\n",
    "#         for node in sub_nodes:\n",
    "#             new_path = current_path + [node.get('title', 'Untitled')]\n",
    "#             if not node.get('children'):\n",
    "#                 if len(new_path) > 2: # Ensure it's at least level 2 deep for a meaningful test\n",
    "#                     leaf_paths.append(new_path)\n",
    "#             else:\n",
    "#                 _traverse(node['children'], new_path)\n",
    "#     _traverse(nodes, [])\n",
    "#     return random.choice(leaf_paths) if leaf_paths else None\n",
    "\n",
    "# # --- TEST CASE FUNCTIONS ---\n",
    "# def run_test(name: str, goal: str, func, *args):\n",
    "#     \"\"\"A wrapper to run each test and print its final status.\"\"\"\n",
    "#     print_header(name, char=\"-\")\n",
    "#     logger.info(f\" GOAL: {goal}\")\n",
    "#     status = \" FAILED\"\n",
    "#     try:\n",
    "#         if func(*args):\n",
    "#             status = \" PASSED\"\n",
    "#             return True\n",
    "#         return False\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"ERROR: {e}\", exc_info=False)\n",
    "#         return False\n",
    "#     finally:\n",
    "#         print(f\"\\n--> {name} Status: {status}\")\n",
    "\n",
    "# def _health_and_hierarchy_report(db):\n",
    "#     \"\"\"Provides a high-level diagnostic overview of the database's structure.\"\"\"\n",
    "#     print_header(\"Database Health & Hierarchy Report\", char=\"*\")\n",
    "#     total_docs = db._collection.count()\n",
    "#     logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "#     retrieved_data = db.get(limit=total_docs, include=[\"metadatas\"])\n",
    "#     all_metadatas = retrieved_data['metadatas']\n",
    "    \n",
    "#     level_0_counts = {}\n",
    "#     for meta in all_metadatas:\n",
    "#         level_0_title = meta.get(\"level_0_title\")\n",
    "#         if level_0_title:\n",
    "#             level_0_counts[level_0_title] = level_0_counts.get(level_0_title, 0) + 1\n",
    "            \n",
    "#     assert level_0_counts, \"CRITICAL: No 'level_0_title' metadata found in any chunks!\"\n",
    "    \n",
    "#     print(\"\\n Found the following top-level sections and their chunk counts:\")\n",
    "#     df = pd.DataFrame(list(level_0_counts.items()), columns=['Top-Level Section (level_0_title)', 'Chunk Count'])\n",
    "#     df = df.sort_values(by='Chunk Count', ascending=False).reset_index(drop=True)\n",
    "#     print(df.to_string())\n",
    "#     return True\n",
    "\n",
    "# def _deep_hierarchy_test(db, toc):\n",
    "#     \"\"\"Verifies a deep leaf section can be retrieved via a similarity search with a strict filter.\"\"\"\n",
    "#     path = find_leaf_section(toc)\n",
    "#     assert path, \"Could not find a leaf section to test.\"\n",
    "#     section_title = path[-1]\n",
    "    \n",
    "#     # Use the normalized path for the filter, as created in Cell 5\n",
    "#     full_path_norm = normalize_text(' > '.join(path))\n",
    "#     w_filter = {\"toc_path_norm\": {\"$eq\": full_path_norm}}\n",
    "    \n",
    "#     # Use similarity search on the raw title for a realistic test\n",
    "#     results = db.similarity_search(section_title, k=1, filter=w_filter)\n",
    "#     print_results(f\"Deep hierarchy check for: '{section_title}'\", results, w_filter)\n",
    "#     assert len(results) > 0, \"Deeply filtered similarity search returned no results.\"\n",
    "#     return True\n",
    "\n",
    "# def _narrative_flow_test(db, toc):\n",
    "#     \"\"\"Verifies chunks for a specific leaf section are sequentially ordered.\"\"\"\n",
    "#     path = find_leaf_section(toc)\n",
    "#     assert path, \"Could not find a leaf section in ToC to test.\"\n",
    "    \n",
    "#     full_path_norm = normalize_text(' > '.join(path))\n",
    "#     w_filter = {\"toc_path_norm\": {\"$eq\": full_path_norm}}\n",
    "    \n",
    "#     operation_name = f\"Narrative flow check for: {' > '.join(path)}\"\n",
    "#     print_results(operation_name, [], w_filter) # Announce the operation\n",
    "    \n",
    "#     retrieved_data = db.get(where=w_filter, limit=200)\n",
    "    \n",
    "#     docs = []\n",
    "#     if retrieved_data and retrieved_data['ids']:\n",
    "#         docs = [Document(page_content=retrieved_data['documents'][i], metadata=retrieved_data['metadatas'][i]) for i in range(len(retrieved_data['ids']))]\n",
    "    \n",
    "#     print_results(f\"Results for '{path[-1]}'\", docs) # Show what was found\n",
    "#     assert len(docs) > 1, \"Not enough chunks retrieved to test sequence.\"\n",
    "    \n",
    "#     docs.sort(key=lambda x: x.metadata.get('global_chunk_sequence_id', -1))\n",
    "#     sequence_numbers = [doc.metadata['global_chunk_sequence_id'] for doc in docs]\n",
    "    \n",
    "#     print(f\"\\nANALYSIS: Retrieved and sorted global sequence numbers: {sequence_numbers}\")\n",
    "#     assert all(sequence_numbers[i] < sequence_numbers[i+1] for i in range(len(sequence_numbers)-1)), \"Global sequence is not strictly increasing.\"\n",
    "#     return True\n",
    "\n",
    "# # --- MAIN VERIFICATION EXECUTION ---\n",
    "# def run_verification():\n",
    "#     print_header(\"Database Verification Process\")\n",
    "#     if not langchain_available: logger.error(\"LangChain libraries not found.\"); return\n",
    "#     required_paths = [CHROMA_PERSIST_DIR, PRE_EXTRACTED_TOC_JSON_PATH, PARSED_UO_JSON_PATH]\n",
    "#     if not all(os.path.exists(p) for p in required_paths): logger.error(\"Missing file/dir. Run Cells 4 & 5.\"); return\n",
    "#     with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: toc_data = json.load(f)\n",
    "    \n",
    "#     logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' to connect to DB...\")\n",
    "#     embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "#     vector_store = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings, collection_name=CHROMA_COLLECTION_NAME)\n",
    "    \n",
    "#     tests = [\n",
    "#         (\"Diagnostic: Health & Hierarchy Report\", \"Provides a high-level overview of the DB's structure.\", _health_and_hierarchy_report, (vector_store,)),\n",
    "#         (\"Test 1: Deep Hierarchy & Filter\", \"Checks if a deep section can be retrieved with a strict filter.\", _deep_hierarchy_test, (vector_store, toc_data)),\n",
    "#         (\"Test 2: Narrative Flow\", \"Checks if chunks within a specific leaf sub-section are correctly ordered.\", _narrative_flow_test, (vector_store, toc_data))\n",
    "#     ]\n",
    "#     results_summary = [run_test(name, goal, func, *args) for name, goal, func, args in tests]\n",
    "    \n",
    "#     passed_count = sum(filter(None, results_summary))\n",
    "#     failed_count = len(results_summary) - passed_count\n",
    "#     print_header(\"Final Verification Summary\")\n",
    "#     print(f\"Total Tests Run: {len(results_summary)} |  Passed: {passed_count} |  Failed: {failed_count}\")\n",
    "#     print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# # --- Execute Verification ---\n",
    "# run_verification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cf3ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 05:18:23,578 - INFO - Initializing embedding model 'nomic-embed-text' to connect to DB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 05:18:23,588 - INFO -  GOAL: Provides a high-level overview of the DB's structure.\n",
      "2025-06-26 05:18:23,591 - INFO - Retrieving metadata for all 12498 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         Database Verification Process                          \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Diagnostic: Health & Hierarchy Report                      \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 05:18:24,404 - INFO -  GOAL: Checks if chunks within a leaf section are found and correctly ordered.\n",
      "2025-06-26 05:18:24,486 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 05:18:24,487 - ERROR - ERROR: Expected where operator to be one of $gt, $gte, $lt, $lte, $ne, $eq, $in, $nin, got $contains in query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Found the following top-level sections and their chunk counts:\n",
      "                                                                  Top-Level Section  Chunk Count\n",
      "0                     Lab Manual for Guide to Computer Forensics and Investigations         5444\n",
      "1                                        Chapter 6. Current Digital Forensics Tools         1440\n",
      "2                                   Chapter 5. Working with Windows and CLI Systems         1310\n",
      "3   Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics          699\n",
      "4                                         Chapter 16. Ethics for the Expert Witness          593\n",
      "5                                                       Chapter 13. Cloud Forensics          575\n",
      "6      Chapter 1. Understanding the Digital Forensics Profession and Investigations          365\n",
      "7                               Chapter 2. The Investigators Office and Laboratory          300\n",
      "8                            Chapter 15. Expert Testimony in Digital Investigations          286\n",
      "9                                   Chapter 4. Processing Crime and Incident Scenes          252\n",
      "10                                                      Chapter 3. Data Acquisition          231\n",
      "11                                             Chapter 8. Recovering Graphics Files          221\n",
      "12                               Chapter 11. E-mail and Social Media Investigations          176\n",
      "13                                      Chapter 7. Linux and Macintosh File Systems          144\n",
      "14                             Chapter 9. Digital Forensics Analysis and Validation          144\n",
      "15                          Chapter 14. Report Writing for High-Tech Investigations          140\n",
      "16                 Chapter 12. Mobile Device Forensics and the Internet of Anything           93\n",
      "17                                                                About the Authors           18\n",
      "18                                                                   Copyright Page           16\n",
      "19                                                                          Preface           15\n",
      "20                                                                  Acknowledgments           10\n",
      "21                                                                       Title Page            7\n",
      "22                               Appendix D. Legacy File System and Forensics Tools            6\n",
      "23                                         Appendix B. Digital Forensics References            6\n",
      "24                                 Appendix C. Digital Forensics Lab Considerations            5\n",
      "25                                                                       Cover Page            1\n",
      "26                                        Appendix A. Certification Test References            1\n",
      "\n",
      "--> Diagnostic: Health & Hierarchy Report Status:  PASSED\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Test: Deep Hierarchy & Narrative Flow                      \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      " Operation: 'Deep Hierarchy & Narrative Flow check for: Chapter 9. Digital Forensics Analysis and Validation > Addressing Data-Hiding Techniques > Hiding Files by Using the OS'\n",
      " Filter: {\n",
      "  \"$and\": [\n",
      "    {\n",
      "      \"toc_path_full_norm\": {\n",
      "        \"$contains\": \"addressing datahiding techniques\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"toc_path_full_norm\": {\n",
      "        \"$contains\": \"hiding files by using the os\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      " RESULTS: No documents found for this operation.\n",
      "--------------------------------------------------\n",
      "\n",
      "--> Test: Deep Hierarchy & Narrative Flow Status:  FAILED\n",
      "\n",
      "================================================================================\n",
      "                           Final Verification Summary                           \n",
      "================================================================================\n",
      "Total Tests Run: 2 |  Passed: 1 |  Failed: 1\n",
      "\n",
      "================================================================================\n",
      "                             Verification Complete                              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Vector Database (Definitive Final Version)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Converts text to a canonical form for matching: lowercase, no punctuation, single spaces.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def print_results(operation_name: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    \"\"\"Prints the results of a vector store operation in a clear, readable format.\"\"\"\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(f\" Operation: '{operation_name}'\")\n",
    "    if where_filter:\n",
    "        print(f\" Filter: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"\\n RESULTS: No documents found for this operation.\")\n",
    "        print(\"-\" * 50)\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n RESULTS: Found {len(results)} documents. Displaying details for top 3:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n--- Result {i+1} ---\")\n",
    "        print(f\"Content: '{doc.page_content.replace('', ' ').strip()[:150]}...'\")\n",
    "        print(f\"Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def find_leaf_section(nodes: List[Dict]) -> Optional[List[str]]:\n",
    "    \"\"\"Finds a random, deep, leaf-node section from the ToC.json.\"\"\"\n",
    "    leaf_paths = []\n",
    "    def _traverse(sub_nodes, current_path):\n",
    "        for node in sub_nodes:\n",
    "            new_path = current_path + [node.get('title', 'Untitled')]\n",
    "            if not node.get('children'):\n",
    "                if len(new_path) > 2:\n",
    "                    leaf_paths.append(new_path)\n",
    "            else:\n",
    "                _traverse(node['children'], new_path)\n",
    "    _traverse(nodes, [])\n",
    "    return random.choice(leaf_paths) if leaf_paths else None\n",
    "\n",
    "# --- TEST CASE FUNCTIONS ---\n",
    "def run_test(name: str, goal: str, func, *args):\n",
    "    \"\"\"A wrapper to run each test and print its final status.\"\"\"\n",
    "    print_header(name, char=\"-\")\n",
    "    logger.info(f\" GOAL: {goal}\")\n",
    "    status = \" FAILED\"\n",
    "    try:\n",
    "        if func(*args):\n",
    "            status = \" PASSED\"\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ERROR: {e}\", exc_info=False)\n",
    "        return False\n",
    "    finally:\n",
    "        print(f\"\\n--> {name} Status: {status}\")\n",
    "\n",
    "def _health_and_hierarchy_report(db):\n",
    "    \"\"\"Provides a high-level diagnostic overview of the database's structure.\"\"\"\n",
    "    total_docs = db._collection.count()\n",
    "    logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "    all_metadatas = db.get(limit=total_docs, include=[\"metadatas\"])['metadatas']\n",
    "    \n",
    "    level_0_counts = {}\n",
    "    for meta in all_metadatas:\n",
    "        level_0_title = meta.get(\"level_0_title\")\n",
    "        if level_0_title:\n",
    "            level_0_counts[level_0_title] = level_0_counts.get(level_0_title, 0) + 1\n",
    "            \n",
    "    assert level_0_counts, \"CRITICAL: No 'level_0_title' metadata found!\"\n",
    "    \n",
    "    print(\"\\n Found the following top-level sections and their chunk counts:\")\n",
    "    df = pd.DataFrame(list(level_0_counts.items()), columns=['Top-Level Section', 'Chunk Count'])\n",
    "    print(df.sort_values(by='Chunk Count', ascending=False).reset_index(drop=True).to_string())\n",
    "    return True\n",
    "\n",
    "def _deep_hierarchy_and_flow_test(db, toc):\n",
    "    \"\"\"The definitive test. It verifies both hierarchy and sequence.\"\"\"\n",
    "    path = find_leaf_section(toc)\n",
    "    assert path, \"Could not find a suitable leaf section in ToC to test.\"\n",
    "\n",
    "    # --- [PATCH B - FINAL VERSION] ---\n",
    "    # Use a tolerant '$contains' filter on the new authoritative metadata field.\n",
    "    assert len(path) >= 2, f\"Found path '{path}' is not deep enough for this test.\"\n",
    "    leaf_norm   = normalize_text(path[-1])\n",
    "    parent_norm = normalize_text(path[-2])\n",
    "\n",
    "    w_filter = {\n",
    "        \"$and\": [\n",
    "            {\"toc_path_full_norm\": {\"$contains\": parent_norm}},\n",
    "            {\"toc_path_full_norm\": {\"$contains\": leaf_norm}}\n",
    "        ]\n",
    "    }\n",
    "    # --- [END OF PATCH B] ---\n",
    "\n",
    "    operation_name = f\"Deep Hierarchy & Narrative Flow check for: {' > '.join(path)}\"\n",
    "    # Announce the operation and show the filter *before* executing the query.\n",
    "    print_results(operation_name, [], where_filter=w_filter) \n",
    "\n",
    "    # --- [FIX] ---\n",
    "    # Use similarity_search, which supports complex filters, instead of .get().\n",
    "    # We provide a simple query string because our goal is to test the filter, not semantic similarity.\n",
    "    docs = db.similarity_search(\n",
    "        query=path[-1], # Use the leaf title as the query\n",
    "        k=200,          # Retrieve up to 200 documents\n",
    "        filter=w_filter\n",
    "    )\n",
    "    # --- [END FIX] ---\n",
    "\n",
    "    print_results(f\"Results for '{path[-1]}'\", docs)\n",
    "    assert len(docs) > 0, \"Deep hierarchy search with '$contains' returned no results.\"\n",
    "    \n",
    "    if len(docs) > 1:\n",
    "        # Sort by the sequence ID to verify chronological order\n",
    "        docs.sort(key=lambda x: x.metadata.get('global_chunk_sequence_id', -1))\n",
    "        sequence_numbers = [doc.metadata.get('global_chunk_sequence_id') for doc in docs]\n",
    "\n",
    "        print(f\"\\nANALYSIS: Retrieved and sorted global sequence numbers:\\n{sequence_numbers}\")\n",
    "        # Ensure the sequence is strictly increasing\n",
    "        assert all(sequence_numbers[i] < sequence_numbers[i+1] for i in\n",
    "                   range(len(sequence_numbers)-1)), \"Global sequence is not strictly increasing.\"\n",
    "    else:\n",
    "        logger.warning(\"Only one chunk was retrieved; cannot test narrative flow sequence.\")\n",
    "        \n",
    "    return True\n",
    "\n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    if not langchain_available: logger.error(\"LangChain libraries not found.\"); return\n",
    "    required_paths = [CHROMA_PERSIST_DIR, PRE_EXTRACTED_TOC_JSON_PATH]\n",
    "    if not all(os.path.exists(p) for p in required_paths): logger.error(\"Missing file/dir. Run Cells 4 & 5.\"); return\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: toc_data = json.load(f)\n",
    "    \n",
    "    logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' to connect to DB...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings, collection_name=CHROMA_COLLECTION_NAME)\n",
    "    \n",
    "    tests = [\n",
    "        (\"Diagnostic: Health & Hierarchy Report\", \"Provides a high-level overview of the DB's structure.\", _health_and_hierarchy_report, (vector_store,)),\n",
    "        (\"Test: Deep Hierarchy & Narrative Flow\", \"Checks if chunks within a leaf section are found and correctly ordered.\", _deep_hierarchy_and_flow_test, (vector_store, toc_data))\n",
    "    ]\n",
    "    results_summary = [run_test(name, goal, func, *args) for name, goal, func, args in tests]\n",
    "    \n",
    "    passed_count = sum(filter(None, results_summary))\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    print_header(\"Final Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)} |  Passed: {passed_count} |  Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "run_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4e32f",
   "metadata": {},
   "source": [
    "# Full Database Health & Hierarchy Diagnostic Report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8878d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 05:14:02,922 - INFO - Connecting to the vector database...\n",
      "2025-06-26 05:14:02,937 - INFO - Retrieving metadata for all 12498 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "               Full Database Health & Hierarchy Diagnostic Report               \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 05:14:03,841 - INFO - Successfully retrieved all metadata.\n",
      "2025-06-26 05:14:03,842 - INFO - Reconstructing hierarchy from chunk metadata...\n",
      "2025-06-26 05:14:03,851 - INFO - Hierarchy reconstruction complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                         Reconstructed Hierarchy Report                         \n",
      "--------------------------------------------------------------------------------\n",
      "This report shows the full hierarchical structure discovered from the chunk metadata.\n",
      "'(Total Chunks: X, Direct: Y)' means X chunks in the whole branch, Y directly tagged to this heading.\n",
      "\n",
      "|__ Lab Manual for Guide to Computer Forensics and Investigations  (Total Chunks: 5444, Direct: 3)\n",
      "    |__ Chapter 16. Ethics for the Expert Witness  (Total Chunks: 1698, Direct: 118)\n",
      "        |__ Lab 16.1. Rebuilding an MFT Record from a Corrupt Image  (Total Chunks: 1568, Direct: 5)\n",
      "            |__ Review Questions  (Total Chunks: 788, Direct: 788)\n",
      "            |__ Objectives  (Total Chunks: 425, Direct: 298)\n",
      "                |__ Materials Required  (Total Chunks: 127, Direct: 127)\n",
      "            |__ Activity  (Total Chunks: 350, Direct: 337)\n",
      "                |__ Copying the Corrected MFT Record  (Total Chunks: 4, Direct: 4)\n",
      "                |__ Creating a Duplicate Forensic Image  (Total Chunks: 4, Direct: 4)\n",
      "                |__ Determining the Offset Byte Address of the Corrupt MFT Record  (Total Chunks: 3, Direct: 3)\n",
      "                |__ Extracting Additional Evidence  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Chapter Introduction  (Total Chunks: 12, Direct: 12)\n",
      "    |__ Chapter 1. Understanding the Digital Forensics Profession and Investigations  (Total Chunks: 933, Direct: 469)\n",
      "        |__ Lab 1.1. Installing Autopsy for Windows  (Total Chunks: 262, Direct: 262)\n",
      "        |__ Lab 1.2. Downloading FTK Imager Lite  (Total Chunks: 100, Direct: 100)\n",
      "        |__ Lab 1.4. Using Autopsy for Windows  (Total Chunks: 99, Direct: 99)\n",
      "        |__ Lab 1.3. Downloading WinHex  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 3. Data Acquisition  (Total Chunks: 537, Direct: 508)\n",
      "        |__ Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive  (Total Chunks: 16, Direct: 4)\n",
      "            |__ Activity  (Total Chunks: 12, Direct: 0)\n",
      "                |__ Learning DEFT Zero Features  (Total Chunks: 6, Direct: 6)\n",
      "                |__ Creating a DEFT Zero Boot CD  (Total Chunks: 4, Direct: 4)\n",
      "                |__ Creating a Bootable USB DEFT Zero Drive  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Lab 3.4. Examining an HFS+ Image  (Total Chunks: 7, Direct: 7)\n",
      "        |__ Lab 3.2. Examining a FAT Image  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Lab 3.3. Examining an NTFS Image  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 4. Processing Crime and Incident Scenes  (Total Chunks: 497, Direct: 462)\n",
      "        |__ Lab 4.2. Using Mini-WinFE to Boot and Image a Windows Computer  (Total Chunks: 20, Direct: 20)\n",
      "        |__ Lab 4.1. Creating a Mini-WinFE Boot CD  (Total Chunks: 7, Direct: 3)\n",
      "            |__ Activity  (Total Chunks: 4, Direct: 0)\n",
      "                |__ Creating a Mini-WinFE ISO Image  (Total Chunks: 3, Direct: 3)\n",
      "                |__ Setting Up Mini-WinFE  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Lab 4.4. Creating an Image with Guymager  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Lab 4.3. Testing the Mini-WinFE Write-Protection Feature  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 5. Working with Windows and CLI Systems  (Total Chunks: 492, Direct: 438)\n",
      "        |__ Lab 5.4. Examining the ntuser.dat Registry File  (Total Chunks: 31, Direct: 31)\n",
      "        |__ Lab 5.1. Using DART to Export Windows Registry Files  (Total Chunks: 15, Direct: 15)\n",
      "        |__ Lab 5.2. Examining the SAM Hive  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Lab 5.3. Examining the SYSTEM Hive  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Chapter 6. Current Digital Forensics Tools  (Total Chunks: 448, Direct: 349)\n",
      "        |__ Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex  (Total Chunks: 76, Direct: 8)\n",
      "            |__ Activity  (Total Chunks: 68, Direct: 0)\n",
      "                |__ Testing an Image File in Autopsy 4.3.0  (Total Chunks: 62, Direct: 62)\n",
      "                |__ Examining Image Files in WinHex  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Lab 6.1. Using Autopsy 4.7.0 to Search an Image File  (Total Chunks: 19, Direct: 3)\n",
      "            |__ Activity  (Total Chunks: 16, Direct: 0)\n",
      "                |__ Installing Autopsy 4.7.0  (Total Chunks: 14, Direct: 14)\n",
      "                |__ Searching E-mail in Autopsy 4.7.0  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Lab 6.2. Using OSForensics to Search an Image of a Hard Drive  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Chapter 9. Digital Forensics Analysis and Validation  (Total Chunks: 280, Direct: 231)\n",
      "        |__ Lab 9.2. Validating File Hash Values with FTK Imager Lite  (Total Chunks: 41, Direct: 41)\n",
      "        |__ Lab 9.3. Validating File Hash Values with WinHex  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Lab 9.1. Using Autopsy to Search for Keywords in an Image  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 15. Expert Testimony in Digital Investigations  (Total Chunks: 200, Direct: 81)\n",
      "        |__ Lab 15.3. Recovering a Password from Password-Protected Files  (Total Chunks: 113, Direct: 7)\n",
      "            |__ Activity  (Total Chunks: 106, Direct: 0)\n",
      "                |__ Verifying the Existence of a Warning Banner  (Total Chunks: 105, Direct: 105)\n",
      "                |__ Recovering a Password from Password-Protected Files  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Lab 15.1. Conducting a Preliminary Investigation  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Lab 15.2. Investigating an Arsonist  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 13. Cloud Forensics  (Total Chunks: 120, Direct: 112)\n",
      "        |__ Lab 13.3. Examining OneDrive Cloud Storage  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Lab 13.1. Examining Dropbox Cloud Storage  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Lab 13.2. Examining Google Drive Cloud Storage  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Chapter 2. The Investigators Office and Laboratory  (Total Chunks: 60, Direct: 6)\n",
      "        |__ Lab 2.5. Viewing Images in FTK Imager Lite  (Total Chunks: 23, Direct: 23)\n",
      "        |__ Lab 2.1. Wiping a USB Drive Securely  (Total Chunks: 9, Direct: 9)\n",
      "        |__ Lab 2.2. Using Directory Snoop to Image a USB Drive  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Lab 2.4. Imaging Evidence with FTK Imager Lite  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Lab 2.3. Converting a Raw Image to an .E01 Image  (Total Chunks: 6, Direct: 6)\n",
      "    |__ Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics  (Total Chunks: 35, Direct: 7)\n",
      "        |__ Lab 10.3. Using Kali Linux for Network Forensics  (Total Chunks: 12, Direct: 1)\n",
      "            |__ Activity  (Total Chunks: 11, Direct: 0)\n",
      "                |__ Mounting Drives in Kali Linux  (Total Chunks: 9, Direct: 9)\n",
      "                |__ Identifying Open Ports and Making a Screen Capture  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine  (Total Chunks: 11, Direct: 3)\n",
      "            |__ Activity  (Total Chunks: 8, Direct: 0)\n",
      "                |__ Installing MD5 Hashes in Autopsy  (Total Chunks: 6, Direct: 6)\n",
      "                |__ Analyzing a Windows Image Containing a Virtual Machine  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Lab 10.2. Conducting a Live Acquisition  (Total Chunks: 5, Direct: 1)\n",
      "            |__ Activity  (Total Chunks: 4, Direct: 0)\n",
      "                |__ Installing Tools for Live Acquisitions  (Total Chunks: 2, Direct: 2)\n",
      "                |__ Exploring Tools for Live Acquisitions  (Total Chunks: 2, Direct: 2)\n",
      "    |__ Chapter 8. Recovering Graphics Files  (Total Chunks: 33, Direct: 13)\n",
      "        |__ Lab 8.3. Using WinHex to Analyze Multimedia Files  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Lab 8.2. Using OSForensics to Analyze Multimedia Files  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Lab 8.1. Using Autopsy to Analyze Multimedia Files  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 12. Mobile Device Forensics  (Total Chunks: 33, Direct: 17)\n",
      "        |__ Lab 12.2. Using FTK Imager Lite to View Text Messages, Phone Numbers, and Photos  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Lab 12.3. Using Autopsy to Search Cloud Backups of Mobile Devices  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Lab 12.1. Examining Cell Phone Storage Devices  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 11. E-mail and Social Media Investigations  (Total Chunks: 32, Direct: 4)\n",
      "        |__ Lab 11.3. Finding Google Searches and Multiple E-mail Accounts  (Total Chunks: 22, Direct: 22)\n",
      "        |__ Lab 11.1. Using OSForensics to Search for E-mails and Mailboxes  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Lab 11.2. Using Autopsy to Search for E-mails and Mailboxes  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 7. Linux and Macintosh File Systems  (Total Chunks: 17, Direct: 6)\n",
      "        |__ Lab 7.3. Using Autopsy to Process a Linux Image  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Lab 7.1. Using Autopsy to Process a Mac OS X Image  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Lab 7.2. Using Autopsy to Process a Mac OS 9 Image  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Chapter 14. Report Writing for High-Tech Investigations  (Total Chunks: 16, Direct: 5)\n",
      "        |__ Lab 14.3. Preparing a Report  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Lab 14.1. Investigating Corporate Espionage  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Lab 14.2. Adding Evidence to a Case  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Introduction  (Total Chunks: 10, Direct: 10)\n",
      "|__ Chapter 6. Current Digital Forensics Tools  (Total Chunks: 1440, Direct: 0)\n",
      "    |__ Evaluating Digital Forensics Tool Needs  (Total Chunks: 1090, Direct: 1)\n",
      "        |__ Tasks Performed by Digital Forensics Tools  (Total Chunks: 743, Direct: 1)\n",
      "            |__ Acquisition  (Total Chunks: 637, Direct: 637)\n",
      "            |__ Reporting  (Total Chunks: 38, Direct: 38)\n",
      "            |__ Extraction  (Total Chunks: 33, Direct: 33)\n",
      "            |__ Reconstruction  (Total Chunks: 29, Direct: 29)\n",
      "            |__ Validation and Verification  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Types of Digital Forensics Tools  (Total Chunks: 341, Direct: 9)\n",
      "            |__ Software Forensics Tools  (Total Chunks: 308, Direct: 308)\n",
      "            |__ Hardware Forensics Tools  (Total Chunks: 24, Direct: 24)\n",
      "        |__ Tool Comparisons  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Other Considerations for Tools  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Digital Forensics Software Tools  (Total Chunks: 197, Direct: 4)\n",
      "        |__ Linux Forensics Tools  (Total Chunks: 166, Direct: 27)\n",
      "            |__ Kali Linux  (Total Chunks: 66, Direct: 66)\n",
      "            |__ Smart  (Total Chunks: 51, Direct: 51)\n",
      "            |__ Autopsy and Sleuth Kit  (Total Chunks: 17, Direct: 17)\n",
      "            |__ Forcepoint Threat Protection  (Total Chunks: 3, Direct: 3)\n",
      "            |__ Helix 3  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Command-Line Forensics Tools  (Total Chunks: 22, Direct: 22)\n",
      "        |__ Other GUI Forensics Tools  (Total Chunks: 5, Direct: 5)\n",
      "    |__ Digital Forensics Hardware Tools  (Total Chunks: 133, Direct: 18)\n",
      "        |__ Forensic Workstations  (Total Chunks: 100, Direct: 97)\n",
      "            |__ Building Your Own Workstation  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Recommendations for a Forensic Workstation  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Using a Write-Blocker  (Total Chunks: 7, Direct: 7)\n",
      "    |__ Validating and Testing Forensics Software  (Total Chunks: 20, Direct: 10)\n",
      "        |__ Using Validation Protocols  (Total Chunks: 9, Direct: 1)\n",
      "            |__ Digital Forensics Examination Protocol  (Total Chunks: 4, Direct: 4)\n",
      "            |__ Digital Forensics Tool Upgrade Protocol  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Using National Institute of Standards and Technology Tools  (Total Chunks: 1, Direct: 1)\n",
      "|__ Chapter 5. Working with Windows and CLI Systems  (Total Chunks: 1310, Direct: 0)\n",
      "    |__ Understanding Microsoft Startup Tasks  (Total Chunks: 785, Direct: 3)\n",
      "        |__ Startup in Windows 7, Windows 8, and Windows 10  (Total Chunks: 745, Direct: 745)\n",
      "        |__ Startup in Windows NT and Later  (Total Chunks: 37, Direct: 14)\n",
      "            |__ Windows XP System Files  (Total Chunks: 14, Direct: 14)\n",
      "            |__ Startup Files for Windows Vista  (Total Chunks: 6, Direct: 6)\n",
      "            |__ Startup Files for Windows XP  (Total Chunks: 2, Direct: 2)\n",
      "            |__ Contamination Concerns with Windows XP  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Examining NTFS Disks  (Total Chunks: 258, Direct: 21)\n",
      "        |__ MFT Structures for File Data  (Total Chunks: 70, Direct: 2)\n",
      "            |__ Attribute 0x30: File Name  (Total Chunks: 18, Direct: 18)\n",
      "            |__ Attribute 0x80: Data for a Nonresident File  (Total Chunks: 16, Direct: 16)\n",
      "            |__ MFT Header Fields  (Total Chunks: 14, Direct: 14)\n",
      "            |__ Attribute 0x40: Object_ID  (Total Chunks: 10, Direct: 10)\n",
      "            |__ Attribute 0x80: Data for a Resident File  (Total Chunks: 8, Direct: 8)\n",
      "            |__ Attribute 0x10: Standard Information  (Total Chunks: 1, Direct: 1)\n",
      "            |__ Interpreting a Data Run  (Total Chunks: 1, Direct: 1)\n",
      "        |__ MFT and File Attributes  (Total Chunks: 46, Direct: 46)\n",
      "        |__ Deleting NTFS Files  (Total Chunks: 31, Direct: 31)\n",
      "        |__ NTFS Alternate Data Streams  (Total Chunks: 29, Direct: 29)\n",
      "        |__ Resilient File System  (Total Chunks: 28, Direct: 28)\n",
      "        |__ NTFS System Files  (Total Chunks: 19, Direct: 19)\n",
      "        |__ EFS Recovery Key Agent  (Total Chunks: 8, Direct: 8)\n",
      "        |__ NTFS Compressed Files  (Total Chunks: 4, Direct: 4)\n",
      "        |__ NTFS Encrypting File System  (Total Chunks: 2, Direct: 2)\n",
      "    |__ Exploring Microsoft File Structures  (Total Chunks: 160, Direct: 1)\n",
      "        |__ Disk Partitions  (Total Chunks: 148, Direct: 148)\n",
      "        |__ Examining FAT Disks  (Total Chunks: 11, Direct: 5)\n",
      "            |__ Deleting FAT Files  (Total Chunks: 6, Direct: 6)\n",
      "    |__ Understanding File Systems  (Total Chunks: 58, Direct: 21)\n",
      "        |__ Understanding Disk Drives  (Total Chunks: 16, Direct: 16)\n",
      "        |__ Solid-State Storage Devices  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Understanding the Boot Sequence  (Total Chunks: 10, Direct: 10)\n",
      "    |__ Understanding the Windows Registry  (Total Chunks: 22, Direct: 10)\n",
      "        |__ Exploring the Organization of the Windows Registry  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Examining the Windows Registry  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Understanding Whole Disk Encryption  (Total Chunks: 21, Direct: 12)\n",
      "        |__ Examining Third-Party Disk Encryption Tools  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Examining Microsoft BitLocker  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Understanding Virtual Machines  (Total Chunks: 6, Direct: 2)\n",
      "        |__ Creating a Virtual Machine  (Total Chunks: 4, Direct: 4)\n",
      "|__ Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics  (Total Chunks: 699, Direct: 0)\n",
      "    |__ An Overview of Virtual Machine Forensics  (Total Chunks: 618, Direct: 2)\n",
      "        |__ Type 2 Hypervisors  (Total Chunks: 608, Direct: 502)\n",
      "            |__ VirtualBox  (Total Chunks: 73, Direct: 73)\n",
      "            |__ VMware Workstation and Workstation Player  (Total Chunks: 13, Direct: 13)\n",
      "            |__ Microsoft Hyper-V  (Total Chunks: 12, Direct: 12)\n",
      "            |__ Parallels Desktop  (Total Chunks: 7, Direct: 7)\n",
      "            |__ KVM  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Working with Type 1 Hypervisors  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Conducting an Investigation with Type 2 Hypervisors  (Total Chunks: 2, Direct: 0)\n",
      "            |__ Other VM Examination Methods  (Total Chunks: 1, Direct: 1)\n",
      "            |__ Using VMs as Forensics Tools  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Network Forensics Overview  (Total Chunks: 76, Direct: 3)\n",
      "        |__ Developing Procedures for Network Forensics  (Total Chunks: 55, Direct: 7)\n",
      "            |__ Using Network Tools  (Total Chunks: 29, Direct: 29)\n",
      "            |__ Using Packet Analyzers  (Total Chunks: 10, Direct: 10)\n",
      "            |__ Reviewing Network Logs  (Total Chunks: 9, Direct: 9)\n",
      "        |__ Investigating Virtual Networks  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Securing a Network  (Total Chunks: 5, Direct: 5)\n",
      "        |__ The Need for Established Procedures  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Examining the Honeynet Project  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Performing Live Acquisitions  (Total Chunks: 5, Direct: 1)\n",
      "        |__ Performing a Live Acquisition in Windows  (Total Chunks: 4, Direct: 4)\n",
      "|__ Chapter 16. Ethics for the Expert Witness  (Total Chunks: 593, Direct: 0)\n",
      "    |__ Chapter Review  (Total Chunks: 400, Direct: 9)\n",
      "        |__ Key Terms  (Total Chunks: 125, Direct: 125)\n",
      "        |__ Case Projects  (Total Chunks: 110, Direct: 110)\n",
      "        |__ Hands-On Projects  (Total Chunks: 80, Direct: 80)\n",
      "        |__ Chapter Summary  (Total Chunks: 76, Direct: 76)\n",
      "    |__ Organizations with Codes of Ethics  (Total Chunks: 110, Direct: 3)\n",
      "        |__ International High Technology Crime Investigation Association  (Total Chunks: 43, Direct: 43)\n",
      "        |__ International Association of Computer Investigative Specialists  (Total Chunks: 24, Direct: 24)\n",
      "        |__ International Society of Forensic Computer Examiners  (Total Chunks: 20, Direct: 20)\n",
      "        |__ American Bar Association  (Total Chunks: 17, Direct: 17)\n",
      "        |__ American Psychological Association  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Applying Ethics and Codes to Expert Witnesses  (Total Chunks: 43, Direct: 11)\n",
      "        |__ Traps for Unwary Experts  (Total Chunks: 17, Direct: 17)\n",
      "        |__ Determining Admissibility of Evidence  (Total Chunks: 7, Direct: 7)\n",
      "        |__ Forensics Examiners Roles in Testifying  (Total Chunks: 7, Direct: 7)\n",
      "        |__ Considerations in Disqualification  (Total Chunks: 1, Direct: 1)\n",
      "    |__ An Ethics Exercise  (Total Chunks: 26, Direct: 4)\n",
      "        |__ Interpreting Attribute 0x80 Data Runs  (Total Chunks: 14, Direct: 1)\n",
      "            |__ Calculating Data Runs  (Total Chunks: 7, Direct: 7)\n",
      "            |__ Finding Attribute 0x80 an MFT Record  (Total Chunks: 4, Direct: 4)\n",
      "            |__ Configuring Data Interpreter Options in WinHex  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Carving Data Run Clusters Manually  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Performing the Exam  (Total Chunks: 2, Direct: 1)\n",
      "            |__ Preparing for an Examination  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Performing a Cursory Exam of a Forensic Image  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Performing a Detailed Exam of a Forensic Image  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Ethical Difficulties in Expert Testimony  (Total Chunks: 14, Direct: 3)\n",
      "        |__ Ethical Responsibilities Owed to You  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Standard Forensics Tools and Tools You Create  (Total Chunks: 5, Direct: 5)\n",
      "|__ Chapter 13. Cloud Forensics  (Total Chunks: 575, Direct: 0)\n",
      "    |__ Conducting a Cloud Investigation  (Total Chunks: 201, Direct: 2)\n",
      "        |__ Examining Stored Cloud Data on a PC  (Total Chunks: 182, Direct: 1)\n",
      "            |__ Google Drive  (Total Chunks: 80, Direct: 80)\n",
      "            |__ OneDrive  (Total Chunks: 54, Direct: 54)\n",
      "            |__ Dropbox  (Total Chunks: 47, Direct: 47)\n",
      "        |__ Investigating CSPs  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Understanding Prefetch Files  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Windows Prefetch Artifacts  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Investigating Cloud Customers  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Technical Challenges in Cloud Forensics  (Total Chunks: 168, Direct: 9)\n",
      "        |__ Architecture  (Total Chunks: 95, Direct: 95)\n",
      "        |__ Standards and Training  (Total Chunks: 31, Direct: 31)\n",
      "        |__ Anti-Forensics  (Total Chunks: 15, Direct: 15)\n",
      "        |__ Role Management  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Incident First Responders  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Analysis of Cloud Forensic Data  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Legal Challenges in Cloud Forensics  (Total Chunks: 132, Direct: 2)\n",
      "        |__ Accessing Evidence in the Cloud  (Total Chunks: 84, Direct: 5)\n",
      "            |__ Search Warrants  (Total Chunks: 52, Direct: 52)\n",
      "            |__ Subpoenas and Court Orders  (Total Chunks: 27, Direct: 27)\n",
      "        |__ Jurisdiction Issues  (Total Chunks: 23, Direct: 23)\n",
      "        |__ Service Level Agreements  (Total Chunks: 23, Direct: 15)\n",
      "            |__ Policies, Standards, and Guidelines for CSPs  (Total Chunks: 6, Direct: 6)\n",
      "            |__ CSP Processes and Procedures  (Total Chunks: 2, Direct: 2)\n",
      "    |__ An Overview of Cloud Computing  (Total Chunks: 38, Direct: 1)\n",
      "        |__ Cloud Vendors  (Total Chunks: 18, Direct: 18)\n",
      "        |__ Cloud Service Levels and Deployment Methods  (Total Chunks: 12, Direct: 12)\n",
      "        |__ History of the Cloud  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Basic Concepts of Cloud Forensics  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Tools for Cloud Forensics  (Total Chunks: 23, Direct: 19)\n",
      "        |__ Magnet AXIOM Cloud  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Forensic Open-Stack Tools  (Total Chunks: 1, Direct: 1)\n",
      "        |__ F-Response for the Cloud  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Acquisitions in the Cloud  (Total Chunks: 13, Direct: 9)\n",
      "        |__ Encryption in the Cloud  (Total Chunks: 4, Direct: 4)\n",
      "|__ Chapter 1. Understanding the Digital Forensics Profession and Investigations  (Total Chunks: 365, Direct: 0)\n",
      "    |__ Conducting an Investigation  (Total Chunks: 120, Direct: 28)\n",
      "        |__ Gathering the Evidence  (Total Chunks: 40, Direct: 40)\n",
      "        |__ Completing the Case  (Total Chunks: 20, Direct: 16)\n",
      "            |__ Autopsys Report Generator  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Analyzing Your Digital Evidence  (Total Chunks: 19, Direct: 7)\n",
      "            |__ Some Additional Features of Autopsy  (Total Chunks: 12, Direct: 12)\n",
      "        |__ Critiquing the Case  (Total Chunks: 9, Direct: 9)\n",
      "        |__ Understanding Bit-stream Copies  (Total Chunks: 4, Direct: 3)\n",
      "            |__ Acquiring an Image of Evidence Media  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Preparing a Digital Forensics Investigation  (Total Chunks: 79, Direct: 5)\n",
      "        |__ Taking a Systematic Approach  (Total Chunks: 67, Direct: 19)\n",
      "            |__ Securing Your Evidence  (Total Chunks: 35, Direct: 35)\n",
      "            |__ Assessing the Case  (Total Chunks: 10, Direct: 10)\n",
      "            |__ Planning Your Investigation  (Total Chunks: 3, Direct: 3)\n",
      "        |__ An Overview of a Company Policy Violation  (Total Chunks: 4, Direct: 4)\n",
      "        |__ An Overview of a Computer Crime  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Procedures for Private-Sector High-Tech Investigations  (Total Chunks: 76, Direct: 2)\n",
      "        |__ Industrial Espionage Investigations  (Total Chunks: 28, Direct: 14)\n",
      "            |__ Interviews and Interrogations in High-Tech Investigations  (Total Chunks: 14, Direct: 14)\n",
      "        |__ Internet Abuse Investigations  (Total Chunks: 26, Direct: 26)\n",
      "        |__ E-mail Abuse Investigations  (Total Chunks: 9, Direct: 9)\n",
      "        |__ Attorney-Client Privilege Investigations  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Employee Termination Cases  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Preparing for Digital Investigations  (Total Chunks: 43, Direct: 8)\n",
      "        |__ Understanding Private-Sector Investigations  (Total Chunks: 27, Direct: 1)\n",
      "            |__ Designating an Authorized Requester  (Total Chunks: 10, Direct: 10)\n",
      "            |__ Establishing Company Policies  (Total Chunks: 5, Direct: 5)\n",
      "            |__ Conducting Security Investigations  (Total Chunks: 5, Direct: 5)\n",
      "            |__ Displaying Warning Banners  (Total Chunks: 3, Direct: 3)\n",
      "            |__ Distinguishing Personal and Company Property  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Following Legal Processes  (Total Chunks: 7, Direct: 7)\n",
      "        |__ Understanding Law Enforcement Agency Investigations  (Total Chunks: 1, Direct: 1)\n",
      "    |__ An Overview of Digital Forensics  (Total Chunks: 24, Direct: 4)\n",
      "        |__ A Brief History of Digital Forensics  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Digital Forensics and Other Related Disciplines  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Developing Digital Forensics Resources  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Understanding Case Law  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Maintaining Professional Conduct  (Total Chunks: 18, Direct: 18)\n",
      "    |__ Understanding Data Recovery Workstations and Software  (Total Chunks: 5, Direct: 1)\n",
      "        |__ Setting Up Your Workstation for Digital Forensics  (Total Chunks: 4, Direct: 4)\n",
      "|__ Chapter 2. The Investigators Office and Laboratory  (Total Chunks: 300, Direct: 0)\n",
      "    |__ Building a Business Case for Developing a Forensics Lab  (Total Chunks: 165, Direct: 1)\n",
      "        |__ Preparing a Business Case for a Digital Forensics Lab  (Total Chunks: 164, Direct: 3)\n",
      "            |__ Production  (Total Chunks: 39, Direct: 39)\n",
      "            |__ Hardware Requirements  (Total Chunks: 36, Direct: 36)\n",
      "            |__ Justification  (Total Chunks: 32, Direct: 32)\n",
      "            |__ Facility Cost  (Total Chunks: 18, Direct: 18)\n",
      "            |__ Implementation  (Total Chunks: 14, Direct: 14)\n",
      "            |__ Software Requirements  (Total Chunks: 11, Direct: 11)\n",
      "            |__ Miscellaneous Budget Needs  (Total Chunks: 4, Direct: 4)\n",
      "            |__ Acceptance Testing  (Total Chunks: 4, Direct: 4)\n",
      "            |__ Budget Development  (Total Chunks: 2, Direct: 2)\n",
      "            |__ Correction for Acceptance  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Understanding Forensics Lab Accreditation Requirements  (Total Chunks: 50, Direct: 1)\n",
      "        |__ Acquiring Certification and Training  (Total Chunks: 42, Direct: 4)\n",
      "            |__ High Tech Crime Network  (Total Chunks: 25, Direct: 25)\n",
      "            |__ ISC2 Certified Cyber Forensics Professional  (Total Chunks: 5, Direct: 5)\n",
      "            |__ Other Training and Certifications  (Total Chunks: 5, Direct: 5)\n",
      "            |__ AccessData Certified Examiner  (Total Chunks: 2, Direct: 2)\n",
      "            |__ EnCase Certified Examiner Certification  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Lab Budget Planning  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Identifying Duties of the Lab Manager and Staff  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Determining the Physical Requirements for a Digital Forensics Lab  (Total Chunks: 45, Direct: 6)\n",
      "        |__ Auditing a Digital Forensics Lab  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Identifying Lab Security Needs  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Considering Physical Security Needs  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Overseeing Facility Maintenance  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Conducting High-Risk Investigations  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Using Evidence Containers  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Determining Floor Plans for Digital Forensics Labs  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Selecting a Basic Forensic Workstation  (Total Chunks: 40, Direct: 5)\n",
      "        |__ Stocking Hardware Peripherals  (Total Chunks: 14, Direct: 14)\n",
      "        |__ Using a Disaster Recovery Plan  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Selecting Workstations for Private-Sector Labs  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Planning for Equipment Upgrades  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Maintaining Operating Systems and Software Inventories  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Selecting Workstations for a Lab  (Total Chunks: 1, Direct: 1)\n",
      "|__ Chapter 15. Expert Testimony in Digital Investigations  (Total Chunks: 286, Direct: 0)\n",
      "    |__ Testifying in Court  (Total Chunks: 146, Direct: 36)\n",
      "        |__ General Guidelines on Testifying  (Total Chunks: 49, Direct: 13)\n",
      "            |__ Using Graphics During Testimony  (Total Chunks: 26, Direct: 26)\n",
      "            |__ Avoiding Testimony Problems  (Total Chunks: 7, Direct: 7)\n",
      "            |__ Understanding Prosecutorial Misconduct  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Testifying During Cross-Examination  (Total Chunks: 31, Direct: 31)\n",
      "        |__ Understanding the Trial Process  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Testifying During Direct Examination  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Providing Qualifications for Your Testimony  (Total Chunks: 8, Direct: 8)\n",
      "    |__ Preparing for Testimony  (Total Chunks: 50, Direct: 1)\n",
      "        |__ Reviewing Your Role as a Consulting Expert or an Expert Witness  (Total Chunks: 14, Direct: 14)\n",
      "        |__ Creating and Maintaining Your CV  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Preparing Technical Definitions  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Documenting and Preparing Evidence  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Preparing to Deal with the News Media  (Total Chunks: 7, Direct: 7)\n",
      "    |__ Preparing Forensics Evidence for Testimony  (Total Chunks: 48, Direct: 17)\n",
      "        |__ Preparing a Defense of Your Evidence-Collection Methods  (Total Chunks: 31, Direct: 31)\n",
      "    |__ Preparing for a Deposition or Hearing  (Total Chunks: 42, Direct: 8)\n",
      "        |__ Guidelines for Testifying at Depositions  (Total Chunks: 21, Direct: 11)\n",
      "            |__ Recognizing Deposition Problems  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Guidelines for Testifying at Hearings  (Total Chunks: 13, Direct: 13)\n",
      "|__ Chapter 4. Processing Crime and Incident Scenes  (Total Chunks: 252, Direct: 0)\n",
      "    |__ Reviewing a Case  (Total Chunks: 58, Direct: 4)\n",
      "        |__ An Example of a Criminal Investigation  (Total Chunks: 23, Direct: 23)\n",
      "        |__ Conducting the Investigation: Acquiring Evidence with OSForensics  (Total Chunks: 16, Direct: 16)\n",
      "        |__ Planning the Investigation  (Total Chunks: 14, Direct: 14)\n",
      "        |__ Sample Civil Investigation  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Storing Digital Evidence  (Total Chunks: 47, Direct: 23)\n",
      "        |__ Documenting Evidence  (Total Chunks: 18, Direct: 18)\n",
      "        |__ Evidence Retention and Media Storage Needs  (Total Chunks: 6, Direct: 6)\n",
      "    |__ Seizing Digital Evidence at the Scene  (Total Chunks: 37, Direct: 2)\n",
      "        |__ Processing Incident or Crime Scenes  (Total Chunks: 19, Direct: 19)\n",
      "        |__ Using a Technical Advisor  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Processing and Handling Digital Evidence  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Preparing to Acquire Digital Evidence  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Processing Data Centers with RAID Systems  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Preparing for a Search  (Total Chunks: 34, Direct: 3)\n",
      "        |__ Determining the Tools You Need  (Total Chunks: 12, Direct: 12)\n",
      "        |__ Getting a Detailed Description of the Location  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Identifying the Type of OS or Digital Device  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Determining Who Is in Charge  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Using Additional Technical Expertise  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Identifying the Nature of the Case  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Determining Whether You Can Seize Computers and Digital Devices  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Preparing the Investigation Team  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Processing Law Enforcement Crime Scenes  (Total Chunks: 32, Direct: 12)\n",
      "        |__ Understanding Concepts and Terms Used in Warrants  (Total Chunks: 20, Direct: 20)\n",
      "    |__ Identifying Digital Evidence  (Total Chunks: 19, Direct: 14)\n",
      "        |__ Understanding Rules of Evidence  (Total Chunks: 5, Direct: 5)\n",
      "    |__ Obtaining a Digital Hash  (Total Chunks: 9, Direct: 9)\n",
      "    |__ Collecting Evidence in Private-Sector Incident Scenes  (Total Chunks: 8, Direct: 8)\n",
      "    |__ Securing a Digital Incident or Crime Scene  (Total Chunks: 8, Direct: 8)\n",
      "|__ Chapter 3. Data Acquisition  (Total Chunks: 231, Direct: 0)\n",
      "    |__ Understanding Storage Formats for Digital Evidence  (Total Chunks: 94, Direct: 1)\n",
      "        |__ Raw Format  (Total Chunks: 46, Direct: 46)\n",
      "        |__ Proprietary Formats  (Total Chunks: 39, Direct: 39)\n",
      "        |__ Advanced Forensic Format  (Total Chunks: 8, Direct: 8)\n",
      "    |__ Validating Data Acquisitions  (Total Chunks: 42, Direct: 5)\n",
      "        |__ Linux Validation Methods  (Total Chunks: 36, Direct: 1)\n",
      "            |__ Validating dcfldd-Acquired Data  (Total Chunks: 31, Direct: 31)\n",
      "            |__ Validating dd-Acquired Data  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Windows Validation Methods  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Using Acquisition Tools  (Total Chunks: 39, Direct: 0)\n",
      "        |__ Acquiring Data with a Linux Boot CD  (Total Chunks: 34, Direct: 1)\n",
      "            |__ Acquiring Data with dcfldd in Linux  (Total Chunks: 14, Direct: 14)\n",
      "            |__ Using Linux Live CD Distributions  (Total Chunks: 11, Direct: 11)\n",
      "            |__ Acquiring Data with dd in Linux  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Capturing an Image with AccessData FTK Imager Lite  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Mini-WinFE Boot CDs and USB Drives  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Performing RAID Data Acquisitions  (Total Chunks: 27, Direct: 6)\n",
      "        |__ Understanding RAID  (Total Chunks: 16, Direct: 16)\n",
      "        |__ Acquiring RAID Disks  (Total Chunks: 5, Direct: 5)\n",
      "    |__ Using Other Forensics Acquisition Tools  (Total Chunks: 20, Direct: 0)\n",
      "        |__ ASR Data SMART  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Runtime Software  (Total Chunks: 6, Direct: 6)\n",
      "        |__ SourceForge  (Total Chunks: 3, Direct: 3)\n",
      "        |__ ILookIX IXImager  (Total Chunks: 2, Direct: 2)\n",
      "        |__ PassMark Software ImageUSB  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Using Remote Network Acquisition Tools  (Total Chunks: 8, Direct: 5)\n",
      "        |__ Remote Acquisition with F-Response  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Contingency Planning for Image Acquisitions  (Total Chunks: 1, Direct: 1)\n",
      "|__ Chapter 8. Recovering Graphics Files  (Total Chunks: 221, Direct: 0)\n",
      "    |__ Understanding Data Compression  (Total Chunks: 107, Direct: 1)\n",
      "        |__ Searching for and Carving Data from Unallocated Space  (Total Chunks: 56, Direct: 52)\n",
      "            |__ Planning Your Examination  (Total Chunks: 3, Direct: 3)\n",
      "            |__ Searching for and Recovering Digital Photograph Evidence  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Lossless and Lossy Compression  (Total Chunks: 26, Direct: 26)\n",
      "        |__ Rebuilding File Headers  (Total Chunks: 7, Direct: 7)\n",
      "        |__ Reconstructing File Fragments  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Repairing Damaged Headers  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Locating and Recovering Graphics Files  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Identifying Graphics File Fragments  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Recognizing a Graphics File  (Total Chunks: 69, Direct: 14)\n",
      "        |__ Understanding Digital Photograph File Formats  (Total Chunks: 29, Direct: 1)\n",
      "            |__ Examining the Exchangeable Image File Format  (Total Chunks: 19, Direct: 19)\n",
      "            |__ Examining the Raw File Format  (Total Chunks: 9, Direct: 9)\n",
      "        |__ Understanding Bitmap and Raster Images  (Total Chunks: 16, Direct: 16)\n",
      "        |__ Understanding Graphics File Formats  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Understanding Vector Graphics  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Understanding Metafile Graphics  (Total Chunks: 3, Direct: 3)\n",
      "    |__ Identifying Unknown File Formats  (Total Chunks: 44, Direct: 4)\n",
      "        |__ Understanding Steganography in Graphics Files  (Total Chunks: 19, Direct: 19)\n",
      "        |__ Analyzing Graphics File Headers  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Using Steganalysis Tools  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Tools for Viewing Images  (Total Chunks: 2, Direct: 2)\n",
      "    |__ Understanding Copyright Issues with Graphics  (Total Chunks: 1, Direct: 1)\n",
      "|__ Chapter 11. E-mail and Social Media Investigations  (Total Chunks: 176, Direct: 0)\n",
      "    |__ Investigating E-mail Crimes and Violations  (Total Chunks: 90, Direct: 23)\n",
      "        |__ Using Network E-mail Logs  (Total Chunks: 17, Direct: 17)\n",
      "        |__ Tracing an E-mail Message  (Total Chunks: 14, Direct: 14)\n",
      "        |__ Examining E-mail Messages  (Total Chunks: 12, Direct: 10)\n",
      "            |__ Copying an E-mail Message  (Total Chunks: 2, Direct: 2)\n",
      "        |__ Understanding Forensic Linguistics  (Total Chunks: 9, Direct: 9)\n",
      "        |__ Examining E-mail Headers  (Total Chunks: 6, Direct: 6)\n",
      "        |__ Viewing E-mail Headers  (Total Chunks: 5, Direct: 5)\n",
      "        |__ Examining Additional E-mail Files  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Using Specialized E-mail Forensics Tools  (Total Chunks: 37, Direct: 22)\n",
      "        |__ Using a Hex Editor to Carve E-mail Messages  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Recovering Outlook Files  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Using Magnet AXIOM to Recover E-mail  (Total Chunks: 2, Direct: 2)\n",
      "        |__ E-mail Case Studies  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Understanding E-mail Servers  (Total Chunks: 24, Direct: 2)\n",
      "        |__ Examining UNIX E-mail Server Logs  (Total Chunks: 14, Direct: 14)\n",
      "        |__ Examining Microsoft E-mail Server Logs  (Total Chunks: 8, Direct: 8)\n",
      "    |__ Applying Digital Forensics Methods to Social Media Communications  (Total Chunks: 20, Direct: 19)\n",
      "        |__ Forensics Tools for Social Media Investigations  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Exploring the Role of E-mail in Investigations  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Exploring the Roles of the Client and Server in E-mail  (Total Chunks: 1, Direct: 1)\n",
      "|__ Chapter 7. Linux and Macintosh File Systems  (Total Chunks: 144, Direct: 0)\n",
      "    |__ Examining Linux File Structures  (Total Chunks: 104, Direct: 17)\n",
      "        |__ File Structures in Ext4  (Total Chunks: 87, Direct: 7)\n",
      "            |__ Inodes  (Total Chunks: 63, Direct: 63)\n",
      "            |__ Hard Links and Symbolic Links  (Total Chunks: 17, Direct: 17)\n",
      "    |__ Understanding Macintosh File Structures  (Total Chunks: 30, Direct: 1)\n",
      "        |__ An Overview of Mac File Structures  (Total Chunks: 28, Direct: 28)\n",
      "        |__ Forensics Procedures in Mac  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Using Linux Forensics Tools  (Total Chunks: 10, Direct: 8)\n",
      "        |__ Examining a Case with Sleuth Kit and Autopsy  (Total Chunks: 2, Direct: 2)\n",
      "|__ Chapter 9. Digital Forensics Analysis and Validation  (Total Chunks: 144, Direct: 0)\n",
      "    |__ Addressing Data-Hiding Techniques  (Total Chunks: 98, Direct: 1)\n",
      "        |__ Recovering Passwords  (Total Chunks: 27, Direct: 27)\n",
      "        |__ Examining Encrypted Files  (Total Chunks: 22, Direct: 22)\n",
      "        |__ Bit-Shifting  (Total Chunks: 20, Direct: 20)\n",
      "        |__ Understanding Steganalysis Methods  (Total Chunks: 12, Direct: 12)\n",
      "        |__ Marking Bad Clusters  (Total Chunks: 8, Direct: 8)\n",
      "        |__ Hiding Partitions  (Total Chunks: 4, Direct: 4)\n",
      "        |__ Hiding Files by Using the OS  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Validating Forensic Data  (Total Chunks: 23, Direct: 9)\n",
      "        |__ Validating with Digital Forensics Tools  (Total Chunks: 10, Direct: 10)\n",
      "        |__ Validating with Hexadecimal Editors  (Total Chunks: 4, Direct: 3)\n",
      "            |__ Using Hash Values to Discriminate Data  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Determining What Data to Collect and Analyze  (Total Chunks: 23, Direct: 8)\n",
      "        |__ Using Autopsy to Validate Data  (Total Chunks: 11, Direct: 8)\n",
      "            |__ Installing NSRL Hashes in Autopsy  (Total Chunks: 3, Direct: 3)\n",
      "        |__ Approaching Digital Forensics Cases  (Total Chunks: 3, Direct: 2)\n",
      "            |__ Refining and Modifying the Investigation Plan  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Collecting Hash Values in Autopsy  (Total Chunks: 1, Direct: 1)\n",
      "|__ Chapter 14. Report Writing for High-Tech Investigations  (Total Chunks: 140, Direct: 0)\n",
      "    |__ Guidelines for Writing Reports  (Total Chunks: 113, Direct: 25)\n",
      "        |__ Designing the Layout and Presentation of Reports  (Total Chunks: 52, Direct: 5)\n",
      "            |__ Providing References  (Total Chunks: 23, Direct: 23)\n",
      "            |__ Including Appendixes  (Total Chunks: 10, Direct: 10)\n",
      "            |__ Explaining Examination and Data Collection Methods  (Total Chunks: 4, Direct: 4)\n",
      "            |__ Providing Supporting Material  (Total Chunks: 3, Direct: 3)\n",
      "            |__ Providing for Uncertainty and Error Analysis  (Total Chunks: 3, Direct: 3)\n",
      "            |__ Formatting Consistently  (Total Chunks: 2, Direct: 2)\n",
      "            |__ Including Calculations  (Total Chunks: 1, Direct: 1)\n",
      "            |__ Explaining Results and Conclusions  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Writing Reports Clearly  (Total Chunks: 16, Direct: 9)\n",
      "            |__ Including Signposts  (Total Chunks: 4, Direct: 4)\n",
      "            |__ Considering Writing Style  (Total Chunks: 3, Direct: 3)\n",
      "        |__ What to Include in Written Preliminary Reports  (Total Chunks: 11, Direct: 11)\n",
      "        |__ Report Structure  (Total Chunks: 9, Direct: 9)\n",
      "    |__ Understanding the Importance of Reports  (Total Chunks: 25, Direct: 1)\n",
      "        |__ Types of Reports  (Total Chunks: 23, Direct: 23)\n",
      "        |__ Limiting a Report to Specifics  (Total Chunks: 1, Direct: 1)\n",
      "    |__ Generating Report Findings with Forensics Software Tools  (Total Chunks: 2, Direct: 0)\n",
      "        |__ Using Autopsy to Generate Reports  (Total Chunks: 2, Direct: 2)\n",
      "|__ Chapter 12. Mobile Device Forensics and the Internet of Anything  (Total Chunks: 93, Direct: 0)\n",
      "    |__ Understanding Mobile Device Forensics  (Total Chunks: 64, Direct: 4)\n",
      "        |__ Inside Mobile Devices  (Total Chunks: 32, Direct: 9)\n",
      "            |__ SIM Cards  (Total Chunks: 23, Direct: 23)\n",
      "        |__ Mobile Phone Basics  (Total Chunks: 28, Direct: 28)\n",
      "    |__ Understanding Acquisition Procedures for Mobile Devices  (Total Chunks: 28, Direct: 0)\n",
      "        |__ Mobile Forensics Equipment  (Total Chunks: 24, Direct: 2)\n",
      "            |__ SIM Card Readers  (Total Chunks: 21, Direct: 21)\n",
      "            |__ Mobile Phone Forensics Tools and Methods  (Total Chunks: 1, Direct: 1)\n",
      "        |__ Using Mobile Forensics Tools  (Total Chunks: 4, Direct: 4)\n",
      "    |__ Understanding Forensics in the Internet of Anything  (Total Chunks: 1, Direct: 1)\n",
      "|__ About the Authors  (Total Chunks: 18, Direct: 18)\n",
      "|__ Copyright Page  (Total Chunks: 16, Direct: 16)\n",
      "|__ Preface  (Total Chunks: 15, Direct: 15)\n",
      "|__ Acknowledgments  (Total Chunks: 10, Direct: 10)\n",
      "|__ Title Page  (Total Chunks: 7, Direct: 7)\n",
      "|__ Appendix B. Digital Forensics References  (Total Chunks: 6, Direct: 6)\n",
      "|__ Appendix D. Legacy File System and Forensics Tools  (Total Chunks: 6, Direct: 6)\n",
      "|__ Appendix C. Digital Forensics Lab Considerations  (Total Chunks: 5, Direct: 5)\n",
      "|__ Cover Page  (Total Chunks: 1, Direct: 1)\n",
      "|__ Appendix A. Certification Test References  (Total Chunks: 1, Direct: 1)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Diagnostic Summary                               \n",
      "--------------------------------------------------------------------------------\n",
      " Found 27 distinct top-level sections.\n",
      " All chunks contain 'toc_path' metadata.\n",
      "\n",
      "================================================================================\n",
      "                              Diagnostic Complete                               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Full Database Health & Hierarchy Diagnostic Report\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def count_total_chunks(node: Dict) -> int:\n",
    "    \"\"\"Recursively counts all chunks in a node and its children.\"\"\"\n",
    "    total = node.get('_chunks', 0)\n",
    "    for child_node in node.get('_children', {}).values():\n",
    "        total += count_total_chunks(child_node)\n",
    "    return total\n",
    "\n",
    "def print_hierarchy_report(node: Dict, indent_level: int = 0):\n",
    "    \"\"\"Recursively prints the reconstructed hierarchy with chunk counts.\"\"\"\n",
    "    # Sort children by their total chunk count for a more organized report\n",
    "    sorted_children = sorted(\n",
    "        node.get('_children', {}).items(),\n",
    "        key=lambda item: count_total_chunks(item[1]),\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for title, child_node in sorted_children:\n",
    "        prefix = \"    \" * indent_level + \"|__ \"\n",
    "        \n",
    "        # Total chunks includes the node itself and all descendants\n",
    "        total_chunks_in_branch = count_total_chunks(child_node)\n",
    "        \n",
    "        # Chunks directly assigned to this node (should be low if it has children)\n",
    "        direct_chunks = child_node.get('_chunks', 0)\n",
    "        \n",
    "        print(f\"{prefix}{title}  (Total Chunks: {total_chunks_in_branch}, Direct: {direct_chunks})\")\n",
    "        \n",
    "        # Recursive call for the next level\n",
    "        print_hierarchy_report(child_node, indent_level + 1)\n",
    "\n",
    "# --- MAIN DIAGNOSTIC FUNCTION ---\n",
    "def run_full_diagnostics():\n",
    "    print_header(\"Full Database Health & Hierarchy Diagnostic Report\")\n",
    "    \n",
    "    # 1. Connect to the Database\n",
    "    logger.info(\"Connecting to the vector database...\")\n",
    "    if not os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        logger.error(f\"FATAL: Chroma DB directory not found at {CHROMA_PERSIST_DIR}. Please run Cell 5 first.\"); return\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings, collection_name=CHROMA_COLLECTION_NAME)\n",
    "    \n",
    "    # 2. Retrieve ALL Metadata\n",
    "    total_docs = vector_store._collection.count()\n",
    "    logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "    retrieved_data = vector_store.get(limit=total_docs, include=[\"metadatas\"])\n",
    "    all_metadatas = retrieved_data['metadatas']\n",
    "    logger.info(\"Successfully retrieved all metadata.\")\n",
    "        \n",
    "    # 3. Reconstruct the Hierarchy Tree from Metadata\n",
    "    logger.info(\"Reconstructing hierarchy from chunk metadata...\")\n",
    "    hierarchy_tree = {'_children': {}} # A root node to hold everything\n",
    "    chunks_without_path = 0\n",
    "    \n",
    "    for meta in all_metadatas:\n",
    "        path = meta.get(\"toc_path\")\n",
    "        if not path:\n",
    "            chunks_without_path += 1\n",
    "            continue\n",
    "            \n",
    "        path_parts = path.split(\" > \")\n",
    "        current_node = hierarchy_tree\n",
    "        \n",
    "        for part in path_parts:\n",
    "            # Navigate or create the path in our tree\n",
    "            current_node = current_node['_children'].setdefault(part, {'_chunks': 0, '_children': {}})\n",
    "        \n",
    "        # Increment the count for the leaf node\n",
    "        current_node['_chunks'] += 1\n",
    "        \n",
    "    logger.info(\"Hierarchy reconstruction complete.\")\n",
    "    \n",
    "    # 4. Print the Report\n",
    "    print_header(\"Reconstructed Hierarchy Report\", char=\"-\")\n",
    "    print(\"This report shows the full hierarchical structure discovered from the chunk metadata.\")\n",
    "    print(\"'(Total Chunks: X, Direct: Y)' means X chunks in the whole branch, Y directly tagged to this heading.\\n\")\n",
    "    \n",
    "    print_hierarchy_report(hierarchy_tree)\n",
    "    \n",
    "    print_header(\"Diagnostic Summary\", char=\"-\")\n",
    "    top_level_sections = len(hierarchy_tree.get('_children', {}))\n",
    "    print(f\" Found {top_level_sections} distinct top-level sections.\")\n",
    "    if chunks_without_path > 0:\n",
    "        logger.warning(f\" Found {chunks_without_path} chunks MISSING the 'toc_path' metadata. This indicates an error in the enrichment pipeline.\")\n",
    "    else:\n",
    "        print(\" All chunks contain 'toc_path' metadata.\")\n",
    "        \n",
    "    print_header(\"Diagnostic Complete\")\n",
    "\n",
    "# --- Execute Diagnostics ---\n",
    "run_full_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae852dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:58:57,716 - INFO - Connecting to the vector database...\n",
      "2025-06-26 03:58:57,731 - INFO - Successfully connected to the database.\n",
      "2025-06-26 03:58:57,733 - INFO - Retrieving all metadata from the database. This may take a moment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                 Database Health & Hierarchy Diagnostic Report                  \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 03:58:58,380 - INFO - Successfully retrieved metadata for all 12498 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                        Hierarchy Distribution Analysis                         \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " Found the following top-level sections (level_0_title) and their chunk counts:\n",
      "                                                  Top-Level Section (level_0_title)  Chunk Count\n",
      "0                     Lab Manual for Guide to Computer Forensics and Investigations         4178\n",
      "1                                         Chapter 16. Ethics for the Expert Witness         2550\n",
      "2      Chapter 1. Understanding the Digital Forensics Profession and Investigations          594\n",
      "3                                   Chapter 5. Working with Windows and CLI Systems          483\n",
      "4                                        Chapter 6. Current Digital Forensics Tools          465\n",
      "5                                   Chapter 4. Processing Crime and Incident Scenes          450\n",
      "6                                                       Chapter 3. Data Acquisition          410\n",
      "7                                                       Chapter 13. Cloud Forensics          365\n",
      "8                            Chapter 15. Expert Testimony in Digital Investigations          340\n",
      "9                               Chapter 2. The Investigators Office and Laboratory          339\n",
      "10  Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics          330\n",
      "11                               Chapter 11. E-mail and Social Media Investigations          319\n",
      "12                                             Chapter 8. Recovering Graphics Files          289\n",
      "13                                      Chapter 7. Linux and Macintosh File Systems          287\n",
      "14                          Chapter 14. Report Writing for High-Tech Investigations          267\n",
      "15                             Chapter 9. Digital Forensics Analysis and Validation          263\n",
      "16                 Chapter 12. Mobile Device Forensics and the Internet of Anything          233\n",
      "17                                         Appendix B. Digital Forensics References           85\n",
      "18                               Appendix D. Legacy File System and Forensics Tools           73\n",
      "19                                 Appendix C. Digital Forensics Lab Considerations           66\n",
      "20                                        Appendix A. Certification Test References           62\n",
      "21                                                                  Acknowledgments           28\n",
      "22                                                                          Preface           14\n",
      "23                                                                About the Authors            8\n",
      "\n",
      "================================================================================\n",
      "                              Diagnostic Complete                               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Database Health & Hierarchy Diagnostic Report\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "import pandas as pd # You might need to run: pip install pandas\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "# --- Main Diagnostic Function ---\n",
    "def run_diagnostics():\n",
    "    print_header(\"Database Health & Hierarchy Diagnostic Report\")\n",
    "    \n",
    "    # --- 1. Connect to the Database ---\n",
    "    logger.info(\"Connecting to the vector database...\")\n",
    "    if not os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        logger.error(f\"FATAL: Chroma DB directory not found at {CHROMA_PERSIST_DIR}. Please run Cell 5 first.\"); return\n",
    "    \n",
    "    # This assumes your global variables (CHROMA_PERSIST_DIR, etc.) are available\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings, collection_name=CHROMA_COLLECTION_NAME)\n",
    "    logger.info(\"Successfully connected to the database.\")\n",
    "\n",
    "    # --- 2. Retrieve ALL Metadata ---\n",
    "    logger.info(\"Retrieving all metadata from the database. This may take a moment...\")\n",
    "    try:\n",
    "        total_docs = vector_store._collection.count()\n",
    "        if total_docs == 0:\n",
    "            logger.error(\"Database is empty. Cannot run diagnostics.\"); return\n",
    "        retrieved_data = vector_store.get(limit=total_docs, include=[\"metadatas\"])\n",
    "        all_metadatas = retrieved_data['metadatas']\n",
    "        logger.info(f\"Successfully retrieved metadata for all {len(all_metadatas)} chunks.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to retrieve data from ChromaDB: {e}\"); return\n",
    "        \n",
    "    # --- 3. Analyze Hierarchy Distribution ---\n",
    "    print_header(\"Hierarchy Distribution Analysis\", char=\"-\")\n",
    "    \n",
    "    level_0_counts = {}\n",
    "    chunks_without_level_0 = 0\n",
    "    for meta in all_metadatas:\n",
    "        level_0_title = meta.get(\"level_0_title\")\n",
    "        if level_0_title:\n",
    "            level_0_counts[level_0_title] = level_0_counts.get(level_0_title, 0) + 1\n",
    "        else:\n",
    "            chunks_without_level_0 += 1\n",
    "            \n",
    "    if level_0_counts:\n",
    "        print(\"\\n Found the following top-level sections (level_0_title) and their chunk counts:\")\n",
    "        # Use pandas to create a nicely formatted table\n",
    "        df = pd.DataFrame(list(level_0_counts.items()), columns=['Top-Level Section (level_0_title)', 'Chunk Count'])\n",
    "        df = df.sort_values(by='Chunk Count', ascending=False).reset_index(drop=True)\n",
    "        print(df.to_string())\n",
    "    else:\n",
    "        logger.error(\" CRITICAL ERROR: No chunks with 'level_0_title' metadata were found!\")\n",
    "\n",
    "    if chunks_without_level_0 > 0:\n",
    "        logger.warning(f\"\\n Found {chunks_without_level_0} chunks that are MISSING the 'level_0_title' metadata entirely. This is a major sign of a data processing error.\")\n",
    "        \n",
    "    print_header(\"Diagnostic Complete\")\n",
    "\n",
    "# --- Execute Diagnostics ---\n",
    "run_diagnostics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
