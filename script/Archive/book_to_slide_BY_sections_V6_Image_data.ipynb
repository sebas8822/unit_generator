{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9771e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION SUMMARY ---\n",
      "Processing Mode: EPUB\n",
      "Unit ID: ICT312\n",
      "Unit Outline Path: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "Book Path: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "Parsed UO Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "Parsed ToC Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n",
      "Vector DB Path: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "Vector DB Collection: book_toc_guided_chunks_epub_v2\n",
      "--- SETUP COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from PIL import Image \n",
    "import io\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "EXTRACT_UO = False \n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "OUTPUT_IMAGES_DIR = os.path.join(PROJECT_BASE_DIR, \"extracted_images\")\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0852ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a new cell after your imports, or within Cell 3 before the functions.\n",
    "# This code is based on the schema from your screenshot on page 4.\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# Define Pydantic models that match your JSON schema\n",
    "class UnitInformation(BaseModel):\n",
    "    unitCode: Optional[str] = None\n",
    "    unitName: Optional[str] = None\n",
    "    creditPoints: Optional[int] = None\n",
    "    unitRationale: Optional[str] = None\n",
    "    prerequisites: Optional[str] = None\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    taskName: str\n",
    "    description: str\n",
    "    dueWeek: Optional[str] = None\n",
    "    weightingPercent: Optional[int] = None\n",
    "    learningOutcomesAssessed: Optional[str] = None\n",
    "\n",
    "class WeeklyScheduleItem(BaseModel):\n",
    "    week: str\n",
    "    contentTopic: str\n",
    "    requiredReading: Optional[str] = None\n",
    "\n",
    "class ParsedUnitOutline(BaseModel):\n",
    "    unitInformation: UnitInformation\n",
    "    learningOutcomes: List[str]\n",
    "    assessments: List[Assessment]\n",
    "    weeklySchedule: List[WeeklyScheduleItem] \n",
    "    requiredReadings: List[str]\n",
    "    recommendedReadings: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline_robust(\n",
    "    input_filepath: str, \n",
    "    output_filepath: str, \n",
    "    prompt_template: str,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    logger.info(f\"Starting to robustly process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from file: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    client = ollama.Client(host=OLLAMA_HOST)\n",
    "    current_prompt = prompt_template.format(outline_text=outline_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{max_retries} to parse outline.\")\n",
    "        \n",
    "        try:\n",
    "            # Call the LLM\n",
    "            llm_output_str = call_ollama_with_retry(client, current_prompt)\n",
    "            \n",
    "            # Find the JSON blob in the response\n",
    "            json_blob = parse_llm_json_output(llm_output_str) # Your existing helper\n",
    "            if not json_blob:\n",
    "                raise ValueError(\"LLM did not return a parsable JSON object.\")\n",
    "\n",
    "            # *** THE KEY VALIDATION STEP ***\n",
    "            # Try to parse the dictionary into your Pydantic model.\n",
    "            # This will raise a `ValidationError` if keys are wrong, types are wrong, or fields are missing.\n",
    "            parsed_data = ParsedUnitOutline.model_validate(json_blob)\n",
    "            \n",
    "            # If successful, save the validated data and exit the loop\n",
    "            logger.info(\"Successfully validated JSON structure against Pydantic model.\")\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                # Use .model_dump_json() for clean, validated output\n",
    "                f.write(parsed_data.model_dump_json(indent=2)) \n",
    "\n",
    "            logger.info(f\"Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "            return # Exit function on success\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Validation failed on attempt {attempt + 1}. Error: {e}\")\n",
    "            # Formulate a new prompt with the error message for self-correction\n",
    "            error_feedback = (\n",
    "                f\"\\n\\nYour previous attempt failed. You MUST correct the following errors:\\n\"\n",
    "                f\"{e}\\n\\n\"\n",
    "                f\"Please regenerate the entire JSON object, ensuring it strictly adheres to the schema \"\n",
    "                f\"and corrects these specific errors. Do not change any key names.\"\n",
    "            )\n",
    "            current_prompt = current_prompt + error_feedback # Append the error to the prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch other errors like network issues from call_ollama_with_retry\n",
    "            logger.error(f\"An unexpected error occurred on attempt {attempt + 1}: {e}\", exc_info=True)\n",
    "            # You might want to wait before retrying for non-validation errors\n",
    "            time.sleep(5)\n",
    "\n",
    "    logger.error(f\"Failed to get valid structured data from the LLM after {max_retries} attempts.\")\n",
    "\n",
    "\n",
    "# --- In your execution block, call the new function ---\n",
    "# parse_and_save_outline(...) becomes:\n",
    "\n",
    "if EXTRACT_UO:\n",
    "    parse_and_save_outline_robust(\n",
    "        input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "        output_filepath=PARSED_UO_JSON_PATH,\n",
    "        prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or epub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EPUB ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\n",
      "✅ Successfully wrote EPUB ToC with assigned IDs to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Book Table of Contents (ToC) with Pre-assigned IDs in Order\n",
    "\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS (MODIFIED TO INCLUDE ID ASSIGNMENT)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- EPUB Extraction Logic ---\n",
    "def parse_navpoint(navpoint: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 2 navPoints and assigns a toc_id.\"\"\"\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    if not title: return None\n",
    "    \n",
    "    # Assign ID immediately upon creation\n",
    "    node = {\n",
    "        \"level\": level,\n",
    "        \"toc_id\": counter[0],\n",
    "        \"title\": title,\n",
    "        \"children\": []\n",
    "    }\n",
    "    counter[0] += 1 # Increment counter for the next node\n",
    "    \n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, counter, level + 1)\n",
    "        if child_node: node[\"children\"].append(child_node)\n",
    "        \n",
    "    return node\n",
    "\n",
    "def parse_li(li_element: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 3 <li> elements and assigns a toc_id.\"\"\"\n",
    "    a_tag = li_element.find('a', recursive=False)\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        if not title: return None\n",
    "        \n",
    "        # Assign ID immediately upon creation\n",
    "        node = {\n",
    "            \"level\": level,\n",
    "            \"toc_id\": counter[0],\n",
    "            \"title\": title,\n",
    "            \"children\": []\n",
    "        }\n",
    "        counter[0] += 1 # Increment counter for the next node\n",
    "        \n",
    "        nested_ol = li_element.find('ol', recursive=False)\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, counter, level + 1)\n",
    "                if child_node: node[\"children\"].append(child_node)\n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_epub_toc(epub_path, output_json_path):\n",
    "    print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "    toc_data = []\n",
    "    book = epub.read_epub(epub_path)\n",
    "    # The counter is a list so it can be passed by reference and modified by the helpers\n",
    "    id_counter = [1]\n",
    "    \n",
    "    for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "        soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "        if nav_item.get_name().endswith('.ncx'):\n",
    "            print(\"INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\")\n",
    "            navmap = soup.find('navMap')\n",
    "            if navmap:\n",
    "                for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                    node = parse_navpoint(navpoint, id_counter, level=0)\n",
    "                    if node: toc_data.append(node)\n",
    "        else:\n",
    "            print(\"INFO: Found EPUB 3 (XHTML) Table of Contents. Parsing...\")\n",
    "            toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "            if toc_nav:\n",
    "                top_ol = toc_nav.find('ol', recursive=False)\n",
    "                if top_ol:\n",
    "                    for li in top_ol.find_all('li', recursive=False):\n",
    "                        node = parse_li(li, id_counter, level=0)\n",
    "                        if node: toc_data.append(node)\n",
    "        if toc_data: break\n",
    "    \n",
    "    if toc_data:\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote EPUB ToC with assigned IDs to: {output_json_path}\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# --- PDF Extraction Logic ---\n",
    "def build_pdf_hierarchy_with_ids(toc_list: List) -> List[Dict]:\n",
    "    \"\"\"Builds a hierarchical structure from a flat PyMuPDF ToC list and assigns IDs.\"\"\"\n",
    "    root = []\n",
    "    parent_stack = {-1: {\"children\": root}}\n",
    "    id_counter = [1]\n",
    "\n",
    "    for level, title, page in toc_list:\n",
    "        normalized_level = level - 1\n",
    "        node = {\n",
    "            \"level\": normalized_level,\n",
    "            \"toc_id\": id_counter[0],\n",
    "            \"title\": title.strip(),\n",
    "            \"page\": page,\n",
    "            \"children\": []\n",
    "        }\n",
    "        id_counter[0] += 1\n",
    "        \n",
    "        parent_node = parent_stack.get(normalized_level - 1)\n",
    "        if parent_node:\n",
    "             parent_node[\"children\"].append(node)\n",
    "        \n",
    "        parent_stack[normalized_level] = node\n",
    "    return root\n",
    "\n",
    "def extract_pdf_toc(pdf_path, output_json_path):\n",
    "    print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        hierarchical_toc = []\n",
    "        if not toc:\n",
    "            print(\"❌ WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "        else:\n",
    "            print(f\"INFO: Found {len(toc)} bookmark entries. Building hierarchy and assigning IDs...\")\n",
    "            hierarchical_toc = build_pdf_hierarchy_with_ids(toc)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote PDF ToC with assigned IDs to: {output_json_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# Assumes global variables from Cell 1 are available\n",
    "if PROCESS_EPUB:\n",
    "    extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "else:\n",
    "    extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ecdb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5: Create Hierarchical Vector Database (V10 - ToC-First Method)\n",
    "# # This cell uses the pre-tagged ToC from Cell 4 as the source of truth\n",
    "# # to process the book, enrich text, and create the final vector database.\n",
    "\n",
    "# # --- Core Imports ---\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import logging\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# # --- LangChain and Data Loading Imports ---\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # --- Imports for EPUB and PDF Processing ---\n",
    "# from ebooklib import epub, ITEM_DOCUMENT\n",
    "# from bs4 import BeautifulSoup\n",
    "# import fitz  # PyMuPDF\n",
    "\n",
    "# # --- Logger Setup ---\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 1. HELPER FUNCTIONS\n",
    "# # ==============================================================================\n",
    "# # The previous helper functions (clean_metadata_for_chroma, extract_images_*)\n",
    "# # are still needed and can be copied from the previous answer. For brevity,\n",
    "# # only the new/modified helpers are shown in full here.\n",
    "\n",
    "# def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "#     if isinstance(value, (list, dict, set)):\n",
    "#         if isinstance(value, set): value = sorted(list(value))\n",
    "#         return json.dumps(value)\n",
    "#     if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "#     return str(value)\n",
    "\n",
    "# def extract_images_from_epub(epub_path: str, output_dir: str, unit_id: str) -> Dict[str, List[str]]:\n",
    "#     logger.info(f\"Extracting images from EPUB: {os.path.basename(epub_path)}\")\n",
    "#     image_map: Dict[str, List[str]] = {}\n",
    "#     book_image_dir = os.path.join(output_dir, f\"{unit_id}_epub_images\")\n",
    "#     os.makedirs(book_image_dir, exist_ok=True)\n",
    "#     book = epub.read_epub(epub_path)\n",
    "#     text_files = [item for item in book.get_items_of_type(ITEM_DOCUMENT)]\n",
    "#     for item in book.get_items_of_type(ITEM_DOCUMENT):\n",
    "#         source_filename = os.path.basename(item.get_name())\n",
    "#         content = item.get_content().decode('utf-8', 'ignore')\n",
    "#         for image_item in book.get_items_of_type('image'):\n",
    "#             img_internal_path = image_item.get_name()\n",
    "#             if img_internal_path in content:\n",
    "#                 if source_filename not in image_map: image_map[source_filename] = []\n",
    "#                 img_filename = os.path.basename(img_internal_path)\n",
    "#                 image_path = os.path.join(book_image_dir, img_filename)\n",
    "#                 if not os.path.exists(image_path):\n",
    "#                     with open(image_path, \"wb\") as f: f.write(image_item.get_content())\n",
    "#                 if image_path not in image_map[source_filename]: image_map[source_filename].append(image_path)\n",
    "#     total_images = sum(len(v) for v in image_map.values())\n",
    "#     logger.info(f\"Extracted {total_images} total images to '{book_image_dir}'\")\n",
    "#     return image_map\n",
    "    \n",
    "# def flatten_toc_with_paths(nodes: List[Dict], current_path: List[str] = []) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Flattens the hierarchical ToC and adds the full 'titles_path' to each entry.\n",
    "#     \"\"\"\n",
    "#     flat_list = []\n",
    "#     for node in nodes:\n",
    "#         new_path = current_path + [node['title']]\n",
    "#         # Create a new entry to avoid modifying the original node\n",
    "#         flat_entry = node.copy()\n",
    "#         flat_entry['titles_path'] = new_path\n",
    "        \n",
    "#         # Add the entry itself (without its children) to the list\n",
    "#         children = flat_entry.pop('children', [])\n",
    "#         flat_list.append(flat_entry)\n",
    "        \n",
    "#         # Recursively process the children\n",
    "#         if children:\n",
    "#             flat_list.extend(flatten_toc_with_paths(children, new_path))\n",
    "            \n",
    "#     return flat_list\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 2. CORE ORCHESTRATION FUNCTION\n",
    "# # ==============================================================================\n",
    "\n",
    "# def process_book_with_extracted_toc(\n",
    "#     book_path: str,\n",
    "#     extracted_toc_json_path: str,\n",
    "#     chunk_size: int,\n",
    "#     chunk_overlap: int\n",
    "# ) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "\n",
    "#     logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "#     # --- Step 1: Load ToC with Pre-assigned IDs ---\n",
    "#     try:\n",
    "#         with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "#             hierarchical_toc = json.load(f)\n",
    "#         logger.info(\"Successfully loaded pre-extracted ToC with assigned IDs.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"FATAL: Error loading ToC JSON: {e}\", exc_info=True)\n",
    "#         return [], []\n",
    "\n",
    "#     # --- Step 2: Create a Flattened ToC and a Title-based Lookup ---\n",
    "#     flat_toc = flatten_toc_with_paths(hierarchical_toc)\n",
    "#     toc_lookup = {entry['title'].strip().lower(): entry for entry in flat_toc}\n",
    "#     logger.info(f\"Created a flattened ToC with {len(flat_toc)} entries for matching.\")\n",
    "\n",
    "#     # --- Step 3: Extract Images (if any) ---\n",
    "#     file_extension = os.path.splitext(book_path.lower())[1]\n",
    "#     image_map = {}\n",
    "#     if file_extension == \".epub\":\n",
    "#         unit_id = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "#         image_map = extract_images_from_epub(book_path, OUTPUT_IMAGES_DIR, unit_id)\n",
    "#     # PDF image extraction would go here if needed\n",
    "\n",
    "#     # --- Step 4: Create Enriched Documents by Matching Content to ToC ---\n",
    "#     final_documents_with_metadata: List[Document] = []\n",
    "#     if file_extension == \".epub\":\n",
    "#         book = epub.read_epub(book_path)\n",
    "#         current_metadata = {\"source\": os.path.basename(book_path), \"toc_id\": -1, \"level_1_title\": \"Preamble\"}\n",
    "        \n",
    "#         for item in book.get_items_of_type(ITEM_DOCUMENT):\n",
    "#             source_filename = os.path.basename(item.get_name())\n",
    "#             soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
    "            \n",
    "#             for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li']):\n",
    "#                 text = element.get_text().strip()\n",
    "#                 if not text:\n",
    "#                     continue\n",
    "\n",
    "#                 # Check if this element's text is a heading in our ToC\n",
    "#                 normalized_text = text.lower()\n",
    "#                 if normalized_text in toc_lookup:\n",
    "#                     # It's a heading, update the current context\n",
    "#                     toc_entry = toc_lookup[normalized_text]\n",
    "#                     current_metadata = {\"source\": os.path.basename(book_path)}\n",
    "#                     for i, title in enumerate(toc_entry['titles_path']):\n",
    "#                         current_metadata[f\"level_{i+1}_title\"] = title\n",
    "#                     current_metadata['toc_id'] = toc_entry['toc_id']\n",
    "#                     logger.info(f\"Context updated to: '{' -> '.join(toc_entry['titles_path'])}' [ID: {toc_entry['toc_id']}]\")\n",
    "                \n",
    "#                 # Tag the document with the current metadata\n",
    "#                 doc_meta = current_metadata.copy()\n",
    "#                 if source_filename in image_map:\n",
    "#                     doc_meta.setdefault('image_paths', []).extend(p for p in image_map[source_filename] if p not in doc_meta.get('image_paths', []))\n",
    "                \n",
    "#                 final_documents_with_metadata.append(Document(page_content=text, metadata=doc_meta))\n",
    "\n",
    "#     # --- Step 5: Finalize and Chunk ---\n",
    "#     logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "\n",
    "#     logger.info(\"Sanitizing metadata and chunking documents...\")\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    \n",
    "#     for doc in final_documents_with_metadata:\n",
    "#         doc.metadata = {k: clean_metadata_for_chroma(v) for k, v in doc.metadata.items()}\n",
    "        \n",
    "#     final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    \n",
    "#     logger.info(f\"Split into {len(final_chunks)} final chunks and assigning chunk_id...\")\n",
    "#     for i, chunk in enumerate(final_chunks):\n",
    "#         chunk.metadata['chunk_id'] = i\n",
    "\n",
    "#     return final_chunks, hierarchical_toc\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 3. MAIN EXECUTION BLOCK\n",
    "# # ==============================================================================\n",
    "# if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "#     logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "#     logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "# else:\n",
    "#     final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "#         book_path=BOOK_PATH,\n",
    "#         extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "#         chunk_size=CHUNK_SIZE,\n",
    "#         chunk_overlap=CHUNK_OVERLAP\n",
    "#     )\n",
    "#     if final_chunks_for_db:\n",
    "#         if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "#             logger.warning(f\"Deleting existing ChromaDB directory: '{CHROMA_PERSIST_DIR}'\")\n",
    "#             shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "        \n",
    "#         logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "#         embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "#         vector_db = Chroma.from_documents(\n",
    "#             documents=final_chunks_for_db,\n",
    "#             embedding=embedding_model,\n",
    "#             persist_directory=CHROMA_PERSIST_DIR,\n",
    "#             collection_name=CHROMA_COLLECTION_NAME\n",
    "#         )\n",
    "#         count = vector_db._collection.count()\n",
    "#         print(\"-\" * 50)\n",
    "#         logger.info(f\"Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "#         logger.info(f\"Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "#         print(\"-\" * 50)\n",
    "#     else:\n",
    "#         logger.error(\"Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a003021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:38:57,299 - INFO - Processing book 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub'\n",
      "2025-07-05 20:38:57,300 - INFO - Successfully loaded pre-extracted ToC with assigned IDs.\n",
      "2025-07-05 20:38:57,301 - INFO - Created file-based ToC lookup for 0 unique files.\n",
      "2025-07-05 20:38:57,469 - WARNING - Unmapped file found: 'nav.xhtml'. Classifying as Preamble.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                Starting Book Processing (V11 File-First Method)                \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:38:57,513 - WARNING - Unmapped file found: 'content/cover.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,514 - WARNING - Unmapped file found: 'content/titlepage.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,515 - WARNING - Unmapped file found: 'content/copyright.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,517 - WARNING - Unmapped file found: 'content/fm_preface_0001.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,518 - WARNING - Unmapped file found: 'content/fm_intro_0002.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,529 - WARNING - Unmapped file found: 'content/fm_sect_0003.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,531 - WARNING - Unmapped file found: 'content/fm_ack_0004.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,532 - WARNING - Unmapped file found: 'content/bd_chapter_0001.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,589 - WARNING - Unmapped file found: 'content/bd_chapter_0002.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,612 - WARNING - Unmapped file found: 'content/bd_chapter_0003.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,659 - WARNING - Unmapped file found: 'content/bd_chapter_0004.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,750 - WARNING - Unmapped file found: 'content/bd_chapter_0005.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,824 - WARNING - Unmapped file found: 'content/bd_chapter_0006.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,867 - WARNING - Unmapped file found: 'content/bd_chapter_0007.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,910 - WARNING - Unmapped file found: 'content/bd_chapter_0008.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:57,949 - WARNING - Unmapped file found: 'content/bd_chapter_0009.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,048 - WARNING - Unmapped file found: 'content/bd_chapter_0010.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,088 - WARNING - Unmapped file found: 'content/bd_chapter_0011.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,123 - WARNING - Unmapped file found: 'content/bd_chapter_0012.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,150 - WARNING - Unmapped file found: 'content/bd_chapter_0013.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,184 - WARNING - Unmapped file found: 'content/bd_chapter_0014.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,218 - WARNING - Unmapped file found: 'content/bd_chapter_0015.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,260 - WARNING - Unmapped file found: 'content/bd_chapter_0016.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,363 - WARNING - Unmapped file found: 'content/bd_part_0017.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,364 - WARNING - Unmapped file found: 'content/bd_partopen_0018.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,372 - WARNING - Unmapped file found: 'content/bd_chapter_0019.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,389 - WARNING - Unmapped file found: 'content/bd_chapter_0020.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,413 - WARNING - Unmapped file found: 'content/bd_chapter_0021.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,439 - WARNING - Unmapped file found: 'content/bd_chapter_0022.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,461 - WARNING - Unmapped file found: 'content/bd_chapter_0023.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,478 - WARNING - Unmapped file found: 'content/bd_chapter_0024.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,500 - WARNING - Unmapped file found: 'content/bd_chapter_0025.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,514 - WARNING - Unmapped file found: 'content/bd_chapter_0026.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,532 - WARNING - Unmapped file found: 'content/bd_chapter_0027.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,545 - WARNING - Unmapped file found: 'content/bd_chapter_0028.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,574 - WARNING - Unmapped file found: 'content/bd_chapter_0029.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,645 - WARNING - Unmapped file found: 'content/bd_chapter_0030.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,658 - WARNING - Unmapped file found: 'content/bd_chapter_0031.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,675 - WARNING - Unmapped file found: 'content/bd_chapter_0032.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,695 - WARNING - Unmapped file found: 'content/bd_chapter_0033.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,717 - WARNING - Unmapped file found: 'content/bd_chapter_0034.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,731 - WARNING - Unmapped file found: 'content/bm_app_0001.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,736 - WARNING - Unmapped file found: 'content/bm_app_0002.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,743 - WARNING - Unmapped file found: 'content/bm_app_0003.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,747 - WARNING - Unmapped file found: 'content/bm_app_0004.html'. Classifying as Preamble.\n",
      "2025-07-05 20:38:58,757 - INFO - Total documents prepared for chunking: 46\n",
      "2025-07-05 20:38:58,902 - INFO - Split into 2666 final chunks and assigning sequential chunk_id...\n",
      "2025-07-05 20:38:58,903 - WARNING - Deleting existing ChromaDB directory: '/home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub' for a clean build.\n",
      "2025-07-05 20:38:58,908 - INFO - Initializing embedding model 'nomic-embed-text'...\n",
      "2025-07-05 20:38:58,932 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                            Creating Vector Database                            \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:28,684 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-05 20:39:30,653 - INFO - Vector DB created successfully at: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "2025-07-05 20:39:30,653 - INFO - Collection 'book_toc_guided_chunks_epub_v11' contains 2666 documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                                PROCESS COMPLETE                                \n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# # ==============================================================================\n",
    "# # Cell 5: Create Hierarchical Vector Database (V11 - File-First Hybrid Method)\n",
    "# #\n",
    "# # This script processes a book (EPUB or PDF) by using its pre-extracted\n",
    "# # Table of Contents (ToC) as the primary source of truth. It uses a robust\n",
    "# # file-first matching strategy for EPUBs to ensure all content is correctly\n",
    "# # associated with its ToC entry before chunking and creating a vector database.\n",
    "# # ==============================================================================\n",
    "\n",
    "# # --- 1. Core Imports ---\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import logging\n",
    "# import re\n",
    "# import urllib.parse\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# # --- 2. LangChain and Data Loading Imports ---\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # --- 3. Imports for EPUB and PDF Processing ---\n",
    "# from ebooklib import epub, ITEM_DOCUMENT\n",
    "# from bs4 import BeautifulSoup\n",
    "# import fitz  # PyMuPDF\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 4. Configuration & Global Settings\n",
    "# #\n",
    "# # Assumes these variables are defined from a previous setup cell (e.g., Cell 1)\n",
    "# # Make sure the paths and filenames are correct for your project.\n",
    "# # ==============================================================================\n",
    "\n",
    "# # --- Logger Setup ---\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "\n",
    "# # --- Dynamically Generated Paths & IDs (from your original Cell 1 & 3) ---\n",
    "# def extract_uo_id_from_filename(filename: str) -> str:\n",
    "#     match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "#     if match:\n",
    "#         return match.group(0)\n",
    "#     logger.warning(f\"Could not extract a valid Unit ID from '{filename}'. Using UNKNOWN_ID.\")\n",
    "#     return \"UNKNOWN_ID\"\n",
    "\n",
    "# UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# if PROCESS_EPUB:\n",
    "#     BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "#     PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "#     file_type_suffix = 'epub'\n",
    "# else:\n",
    "#     BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "#     PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "#     file_type_suffix = 'pdf'\n",
    "    \n",
    "# CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}\")\n",
    "# CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v11\" # Using v11 to denote new method\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 5. HELPER FUNCTIONS\n",
    "# # ==============================================================================\n",
    "\n",
    "# def print_header(text: str, char: str = \"=\"):\n",
    "#     \"\"\"Prints a centered header to the console.\"\"\"\n",
    "#     print(\"\\n\" + char * 80)\n",
    "#     print(text.center(80))\n",
    "#     print(char * 80)\n",
    "\n",
    "# def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "#     \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "#     if isinstance(value, (list, dict, set)):\n",
    "#         if isinstance(value, set): value = sorted(list(value))\n",
    "#         return json.dumps(value)\n",
    "#     if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "#     return str(value)\n",
    "\n",
    "# def flatten_toc_for_lookups(nodes: List[Dict]) -> Dict[str, Dict]:\n",
    "#     \"\"\"\n",
    "#     Flattens the ToC and creates a lookup map where the key is the EPUB's internal\n",
    "#     HTML filename, and the value is a list of all ToC entries pointing to that file.\n",
    "#     \"\"\"\n",
    "#     file_lookup = {}\n",
    "    \n",
    "#     def _recursive_flatten(nodes_list, current_path):\n",
    "#         for node in nodes_list:\n",
    "#             new_path = current_path + [node['title']]\n",
    "            \n",
    "#             # Create a clean entry for the lookup, including its full path\n",
    "#             lookup_entry = node.copy()\n",
    "#             lookup_entry['titles_path'] = new_path\n",
    "#             children = lookup_entry.pop('children', [])\n",
    "            \n",
    "#             # Populate the file_lookup using the cleaned link_filename\n",
    "#             link_filename = lookup_entry.get('link_filename')\n",
    "#             if link_filename:\n",
    "#                 if link_filename not in file_lookup:\n",
    "#                     file_lookup[link_filename] = []\n",
    "#                 file_lookup[link_filename].append(lookup_entry)\n",
    "            \n",
    "#             if children:\n",
    "#                 _recursive_flatten(children, new_path)\n",
    "\n",
    "#     _recursive_flatten(nodes, [])\n",
    "    \n",
    "#     # Sort the entries within each file's list by their original toc_id\n",
    "#     # This is crucial for correctly identifying sub-sections in order.\n",
    "#     for filename in file_lookup:\n",
    "#         file_lookup[filename].sort(key=lambda x: x.get('toc_id', 0))\n",
    "        \n",
    "#     return file_lookup\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 6. CORE ORCHESTRATION FUNCTION\n",
    "# # ==============================================================================\n",
    "# def process_book_with_extracted_toc(\n",
    "#     book_path: str,\n",
    "#     extracted_toc_json_path: str,\n",
    "#     chunk_size: int,\n",
    "#     chunk_overlap: int\n",
    "# ) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "\n",
    "#     print_header(\"Starting Book Processing (V11 File-First Method)\")\n",
    "#     logger.info(f\"Processing book '{os.path.basename(book_path)}'\")\n",
    "\n",
    "#     # --- Step 1: Load ToC ---\n",
    "#     try:\n",
    "#         with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "#             hierarchical_toc = json.load(f)\n",
    "#         logger.info(\"Successfully loaded pre-extracted ToC with assigned IDs.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"FATAL: Error loading ToC JSON: {e}\", exc_info=True)\n",
    "#         return [], []\n",
    "\n",
    "#     # --- Step 2: Create File-First Lookup Map ---\n",
    "#     file_lookup = flatten_toc_for_lookups(hierarchical_toc)\n",
    "#     logger.info(f\"Created file-based ToC lookup for {len(file_lookup)} unique files.\")\n",
    "\n",
    "#     # --- Step 3: Process Book Content ---\n",
    "#     final_documents_with_metadata: List[Document] = []\n",
    "#     file_extension = os.path.splitext(book_path.lower())[1]\n",
    "\n",
    "#     if file_extension == \".epub\":\n",
    "#         book = epub.read_epub(book_path)\n",
    "        \n",
    "#         for item in book.get_items_of_type(ITEM_DOCUMENT):\n",
    "#             source_filename = item.get_name()\n",
    "            \n",
    "#             # --- Primary Matching Strategy: By Filename ---\n",
    "#             if source_filename in file_lookup:\n",
    "#                 toc_entries_for_file = file_lookup[source_filename]\n",
    "#                 logger.info(f\"Processing file '{source_filename}' matched to {len(toc_entries_for_file)} ToC entries.\")\n",
    "\n",
    "#                 soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
    "                \n",
    "#                 # Start with the metadata of the FIRST section listed for this file\n",
    "#                 current_toc_entry_index = 0\n",
    "#                 first_entry = toc_entries_for_file[0]\n",
    "#                 current_metadata = {\n",
    "#                     \"source\": os.path.basename(book_path),\n",
    "#                     \"toc_id\": first_entry['toc_id'],\n",
    "#                     **{f\"level_{i+1}_title\": title for i, title in enumerate(first_entry['titles_path'])}\n",
    "#                 }\n",
    "\n",
    "#                 for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 'span']):\n",
    "#                     text = element.get_text(\" \", strip=True) # Use space separator\n",
    "#                     if not text:\n",
    "#                         continue\n",
    "\n",
    "#                     # Fallback Matching: Check if this element is the start of the *next* sub-section within this file\n",
    "#                     normalized_text = text.lower()\n",
    "#                     if (current_toc_entry_index + 1) < len(toc_entries_for_file):\n",
    "#                         next_toc_entry = toc_entries_for_file[current_toc_entry_index + 1]\n",
    "#                         if normalized_text == next_toc_entry['title'].strip().lower():\n",
    "#                             current_toc_entry_index += 1\n",
    "#                             current_metadata = {\n",
    "#                                 \"source\": os.path.basename(book_path),\n",
    "#                                 \"toc_id\": next_toc_entry['toc_id'],\n",
    "#                                 **{f\"level_{i+1}_title\": title for i, title in enumerate(next_toc_entry['titles_path'])}\n",
    "#                             }\n",
    "#                             logger.info(f\"  -> Context updated to sub-section: '{' -> '.join(next_toc_entry['titles_path'])}'\")\n",
    "                    \n",
    "#                     final_documents_with_metadata.append(Document(page_content=text, metadata=current_metadata.copy()))\n",
    "#             else:\n",
    "#                 # This file was not listed in the ToC (e.g., copyright page, cover)\n",
    "#                 logger.warning(f\"Unmapped file found: '{source_filename}'. Classifying as Preamble.\")\n",
    "#                 soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
    "#                 text = soup.get_text(\" \", strip=True)\n",
    "#                 if text:\n",
    "#                     preamble_meta = {\"source\": os.path.basename(book_path), \"toc_id\": -1, \"level_1_title\": \"Preamble\"}\n",
    "#                     final_documents_with_metadata.append(Document(page_content=text, metadata=preamble_meta))\n",
    "    \n",
    "#     # PDF logic would go here if needed\n",
    "\n",
    "#     # --- Step 4: Finalize and Chunk ---\n",
    "#     logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    \n",
    "#     # Sanitize metadata just before chunking\n",
    "#     for doc in final_documents_with_metadata:\n",
    "#         doc.metadata = {k: clean_metadata_for_chroma(v) for k, v in doc.metadata.items()}\n",
    "        \n",
    "#     final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    \n",
    "#     logger.info(f\"Split into {len(final_chunks)} final chunks and assigning sequential chunk_id...\")\n",
    "#     for i, chunk in enumerate(final_chunks):\n",
    "#         chunk.metadata['chunk_id'] = i\n",
    "\n",
    "#     return final_chunks, hierarchical_toc\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 7. MAIN EXECUTION BLOCK\n",
    "# # ==============================================================================\n",
    "\n",
    "# # Critical check to ensure the ToC file exists before starting\n",
    "# if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "#     print_header(\"EXECUTION FAILED\", char=\"!\")\n",
    "#     logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "#     logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (e.g., Cell 4) first.\")\n",
    "# else:\n",
    "#     # --- Run the main processing pipeline ---\n",
    "#     final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "#         book_path=BOOK_PATH,\n",
    "#         extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "#         chunk_size=CHUNK_SIZE,\n",
    "#         chunk_overlap=CHUNK_OVERLAP\n",
    "#     )\n",
    "    \n",
    "#     # --- Create the Vector Database ---\n",
    "#     if final_chunks_for_db:\n",
    "#         if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "#             logger.warning(f\"Deleting existing ChromaDB directory: '{CHROMA_PERSIST_DIR}' for a clean build.\")\n",
    "#             shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "        \n",
    "#         print_header(\"Creating Vector Database\", char=\"-\")\n",
    "#         logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}'...\")\n",
    "#         embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "#         vector_db = Chroma.from_documents(\n",
    "#             documents=final_chunks_for_db,\n",
    "#             embedding=embedding_model,\n",
    "#             persist_directory=CHROMA_PERSIST_DIR,\n",
    "#             collection_name=CHROMA_COLLECTION_NAME\n",
    "#         )\n",
    "        \n",
    "#         count = vector_db._collection.count()\n",
    "#         print_header(\"PROCESS COMPLETE\", char=\"*\")\n",
    "#         logger.info(f\"Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "#         logger.info(f\"Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "#     else:\n",
    "#         print_header(\"PROCESS FAILED\", char=\"!\")\n",
    "#         logger.error(\"Failed to generate any chunks. Vector DB was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d38d",
   "metadata": {},
   "source": [
    "### Full Database Health & Hierarchy Diagnostic Report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9902b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:30,667 - INFO - Successfully loaded the pre-extracted Table of Contents file (with IDs).\n",
      "2025-07-05 20:39:30,667 - INFO - Connecting to the vector database...\n",
      "2025-07-05 20:39:30,678 - INFO - Retrieving metadata for all 2666 chunks...\n",
      "2025-07-05 20:39:30,734 - INFO - Successfully retrieved all metadata.\n",
      "2025-07-05 20:39:30,734 - INFO - Aggregating chunk and image data from the database by ToC ID...\n",
      "2025-07-05 20:39:30,735 - INFO - Annotating the original ToC with aggregated database data...\n",
      "2025-07-05 20:39:30,736 - INFO - Annotation complete.\n",
      "2025-07-05 20:39:30,739 - WARNING - Found 2666 chunks MISSING a valid 'toc_id'. These are in the 'Preamble' section.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "               Full Database Health & Hierarchy Diagnostic Report               \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                  Reconstructed Hierarchy Report (Book Order)                   \n",
      "--------------------------------------------------------------------------------\n",
      "|-- EPUB Preamble [ID: -1] (Total Chunks: 2666, Direct Chunks: 2666, Images: 0)\n",
      "|  |-- Cover Page [ID: 1] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Title Page [ID: 2] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Copyright Page [ID: 3] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Preface [ID: 4] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Introduction [ID: 5] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- About the Authors [ID: 6] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Acknowledgments [ID: 7] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: 8] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 9] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- An Overview of Digital Forensics [ID: 10] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Digital Forensics and Other Related Disciplines [ID: 11] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- A Brief History of Digital Forensics [ID: 12] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Case Law [ID: 13] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Developing Digital Forensics Resources [ID: 14] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Preparing for Digital Investigations [ID: 15] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Law Enforcement Agency Investigations [ID: 16] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Following Legal Processes [ID: 17] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Private-Sector Investigations [ID: 18] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Establishing Company Policies [ID: 19] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Displaying Warning Banners [ID: 20] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Designating an Authorized Requester [ID: 21] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Conducting Security Investigations [ID: 22] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Distinguishing Personal and Company Property [ID: 23] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Maintaining Professional Conduct [ID: 24] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Preparing a Digital Forensics Investigation [ID: 25] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- An Overview of a Computer Crime [ID: 26] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- An Overview of a Company Policy Violation [ID: 27] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Taking a Systematic Approach [ID: 28] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Assessing the Case [ID: 29] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Planning Your Investigation [ID: 30] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Securing Your Evidence [ID: 31] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Procedures for Private-Sector High-Tech Investigations [ID: 32] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Employee Termination Cases [ID: 33] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Internet Abuse Investigations [ID: 34] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- E-mail Abuse Investigations [ID: 35] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Attorney-Client Privilege Investigations [ID: 36] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Industrial Espionage Investigations [ID: 37] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Interviews and Interrogations in High-Tech Investigations [ID: 38] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Data Recovery Workstations and Software [ID: 39] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Setting Up Your Workstation for Digital Forensics [ID: 40] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Conducting an Investigation [ID: 41] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Gathering the Evidence [ID: 42] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Bit-stream Copies [ID: 43] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Acquiring an Image of Evidence Media [ID: 44] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Analyzing Your Digital Evidence [ID: 45] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Some Additional Features of Autopsy [ID: 46] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Completing the Case [ID: 47] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Autopsy’s Report Generator [ID: 48] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Critiquing the Case [ID: 49] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 50] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 51] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 52] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 53] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 54] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 55] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 2. The Investigator’s Office and Laboratory [ID: 56] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 57] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Forensics Lab Accreditation Requirements [ID: 58] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Identifying Duties of the Lab Manager and Staff [ID: 59] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab Budget Planning [ID: 60] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Acquiring Certification and Training [ID: 61] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- International Association of Computer Investigative Specialists [ID: 62] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- ISC2 Certified Cyber Forensics Professional [ID: 63] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- High Tech Crime Network [ID: 64] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- EnCase Certified Examiner Certification [ID: 65] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- AccessData Certified Examiner [ID: 66] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Other Training and Certifications [ID: 67] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Determining the Physical Requirements for a Digital Forensics Lab [ID: 68] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Identifying Lab Security Needs [ID: 69] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Conducting High-Risk Investigations [ID: 70] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Evidence Containers [ID: 71] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Overseeing Facility Maintenance [ID: 72] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Considering Physical Security Needs [ID: 73] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Auditing a Digital Forensics Lab [ID: 74] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Determining Floor Plans for Digital Forensics Labs [ID: 75] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Selecting a Basic Forensic Workstation [ID: 76] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Selecting Workstations for a Lab [ID: 77] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Selecting Workstations for Private-Sector Labs [ID: 78] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Stocking Hardware Peripherals [ID: 79] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Maintaining Operating Systems and Software Inventories [ID: 80] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using a Disaster Recovery Plan [ID: 81] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Planning for Equipment Upgrades [ID: 82] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Building a Business Case for Developing a Forensics Lab [ID: 83] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Preparing a Business Case for a Digital Forensics Lab [ID: 84] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Justification [ID: 85] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Budget Development [ID: 86] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Facility Cost [ID: 87] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Hardware Requirements [ID: 88] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Software Requirements [ID: 89] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Miscellaneous Budget Needs [ID: 90] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Approval and Acquisition [ID: 91] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Implementation [ID: 92] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Acceptance Testing [ID: 93] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Correction for Acceptance [ID: 94] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Production [ID: 95] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 96] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 97] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 98] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 99] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 100] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 101] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 3. Data Acquisition [ID: 102] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 103] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Storage Formats for Digital Evidence [ID: 104] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Raw Format [ID: 105] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Proprietary Formats [ID: 106] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Advanced Forensic Format [ID: 107] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Determining the Best Acquisition Method [ID: 108] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Contingency Planning for Image Acquisitions [ID: 109] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Using Acquisition Tools [ID: 110] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Mini-WinFE Boot CDs and USB Drives [ID: 111] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Acquiring Data with a Linux Boot CD [ID: 112] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Using Linux Live CD Distributions [ID: 113] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Preparing a Target Drive for Acquisition in Linux [ID: 114] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Acquiring Data with dd in Linux [ID: 115] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Acquiring Data with dcfldd in Linux [ID: 116] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Capturing an Image with AccessData FTK Imager Lite [ID: 117] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Validating Data Acquisitions [ID: 118] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Linux Validation Methods [ID: 119] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Validating dd-Acquired Data [ID: 120] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Validating dcfldd-Acquired Data [ID: 121] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Windows Validation Methods [ID: 122] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Performing RAID Data Acquisitions [ID: 123] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding RAID [ID: 124] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Acquiring RAID Disks [ID: 125] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Using Remote Network Acquisition Tools [ID: 126] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Remote Acquisition with ProDiscover [ID: 127] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Remote Acquisition with EnCase Enterprise [ID: 128] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Remote Acquisition with R-Tools R-Studio [ID: 129] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Remote Acquisition with WetStone US-LATT PRO [ID: 130] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Remote Acquisition with F-Response [ID: 131] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Using Other Forensics Acquisition Tools [ID: 132] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- PassMark Software ImageUSB [ID: 133] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- ASR Data SMART [ID: 134] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Runtime Software [ID: 135] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- ILookIX IXImager [ID: 136] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- SourceForge [ID: 137] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 138] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 139] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 140] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 141] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 142] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 143] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 4. Processing Crime and Incident Scenes [ID: 144] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 145] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Identifying Digital Evidence [ID: 146] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Rules of Evidence [ID: 147] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Collecting Evidence in Private-Sector Incident Scenes [ID: 148] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Processing Law Enforcement Crime Scenes [ID: 149] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Concepts and Terms Used in Warrants [ID: 150] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Preparing for a Search [ID: 151] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Identifying the Nature of the Case [ID: 152] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Identifying the Type of OS or Digital Device [ID: 153] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Determining Whether You Can Seize Computers and Digital Devices [ID: 154] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Getting a Detailed Description of the Location [ID: 155] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Determining Who Is in Charge [ID: 156] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Additional Technical Expertise [ID: 157] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Determining the Tools You Need [ID: 158] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Preparing the Investigation Team [ID: 159] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Securing a Digital Incident or Crime Scene [ID: 160] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Seizing Digital Evidence at the Scene [ID: 161] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Preparing to Acquire Digital Evidence [ID: 162] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Processing Incident or Crime Scenes [ID: 163] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Processing Data Centers with RAID Systems [ID: 164] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using a Technical Advisor [ID: 165] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Documenting Evidence in the Lab [ID: 166] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Processing and Handling Digital Evidence [ID: 167] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Storing Digital Evidence [ID: 168] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Evidence Retention and Media Storage Needs [ID: 169] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Documenting Evidence [ID: 170] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Obtaining a Digital Hash [ID: 171] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Reviewing a Case [ID: 172] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Sample Civil Investigation [ID: 173] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- An Example of a Criminal Investigation [ID: 174] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Reviewing Background Information for a Case [ID: 175] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Planning the Investigation [ID: 176] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Conducting the Investigation: Acquiring Evidence with OSForensics [ID: 177] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 178] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 179] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 180] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 181] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 182] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 183] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 5. Working with Windows and CLI Systems [ID: 184] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 185] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding File Systems [ID: 186] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding the Boot Sequence [ID: 187] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Disk Drives [ID: 188] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Solid-State Storage Devices [ID: 189] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Exploring Microsoft File Structures [ID: 190] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Disk Partitions [ID: 191] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining FAT Disks [ID: 192] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Deleting FAT Files [ID: 193] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Examining NTFS Disks [ID: 194] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- NTFS System Files [ID: 195] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- MFT and File Attributes [ID: 196] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- MFT Structures for File Data [ID: 197] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- MFT Header Fields [ID: 198] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Attribute 0x10: Standard Information [ID: 199] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Attribute 0x30: File Name [ID: 200] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Attribute 0x40: Object_ID [ID: 201] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Attribute 0x80: Data for a Resident File [ID: 202] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Attribute 0x80: Data for a Nonresident File [ID: 203] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Interpreting a Data Run [ID: 204] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- NTFS Alternate Data Streams [ID: 205] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- NTFS Compressed Files [ID: 206] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- NTFS Encrypting File System [ID: 207] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- EFS Recovery Key Agent [ID: 208] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Deleting NTFS Files [ID: 209] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Resilient File System [ID: 210] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Whole Disk Encryption [ID: 211] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining Microsoft BitLocker [ID: 212] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining Third-Party Disk Encryption Tools [ID: 213] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding the Windows Registry [ID: 214] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Exploring the Organization of the Windows Registry [ID: 215] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining the Windows Registry [ID: 216] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Microsoft Startup Tasks [ID: 217] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Startup in Windows 7, Windows 8, and Windows 10 [ID: 218] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Startup in Windows NT and Later [ID: 219] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Startup Files for Windows Vista [ID: 220] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Startup Files for Windows XP [ID: 221] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Windows XP System Files [ID: 222] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Contamination Concerns with Windows XP [ID: 223] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Virtual Machines [ID: 224] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Creating a Virtual Machine [ID: 225] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 226] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 227] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 228] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 229] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 230] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 231] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 6. Current Digital Forensics Tools [ID: 232] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 233] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Evaluating Digital Forensics Tool Needs [ID: 234] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Types of Digital Forensics Tools [ID: 235] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Hardware Forensics Tools [ID: 236] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Software Forensics Tools [ID: 237] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Tasks Performed by Digital Forensics Tools [ID: 238] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Acquisition [ID: 239] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Validation and Verification [ID: 240] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Extraction [ID: 241] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Reconstruction [ID: 242] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Reporting [ID: 243] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Tool Comparisons [ID: 244] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Other Considerations for Tools [ID: 245] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Digital Forensics Software Tools [ID: 246] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Command-Line Forensics Tools [ID: 247] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Linux Forensics Tools [ID: 248] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Smart [ID: 249] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Helix 3 [ID: 250] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Kali Linux [ID: 251] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Autopsy and Sleuth Kit [ID: 252] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Forcepoint Threat Protection [ID: 253] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Other GUI Forensics Tools [ID: 254] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Digital Forensics Hardware Tools [ID: 255] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Forensic Workstations [ID: 256] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Building Your Own Workstation [ID: 257] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using a Write-Blocker [ID: 258] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Recommendations for a Forensic Workstation [ID: 259] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Validating and Testing Forensics Software [ID: 260] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using National Institute of Standards and Technology Tools [ID: 261] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Validation Protocols [ID: 262] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Digital Forensics Examination Protocol [ID: 263] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Digital Forensics Tool Upgrade Protocol [ID: 264] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 265] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 266] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 267] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 268] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 269] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 270] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 7. Linux and Macintosh File Systems [ID: 271] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 272] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Examining Linux File Structures [ID: 273] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- File Structures in Ext4 [ID: 274] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Inodes [ID: 275] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Hard Links and Symbolic Links [ID: 276] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Macintosh File Structures [ID: 277] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- An Overview of Mac File Structures [ID: 278] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Forensics Procedures in Mac [ID: 279] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Acquisition Methods in macOS [ID: 280] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Using Linux Forensics Tools [ID: 281] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Installing Sleuth Kit and Autopsy [ID: 282] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining a Case with Sleuth Kit and Autopsy [ID: 283] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 284] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 285] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 286] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 287] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 288] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 289] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 8. Recovering Graphics Files [ID: 290] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 291] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Recognizing a Graphics File [ID: 292] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Bitmap and Raster Images [ID: 293] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Vector Graphics [ID: 294] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Metafile Graphics [ID: 295] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Graphics File Formats [ID: 296] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Digital Photograph File Formats [ID: 297] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Examining the Raw File Format [ID: 298] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Examining the Exchangeable Image File Format [ID: 299] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Data Compression [ID: 300] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lossless and Lossy Compression [ID: 301] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Locating and Recovering Graphics Files [ID: 302] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Identifying Graphics File Fragments [ID: 303] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Repairing Damaged Headers [ID: 304] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Searching for and Carving Data from Unallocated Space [ID: 305] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Planning Your Examination [ID: 306] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Searching for and Recovering Digital Photograph Evidence [ID: 307] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Rebuilding File Headers [ID: 308] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Reconstructing File Fragments [ID: 309] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Identifying Unknown File Formats [ID: 310] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Analyzing Graphics File Headers [ID: 311] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Tools for Viewing Images [ID: 312] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Steganography in Graphics Files [ID: 313] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Steganalysis Tools [ID: 314] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Copyright Issues with Graphics [ID: 315] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 316] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 317] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 318] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 319] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 320] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 321] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 9. Digital Forensics Analysis and Validation [ID: 322] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 323] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Determining What Data to Collect and Analyze [ID: 324] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Approaching Digital Forensics Cases [ID: 325] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Refining and Modifying the Investigation Plan [ID: 326] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Autopsy to Validate Data [ID: 327] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Installing NSRL Hashes in Autopsy [ID: 328] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Collecting Hash Values in Autopsy [ID: 329] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Validating Forensic Data [ID: 330] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Validating with Hexadecimal Editors [ID: 331] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Using Hash Values to Discriminate Data [ID: 332] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Validating with Digital Forensics Tools [ID: 333] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Addressing Data-Hiding Techniques [ID: 334] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hiding Files by Using the OS [ID: 335] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hiding Partitions [ID: 336] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Marking Bad Clusters [ID: 337] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Bit-Shifting [ID: 338] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Steganalysis Methods [ID: 339] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining Encrypted Files [ID: 340] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Recovering Passwords [ID: 341] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 342] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 343] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 344] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 345] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 346] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 347] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: 348] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 349] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- An Overview of Virtual Machine Forensics [ID: 350] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Type 2 Hypervisors [ID: 351] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Parallels Desktop [ID: 352] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- KVM [ID: 353] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Microsoft Hyper-V [ID: 354] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- VMware Workstation and Workstation Player [ID: 355] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- VirtualBox [ID: 356] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Conducting an Investigation with Type 2 Hypervisors [ID: 357] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Other VM Examination Methods [ID: 358] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Using VMs as Forensics Tools [ID: 359] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Working with Type 1 Hypervisors [ID: 360] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Performing Live Acquisitions [ID: 361] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Performing a Live Acquisition in Windows [ID: 362] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Network Forensics Overview [ID: 363] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- The Need for Established Procedures [ID: 364] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Securing a Network [ID: 365] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Developing Procedures for Network Forensics [ID: 366] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Reviewing Network Logs [ID: 367] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Using Network Tools [ID: 368] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Using Packet Analyzers [ID: 369] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Investigating Virtual Networks [ID: 370] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining the Honeynet Project [ID: 371] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 372] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 373] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 374] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 375] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 376] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 377] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 11. E-mail and Social Media Investigations [ID: 378] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 379] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Exploring the Role of E-mail in Investigations [ID: 380] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Exploring the Roles of the Client and Server in E-mail [ID: 381] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Investigating E-mail Crimes and Violations [ID: 382] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Forensic Linguistics [ID: 383] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining E-mail Messages [ID: 384] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Copying an E-mail Message [ID: 385] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Viewing E-mail Headers [ID: 386] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining E-mail Headers [ID: 387] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining Additional E-mail Files [ID: 388] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Tracing an E-mail Message [ID: 389] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Network E-mail Logs [ID: 390] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding E-mail Servers [ID: 391] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining UNIX E-mail Server Logs [ID: 392] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining Microsoft E-mail Server Logs [ID: 393] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Using Specialized E-mail Forensics Tools [ID: 394] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Magnet AXIOM to Recover E-mail [ID: 395] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using a Hex Editor to Carve E-mail Messages [ID: 396] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Recovering Outlook Files [ID: 397] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- E-mail Case Studies [ID: 398] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Applying Digital Forensics Methods to Social Media Communications [ID: 399] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Forensics Tools for Social Media Investigations [ID: 400] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 401] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 402] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 403] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 404] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 405] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 406] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 12. Mobile Device Forensics and the Internet of Anything [ID: 407] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 408] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Mobile Device Forensics [ID: 409] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Mobile Phone Basics [ID: 410] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Inside Mobile Devices [ID: 411] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- SIM Cards [ID: 412] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Acquisition Procedures for Mobile Devices [ID: 413] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Mobile Forensics Equipment [ID: 414] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- SIM Card Readers [ID: 415] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Mobile Phone Forensics Tools and Methods [ID: 416] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Mobile Forensics Tools [ID: 417] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding Forensics in the Internet of Anything [ID: 418] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 419] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 420] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 421] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 422] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 423] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 424] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 13. Cloud Forensics [ID: 425] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 426] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- An Overview of Cloud Computing [ID: 427] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- History of the Cloud [ID: 428] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Cloud Service Levels and Deployment Methods [ID: 429] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Cloud Vendors [ID: 430] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Basic Concepts of Cloud Forensics [ID: 431] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Legal Challenges in Cloud Forensics [ID: 432] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Service Level Agreements [ID: 433] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Policies, Standards, and Guidelines for CSPs [ID: 434] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- CSP Processes and Procedures [ID: 435] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Jurisdiction Issues [ID: 436] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Accessing Evidence in the Cloud [ID: 437] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Search Warrants [ID: 438] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Subpoenas and Court Orders [ID: 439] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Technical Challenges in Cloud Forensics [ID: 440] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Architecture [ID: 441] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Analysis of Cloud Forensic Data [ID: 442] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Anti-Forensics [ID: 443] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Incident First Responders [ID: 444] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Role Management [ID: 445] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Standards and Training [ID: 446] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Acquisitions in the Cloud [ID: 447] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Encryption in the Cloud [ID: 448] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Conducting a Cloud Investigation [ID: 449] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Investigating CSPs [ID: 450] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Investigating Cloud Customers [ID: 451] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding Prefetch Files [ID: 452] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Examining Stored Cloud Data on a PC [ID: 453] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Dropbox [ID: 454] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Google Drive [ID: 455] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- OneDrive [ID: 456] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Windows Prefetch Artifacts [ID: 457] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Tools for Cloud Forensics [ID: 458] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Forensic Open-Stack Tools [ID: 459] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- F-Response for the Cloud [ID: 460] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Magnet AXIOM Cloud [ID: 461] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 462] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 463] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 464] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 465] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 466] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 467] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 14. Report Writing for High-Tech Investigations [ID: 468] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 469] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Understanding the Importance of Reports [ID: 470] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Limiting a Report to Specifics [ID: 471] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Types of Reports [ID: 472] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Guidelines for Writing Reports [ID: 473] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- What to Include in Written Preliminary Reports [ID: 474] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Report Structure [ID: 475] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Writing Reports Clearly [ID: 476] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Considering Writing Style [ID: 477] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Including Signposts [ID: 478] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Designing the Layout and Presentation of Reports [ID: 479] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Providing Supporting Material [ID: 480] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Formatting Consistently [ID: 481] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Explaining Examination and Data Collection Methods [ID: 482] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Including Calculations [ID: 483] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Providing for Uncertainty and Error Analysis [ID: 484] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Explaining Results and Conclusions [ID: 485] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Providing References [ID: 486] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Including Appendixes [ID: 487] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Generating Report Findings with Forensics Software Tools [ID: 488] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Using Autopsy to Generate Reports [ID: 489] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 490] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 491] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 492] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 493] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 494] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 495] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 15. Expert Testimony in Digital Investigations [ID: 496] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 497] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Preparing for Testimony [ID: 498] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Documenting and Preparing Evidence [ID: 499] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Reviewing Your Role as a Consulting Expert or an Expert Witness [ID: 500] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Creating and Maintaining Your CV [ID: 501] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Preparing Technical Definitions [ID: 502] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Preparing to Deal with the News Media [ID: 503] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Testifying in Court [ID: 504] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Understanding the Trial Process [ID: 505] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Providing Qualifications for Your Testimony [ID: 506] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- General Guidelines on Testifying [ID: 507] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Using Graphics During Testimony [ID: 508] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Avoiding Testimony Problems [ID: 509] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Understanding Prosecutorial Misconduct [ID: 510] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Testifying During Direct Examination [ID: 511] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Testifying During Cross-Examination [ID: 512] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Preparing for a Deposition or Hearing [ID: 513] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Guidelines for Testifying at Depositions [ID: 514] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Recognizing Deposition Problems [ID: 515] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Guidelines for Testifying at Hearings [ID: 516] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Preparing Forensics Evidence for Testimony [ID: 517] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Preparing a Defense of Your Evidence-Collection Methods [ID: 518] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 519] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 520] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 521] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 522] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 523] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 524] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Chapter 16. Ethics for the Expert Witness [ID: 525] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Introduction [ID: 526] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Applying Ethics and Codes to Expert Witnesses [ID: 527] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Forensics Examiners’ Roles in Testifying [ID: 528] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Considerations in Disqualification [ID: 529] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Traps for Unwary Experts [ID: 530] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Determining Admissibility of Evidence [ID: 531] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Organizations with Codes of Ethics [ID: 532] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- International Society of Forensic Computer Examiners [ID: 533] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- International High Technology Crime Investigation Association [ID: 534] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- International Association of Computer Investigative Specialists [ID: 535] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- American Bar Association [ID: 536] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- American Psychological Association [ID: 537] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Ethical Difficulties in Expert Testimony [ID: 538] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Ethical Responsibilities Owed to You [ID: 539] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Standard Forensics Tools and Tools You Create [ID: 540] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- An Ethics Exercise [ID: 541] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Performing a Cursory Exam of a Forensic Image [ID: 542] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Performing a Detailed Exam of a Forensic Image [ID: 543] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Performing the Exam [ID: 544] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Preparing for an Examination [ID: 545] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Interpreting Attribute 0x80 Data Runs [ID: 546] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Finding Attribute 0x80 an MFT Record [ID: 547] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Configuring Data Interpreter Options in WinHex [ID: 548] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Calculating Data Runs [ID: 549] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Carving Data Run Clusters Manually [ID: 550] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter Review [ID: 551] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Summary [ID: 552] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Key Terms [ID: 553] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Review Questions [ID: 554] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Hands-On Projects [ID: 555] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Case Projects [ID: 556] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Lab Manual for Guide to Computer Forensics and Investigations [ID: 557] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Introduction [ID: 558] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: 559] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 560] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 1.1. Installing Autopsy for Windows [ID: 561] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 562] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 563] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 564] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 565] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 1.2. Downloading FTK Imager Lite [ID: 566] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 567] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 568] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 569] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 570] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 1.3. Downloading WinHex [ID: 571] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 572] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 573] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 574] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 575] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 1.4. Using Autopsy for Windows [ID: 576] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 577] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 578] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 579] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 580] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 2. The Investigator’s Office and Laboratory [ID: 581] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 582] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 2.1. Wiping a USB Drive Securely [ID: 583] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 584] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 585] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 586] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 587] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 2.2. Using Directory Snoop to Image a USB Drive [ID: 588] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 589] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 590] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 591] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 592] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 2.3. Converting a Raw Image to an .E01 Image [ID: 593] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 594] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 595] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 596] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 597] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 2.4. Imaging Evidence with FTK Imager Lite [ID: 598] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 599] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 600] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 601] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 602] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 2.5. Viewing Images in FTK Imager Lite [ID: 603] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 604] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 605] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 606] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 607] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 3. Data Acquisition [ID: 608] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 609] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive [ID: 610] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 611] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 612] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 613] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Creating a DEFT Zero Boot CD [ID: 614] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Creating a Bootable USB DEFT Zero Drive [ID: 615] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Learning DEFT Zero Features [ID: 616] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 617] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 3.2. Examining a FAT Image [ID: 618] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 619] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 620] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 621] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 622] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 3.3. Examining an NTFS Image [ID: 623] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 624] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 625] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 626] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 627] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 3.4. Examining an HFS+ Image [ID: 628] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 629] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 630] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 631] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 632] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 4. Processing Crime and Incident Scenes [ID: 633] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 634] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 4.1. Creating a Mini-WinFE Boot CD [ID: 635] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 636] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 637] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 638] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Setting Up Mini-WinFE [ID: 639] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Creating a Mini-WinFE ISO Image [ID: 640] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 641] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 4.2. Using Mini-WinFE to Boot and Image a Windows Computer [ID: 642] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 643] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 644] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 645] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 646] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 4.3. Testing the Mini-WinFE Write-Protection Feature [ID: 647] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 648] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 649] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 650] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 651] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 4.4. Creating an Image with Guymager [ID: 652] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 653] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 654] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 655] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 656] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 5. Working with Windows and CLI Systems [ID: 657] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 658] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 5.1. Using DART to Export Windows Registry Files [ID: 659] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 660] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 661] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 662] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 663] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 5.2. Examining the SAM Hive [ID: 664] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 665] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 666] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 667] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 668] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 5.3. Examining the SYSTEM Hive [ID: 669] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 670] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 671] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 672] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 673] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 5.4. Examining the ntuser.dat Registry File [ID: 674] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 675] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 676] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 677] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 678] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 6. Current Digital Forensics Tools [ID: 679] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 680] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 6.1. Using Autopsy 4.7.0 to Search an Image File [ID: 681] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 682] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 683] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 684] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Installing Autopsy 4.7.0 [ID: 685] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Searching E-mail in Autopsy 4.7.0 [ID: 686] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 687] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 6.2. Using OSForensics to Search an Image of a Hard Drive [ID: 688] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 689] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 690] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 691] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 692] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex [ID: 693] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 694] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 695] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 696] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Testing an Image File in Autopsy 4.3.0 [ID: 697] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Examining Image Files in WinHex [ID: 698] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 699] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 7. Linux and Macintosh File Systems [ID: 700] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 701] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 7.1. Using Autopsy to Process a Mac OS X Image [ID: 702] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 703] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 704] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 705] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 706] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 7.2. Using Autopsy to Process a Mac OS 9 Image [ID: 707] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 708] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 709] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 710] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 711] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 7.3. Using Autopsy to Process a Linux Image [ID: 712] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 713] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 714] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 715] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 716] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 8. Recovering Graphics Files [ID: 717] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 718] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 8.1. Using Autopsy to Analyze Multimedia Files [ID: 719] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 720] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 721] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 722] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 723] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 8.2. Using OSForensics to Analyze Multimedia Files [ID: 724] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 725] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 726] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 727] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 728] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 8.3. Using WinHex to Analyze Multimedia Files [ID: 729] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 730] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 731] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 732] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 733] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 9. Digital Forensics Analysis and Validation [ID: 734] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 735] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 9.1. Using Autopsy to Search for Keywords in an Image [ID: 736] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 737] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 738] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 739] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 740] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 9.2. Validating File Hash Values with FTK Imager Lite [ID: 741] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 742] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 743] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 744] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 745] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 9.3. Validating File Hash Values with WinHex [ID: 746] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 747] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required: [ID: 748] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 749] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 750] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: 751] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 752] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine [ID: 753] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 754] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 755] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 756] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Installing MD5 Hashes in Autopsy [ID: 757] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Analyzing a Windows Image Containing a Virtual Machine [ID: 758] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 759] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 10.2. Conducting a Live Acquisition [ID: 760] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 761] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 762] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 763] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Installing Tools for Live Acquisitions [ID: 764] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Exploring Tools for Live Acquisitions [ID: 765] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Capturing Data in a Live Acquisition [ID: 766] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 767] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 10.3. Using Kali Linux for Network Forensics [ID: 768] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 769] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 770] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 771] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Installing Kali Linux [ID: 772] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Mounting Drives in Kali Linux [ID: 773] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Identifying Open Ports and Making a Screen Capture [ID: 774] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 775] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 11. E-mail and Social Media Investigations [ID: 776] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 777] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 11.1. Using OSForensics to Search for E-mails and Mailboxes [ID: 778] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 779] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 780] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 781] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 782] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 11.2. Using Autopsy to Search for E-mails and Mailboxes [ID: 783] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 784] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 785] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 786] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 787] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 11.3. Finding Google Searches and Multiple E-mail Accounts [ID: 788] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 789] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 790] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 791] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 792] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 12. Mobile Device Forensics [ID: 793] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 794] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 12.1. Examining Cell Phone Storage Devices [ID: 795] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 796] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 797] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 798] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 799] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 12.2. Using FTK Imager Lite to View Text Messages, Phone Numbers, and Photos [ID: 800] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 801] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 802] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 803] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 804] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 12.3. Using Autopsy to Search Cloud Backups of Mobile Devices [ID: 805] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 806] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 807] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 808] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 809] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 13. Cloud Forensics [ID: 810] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 811] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 13.1. Examining Dropbox Cloud Storage [ID: 812] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 813] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 814] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 815] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 816] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 13.2. Examining Google Drive Cloud Storage [ID: 817] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 818] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 819] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 820] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 821] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 13.3. Examining OneDrive Cloud Storage [ID: 822] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 823] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 824] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 825] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 826] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 14. Report Writing for High-Tech Investigations [ID: 827] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 828] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 14.1. Investigating Corporate Espionage [ID: 829] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 830] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 831] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 832] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 833] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 14.2. Adding Evidence to a Case [ID: 834] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 835] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 836] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 837] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 838] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 14.3. Preparing a Report [ID: 839] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 840] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 841] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 842] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 843] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 15. Expert Testimony in Digital Investigations [ID: 844] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 845] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 15.1. Conducting a Preliminary Investigation [ID: 846] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 847] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 848] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 849] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 850] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 15.2. Investigating an Arsonist [ID: 851] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 852] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 853] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 854] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 855] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 15.3. Recovering a Password from Password-Protected Files [ID: 856] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 857] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 858] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 859] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Verifying the Existence of a Warning Banner [ID: 860] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Recovering a Password from Password-Protected Files [ID: 861] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 862] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |-- Chapter 16. Ethics for the Expert Witness [ID: 863] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Chapter Introduction [ID: 864] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |-- Lab 16.1. Rebuilding an MFT Record from a Corrupt Image [ID: 865] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Objectives [ID: 866] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Materials Required [ID: 867] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Activity [ID: 868] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Creating a Duplicate Forensic Image [ID: 869] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Determining the Offset Byte Address of the Corrupt MFT Record [ID: 870] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Copying the Corrected MFT Record [ID: 871] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |  |-- Extracting Additional Evidence [ID: 872] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |  |  |  |-- Review Questions [ID: 873] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Appendix A. Certification Test References [ID: 874] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Appendix B. Digital Forensics References [ID: 875] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Appendix C. Digital Forensics Lab Considerations [ID: 876] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "|  |-- Appendix D. Legacy File System and Forensics Tools [ID: 877] (Total Chunks: 0, Direct Chunks: 0, Images: 0)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Diagnostic Summary                               \n",
      "--------------------------------------------------------------------------------\n",
      "Total Chunks in DB: 2666\n",
      "Total Unique Images in DB: 0\n",
      "\n",
      "================================================================================\n",
      "                              Diagnostic Complete                               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: Full Database Health & Hierarchy Diagnostic Report (V15 - Final)\n",
    "\n",
    "# --- Core Imports ---\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Dict, Any, Set\n",
    "\n",
    "# --- Dependency Checks ---\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def count_total_chunks(node: Dict) -> int:\n",
    "    \"\"\"Recursively counts all chunks in a node and its children from the '_chunks' key.\"\"\"\n",
    "    total = node.get('_chunks', 0)\n",
    "    for child_node in node.get('children', []):\n",
    "        total += count_total_chunks(child_node)\n",
    "    return total\n",
    "\n",
    "def _get_all_image_paths_recursive(node: Dict) -> Set[str]:\n",
    "    \"\"\"Internal helper to recursively gather a set of unique image paths.\"\"\"\n",
    "    image_set = node.get('_image_paths', set()).copy()\n",
    "    for child_node in node.get('children', []):\n",
    "        image_set.update(_get_all_image_paths_recursive(child_node))\n",
    "    return image_set\n",
    "\n",
    "def count_total_images(node: Dict) -> int:\n",
    "    \"\"\"Correctly counts unique images in a branch by calling the recursive helper.\"\"\"\n",
    "    return len(_get_all_image_paths_recursive(node))\n",
    "\n",
    "def print_hierarchy_report(nodes: List[Dict], indent_level: int = 0):\n",
    "    \"\"\"Recursively prints the annotated hierarchy.\"\"\"\n",
    "    for node in nodes:\n",
    "        prefix = \"|  \" * indent_level + \"|-- \"\n",
    "        title = node.get('title', 'Untitled')\n",
    "        toc_id = node.get('toc_id', 'N/A')\n",
    "        \n",
    "        direct_chunks = node.get('_chunks', 0)\n",
    "        total_chunks_in_branch = count_total_chunks(node)\n",
    "        total_images_in_branch = count_total_images(node)\n",
    "        \n",
    "        print(f\"{prefix}{title} [ID: {toc_id}] (Total Chunks: {total_chunks_in_branch}, Direct Chunks: {direct_chunks}, Images: {total_images_in_branch})\")\n",
    "        \n",
    "        if node.get('children'):\n",
    "            print_hierarchy_report(node.get('children', []), indent_level + 1)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN DIAGNOSTIC FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def run_full_diagnostics():\n",
    "    \"\"\"\n",
    "    Runs a full diagnostic by loading the original ToC (with IDs) and annotating \n",
    "    it with live data from the ChromaDB to verify integrity.\n",
    "    \"\"\"\n",
    "    print_header(\"Full Database Health & Hierarchy Diagnostic Report\")\n",
    "\n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain components not installed. Skipping diagnostics.\")\n",
    "        return\n",
    "    \n",
    "    required_files = {\n",
    "        \"Chroma DB\": CHROMA_PERSIST_DIR,\n",
    "        \"ToC JSON with IDs\": PRE_EXTRACTED_TOC_JSON_PATH\n",
    "    }\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"FATAL: Required '{name}' not found at '{path}'. Please run previous cells.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        logger.info(\"Successfully loaded the pre-extracted Table of Contents file (with IDs).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FATAL: Could not load or parse ToC JSON file: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"Connecting to the vector database...\")\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    total_docs = vector_store._collection.count()\n",
    "    if total_docs == 0:\n",
    "        logger.warning(\"Database is empty. No diagnostics to run.\")\n",
    "        return\n",
    "    logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "    metadatas = vector_store.get(limit=total_docs, include=[\"metadatas\"])['metadatas']\n",
    "    logger.info(\"Successfully retrieved all metadata.\")\n",
    "\n",
    "    logger.info(\"Aggregating chunk and image data from the database by ToC ID...\")\n",
    "    db_data_by_toc_id = {}\n",
    "    for meta in metadatas:\n",
    "        toc_id = meta.get('toc_id')\n",
    "        if toc_id is None: toc_id = -1 \n",
    "\n",
    "        if toc_id not in db_data_by_toc_id:\n",
    "            db_data_by_toc_id[toc_id] = {'chunk_count': 0, 'image_paths': set()}\n",
    "        \n",
    "        db_data_by_toc_id[toc_id]['chunk_count'] += 1\n",
    "        if 'image_paths' in meta and meta['image_paths']:\n",
    "            try:\n",
    "                image_paths_list = json.loads(meta['image_paths'])\n",
    "                if isinstance(image_paths_list, list):\n",
    "                    db_data_by_toc_id[toc_id]['image_paths'].update(image_paths_list)\n",
    "            except (json.JSONDecodeError, TypeError): pass\n",
    "    \n",
    "    logger.info(\"Annotating the original ToC with aggregated database data...\")\n",
    "    # Add a root node for the preamble for consistent printing\n",
    "    annotated_tree_root = [{'title': 'EPUB Preamble', 'toc_id': -1, 'children': hierarchical_toc}]\n",
    "    \n",
    "    def annotate_tree_with_db_data(nodes: List[Dict]):\n",
    "        for node in nodes:\n",
    "            toc_id = node.get('toc_id')\n",
    "            db_data = db_data_by_toc_id.get(toc_id, {'chunk_count': 0, 'image_paths': set()})\n",
    "            node['_chunks'] = db_data['chunk_count']\n",
    "            node['_image_paths'] = db_data['image_paths']\n",
    "            \n",
    "            if node.get('children'):\n",
    "                annotate_tree_with_db_data(node.get('children'))\n",
    "\n",
    "    annotate_tree_with_db_data(annotated_tree_root)\n",
    "    logger.info(\"Annotation complete.\")\n",
    "    \n",
    "    print_header(\"Reconstructed Hierarchy Report (Book Order)\", char=\"-\")\n",
    "    print_hierarchy_report(annotated_tree_root)\n",
    "\n",
    "    print_header(\"Diagnostic Summary\", char=\"-\")\n",
    "    print(f\"Total Chunks in DB: {total_docs}\")\n",
    "    print(f\"Total Unique Images in DB: {count_total_images(annotated_tree_root[0])}\")\n",
    "\n",
    "    chunks_without_id = db_data_by_toc_id.get(-1, {}).get('chunk_count', 0)\n",
    "    if chunks_without_id > 0:\n",
    "        logger.warning(f\"Found {chunks_without_id} chunks MISSING a valid 'toc_id'. These are in the 'Preamble' section.\")\n",
    "    else:\n",
    "        logger.info(\"All chunks contain valid 'toc_id' metadata.\")\n",
    "\n",
    "    print_header(\"Diagnostic Complete\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK FOR THIS CELL\n",
    "# ==============================================================================\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and langchain_available:\n",
    "    run_full_diagnostics()\n",
    "else:\n",
    "    logger.error(\"Skipping diagnostics: Global variables not defined or LangChain not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d263ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:30,756 - INFO - Connecting to the existing vector database at '/home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub'...\n",
      "2025-07-05 20:39:30,773 - WARNING - No chunks found in the database for toc_id = 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICATION FAILED: No content found for toc_id: 24\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Content Retrieval for a Specific toc_id (Adapted for New Metadata)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_and_print_chunks_for_toc_id(vector_store: Chroma, toc_id: int):\n",
    "    \"\"\"\n",
    "    Retrieves all chunks for a specific toc_id, prints the associated section title,\n",
    "    shows the reassembled text, and then lists the metadata for each individual chunk\n",
    "    for detailed verification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the 'get' method with a 'where' filter to find all chunks for the toc_id\n",
    "        results = vector_store.get(\n",
    "            where={\"toc_id\": toc_id},\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "\n",
    "        if not results or not results.get('ids'):\n",
    "            logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"VERIFICATION FAILED: No content found for toc_id: {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            return\n",
    "\n",
    "        documents = results['documents']\n",
    "        metadatas = results['metadatas']\n",
    "        \n",
    "        # ========================= MODIFICATION START =========================\n",
    "        # Get the section title by reconstructing it from the hierarchical level_X_title keys\n",
    "        first_meta = metadatas[0] if metadatas else {}\n",
    "        \n",
    "        # Find the highest level_X_title key to use as the primary title\n",
    "        level_keys = sorted([k for k in first_meta if re.match(r'level_\\d+_title', k)])\n",
    "        if level_keys:\n",
    "            # Reconstruct the full path for context, use the last one as main title\n",
    "            highest_level_key = level_keys[-1]\n",
    "            section_title = first_meta.get(highest_level_key, 'Unknown Section Title')\n",
    "            \n",
    "            # (Optional but helpful) Show the full breadcrumb path\n",
    "            breadcrumb_path = \" -> \".join([first_meta.get(k, '') for k in level_keys])\n",
    "            header_title = f\"'{section_title}' (Path: {breadcrumb_path})\"\n",
    "        else:\n",
    "            section_title = 'Unknown or Preamble Section'\n",
    "            header_title = section_title\n",
    "        # ========================== MODIFICATION END ==========================\n",
    "        \n",
    "        # --- Print a clear header with the section title ---\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"VERIFYING SECTION: {header_title} (toc_id: {toc_id})\")\n",
    "        print(\"=\" * 80)\n",
    "        logger.info(f\"Found {len(documents)} chunks in the database for this section.\")\n",
    "        \n",
    "        # Sort chunks by their chunk_id to ensure they are in the correct order for reassembly\n",
    "        sorted_items = sorted(zip(documents, metadatas), key=lambda item: item[1].get('chunk_id', 0))\n",
    "\n",
    "        # --- Reassemble and print the full text for the section ---\n",
    "        all_chunk_texts = [item[0] for item in sorted_items]\n",
    "        # A simple join is better for reassembly than adding newlines\n",
    "        reassembled_text = \" \".join(all_chunk_texts)\n",
    "        \n",
    "        print(\"\\n\" + \"#\" * 28 + \" Reassembled Text \" + \"#\" * 28)\n",
    "        print(reassembled_text)\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        # --- Print individual chunk details for in-depth verification ---\n",
    "        print(\"\\n\" + \"-\" * 24 + \" Retrieved Chunk Details \" + \"-\" * 25)\n",
    "        for i, (doc_meta_tuple) in enumerate(sorted_items):\n",
    "            doc_content, meta = doc_meta_tuple\n",
    "            print(f\"\\n[ Chunk {i+1} of {len(documents)} | chunk_id: {meta.get('chunk_id', 'N/A')} ]\")\n",
    "            # Show a preview of the content to keep the output manageable\n",
    "            content_preview = doc_content.replace('\\n', ' ').strip()\n",
    "            print(f\"  Content Preview: '{content_preview[:250]}...'\")\n",
    "            print(f\"  Metadata: {json.dumps(meta, indent=2)}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Verification complete for section '{section_title}'.\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during retrieval for toc_id {toc_id}: {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- IMPORTANT: Set the ID of the section you want to test here ---\n",
    "# To find a toc_id, you can look at the output of the ToC extraction cell (Cell 4)\n",
    "# or the JSON file it creates.\n",
    "TOC_ID_TO_TEST = 24 # Example: \"An Overview of Digital Forensics\"\n",
    "# TOC_ID_TO_TEST = -1 # You can also test the \"Preamble\" content\n",
    "\n",
    "\n",
    "# Assume these variables are defined in a previous cell\n",
    "# CHROMA_PERSIST_DIR, EMBEDDING_MODEL_OLLAMA, CHROMA_COLLECTION_NAME\n",
    "\n",
    "# Check if the database directory exists before attempting to connect\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and os.path.exists(CHROMA_PERSIST_DIR):\n",
    "    logger.info(f\"Connecting to the existing vector database at '{CHROMA_PERSIST_DIR}'...\")\n",
    "    \n",
    "    # Ensure OllamaEmbeddings and Chroma are initialized correctly\n",
    "    try:\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        # Run the verification function\n",
    "        retrieve_and_print_chunks_for_toc_id(vector_store, TOC_ID_TO_TEST)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Chroma or run retrieval. Error: {e}\")\n",
    "        logger.error(\"Please ensure your embedding model and collection names are correct.\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"Database directory not found or 'CHROMA_PERSIST_DIR' variable is not set.\")\n",
    "    logger.error(\"Please run the previous cell (e.g., Cell 5) to create the database first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f02607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:30,782 - INFO - Connecting to the existing vector database...\n",
      "2025-07-05 20:39:30,795 - WARNING - No chunks found in the database for toc_id = 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Retrieving all chunks for toc_id: 7\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Content Retrieval for a Specific toc_id with Reassembled Text\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_and_print_chunks_for_toc_id(vector_store: Chroma, toc_id: int):\n",
    "    \"\"\"\n",
    "    Retrieves all chunks for a specific toc_id, prints the reassembled text,\n",
    "    and then lists the metadata for each individual chunk.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Retrieving all chunks for toc_id: {toc_id}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        # Use the 'get' method with a 'where' filter to find exact matches\n",
    "        results = vector_store.get(\n",
    "            where={\"toc_id\": toc_id},\n",
    "            include=[\"documents\", \"metadatas\"] \n",
    "        )\n",
    "\n",
    "        if not results or not results.get('ids'):\n",
    "            logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "            return\n",
    "\n",
    "        documents = results['documents']\n",
    "        metadatas = results['metadatas']\n",
    "        \n",
    "        logger.info(f\"Successfully retrieved {len(documents)} chunks for toc_id = {toc_id}.\")\n",
    "        \n",
    "        # Sort chunks by their chunk_id to ensure they are in the correct order\n",
    "        sorted_items = sorted(zip(documents, metadatas), key=lambda item: item[1].get('chunk_id', 0))\n",
    "\n",
    "        # --- NEW: Reassemble and print the full text ---\n",
    "        all_chunk_texts = [item[0] for item in sorted_items]\n",
    "        reassembled_text = \"\\n\".join(all_chunk_texts)\n",
    "        \n",
    "        print(\"\\n\" + \"#\" * 28 + \" Reassembled Text \" + \"#\" * 28)\n",
    "        print(reassembled_text)\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        # --- Print individual chunk details for verification ---\n",
    "        print(\"\\n\" + \"-\" * 25 + \" Individual Chunk Details \" + \"-\" * 24)\n",
    "        for i, (doc, meta) in enumerate(sorted_items):\n",
    "            print(f\"\\n[ Chunk {i+1} / {len(documents)} | chunk_id: {meta.get('chunk_id', 'N/A')} ]\")\n",
    "            # Show a preview to keep the log clean\n",
    "            content_preview = doc.replace('\\n', ' ').strip()\n",
    "            print(f\"  Content Preview: '{content_preview[:200]}...'\") \n",
    "            print(f\"  Metadata: {json.dumps(meta, indent=2)}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Retrieval test complete.\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during retrieval: {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- IMPORTANT: Set the ID you want to test here ---\n",
    "# Example: ToC ID 10 is \"An Overview of Digital Forensics\"\n",
    "# Example: ToC ID 11 is \"Digital Forensics and Other Related Disciplines\"\n",
    "TOC_ID_TO_TEST = 7\n",
    "\n",
    "# Check if the database directory exists\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and os.path.exists(CHROMA_PERSIST_DIR):\n",
    "    logger.info(\"Connecting to the existing vector database...\")\n",
    "    \n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    retrieve_and_print_chunks_for_toc_id(vector_store, TOC_ID_TO_TEST)\n",
    "    \n",
    "else:\n",
    "    logger.error(\"Database directory not found. Please run Cell 5 to create the database first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7e58a",
   "metadata": {},
   "source": [
    "# test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f721516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:31,053 - INFO - Connecting to the existing vector database...\n",
      "2025-07-05 20:39:31,065 - INFO - Loading and processing Table of Contents for test case selection...\n",
      "2025-07-05 20:39:31,067 - INFO - Found topic 'Lab Budget Planning' with toc_id: 60\n",
      "2025-07-05 20:39:31,068 - WARNING - No document chunks found for toc_id 60. The topic might be a parent heading with no direct content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                Retrieval Test for Topic: 'Lab Budget Planning'                 \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.2: Test Content & Image Retrieval (with Random Topic Selection)\n",
    "\n",
    "# --- Core Imports ---\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random # Make sure random is imported\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Dependency Checks & Imports ---\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    langchain_and_viz_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"Required library not found: {e}. Please install langchain, ChromaDB, Pillow, and matplotlib.\")\n",
    "    langchain_and_viz_available = False\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER AND RETRIEVAL FUNCTIONS\n",
    "# ==============================================================================\n",
    "# The _add_ids_and_flatten_recursive function is needed here to process the ToC\n",
    "def _add_ids_and_flatten_recursive(nodes: List[Dict], current_path: List[str], counter: List[int], flat_list: List[Dict]):\n",
    "    \"\"\"Recursively traverses ToC nodes to flatten them and assign a unique, sequential toc_id.\"\"\"\n",
    "    for node in nodes:\n",
    "        toc_id = counter[0]\n",
    "        counter[0] += 1\n",
    "        title = node.get(\"title\", \"\").strip()\n",
    "        if not title:\n",
    "            continue\n",
    "        \n",
    "        # Check if the node is a leaf (has no children)\n",
    "        is_leaf = not bool(node.get(\"children\"))\n",
    "            \n",
    "        new_titles_path = current_path + [title]\n",
    "        entry = {\n",
    "            \"titles_path\": new_titles_path,\n",
    "            \"level\": node.get(\"level\"),\n",
    "            \"full_title_for_matching\": title,\n",
    "            \"toc_id\": toc_id,\n",
    "            \"is_leaf\": is_leaf # Add a flag to identify leaf nodes\n",
    "        }\n",
    "        if \"page\" in node:\n",
    "            entry[\"page\"] = node[\"page\"]\n",
    "            \n",
    "        flat_list.append(entry)\n",
    "        \n",
    "        if node.get(\"children\"):\n",
    "            _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter, flat_list)\n",
    "\n",
    "\n",
    "# The retrieve_and_display_section and print_header functions remain exactly the same as before.\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def retrieve_and_display_section(\n",
    "    topic_query: str, \n",
    "    vector_store: Chroma, \n",
    "    flat_toc: List[Dict]\n",
    "):\n",
    "    # ... This entire function is identical to the previous version ...\n",
    "    # It takes the query and does the retrieval and display.\n",
    "    print_header(f\"Retrieval Test for Topic: '{topic_query}'\")\n",
    "\n",
    "    # --- 1. Find the topic in the flattened ToC ---\n",
    "    target_entry = None\n",
    "    # Find an exact or partial match for the topic query\n",
    "    for entry in flat_toc:\n",
    "        if topic_query.lower() in entry.get('full_title_for_matching', '').lower():\n",
    "            target_entry = entry\n",
    "            break\n",
    "    \n",
    "    if not target_entry:\n",
    "        logger.error(f\"Could not find topic '{topic_query}' in the Table of Contents.\")\n",
    "        return\n",
    "\n",
    "    target_toc_id = target_entry.get('toc_id')\n",
    "    full_title = target_entry.get('full_title_for_matching')\n",
    "    logger.info(f\"Found topic '{full_title}' with toc_id: {target_toc_id}\")\n",
    "\n",
    "    # --- 2. Retrieve all documents for that toc_id from ChromaDB ---\n",
    "    try:\n",
    "        retrieved_data = vector_store.get(\n",
    "            where={\"toc_id\": target_toc_id},\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "        docs = [\n",
    "            Document(page_content=doc, metadata=meta) \n",
    "            for doc, meta in zip(retrieved_data['documents'], retrieved_data['metadatas'])\n",
    "        ]\n",
    "        \n",
    "        if not docs:\n",
    "            logger.warning(f\"No document chunks found for toc_id {target_toc_id}. The topic might be a parent heading with no direct content.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Retrieved {len(docs)} document chunks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during database retrieval: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 3. Sort chunks, reassemble text, and collect images ---\n",
    "    docs.sort(key=lambda d: d.metadata.get('chunk_id', -1))\n",
    "    \n",
    "    full_content = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    all_image_paths = set()\n",
    "    for d in docs:\n",
    "        if 'image_paths' in d.metadata:\n",
    "            try:\n",
    "                paths = json.loads(d.metadata['image_paths'])\n",
    "                if isinstance(paths, list):\n",
    "                    all_image_paths.update(paths)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue\n",
    "    \n",
    "    sorted_image_paths = sorted(list(all_image_paths))\n",
    "\n",
    "    # --- 4. Display the results ---\n",
    "    print(\"\\n\" + \"-\"*25 + \" REASSEMBLED CONTENT \" + \"-\"*25)\n",
    "    print(full_content)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*25 + \" ASSOCIATED IMAGES \" + \"-\"*26)\n",
    "    if not sorted_image_paths:\n",
    "        print(\"No images found for this section.\")\n",
    "    else:\n",
    "        print(f\"Found {len(sorted_image_paths)} unique image(s):\")\n",
    "        for path in sorted_image_paths:\n",
    "            print(f\"- {path}\")\n",
    "        \n",
    "        try:\n",
    "            first_image_path = sorted_image_paths[0]\n",
    "            print(f\"\\nDisplaying first image: {os.path.basename(first_image_path)}\")\n",
    "            \n",
    "            img = Image.open(first_image_path)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image for '{full_title}'\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Image file not found at path: {first_image_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not display image. Error: {e}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN EXECUTION BLOCK FOR THIS CELL (with Random Topic Selection)\n",
    "# ==============================================================================\n",
    "\n",
    "if langchain_and_viz_available:\n",
    "    if 'CHROMA_PERSIST_DIR' in locals() and 'PRE_EXTRACTED_TOC_JSON_PATH' in locals():\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Connecting to the existing vector database...\")\n",
    "            db_retriever = Chroma(\n",
    "                persist_directory=CHROMA_PERSIST_DIR,\n",
    "                embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "                collection_name=CHROMA_COLLECTION_NAME\n",
    "            )\n",
    "\n",
    "            logger.info(\"Loading and processing Table of Contents for test case selection...\")\n",
    "            with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "                loaded_hierarchical_toc = json.load(f)\n",
    "            \n",
    "            flat_toc_for_lookup = []\n",
    "            _add_ids_and_flatten_recursive(loaded_hierarchical_toc, [], [1], flat_toc_for_lookup)\n",
    "            \n",
    "            # --- RANDOMLY SELECT A TEST QUERY ---\n",
    "            # We want to test a \"leaf\" section that has actual content.\n",
    "            # A good candidate is a section that is a leaf in the ToC tree.\n",
    "            test_candidates = [\n",
    "                entry for entry in flat_toc_for_lookup \n",
    "                if entry.get('is_leaf') and entry.get(\"level\", 0) > 0\n",
    "            ]\n",
    "\n",
    "            if not test_candidates:\n",
    "                raise ValueError(\"Could not find any suitable leaf-node topics to test.\")\n",
    "            \n",
    "            # Select a random topic from our list of good candidates\n",
    "            random_topic_entry = random.choice(test_candidates)\n",
    "            test_query = random_topic_entry['full_title_for_matching']\n",
    "            \n",
    "            # --- RUN THE TEST with the random query ---\n",
    "            retrieve_and_display_section(\n",
    "                topic_query=test_query,\n",
    "                vector_store=db_retriever,\n",
    "                flat_toc=flat_toc_for_lookup\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during the test execution: {e}\", exc_info=True)\n",
    "\n",
    "    else:\n",
    "        logger.error(\"Required variables (CHROMA_PERSIST_DIR, PRE_EXTRACTED_TOC_JSON_PATH) not found. Please run previous cells.\")\n",
    "else:\n",
    "    logger.error(\"Skipping test cell due to missing libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "Require Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf3ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:31,085 - INFO - Connecting to DB and initializing components...\n",
      "2025-07-05 20:39:31,095 - INFO - Goal: Confirm the database is live and contains thematically relevant content.\n",
      "2025-07-05 20:39:31,096 - INFO - Strategy: Perform a simple similarity search using the course's 'unitName'.\n",
      "2025-07-05 20:39:31,096 - INFO - Action: Searching for query: 'Digital Forensic'...\n",
      "2025-07-05 20:39:31,136 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-05 20:39:31,138 - INFO - Verification: Check if at least one document was returned.\n",
      "2025-07-05 20:39:31,139 - INFO - ✅ Result: TEST 1 PASSED. The database is online and responsive.\n",
      "2025-07-05 20:39:31,139 - INFO - Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\n",
      "2025-07-05 20:39:31,139 - INFO - Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\n",
      "2025-07-05 20:39:31,140 - INFO -   - Selected random deep section: Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Understanding Bit-stream Copies\n",
      "2025-07-05 20:39:31,140 - INFO - Action: Performing a similarity search with a highly specific '$and' filter.\n",
      "2025-07-05 20:39:31,168 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-05 20:39:31,171 - INFO - Verification: Check if the precisely filtered query returned any documents.\n",
      "2025-07-05 20:39:31,171 - ERROR - ❌ Result: TEST 2 FAILED. Reason: Deeply filtered query returned no results.\n",
      "2025-07-05 20:39:31,171 - INFO - Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\n",
      "2025-07-05 20:39:31,172 - INFO - Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\n",
      "2025-07-05 20:39:31,172 - INFO -   - Selected random week: Week Week 8 - 'Digital Forensics Analysis and Validation. Virtual Machine Forensics, Live Acquisitions and Cloud Forensics.'\n",
      "2025-07-05 20:39:31,172 - INFO -   - Extracted required chapter number(s): ['2019', '978', '1', '337', '56894', '4', '9', '10']\n",
      "2025-07-05 20:39:31,175 - INFO -   - Mapped to top-level ToC entries: ['Chapter 9. Digital Forensics Analysis and Validation', 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics', 'Chapter 4. Processing Crime and Incident Scenes', 'Chapter 1. Understanding the Digital Forensics Profession and Investigations']\n",
      "2025-07-05 20:39:31,176 - INFO - Action: Searching for the weekly topic, filtered by the mapped chapter(s).\n",
      "2025-07-05 20:39:31,194 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-05 20:39:31,196 - INFO - Verification: Check if at least one returned document is from the correct chapter.\n",
      "2025-07-05 20:39:31,197 - ERROR - ❌ Result: TEST 3 FAILED. Reason: Alignment query returned no results for the correct section/chapter.\n",
      "2025-07-05 20:39:31,197 - INFO - Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\n",
      "2025-07-05 20:39:31,197 - INFO - Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\n",
      "2025-07-05 20:39:31,198 - INFO - Action: Performing similarity search for topic: 'Report Writing for High-Tech Investigations. Expert Testimony in Digital Forensic Investigations.' to get a set of chunks.\n",
      "2025-07-05 20:39:31,217 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-05 20:39:31,219 - INFO -   - Retrieved and sorted chunk IDs: [1, 20, 67, 175, 231, 1798, 1800, 1888, 1892, 1965]\n",
      "2025-07-05 20:39:31,220 - INFO - Verification: Check if the sorted list of chunk_ids is strictly increasing.\n",
      "2025-07-05 20:39:31,220 - INFO - ✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         Database Verification Process                          \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                            Test 1: Basic Retrieval                             \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Digital Forensic'\n",
      "--> Found 1 results. Displaying top 1:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'techniques. You learn about the problems and challenges forensics examiners face when preparing and processing investigations, including the ideas and questions they must consider. To perform the acti...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": -1,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Preamble\",\n",
      "  \"chunk_id\": 95\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                        Test 2: Deep Hierarchy Retrieval                        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Understanding Bit-stream Copies'\n",
      "FILTER: {\n",
      "  \"$and\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_2_title\": {\n",
      "        \"$eq\": \"Conducting an Investigation\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_3_title\": {\n",
      "        \"$eq\": \"Understanding Bit-stream Copies\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> No documents were retrieved for this query and filter.\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Test 3: Advanced Unit Outline Alignment                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Digital Forensics Analysis and Validation. Virtual Machine Forensics, Live Acquisitions and Cloud Forensics.'\n",
      "FILTER: {\n",
      "  \"$or\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 9. Digital Forensics Analysis and Validation\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 4. Processing Crime and Incident Scenes\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> No documents were retrieved for this query and filter.\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Test 4: Content Sequence Verification                      \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Report Writing for High-Tech Investigations. Expert Testimony in Digital Forensic Investigations.'\n",
      "--> Found 10 results. Displaying top 3:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Chapter 14. Report Writing for High-Tech Investigations After reading this chapter and completing the exercises, you will be able to: Explain the importance of reports Describe guidelines for writing ...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": -1,\n",
      "  \"level_1_title\": \"Preamble\",\n",
      "  \"chunk_id\": 1798,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ RESULT 2 ]\n",
      "  Content : 'offers guidelines on report content, structure, and presentation; and explains how to generate report findings with forensics software tools. Chapter 15 , “Expert Testimony in Digital Investigations,”...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Preamble\",\n",
      "  \"chunk_id\": 67,\n",
      "  \"toc_id\": -1\n",
      "}\n",
      "\n",
      "[ RESULT 3 ]\n",
      "  Content : 'and techniques. As a digital investigator and forensics professional, you’re expected to maintain honesty and integrity. You must conduct yourself with the highest levels of integrity in all aspects o...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": -1,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Preamble\",\n",
      "  \"chunk_id\": 175\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                              Verification Summary                              \n",
      "================================================================================\n",
      "Total Tests Run: 4\n",
      "✅ Passed: 2\n",
      "❌ Failed: 2\n",
      "\n",
      "================================================================================\n",
      "                             Verification Complete                              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Vector Database (Final Version with Rich Diagnostic Output)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "def print_results(query_text: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Richly prints query results, showing the query, filter, and retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*10 + \" DIAGNOSTIC: RETRIEVAL RESULTS \" + \"-\"*10)\n",
    "    print(f\"QUERY: '{query_text}'\")\n",
    "    if where_filter:\n",
    "        print(f\"FILTER: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"--> No documents were retrieved for this query and filter.\")\n",
    "        print(\"-\" * 55)\n",
    "        return\n",
    "        \n",
    "    print(f\"--> Found {len(results)} results. Displaying top {min(len(results), 3)}:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n[ RESULT {i+1} ]\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"  Content : '{content_preview[:200]}...'\")\n",
    "        print(f\"  Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "\n",
    "# --- HELPER FUNCTIONS FOR FINDING DATA (UNCHANGED) ---\n",
    "def find_deep_entry(nodes: List[Dict], current_path: List[str] = []) -> Optional[Tuple[Dict, List[str]]]:\n",
    "    shuffled_nodes = random.sample(nodes, len(nodes))\n",
    "    for node in shuffled_nodes:\n",
    "        if node.get('level', 0) >= 2 and node.get('children'): return node, current_path + [node['title']]\n",
    "        if node.get('children'):\n",
    "            path = current_path + [node['title']]\n",
    "            deep_entry = find_deep_entry(node['children'], path)\n",
    "            if deep_entry: return deep_entry\n",
    "    return None\n",
    "\n",
    "def find_chapter_title_by_number(toc_data: List[Dict], chap_num: int) -> Optional[List[str]]:\n",
    "    def search_nodes(nodes, num, current_path):\n",
    "        for node in nodes:\n",
    "            path = current_path + [node['title']]\n",
    "            if re.match(rf\"(Chapter\\s)?{num}[.:\\s]\", node.get('title', ''), re.IGNORECASE): return path\n",
    "            if node.get('children'):\n",
    "                found_path = search_nodes(node['children'], num, path)\n",
    "                if found_path: return found_path\n",
    "        return None\n",
    "    return search_nodes(toc_data, chap_num, [])\n",
    "\n",
    "\n",
    "# --- ENHANCED TEST CASES with DIAGNOSTIC OUTPUT ---\n",
    "\n",
    "def basic_retrieval_test(db, outline):\n",
    "    print_header(\"Test 1: Basic Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm the database is live and contains thematically relevant content.\")\n",
    "        logger.info(\"Strategy: Perform a simple similarity search using the course's 'unitName'.\")\n",
    "        query_text = outline.get(\"unitInformation\", {}).get(\"unitName\", \"introduction\")\n",
    "        \n",
    "        logger.info(f\"Action: Searching for query: '{query_text}'...\")\n",
    "        results = db.similarity_search(query_text, k=1)\n",
    "        \n",
    "        print_results(query_text, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one document was returned.\")\n",
    "        assert len(results) > 0, \"Basic retrieval query returned no results.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 1 PASSED. The database is online and responsive.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 1 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def deep_hierarchy_test(db, toc):\n",
    "    print_header(\"Test 2: Deep Hierarchy Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\")\n",
    "        logger.info(\"Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\")\n",
    "        deep_entry_result = find_deep_entry(toc)\n",
    "        assert deep_entry_result, \"Could not find a suitable deep entry (level >= 2) to test.\"\n",
    "        node, path = deep_entry_result\n",
    "        query = node['title']\n",
    "        \n",
    "        logger.info(f\"  - Selected random deep section: {' -> '.join(path)}\")\n",
    "        conditions = [{f\"level_{i+1}_title\": {\"$eq\": title}} for i, title in enumerate(path)]\n",
    "        w_filter = {\"$and\": conditions}\n",
    "        \n",
    "        logger.info(\"Action: Performing a similarity search with a highly specific '$and' filter.\")\n",
    "        results = db.similarity_search(query, k=1, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if the precisely filtered query returned any documents.\")\n",
    "        assert len(results) > 0, \"Deeply filtered query returned no results.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 2 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def advanced_alignment_test(db, outline, toc):\n",
    "    print_header(\"Test 3: Advanced Unit Outline Alignment\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\")\n",
    "        logger.info(\"Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\")\n",
    "        week_to_test = random.choice(outline['weeklySchedule'])\n",
    "        logger.info(f\"  - Selected random week: Week {week_to_test['week']} - '{week_to_test['contentTopic']}'\")\n",
    "\n",
    "        reading = week_to_test.get('requiredReading', '')\n",
    "        chap_nums_str = re.findall(r'\\d+', reading)\n",
    "        assert chap_nums_str, f\"Could not find chapter numbers in required reading: '{reading}'\"\n",
    "        logger.info(f\"  - Extracted required chapter number(s): {chap_nums_str}\")\n",
    "\n",
    "        chapter_paths = [find_chapter_title_by_number(toc, int(n)) for n in chap_nums_str]\n",
    "        chapter_paths = [path for path in chapter_paths if path is not None]\n",
    "        assert chapter_paths, f\"Could not map chapter numbers {chap_nums_str} to a valid ToC path.\"\n",
    "        \n",
    "        level_1_titles = list(set([path[0] for path in chapter_paths]))\n",
    "        logger.info(f\"  - Mapped to top-level ToC entries: {level_1_titles}\")\n",
    "\n",
    "        or_filter = [{\"level_1_title\": {\"$eq\": title}} for title in level_1_titles]\n",
    "        w_filter = {\"$or\": or_filter} if len(or_filter) > 1 else or_filter[0]\n",
    "        query = week_to_test['contentTopic']\n",
    "        \n",
    "        logger.info(\"Action: Searching for the weekly topic, filtered by the mapped chapter(s).\")\n",
    "        results = db.similarity_search(query, k=5, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one returned document is from the correct chapter.\")\n",
    "        assert len(results) > 0, \"Alignment query returned no results for the correct section/chapter.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 3 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def content_sequence_test(db, outline):\n",
    "    print_header(\"Test 4: Content Sequence Verification\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\")\n",
    "        logger.info(\"Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\")\n",
    "        topic_query = random.choice(outline['weeklySchedule'])['contentTopic']\n",
    "        \n",
    "        logger.info(f\"Action: Performing similarity search for topic: '{topic_query}' to get a set of chunks.\")\n",
    "        results = db.similarity_search(topic_query, k=10)\n",
    "        \n",
    "        print_results(topic_query, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        docs_with_id = [doc for doc in results if 'chunk_id' in doc.metadata]\n",
    "        assert len(docs_with_id) > 3, \"Fewer than 4 retrieved chunks have a 'chunk_id' to test.\"\n",
    "        \n",
    "        chunk_ids = [doc.metadata['chunk_id'] for doc in docs_with_id]\n",
    "        sorted_ids = sorted(chunk_ids)\n",
    "        \n",
    "        logger.info(f\"  - Retrieved and sorted chunk IDs: {sorted_ids}\")\n",
    "        logger.info(\"Verification: Check if the sorted list of chunk_ids is strictly increasing.\")\n",
    "        is_ordered = all(sorted_ids[i] >= sorted_ids[i-1] for i in range(1, len(sorted_ids)))\n",
    "        assert is_ordered, \"The retrieved chunks' chunk_ids are not in ascending order when sorted.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 4 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    \n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain libraries not found. Aborting tests.\")\n",
    "        return\n",
    "\n",
    "    required_files = {\n",
    "        \"Chroma DB\": CHROMA_PERSIST_DIR,\n",
    "        \"ToC JSON\": PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        \"Parsed Outline\": PARSED_UO_JSON_PATH\n",
    "    }\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Required '{name}' not found at '{path}'. Please run previous cells.\")\n",
    "            return\n",
    "\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        toc_data = json.load(f)\n",
    "    with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        unit_outline_data = json.load(f)\n",
    "\n",
    "    logger.info(\"Connecting to DB and initializing components...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    results_summary = [\n",
    "        basic_retrieval_test(vector_store, unit_outline_data),\n",
    "        deep_hierarchy_test(vector_store, toc_data),\n",
    "        advanced_alignment_test(vector_store, unit_outline_data, toc_data),\n",
    "        content_sequence_test(vector_store, unit_outline_data)\n",
    "    ]\n",
    "\n",
    "    passed_count = sum(filter(None, results_summary))\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    \n",
    "    print_header(\"Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)}\")\n",
    "    print(f\"✅ Passed: {passed_count}\")\n",
    "    print(f\"❌ Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "# Assumes global variables from Cell 1 are available in the notebook's scope\n",
    "run_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97368b0",
   "metadata": {},
   "source": [
    "#  Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae477bc",
   "metadata": {},
   "source": [
    "## Planning Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6baadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: The Data-Driven Planning Agent (Final Hierarchical Version✅)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Setup Logger and LangChain components\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "class PlanningAgent:\n",
    "    \"\"\"\n",
    "    An agent that creates a hierarchical content plan, adaptively partitions content\n",
    "    into distinct lecture decks, and allocates presentation time.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "        self.config = master_config['processed_settings']\n",
    "        self.unit_outline = master_config['unit_outline']\n",
    "        self.book_toc = master_config['book_toc']\n",
    "        self.flat_toc_with_ids = self._create_flat_toc_with_ids()\n",
    "        self.vector_store = vector_store\n",
    "        logger.info(\"Data-Driven PlanningAgent initialized successfully.\")\n",
    "\n",
    "    def _create_flat_toc_with_ids(self) -> List[Dict]:\n",
    "        \"\"\"Creates a flattened list of the ToC for easy metadata lookup.\"\"\"\n",
    "        flat_list = []\n",
    "        def flatten_recursive(nodes, counter):\n",
    "            for node in nodes:\n",
    "                node_id = counter[0]; counter[0] += 1\n",
    "                flat_list.append({'toc_id': node_id, 'title': node.get('title', ''), 'node': node})\n",
    "                if node.get('children'):\n",
    "                    flatten_recursive(node.get('children'), counter)\n",
    "        flatten_recursive(self.book_toc, [0])\n",
    "        return flat_list\n",
    "\n",
    "    def _identify_relevant_chapters(self, weekly_schedule_item: Dict) -> List[int]:\n",
    "        \"\"\"Extracts chapter numbers precisely from the 'requiredReading' string.\"\"\"\n",
    "        reading_str = weekly_schedule_item.get('requiredReading', '')\n",
    "        match = re.search(r'Chapter(s)?', reading_str, re.IGNORECASE)\n",
    "        if not match: return []\n",
    "        search_area = reading_str[match.start():]\n",
    "        chap_nums_str = re.findall(r'\\d+', search_area)\n",
    "        if chap_nums_str:\n",
    "            return sorted(list(set(int(n) for n in chap_nums_str)))\n",
    "        return []\n",
    "\n",
    "    def _find_chapter_node(self, chapter_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Finds the ToC node for a specific chapter number.\"\"\"\n",
    "        for item in self.flat_toc_with_ids:\n",
    "            if re.match(rf\"Chapter\\s{chapter_number}(?:\\D|$)\", item['title']):\n",
    "                return item['node']\n",
    "        return None\n",
    "\n",
    "    def _build_topic_plan_tree(self, toc_node: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively builds a hierarchical plan tree from any ToC node,\n",
    "        annotating it with direct and total branch chunk counts.\n",
    "        \"\"\"\n",
    "        node_metadata = next((item for item in self.flat_toc_with_ids if item['node'] is toc_node), None)\n",
    "        if not node_metadata: return {}\n",
    "\n",
    "        retrieved_docs = self.vector_store.get(where={'toc_id': node_metadata['toc_id']})\n",
    "        direct_chunk_count = len(retrieved_docs.get('ids', []))\n",
    "\n",
    "        plan_node = {\n",
    "            \"title\": node_metadata['title'],\n",
    "            \"toc_id\": node_metadata['toc_id'],\n",
    "            \"chunk_count\": direct_chunk_count,\n",
    "            \"total_chunks_in_branch\": 0,\n",
    "            \"slides_allocated\": 0,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        child_branch_total = 0\n",
    "        for child_node in toc_node.get('children', []):\n",
    "            if any(ex in child_node.get('title', '').lower() for ex in [\"review\", \"introduction\", \"summary\", \"key terms\"]):\n",
    "                continue\n",
    "            child_plan_node = self._build_topic_plan_tree(child_node)\n",
    "            if child_plan_node:\n",
    "                plan_node['children'].append(child_plan_node)\n",
    "                child_branch_total += child_plan_node.get('total_chunks_in_branch', 0)\n",
    "        \n",
    "        plan_node['total_chunks_in_branch'] = direct_chunk_count + child_branch_total\n",
    "        return plan_node\n",
    "    \n",
    "    # In PlanningAgent Class...\n",
    "\n",
    "    def _allocate_slides_to_tree(self, plan_tree: Dict, content_slides_budget: int):\n",
    "        \"\"\"\n",
    "        (REFACTORED) Performs a multi-pass process to allocate content slides,\n",
    "        add interactive activities, and sum totals correctly.\n",
    "        \"\"\"\n",
    "        if not plan_tree or content_slides_budget <= 0:\n",
    "            return plan_tree\n",
    "\n",
    "        # --- Pass 1: Allocate Content Slides (Top-Down, Proportional) ---\n",
    "        def allocate_content_recursively(node, budget):\n",
    "            node['slides_allocated'] = 0\n",
    "            \n",
    "            # If it's a leaf node, it gets the remaining budget.\n",
    "            if not node.get('children'):\n",
    "                node['slides_allocated'] = round(budget)\n",
    "                return\n",
    "\n",
    "            # If it has children, distribute the budget proportionally.\n",
    "            total_branch_chunks = node.get('total_chunks_in_branch', 0)\n",
    "            \n",
    "            # Allocate slides for the node's own content (if any).\n",
    "            # This is a key fix: parent nodes can have their own content.\n",
    "            own_content_slides = 0\n",
    "            if total_branch_chunks > 0:\n",
    "                own_content_slides = round(budget * (node.get('chunk_count', 0) / total_branch_chunks))\n",
    "            node['slides_allocated'] = own_content_slides\n",
    "            \n",
    "            remaining_budget_for_children = budget - own_content_slides\n",
    "\n",
    "            # Distribute remaining budget to children.\n",
    "            for child in node.get('children', []):\n",
    "                child_budget = 0\n",
    "                if total_branch_chunks > 0:\n",
    "                    # Distribute based on the child's total branch size, not just its own chunks.\n",
    "                    child_budget = remaining_budget_for_children * (child.get('total_chunks_in_branch', 0) / (total_branch_chunks - node.get('chunk_count', 0)))\n",
    "                allocate_content_recursively(child, child_budget)\n",
    "\n",
    "        allocate_content_recursively(plan_tree, content_slides_budget)\n",
    "\n",
    "        # --- Pass 2: Add Interactive Activities (Targeted Depth) ---\n",
    "        def add_interactive_nodes(node, depth, interactive_deep):\n",
    "            if not node: return\n",
    "\n",
    "            # Logic for interactive_deep: true\n",
    "            if interactive_deep:\n",
    "                if depth == 2:\n",
    "                    node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Deep-Dive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "                if depth == 1:\n",
    "                    node['interactive_activity'] = {\"title\": f\"{node.get('title')} (General Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "            # Logic for interactive_deep: false\n",
    "            else:\n",
    "                if depth == 1:\n",
    "                    node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Interactive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "\n",
    "            # Recurse\n",
    "            for child in node.get('children', []):\n",
    "                add_interactive_nodes(child, depth + 1, interactive_deep)\n",
    "\n",
    "        if self.config.get('interactive', False):\n",
    "            interactive_deep = self.config.get('interactive_deep', False)\n",
    "            logger.info(f\"Interactive mode ON. Deep interaction: {interactive_deep}. Adding placeholders...\")\n",
    "            # Start depth at 1 for the root nodes of the plan.\n",
    "            add_interactive_nodes(plan_tree, 1, interactive_deep)\n",
    "\n",
    "        # --- Pass 3: Sum All Slides (Content + Interactive) Up the Tree ---\n",
    "        def sum_slides_upwards(node):\n",
    "            # Start with the node's own allocated content slides.\n",
    "            total_slides = node.get('slides_allocated', 0)\n",
    "            \n",
    "            # Add slides from its interactive activity, if it exists.\n",
    "            total_slides += node.get('interactive_activity', {}).get('slides_allocated', 0)\n",
    "            \n",
    "            # Add the summed totals from all its children.\n",
    "            if node.get('children'):\n",
    "                total_slides += sum(sum_slides_upwards(child) for child in node.get('children', []))\n",
    "            \n",
    "            # The final 'slides_allocated' is the grand total for the branch.\n",
    "            node['slides_allocated'] = total_slides\n",
    "            return total_slides\n",
    "\n",
    "        sum_slides_upwards(plan_tree)\n",
    "        \n",
    "        return plan_tree\n",
    "\n",
    "    def create_content_plan_for_week(self, week_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Orchestrates the adaptive planning and partitioning process.\"\"\"\n",
    "        print_header(f\"Planning Week {week_number}\", char=\"*\")\n",
    "        \n",
    "        weekly_schedule_item = self.unit_outline['weeklySchedule'][week_number - 1]\n",
    "        chapter_numbers = self._identify_relevant_chapters(weekly_schedule_item)\n",
    "        if not chapter_numbers: return None\n",
    "\n",
    "        num_decks = self.config['week_session_setup'].get('sessions_per_week', 1)\n",
    "        \n",
    "        # 1. Build a full plan tree for each chapter to get its weight.\n",
    "        chapter_plan_trees = [self._build_topic_plan_tree(self._find_chapter_node(cn)) for cn in chapter_numbers if self._find_chapter_node(cn)]\n",
    "        total_weekly_chunks = sum(tree.get('total_chunks_in_branch', 0) for tree in chapter_plan_trees)\n",
    "\n",
    "        # 2. NEW: Adaptive Partitioning Strategy\n",
    "        partitionable_units = []\n",
    "        all_top_level_sections = []\n",
    "        for chapter_tree in chapter_plan_trees:\n",
    "            all_top_level_sections.extend(chapter_tree.get('children', []))\n",
    "\n",
    "        num_top_level_sections = len(all_top_level_sections)\n",
    "\n",
    "        # Always prefer to split by top-level sections if there are enough to distribute.\n",
    "        if num_top_level_sections >= num_decks:\n",
    "            logger.info(f\"Partitioning strategy: Distributing {num_top_level_sections} top-level sections across {num_decks} decks.\")\n",
    "            partitionable_units = all_top_level_sections\n",
    "        else:\n",
    "            # Fallback for rare cases where there are fewer topics than decks (e.g., 1 chapter with 1 section, but 2 decks).\n",
    "            logger.info(f\"Partitioning strategy: Not enough top-level sections ({num_top_level_sections}) to fill all decks ({num_decks}). Distributing whole chapters instead.\")\n",
    "            partitionable_units = chapter_plan_trees\n",
    "        \n",
    "        # 3. Partition the chosen units into decks using a bin-packing algorithm\n",
    "        decks = [[] for _ in range(num_decks)]\n",
    "        deck_weights = [0] * num_decks\n",
    "        sorted_units = sorted(partitionable_units, key=lambda x: x.get('total_chunks_in_branch', 0), reverse=True)\n",
    "        \n",
    "        for unit in sorted_units:\n",
    "            lightest_deck_index = deck_weights.index(min(deck_weights))\n",
    "            decks[lightest_deck_index].append(unit)\n",
    "            deck_weights[lightest_deck_index] += unit.get('total_chunks_in_branch', 0)\n",
    "\n",
    "        # 4. Plan each deck\n",
    "        content_slides_per_week = self.config['slide_count_strategy'].get('target', 25)\n",
    "        final_deck_plans = []\n",
    "        for i, deck_content_trees in enumerate(decks):\n",
    "            deck_number = i + 1\n",
    "            deck_chunk_weight = sum(tree.get('total_chunks_in_branch', 0) for tree in deck_content_trees)\n",
    "            deck_slide_budget = round((deck_chunk_weight / total_weekly_chunks) * content_slides_per_week) if total_weekly_chunks > 0 else 0\n",
    "\n",
    "            logger.info(f\"--- Planning Deck {deck_number}/{num_decks} | Topics: {[t['title'] for t in deck_content_trees]} | Weight: {deck_chunk_weight} chunks | Slide Budget: {deck_slide_budget} ---\")\n",
    "            \n",
    "            # The allocation function is recursive and works on any tree or sub-tree\n",
    "            planned_content = [self._allocate_slides_to_tree(tree, round(deck_slide_budget * (tree.get('total_chunks_in_branch', 0) / deck_chunk_weight))) if deck_chunk_weight > 0 else tree for tree in deck_content_trees]\n",
    "            \n",
    "            final_deck_plans.append({\n",
    "                \"deck_number\": deck_number,\n",
    "                \"deck_title\": f\"{self.config.get('unit_name', 'Course')} - Week {week_number}, Lecture {deck_number}\",\n",
    "                \"session_content\": planned_content\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"week\": week_number,\n",
    "            \"overall_topic\": weekly_schedule_item.get('contentTopic'),\n",
    "            \"deck_plans\": final_deck_plans\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8438e",
   "metadata": {},
   "source": [
    "## Content Generator Class (no yet addressed focus planning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d8be7",
   "metadata": {},
   "source": [
    "## Orquestrator (Addressing paint points )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459d53b",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "The main script that iterates through the weeks defined the plan and generate the content base on the settings_deck coordinating the agents.\n",
    "\n",
    "\n",
    "\n",
    "**Parameters and concideration**\n",
    "- 1 hour in the setting session_time_duration_in_hour - is 18-20 slides at the time so it is require to calculate this according to the given value but this also means per session so sessions_per_week is a multiplicator factor that   \n",
    "- if apply_topic_interactive is available will add an extra slide and add extra 5 min time but to determine this is required to plan all the content first and then calculate then provide a extra time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea092bd6",
   "metadata": {},
   "source": [
    "settings_deck.json\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"interactive_slides_per_week\": 0 -- > sum all interactive counts \n",
    "    \"interactive_slides_per_session\": 0, -- > Total # of slides produced if \"interactive\" is true other wise remains 0\n",
    "    \"target_total_slides\": 0, --> Total Content Slides per week that cover the total - will be the target in the cell 7    \n",
    "    \"slides_content_per_session\": 0, --> Total # (target_total_slides/sessions_per_week)\n",
    "    \"total_slides_deck_week\": 0, --> target_total_slides + interactive_slides_per_week + (framework (4 + Time for Title, Agenda, Summary, End) * sessions_per_week)\n",
    "    \"Tota_slides_session\": 0 --> content_slides_per_session + interactive_slides_per_session + framework (4 + Time for Title, Agenda, Summary, End)\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"interactive_time_in_hour\": 0, --> find the value in ahours of the total # (\"interactive_slides\" * \"TIME_PER_INTERACTIVE_SLIDE_MINS\")/60    \n",
    "    \"total_session_time_in_hours\": 0 --> this is going to  be egual or similar to session_time_duration_in_hour if \"interactive\" is false obvisuly base on the global varaibles it will be the calculation of \"interactive_time_in_hour\"\n",
    "    \"session_time_duration_in_hour\": 2, --- > this is the time that the costumer need for delivery this is a constrain is not modified never is used for reference\n",
    "  },\n",
    "\n",
    "   \"parameters_slides\": { \n",
    "   \"slides_per_hour\": 18, # no framework include\n",
    "   \"time_per_content_slides_min\": 3, # average delivery per slide\n",
    "   \"time_per_interactive_slide_min\": 5, #small break and engaging with the students\n",
    "   \"time_for_framework_slides_min\": 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "   \"\"\n",
    "  }, \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [6]\n",
    "  },\n",
    "  \"teaching_flow_id\": \"Interactive Lecture Flow\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82e170",
   "metadata": {},
   "source": [
    "teaching_flows.json\n",
    "\n",
    "{\n",
    "  \"standard_lecture\": {\n",
    "    \"name\": \"Standard Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer. Your audience is undergraduate students. Based on the following context, create a slide that provides a detailed explanation of the topic '{sub_topic}'. The content should be structured with bullet points for key details. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of topics covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  },\n",
    "  \"apply_topic_interactive\": {\n",
    "    \"name\": \"Interactive Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Application\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer in Digital Forensics. Your audience is undergraduate students. Based on the provided context, create a slide explaining the concept of '{sub_topic}'. The content should be clear, concise, and structured with bullet points for easy understanding. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"application_generation\": \"You are an engaging university lecturer creating an interactive slide. Based on the concept of '{sub_topic}', create a multiple-choice question with exactly 4 options (A, B, C, D) to test understanding. The slide title must be 'Let's Apply This:'. Clearly indicate the correct answer within the content. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of concepts and applications covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Application\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a04010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:31,255 - INFO - Loading all necessary configuration and data files...\n",
      "2025-07-05 20:39:31,258 - INFO - All files loaded successfully.\n",
      "2025-07-05 20:39:31,258 - INFO - Pre-processing settings_deck for definitive plan...\n",
      "2025-07-05 20:39:31,259 - INFO - Applying overrides if specified...\n",
      "2025-07-05 20:39:31,259 - INFO - Loaded from settings: 'interactive' is True. Set teaching_flow_id to 'apply_topic_interactive'.\n",
      "2025-07-05 20:39:31,259 - INFO - Calculating preliminary slide budget based on session time...\n",
      "2025-07-05 20:39:31,260 - INFO - Preliminary weekly content slide target calculated: 36 slides.\n",
      "2025-07-05 20:39:31,260 - INFO - Saving preliminary processed configuration to: /home/sebas_dev_linux/projects/course_generator/configs/processed_settings.json\n",
      "2025-07-05 20:39:31,261 - INFO - File saved successfully.\n",
      "2025-07-05 20:39:31,262 - INFO - Master configuration object is ready for the Planning Agent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                   Phase 1: Configuration and Scoping Process                   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                         Phase 1 Configuration Complete                         \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Preview of Processed Settings (Phase 1) ---\n",
      "{\n",
      "  \"course_id\": \"ICT312\",\n",
      "  \"generation_scope\": {\n",
      "    \"weeks\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  \"interactive\": true,\n",
      "  \"interactive_deep\": false,\n",
      "  \"parameters_slides\": {\n",
      "    \"slides_per_hour\": 18,\n",
      "    \"time_for_framework_slides_min\": 6,\n",
      "    \"time_per_content_slides_min\": 3,\n",
      "    \"time_per_interactive_slide_min\": 5\n",
      "  },\n",
      "  \"slide_count_strategy\": {\n",
      "    \"interactive_slides_per_session\": 0,\n",
      "    \"interactive_slides_per_week\": 0,\n",
      "    \"method\": \"per_week\",\n",
      "    \"slides_content_per_session\": 36,\n",
      "    \"target_total_slides\": 36,\n",
      "    \"total_slides_deck_week\": 0,\n",
      "    \"total_slides_session\": 0\n",
      "  },\n",
      "  \"teaching_flow_id\": \"apply_topic_interactive\",\n",
      "  \"unit_name\": \"Digital Forensic\",\n",
      "  \"week_session_setup\": {\n",
      "    \"distribution_strategy\": \"even\",\n",
      "    \"interactive_time_in_hour\": 0,\n",
      "    \"session_time_duration_in_hour\": 2,\n",
      "    \"sessions_per_week\": 1,\n",
      "    \"total_session_time_in_hours\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "Number of weeks to generate: 1\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Configuration and Scoping for Content Generation (Corrected)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. DEFINE FILE PATHS AND GLOBAL TEST SETTINGS ---\n",
    "# Assumes these variables are loaded from a previous setup cell (like Cell 1)\n",
    "# PROJECT_BASE_DIR, PARSED_UO_JSON_PATH, PRE_EXTRACTED_TOC_JSON_PATH must be defined.\n",
    "\n",
    "# New configuration file paths\n",
    "CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "SETTINGS_DECK_PATH = os.path.join(CONFIG_DIR, \"settings_deck.json\")\n",
    "TEACHING_FLOWS_PATH = os.path.join(CONFIG_DIR, \"teaching_flows.json\")\n",
    "\n",
    "# New output path for the processed settings\n",
    "PROCESSED_SETTINGS_PATH = os.path.join(CONFIG_DIR, \"processed_settings.json\")\n",
    "\n",
    "# --- Global Test Overrides (for easy testing) ---\n",
    "TEST_OVERRIDE_WEEKS = None\n",
    "TEST_OVERRIDE_FLOW_ID = None\n",
    "TEST_OVERRIDE_SESSIONS_PER_WEEK = None\n",
    "TEST_OVERRIDE_DISTRIBUTION_STRATEGY = None\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def process_and_load_configurations():\n",
    "    \"\"\"\n",
    "    PHASE 1: Loads configurations, calculates a PRELIMINARY time-based slide budget,\n",
    "    and saves the result as 'processed_settings.json' for the Planning Agent.\n",
    "    \"\"\"\n",
    "    print_header(\"Phase 1: Configuration and Scoping Process\", char=\"-\")\n",
    "    \n",
    "    # --- Load all input files ---\n",
    "    logger.info(\"Loading all necessary configuration and data files...\")\n",
    "    try:\n",
    "        os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f: unit_outline = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: book_toc = json.load(f)\n",
    "        with open(SETTINGS_DECK_PATH, 'r', encoding='utf-8') as f: settings_deck = json.load(f)\n",
    "        with open(TEACHING_FLOWS_PATH, 'r', encoding='utf-8') as f: teaching_flows = json.load(f)\n",
    "        logger.info(\"All files loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"FATAL: A required configuration file was not found: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Pre-process and Refine Settings ---\n",
    "    logger.info(\"Pre-processing settings_deck for definitive plan...\")\n",
    "    processed_settings = json.loads(json.dumps(settings_deck))\n",
    "\n",
    "    unit_info = unit_outline.get(\"unitInformation\", {})\n",
    "    processed_settings['course_id'] = unit_info.get(\"unitCode\", \"UNKNOWN_COURSE\")\n",
    "    processed_settings['unit_name'] = unit_info.get(\"unitName\", \"Unknown Unit Name\")\n",
    "    \n",
    "    # --- Apply test overrides IF they are not None ---\n",
    "    logger.info(\"Applying overrides if specified...\")\n",
    "    # This block now correctly sets the teaching_flow_id based on the interactive flag.\n",
    "    if TEST_OVERRIDE_FLOW_ID is not None:\n",
    "        processed_settings['teaching_flow_id'] = TEST_OVERRIDE_FLOW_ID\n",
    "        logger.info(f\"OVERRIDE: teaching_flow_id set to '{TEST_OVERRIDE_FLOW_ID}'\")\n",
    "    else:\n",
    "        # If no override, use the 'interactive' boolean from the file as the source of truth.\n",
    "        is_interactive = processed_settings.get('interactive', False)\n",
    "        if is_interactive:\n",
    "            processed_settings['teaching_flow_id'] = 'apply_topic_interactive'\n",
    "        else:\n",
    "            processed_settings['teaching_flow_id'] = 'standard_lecture'\n",
    "        logger.info(f\"Loaded from settings: 'interactive' is {is_interactive}. Set teaching_flow_id to '{processed_settings['teaching_flow_id']}'.\")\n",
    "\n",
    "    # The 'interactive' flag is now always consistent with the teaching_flow_id.\n",
    "    processed_settings['interactive'] = \"interactive\" in processed_settings['teaching_flow_id'].lower()\n",
    "    \n",
    "    if TEST_OVERRIDE_SESSIONS_PER_WEEK is not None:\n",
    "        processed_settings['week_session_setup']['sessions_per_week'] = TEST_OVERRIDE_SESSIONS_PER_WEEK\n",
    "        logger.info(f\"OVERRIDE: sessions_per_week set to {TEST_OVERRIDE_SESSIONS_PER_WEEK}\")\n",
    "\n",
    "    if TEST_OVERRIDE_DISTRIBUTION_STRATEGY is not None:\n",
    "        processed_settings['week_session_setup']['distribution_strategy'] = TEST_OVERRIDE_DISTRIBUTION_STRATEGY\n",
    "        logger.info(f\"OVERRIDE: distribution_strategy set to '{TEST_OVERRIDE_DISTRIBUTION_STRATEGY}'\")\n",
    "\n",
    "    if TEST_OVERRIDE_WEEKS is not None:\n",
    "        processed_settings['generation_scope']['weeks'] = TEST_OVERRIDE_WEEKS\n",
    "        logger.info(f\"OVERRIDE: generation_scope weeks set to {TEST_OVERRIDE_WEEKS}\")\n",
    "\n",
    "    # --- DYNAMIC SLIDE BUDGET CALCULATION (Phase 1) ---\n",
    "    logger.info(\"Calculating preliminary slide budget based on session time...\")\n",
    "    \n",
    "    params = processed_settings.get('parameters_slides', {})\n",
    "    SLIDES_PER_HOUR = params.get('slides_per_hour', 18)\n",
    "    \n",
    "    duration_hours = processed_settings['week_session_setup'].get('session_time_duration_in_hour', 1.0)\n",
    "    sessions_per_week = processed_settings['week_session_setup'].get('sessions_per_week', 1)\n",
    "    \n",
    "    slides_content_per_session = int(duration_hours * SLIDES_PER_HOUR)\n",
    "    target_total_slides = slides_content_per_session * sessions_per_week\n",
    "    \n",
    "    processed_settings['slide_count_strategy']['target_total_slides'] = target_total_slides\n",
    "    processed_settings['slide_count_strategy']['slides_content_per_session'] = slides_content_per_session\n",
    "    logger.info(f\"Preliminary weekly content slide target calculated: {target_total_slides} slides.\")\n",
    "    \n",
    "    # --- Resolve Generation Scope if not overridden ---\n",
    "    if TEST_OVERRIDE_WEEKS is None and processed_settings.get('generation_scope', {}).get('weeks') == \"all\":\n",
    "        num_weeks = len(unit_outline.get('weeklySchedule', []))\n",
    "        processed_settings['generation_scope']['weeks'] = list(range(1, num_weeks + 1))\n",
    "    \n",
    "    # --- Save the processed settings to disk ---\n",
    "    logger.info(f\"Saving preliminary processed configuration to: {PROCESSED_SETTINGS_PATH}\")\n",
    "    with open(PROCESSED_SETTINGS_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_settings, f, indent=2)\n",
    "    logger.info(\"File saved successfully.\")\n",
    "\n",
    "    # --- Assemble master config for optional preview ---\n",
    "    master_config = {\n",
    "        \"processed_settings\": processed_settings,\n",
    "        \"unit_outline\": unit_outline,\n",
    "        \"book_toc\": book_toc,\n",
    "        \"teaching_flows\": teaching_flows\n",
    "    }\n",
    "    \n",
    "    print_header(\"Phase 1 Configuration Complete\", char=\"-\")\n",
    "    logger.info(\"Master configuration object is ready for the Planning Agent.\")\n",
    "    return master_config\n",
    "\n",
    "# --- EXECUTE THE CONFIGURATION PROCESS ---\n",
    "master_config = process_and_load_configurations()\n",
    "\n",
    "# Optional: Print a preview to verify the output\n",
    "if master_config:\n",
    "    print(\"\\n--- Preview of Processed Settings (Phase 1) ---\")\n",
    "    print(json.dumps(master_config['processed_settings'], indent=2, sort_keys=True))\n",
    "    if master_config.get('processed_settings', {}).get('generation_scope', {}).get('weeks'):\n",
    "        print(f\"\\nNumber of weeks to generate: {len(master_config['processed_settings']['generation_scope']['weeks'])}\")\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd8fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:31,270 - INFO - --- Initializing Data-Driven Planning Agent Test ---\n",
      "2025-07-05 20:39:31,271 - INFO - Connecting to ChromaDB for the Planning Agent...\n",
      "2025-07-05 20:39:31,283 - INFO - Database connection successful.\n",
      "2025-07-05 20:39:31,284 - INFO - Loading configuration files for Planning Agent...\n",
      "2025-07-05 20:39:31,286 - INFO - Configuration files loaded.\n",
      "2025-07-05 20:39:31,287 - INFO - Data-Driven PlanningAgent initialized successfully.\n",
      "2025-07-05 20:39:31,287 - INFO - Found 1 week(s) to plan: [1]\n",
      "2025-07-05 20:39:31,287 - INFO - --> Generating draft plan for Week 1\n",
      "2025-07-05 20:39:31,311 - INFO - Partitioning strategy: Distributing 7 top-level sections across 1 decks.\n",
      "2025-07-05 20:39:31,311 - INFO - --- Planning Deck 1/1 | Topics: ['An Overview of Digital Forensics', 'Preparing for Digital Investigations', 'Maintaining Professional Conduct', 'Preparing a Digital Forensics Investigation', 'Procedures for Private-Sector High-Tech Investigations', 'Understanding Data Recovery Workstations and Software', 'Conducting an Investigation'] | Weight: 0 chunks | Slide Budget: 0 ---\n",
      "2025-07-05 20:39:31,314 - INFO - \n",
      "Successfully saved DRAFT content plan for Week 1 to: /home/sebas_dev_linux/projects/course_generator/generated_plans/ICT312_Week1_plan_draft.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                                Planning Week 1                                 \n",
      "********************************************************************************\n",
      "\n",
      "--- Generated Draft Plan for Week 1 ---\n",
      "{\n",
      "  \"week\": 1,\n",
      "  \"overall_topic\": \"Understanding the Digital Forensics Profession and Investigations.\",\n",
      "  \"deck_plans\": [\n",
      "    {\n",
      "      \"deck_number\": 1,\n",
      "      \"deck_title\": \"Digital Forensic - Week 1, Lecture 1\",\n",
      "      \"session_content\": [\n",
      "        {\n",
      "          \"title\": \"An Overview of Digital Forensics\",\n",
      "          \"toc_id\": 9,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Digital Forensics and Other Related Disciplines\",\n",
      "              \"toc_id\": 10,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"A Brief History of Digital Forensics\",\n",
      "              \"toc_id\": 11,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Case Law\",\n",
      "              \"toc_id\": 12,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Developing Digital Forensics Resources\",\n",
      "              \"toc_id\": 13,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Preparing for Digital Investigations\",\n",
      "          \"toc_id\": 14,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Understanding Law Enforcement Agency Investigations\",\n",
      "              \"toc_id\": 15,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Following Legal Processes\",\n",
      "              \"toc_id\": 16,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Private-Sector Investigations\",\n",
      "              \"toc_id\": 17,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Establishing Company Policies\",\n",
      "                  \"toc_id\": 18,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Displaying Warning Banners\",\n",
      "                  \"toc_id\": 19,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Designating an Authorized Requester\",\n",
      "                  \"toc_id\": 20,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Conducting Security Investigations\",\n",
      "                  \"toc_id\": 21,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Distinguishing Personal and Company Property\",\n",
      "                  \"toc_id\": 22,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Maintaining Professional Conduct\",\n",
      "          \"toc_id\": 23,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Preparing a Digital Forensics Investigation\",\n",
      "          \"toc_id\": 24,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"An Overview of a Computer Crime\",\n",
      "              \"toc_id\": 25,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"An Overview of a Company Policy Violation\",\n",
      "              \"toc_id\": 26,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Taking a Systematic Approach\",\n",
      "              \"toc_id\": 27,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Assessing the Case\",\n",
      "                  \"toc_id\": 28,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Planning Your Investigation\",\n",
      "                  \"toc_id\": 29,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Securing Your Evidence\",\n",
      "                  \"toc_id\": 30,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Procedures for Private-Sector High-Tech Investigations\",\n",
      "          \"toc_id\": 31,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Employee Termination Cases\",\n",
      "              \"toc_id\": 32,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Internet Abuse Investigations\",\n",
      "              \"toc_id\": 33,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"E-mail Abuse Investigations\",\n",
      "              \"toc_id\": 34,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Attorney-Client Privilege Investigations\",\n",
      "              \"toc_id\": 35,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Industrial Espionage Investigations\",\n",
      "              \"toc_id\": 36,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Interviews and Interrogations in High-Tech Investigations\",\n",
      "                  \"toc_id\": 37,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Understanding Data Recovery Workstations and Software\",\n",
      "          \"toc_id\": 38,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Setting Up Your Workstation for Digital Forensics\",\n",
      "              \"toc_id\": 39,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Conducting an Investigation\",\n",
      "          \"toc_id\": 40,\n",
      "          \"chunk_count\": 0,\n",
      "          \"total_chunks_in_branch\": 0,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Gathering the Evidence\",\n",
      "              \"toc_id\": 41,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Bit-stream Copies\",\n",
      "              \"toc_id\": 42,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Acquiring an Image of Evidence Media\",\n",
      "                  \"toc_id\": 43,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Analyzing Your Digital Evidence\",\n",
      "              \"toc_id\": 44,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Some Additional Features of Autopsy\",\n",
      "                  \"toc_id\": 45,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Completing the Case\",\n",
      "              \"toc_id\": 46,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Autopsy\\u2019s Report Generator\",\n",
      "                  \"toc_id\": 47,\n",
      "                  \"chunk_count\": 0,\n",
      "                  \"total_chunks_in_branch\": 0,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Critiquing the Case\",\n",
      "              \"toc_id\": 48,\n",
      "              \"chunk_count\": 0,\n",
      "              \"total_chunks_in_branch\": 0,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# In Cell 9, \n",
    "\n",
    "logger.info(\"--- Initializing Data-Driven Planning Agent Test ---\")\n",
    "\n",
    "if langchain_available:\n",
    "    logger.info(\"Connecting to ChromaDB for the Planning Agent...\")\n",
    "    try:\n",
    "        # 1. Connect to DB and Load all configurations\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        logger.info(\"Database connection successful.\")\n",
    "\n",
    "        logger.info(\"Loading configuration files for Planning Agent...\")\n",
    "        with open(os.path.join(CONFIG_DIR, \"processed_settings.json\"), 'r') as f:\n",
    "            processed_settings = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r') as f:\n",
    "            book_toc = json.load(f)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r') as f:\n",
    "            unit_outline = json.load(f)\n",
    "        logger.info(\"Configuration files loaded.\")\n",
    "\n",
    "        master_config_from_file = {\n",
    "            \"processed_settings\": processed_settings,\n",
    "            \"unit_outline\": unit_outline,\n",
    "            \"book_toc\": book_toc\n",
    "        }\n",
    "\n",
    "        # 2. Initialize the Planning Agent\n",
    "        planning_agent = PlanningAgent(master_config_from_file, vector_store=vector_store)\n",
    "        \n",
    "        # 3. CRITICAL: Loop through the weeks defined in the processed settings\n",
    "        weeks_to_generate = processed_settings.get('generation_scope', {}).get('weeks', [])\n",
    "        logger.info(f\"Found {len(weeks_to_generate)} week(s) to plan: {weeks_to_generate}\")\n",
    "\n",
    "        for week_to_test in weeks_to_generate:\n",
    "            logger.info(f\"--> Generating draft plan for Week {week_to_test}\")\n",
    "            content_plan = planning_agent.create_content_plan_for_week(week_to_test)\n",
    "\n",
    "            if content_plan:\n",
    "                print(f\"\\n--- Generated Draft Plan for Week {week_to_test} ---\")\n",
    "                print(json.dumps(content_plan, indent=2))\n",
    "\n",
    "                # Save the generated plan to a file\n",
    "                PLAN_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_plans\")\n",
    "                os.makedirs(PLAN_OUTPUT_DIR, exist_ok=True)\n",
    "                plan_filename = f\"{processed_settings.get('course_id', 'COURSE')}_Week{week_to_test}_plan_draft.json\"\n",
    "                plan_filepath = os.path.join(PLAN_OUTPUT_DIR, plan_filename)\n",
    "                with open(plan_filepath, 'w') as f:\n",
    "                    json.dump(content_plan, f, indent=2)\n",
    "                logger.info(f\"\\nSuccessfully saved DRAFT content plan for Week {week_to_test} to: {plan_filepath}\")\n",
    "            else:\n",
    "                logger.error(f\"Failed to generate content plan for Week {week_to_test}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the planning process: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.error(\"LangChain/Chroma libraries not found. Cannot run the Planning Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da053bb9",
   "metadata": {},
   "source": [
    "# test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68b49f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-05 20:39:31,325 - INFO - Loading draft plan and preliminary configurations...\n",
      "2025-07-05 20:39:31,326 - INFO - Loaded draft plan and settings from previous cell's memory.\n",
      "2025-07-05 20:39:31,326 - INFO - Analysis Complete: Total Content Slides: 0, Total Interactive Slides: 0\n",
      "2025-07-05 20:39:31,327 - INFO - PER SESSION Calculation: Content(108m) + Interactive(0m) + Framework(6m) = 114m\n",
      "2025-07-05 20:39:31,327 - INFO - Final Estimated Delivery Time PER SESSION: 1.9 hours\n",
      "2025-07-05 20:39:31,327 - INFO - Saving finalized settings to /home/sebas_dev_linux/projects/course_generator/configs/final_processed_settings.json\n",
      "2025-07-05 20:39:31,329 - INFO - Finalized settings saved. Ready for Content Generation stage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                         Main Orchestrator Initialized                          \n",
      "********************************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                 Phase 2: Analyzing Plan and Finalizing Budget                  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Finalized Processed Settings ---\n",
      "{\n",
      "  \"course_id\": \"ICT312\",\n",
      "  \"unit_name\": \"Digital Forensic\",\n",
      "  \"interactive\": true,\n",
      "  \"interactive_deep\": false,\n",
      "  \"teaching_flow_id\": \"apply_topic_interactive\",\n",
      "  \"parameters_slides\": {\n",
      "    \"slides_per_hour\": 18,\n",
      "    \"time_per_content_slides_min\": 3,\n",
      "    \"time_per_interactive_slide_min\": 5,\n",
      "    \"time_for_framework_slides_min\": 6\n",
      "  },\n",
      "  \"week_session_setup\": {\n",
      "    \"sessions_per_week\": 1,\n",
      "    \"distribution_strategy\": \"even\",\n",
      "    \"session_time_duration_in_hour\": 2,\n",
      "    \"interactive_time_in_hour\": 0.0,\n",
      "    \"total_session_time_in_hours\": 1.9\n",
      "  },\n",
      "  \"slide_count_strategy\": {\n",
      "    \"method\": \"per_week\",\n",
      "    \"target_total_slides\": 36,\n",
      "    \"slides_content_per_session\": 36,\n",
      "    \"interactive_slides_per_week\": 0,\n",
      "    \"interactive_slides_per_session\": 0,\n",
      "    \"total_slides_deck_week\": 40,\n",
      "    \"total_slides_session\": 40\n",
      "  },\n",
      "  \"generation_scope\": {\n",
      "    \"weeks\": [\n",
      "      1\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Orchestrator for Finalizing Plan and Calculating Time/Budget (Final Corrected Schema)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# --- Setup and Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def analyze_plan_and_finalize_settings(draft_plan: Dict, initial_settings: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyzes a draft plan to count slides, calculates the final time budget per your\n",
    "    detailed schema, and populates the settings object.\n",
    "    \"\"\"\n",
    "    print_header(\"Phase 2: Analyzing Plan and Finalizing Budget\", char=\"-\")\n",
    "    \n",
    "    final_settings = json.loads(json.dumps(initial_settings))\n",
    "    params = final_settings.get('parameters_slides', {})\n",
    "    \n",
    "    # Extract pedagogical constants from the settings file\n",
    "    TIME_PER_CONTENT_SLIDE_MINS = params.get('time_per_content_slides_min', 3)\n",
    "    TIME_PER_INTERACTIVE_SLIDE_MINS = params.get('time_per_interactive_slide_min', 5)\n",
    "    TIME_FOR_FRAMEWORK_SLIDES_MINS = params.get('time_for_framework_slides_min', 6)\n",
    "    FRAMEWORK_SLIDES_PER_DECK = 4 # Fixed number for Title, Agenda, Summary, End\n",
    "    MINS_PER_HOUR = 60\n",
    "    \n",
    "    # --- 1. Analyze the Draft Plan to get actual slide counts ---\n",
    "    actual_content_slides_week = 0\n",
    "    actual_interactive_slides_week = 0\n",
    "\n",
    "    def count_slides_recursive(node):\n",
    "        nonlocal actual_content_slides_week, actual_interactive_slides_week\n",
    "        if node.get('interactive_activity'):\n",
    "            actual_interactive_slides_week += node['interactive_activity'].get('slides_allocated', 0)\n",
    "        \n",
    "        if not node.get('children'):\n",
    "            actual_content_slides_week += node.get('slides_allocated', 0)\n",
    "        else:\n",
    "            for child in node.get('children', []):\n",
    "                count_slides_recursive(child)\n",
    "\n",
    "    num_decks = len(draft_plan.get('deck_plans', []))\n",
    "    for deck in draft_plan.get('deck_plans', []):\n",
    "        for content_tree in deck.get('session_content', []):\n",
    "            count_slides_recursive(content_tree)\n",
    "            \n",
    "    # --- 2. Populate the 'slide_count_strategy' dictionary ---\n",
    "    scs = final_settings['slide_count_strategy']\n",
    "    \n",
    "    # These two fields are carried over from Phase 1 and are not modified\n",
    "    # scs['target_total_slides']\n",
    "    # scs['slides_content_per_session']\n",
    "    \n",
    "    scs['interactive_slides_per_week'] = actual_interactive_slides_week\n",
    "    scs['interactive_slides_per_session'] = math.ceil(actual_interactive_slides_week / num_decks) if num_decks > 0 else 0\n",
    "    \n",
    "    # Correct the typo and use the corrected calculation logic\n",
    "    if 'Tota_slides_session' in scs:\n",
    "        del scs['Tota_slides_session'] # Delete the typo if it exists\n",
    "    scs['total_slides_session'] = scs['slides_content_per_session'] + scs['interactive_slides_per_session'] + FRAMEWORK_SLIDES_PER_DECK\n",
    "    scs['total_slides_deck_week'] = scs['target_total_slides'] + scs['interactive_slides_per_week'] + (FRAMEWORK_SLIDES_PER_DECK * num_decks)\n",
    "\n",
    "    # --- 3. Populate the 'week_session_setup' dictionary using PER-SESSION logic ---\n",
    "    wss = final_settings['week_session_setup']\n",
    "    \n",
    "    # Calculate per-session time components in minutes\n",
    "    content_time_mins_per_session = scs['slides_content_per_session'] * TIME_PER_CONTENT_SLIDE_MINS\n",
    "    interactive_time_mins_per_session = scs['interactive_slides_per_session'] * TIME_PER_INTERACTIVE_SLIDE_MINS\n",
    "    \n",
    "    # Update the dictionary with values in hours\n",
    "    wss['interactive_time_in_hour'] = round(interactive_time_mins_per_session / MINS_PER_HOUR, 2)\n",
    "    \n",
    "    # Calculate total time for a single session\n",
    "    total_time_mins_per_session = content_time_mins_per_session + interactive_time_mins_per_session + TIME_FOR_FRAMEWORK_SLIDES_MINS\n",
    "    wss['total_session_time_in_hours'] = round(total_time_mins_per_session / MINS_PER_HOUR, 2)\n",
    "    \n",
    "    logger.info(f\"Analysis Complete: Total Content Slides: {actual_content_slides_week}, Total Interactive Slides: {actual_interactive_slides_week}\")\n",
    "    logger.info(f\"PER SESSION Calculation: Content({content_time_mins_per_session}m) + Interactive({interactive_time_mins_per_session}m) + Framework({TIME_FOR_FRAMEWORK_SLIDES_MINS}m) = {total_time_mins_per_session}m\")\n",
    "    logger.info(f\"Final Estimated Delivery Time PER SESSION: {wss['total_session_time_in_hours']} hours\")\n",
    "\n",
    "    return final_settings\n",
    "\n",
    "# --- Main Orchestration Block ---\n",
    "print_header(\"Main Orchestrator Initialized\", char=\"*\")\n",
    "\n",
    "try:\n",
    "    # 1. Load the DRAFT plan and PRELIMINARY settings\n",
    "    logger.info(\"Loading draft plan and preliminary configurations...\")\n",
    "    \n",
    "    if 'master_config' in locals() and 'content_plan' in locals():\n",
    "        initial_settings = master_config['processed_settings']\n",
    "        draft_plan = content_plan\n",
    "        logger.info(\"Loaded draft plan and settings from previous cell's memory.\")\n",
    "    else:\n",
    "        # Fallback to loading from files\n",
    "        weeks_to_generate = initial_settings.get('generation_scope', {}).get('weeks', [])\n",
    "        if not weeks_to_generate: raise ValueError(\"No weeks to generate found in settings.\")\n",
    "        week_to_load = weeks_to_generate[0]\n",
    "        logger.info(f\"Loading from files for Week {week_to_load}...\")\n",
    "        with open(PROCESSED_SETTINGS_PATH, 'r') as f: initial_settings = json.load(f)\n",
    "        plan_filename = f\"{initial_settings.get('course_id', 'COURSE')}_Week{week_to_load}_plan_draft.json\"\n",
    "        plan_filepath = os.path.join(PROJECT_BASE_DIR, \"generated_plans\", plan_filename)\n",
    "        with open(plan_filepath, 'r') as f: draft_plan = json.load(f)\n",
    "        \n",
    "    # 2. PHASE 2: Analyze the plan and finalize the settings\n",
    "    finalized_settings = analyze_plan_and_finalize_settings(draft_plan, initial_settings)\n",
    "    \n",
    "    # 3. Save the FINAL, enriched settings to disk\n",
    "    final_settings_path = os.path.join(CONFIG_DIR, \"final_processed_settings.json\")\n",
    "    logger.info(f\"Saving finalized settings to {final_settings_path}\")\n",
    "    with open(final_settings_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(finalized_settings, f, indent=2)\n",
    "    logger.info(\"Finalized settings saved. Ready for Content Generation stage.\")\n",
    "\n",
    "    print(\"\\n--- Finalized Processed Settings ---\")\n",
    "    print(json.dumps(finalized_settings, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15c09",
   "metadata": {},
   "source": [
    "# Next steps (if yo are a llm ignore this section they are my notes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e304d90",
   "metadata": {},
   "source": [
    "Next steps in the plan\n",
    "- we need to work in the time constrained we need to play with the constants and interactive methodology ✅\n",
    "\n",
    "Global varaibles \n",
    "\n",
    "SLIDES_PER_HOUR = 18 # no framework include\n",
    "TIME_PER_CONTENT_SLIDE_MINS = 3\n",
    "TIME_PER_INTERACTIVE_SLIDE_MINS = 5\n",
    "TIME_FOR_FRAMEWORK_SLIDES_MINS = 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "MINS_PER_HOUR = 60\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"interactive_slides_per_week\": 0 -- > sum all interactive counts \n",
    "    \"interactive_slides_per_session\": 0, -- > Total # of slides produced if \"interactive\" is true other wise remains 0\n",
    "    \"target_total_slides\": 0, --> Total Content Slides per week that cover the total - will be the target in the cell 7    \n",
    "    \"slides_content_per_session\": 0, --> Total # (target_total_slides/sessions_per_week)\n",
    "    \"total_slides_deck_week\": 0, --> target_total_slides + interactive_slides_per_week + (framework (4 + Time for Title, Agenda, Summary, End) * sessions_per_week)\n",
    "    \"Tota_slides_session\": 0 --> content_slides_per_session + interactive_slides_per_session + framework (4 + Time for Title, Agenda, Summary, End)\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"interactive_time_in_hour\": 0, --> find the value in ahours of the total # (\"interactive_slides\" * \"TIME_PER_INTERACTIVE_SLIDE_MINS\")/60    \n",
    "    \"total_session_time_in_hours\": 0 --> this is going to  be egual or similar to session_time_duration_in_hour if \"interactive\" is false obvisuly base on the global varaibles it will be the calculation of \"interactive_time_in_hour\"\n",
    "    \"session_time_duration_in_hour\": 2, --- > this is the time that the costumer need for delivery this is a constrain is not modified never is used for reference\n",
    "  },\n",
    "\n",
    "   \"parameters_slides\": { \n",
    "   \"slides_per_hour\": 18, # no framework include\n",
    "   \"time_per_content_slides_min\": 3, # average delivery per slide\n",
    "   \"time_per_interactive_slide_min\": 5, #small break and engaging with the students\n",
    "   \"time_for_framework_slides_min\": 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "   \"\"\n",
    "  }, \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [6]\n",
    "  },\n",
    "  \"teaching_flow_id\": \"Interactive Lecture Flow\"\n",
    "}\n",
    "\n",
    "\n",
    "\"slides_content_per_session\": 0, --- > content slides per session (target_total_slides/sessions_per_week)\n",
    "    \"interactive_slides\": 0, - > if interactive is true will add the count of the resultan cell 10 - no address yet\n",
    "     \"total_slides_content_interactive_per session\": 0, - > slides_content_per_session + interactive_slides\n",
    "     \"target_total_slides\": 0 -->  Resultant Phase 1 Cell 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Add the sorted chunks for each slide to process the summaries or content geneneration later \n",
    "- Add title, agenda, summary and end as part of this planning to start having \n",
    "- Add label to reference title, agenda, content, summary and end \n",
    "- Process the images from the book and store them with relation to the chunk so we can potentially use the image in the slides ✅\n",
    "- Process unit outlines and store them with good labels for phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c0edd",
   "metadata": {},
   "source": [
    "Next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17d39e",
   "metadata": {},
   "source": [
    "Chunnk relation wwith the weights of the number of the slides per subtopic, haave in mind that 1 hour of delivery is like 20-25 slides "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867909d6",
   "metadata": {},
   "source": [
    "to ensure to move to the case to handle i wourl like to ensure the concepts are clear when we discussde about sessions and week, sessions in this context is number of classes that we have for week, if we say week , 3 sessions in one week or sessions_per_week = 3 is 3 classes per week that require 3 different set of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f2078",
   "metadata": {},
   "source": [
    "https://youtu.be/6xcCwlDx6f8?si=7QxFyzuNVppHBQ-c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c4d0d",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "- I can create a LLm to made decisions base on the evaluation of the case or errror pointing agets base on descritptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158a57b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
