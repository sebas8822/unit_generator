{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9756d96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 21:45:55,596 - INFO - Using book: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "2025-06-15 21:45:55,597 - INFO - Processing book: '/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub' for hierarchical ToC and chunk metadata.\n",
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-06-15 21:46:00,455 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-15 21:46:00,456 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-15 21:46:03,650 - INFO - Loaded 11815 elements from EPUB.\n",
      "2025-06-15 21:46:03,765 - INFO - Total documents processed with hierarchical metadata: 11483\n",
      "2025-06-15 21:46:04,151 - INFO - Split into 11774 final chunks for vector store.\n",
      "2025-06-15 21:46:04,164 - INFO - Hierarchical Table of Contents saved to: ./book_hierarchical_toc_parsed.json\n",
      "2025-06-15 21:46:04,164 - INFO - Initializing Ollama embedding model...\n",
      "2025-06-15 21:46:04,173 - INFO - Creating/Rebuilding vector database at: ./chroma_db_book_hierarchical_chunks_v1\n",
      "2025-06-15 21:46:04,207 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-15 21:47:15,398 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:48:31,514 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:48:47,167 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:48:47,794 - INFO - Vector database created/updated with 11774 chunks successfully.\n",
      "2025-06-15 21:48:47,830 - INFO - ChromaDB collection count: 11774\n",
      "2025-06-15 21:48:47,831 - INFO - Focused script for ToC extraction and DB creation finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# --- LangChain & Pydantic Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field # Direct Pydantic import\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama import OllamaEmbeddings # Assuming Ollama for embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Ensure logging is configured (might be done once per notebook session)\n",
    "if not logging.getLogger().hasHandlers(): # Avoid adding multiple handlers\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO) # Ensure logger level is set\n",
    "\n",
    "# --- Configuration ---\n",
    "BOOK_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "# BOOK_PATH = \"/path/to/your/book.pdf\" # Example for PDF\n",
    "\n",
    "# Output paths for this focused step\n",
    "OUTPUT_HIERARCHICAL_TOC_JSON = \"./book_hierarchical_toc_parsed.json\"\n",
    "CHROMA_PERSIST_DIR_HIERARCHICAL = \"./chroma_db_book_hierarchical_chunks_v1\"\n",
    "CHROMA_COLLECTION_NAME_HIERARCHICAL = \"book_hierarchical_chunks_v1\"\n",
    "\n",
    "# Embedding and Chunking\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- Helper: Clean metadata values for ChromaDB ---\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    if isinstance(value, list): return \", \".join(map(str, value)) # Convert list to string\n",
    "    elif isinstance(value, dict): return json.dumps(value)      # Convert dict to JSON string\n",
    "    elif isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    else: return str(value)\n",
    "\n",
    "# --- Core Function: Load Book, Extract Hierarchical ToC, Create Chunks with Hierarchy ---\n",
    "def process_book_for_hierarchical_toc_and_chunks(book_path: str) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    logger.info(f\"Processing book: '{book_path}' for hierarchical ToC and chunk metadata.\")\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "    all_book_documents_with_hierarchy: List[Document] = []\n",
    "    hierarchical_toc: List[Dict[str, Any]] = []\n",
    "\n",
    "    if file_extension == \".epub\":\n",
    "        epub_loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            raw_elements = epub_loader.load()\n",
    "            if not raw_elements: raise ValueError(\"No elements loaded from EPUB.\")\n",
    "            logger.info(f\"Loaded {len(raw_elements)} elements from EPUB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EPUB: {e}\", exc_info=True); raise\n",
    "\n",
    "        current_paths = {level: None for level in range(1, 5)}\n",
    "        current_numbers = {level: 0 for level in range(1, 5)}\n",
    "\n",
    "        chapter_regex = r\"(?i)^(chapter\\s+(\\d+|[IVXLCDM]+)\\b|part\\s+[A-Z0-9]+|appendix\\s+[A-Z])\"\n",
    "        ignore_title_prefixes = (\"Note:\", \"Tip:\", \"Figure:\", \"Table:\", \"Listing:\", \"Caution:\")\n",
    "\n",
    "        current_chapter_toc_entry = None\n",
    "        current_section_toc_entry = None\n",
    "        # current_subsection_toc_entry = None # Not explicitly used for adding deeper, but for context\n",
    "\n",
    "        for i, element_doc in enumerate(raw_elements):\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            element_category = element_doc.metadata.get(\"category\", \"\").lower()\n",
    "            \n",
    "            if not element_text: continue\n",
    "\n",
    "            is_potential_heading = element_category == \"title\" and len(element_text) < 200\n",
    "            is_ignorable = any(element_text.lower().startswith(p.lower()) for p in ignore_title_prefixes)\n",
    "            level_detected = 0\n",
    "\n",
    "            if is_potential_heading and not is_ignorable:\n",
    "                if re.match(chapter_regex, element_text):\n",
    "                    level_detected = 1\n",
    "                elif current_paths[1]: # Inside a chapter\n",
    "                    if current_paths[2] and not current_paths[3]: # Inside a section, potential subsection\n",
    "                        level_detected = 3\n",
    "                    elif not current_paths[2]: # Not in a section yet, so this is a section\n",
    "                        level_detected = 2\n",
    "                    elif current_paths[3] and not current_paths[4]: # Inside a subsection, potential sub-subsection\n",
    "                        level_detected = 4\n",
    "                    # Add more conditions if you want to detect L3 or L4 more reliably.\n",
    "                    # This basic heuristic might misclassify titles if structure isn't simple H1->H2->H3.\n",
    "\n",
    "            if level_detected > 0:\n",
    "                for L_reset in range(level_detected + 1, 5):\n",
    "                    current_paths[L_reset] = None\n",
    "                    current_numbers[L_reset] = 0\n",
    "                \n",
    "                current_numbers[level_detected] += 1\n",
    "                current_paths[level_detected] = element_text\n",
    "                \n",
    "                # Create new_toc_item with appropriate children key based on level\n",
    "                new_toc_item = {\n",
    "                    \"title\": element_text,\n",
    "                    \"level\": level_detected,\n",
    "                    \"number_hierarchical\": \".\".join(map(str, [current_numbers[l_num] for l_num in range(1, level_detected + 1) if current_paths[l_num]]))\n",
    "                }\n",
    "                if level_detected < 4: # Levels 1, 2, 3 can have children (\"sections\" or \"subsections\")\n",
    "                    # Level 1 (Chapter) has \"sections\"\n",
    "                    # Level 2 (Section) has \"subsections\" (which we'll also call \"sections\" for simplicity in this dict key)\n",
    "                    # Level 3 (Subsection) has \"subsections\"\n",
    "                    new_toc_item[\"sections\"] = [] # Use a generic \"sections\" key for children of the current item\n",
    "\n",
    "                if level_detected == 1:\n",
    "                    hierarchical_toc.append(new_toc_item)\n",
    "                    current_chapter_toc_entry = new_toc_item\n",
    "                    current_section_toc_entry = None \n",
    "                elif level_detected == 2 and current_chapter_toc_entry:\n",
    "                    current_chapter_toc_entry[\"sections\"].append(new_toc_item)\n",
    "                    current_section_toc_entry = new_toc_item\n",
    "                elif level_detected == 3 and current_section_toc_entry: # If it's a subsection, add to current section\n",
    "                    current_section_toc_entry[\"sections\"].append(new_toc_item)\n",
    "                    # current_subsection_toc_entry = new_toc_item # If we were tracking L3 pointers\n",
    "                elif level_detected == 4 and current_section_toc_entry: # Assuming L4 falls under L3, which is under L2.\n",
    "                                                                     # This logic needs L3 pointer for L4.\n",
    "                                                                     # For simplicity, let's say L4 also goes into L2's \"sections\" if no L3 ptr\n",
    "                    # If current_subsection_toc_entry was set when L3 was detected:\n",
    "                    # if current_subsection_toc_entry:\n",
    "                    #    current_subsection_toc_entry[\"sections\"].append(new_toc_item)\n",
    "                    # else: # Fallback: add to current L2 section if no L3 pointer active\n",
    "                    if current_section_toc_entry: # Check again\n",
    "                         current_section_toc_entry[\"sections\"].append(new_toc_item)\n",
    "                \n",
    "                logger.debug(f\"TOC L{level_detected}: {element_text}\")\n",
    "\n",
    "            doc_metadata = {\n",
    "                \"source\": os.path.basename(book_path),\n",
    "                \"original_category\": element_category,\n",
    "                \"raw_text_snippet\": element_text[:100]\n",
    "            }\n",
    "            for L_meta in range(1, 5):\n",
    "                if current_paths[L_meta]:\n",
    "                    doc_metadata[f\"level_{L_meta}_title\"] = current_paths[L_meta]\n",
    "                    doc_metadata[f\"level_{L_meta}_number_hierarchical\"] = \".\".join(map(str, [current_numbers[l_num_meta] for l_num_meta in range(1, L_meta + 1) if current_paths[l_num_meta]]))\n",
    "            \n",
    "            cleaned_doc_metadata = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata.items() if v is not None}\n",
    "            all_book_documents_with_hierarchy.append(Document(page_content=element_text, metadata=cleaned_doc_metadata))\n",
    "\n",
    "    # --- For PDF: Basic page-level processing ---\n",
    "    elif file_extension == \".pdf\":\n",
    "        # ... (your existing PDF code) ...\n",
    "        logger.warning(\"PDF processing: Using pages as primary elements. Hierarchical ToC will be page-based.\")\n",
    "        pdf_loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            pages = pdf_loader.load()\n",
    "            if not pages: raise ValueError(\"No pages loaded from PDF.\")\n",
    "            logger.info(f\"Loaded {len(pages)} pages from PDF.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF: {e}\", exc_info=True); raise\n",
    "\n",
    "        pdf_main_toc_entry = {\"title\": \"PDF Content (Page by Page)\", \"level\": 1, \"number_hierarchical\": \"PDF\", \"sections\": []}\n",
    "        hierarchical_toc.append(pdf_main_toc_entry)\n",
    "\n",
    "        for i, page_doc in enumerate(pages):\n",
    "            page_num = page_doc.metadata.get('page', i + 1)\n",
    "            page_title = f\"Page {page_num}\"\n",
    "            \n",
    "            pdf_main_toc_entry[\"sections\"].append({\"title\": page_title, \"level\": 2, \"number_hierarchical\": f\"PDF.{page_num}\", \"sections\": []}) # Add \"sections\" key here too\n",
    "            \n",
    "            doc_metadata = {\n",
    "                \"source\": os.path.basename(book_path),\n",
    "                \"page_number\": page_num,\n",
    "                \"level_1_title\": \"PDF Content\", \n",
    "                \"level_1_number_hierarchical\": \"PDF\",\n",
    "                \"level_2_title\": page_title,    \n",
    "                \"level_2_number_hierarchical\": f\"PDF.{page_num}\"\n",
    "            }\n",
    "            cleaned_doc_metadata = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata.items() if v is not None}\n",
    "            all_book_documents_with_hierarchy.append(Document(page_content=page_doc.page_content, metadata=cleaned_doc_metadata))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported book file format: {file_extension}\")\n",
    "\n",
    "    if not all_book_documents_with_hierarchy:\n",
    "        logger.error(\"No documents processed from the book.\")\n",
    "        return [], []\n",
    "\n",
    "    logger.info(f\"Total documents processed with hierarchical metadata: {len(all_book_documents_with_hierarchy)}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(all_book_documents_with_hierarchy)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks for vector store.\")\n",
    "    \n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "   \n",
    "\n",
    "# --- Main execution block for Jupyter ---\n",
    "\n",
    "# 1. Delete old DB if it exists (for clean runs during development)\n",
    "if os.path.exists(CHROMA_PERSIST_DIR_HIERARCHICAL):\n",
    "    logger.info(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR_HIERARCHICAL}\")\n",
    "    shutil.rmtree(CHROMA_PERSIST_DIR_HIERARCHICAL)\n",
    "\n",
    "# 2. Process the book: Get chunks with hierarchical metadata and the hierarchical ToC\n",
    "logger.info(f\"Using book: {BOOK_PATH}\")\n",
    "final_chunks_for_db, book_hierarchical_toc = process_book_for_hierarchical_toc_and_chunks(BOOK_PATH)\n",
    "\n",
    "# 3. Save the Hierarchical Table of Contents\n",
    "if book_hierarchical_toc:\n",
    "    try:\n",
    "        with open(OUTPUT_HIERARCHICAL_TOC_JSON, 'w', encoding='utf-8') as f:\n",
    "            json.dump(book_hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Hierarchical Table of Contents saved to: {OUTPUT_HIERARCHICAL_TOC_JSON}\")\n",
    "        # You can print a snippet for quick review in Jupyter\n",
    "        # print(\"\\n--- Sample of Hierarchical ToC ---\")\n",
    "        # print(json.dumps(book_hierarchical_toc[:2], indent=2)) # Print first 2 chapters\n",
    "        # print(\"...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving hierarchical ToC: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"No hierarchical ToC was generated to save.\")\n",
    "\n",
    "# 4. Create and Persist the Vector Database with hierarchical chunks\n",
    "if final_chunks_for_db:\n",
    "    logger.info(\"Initializing Ollama embedding model...\")\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        logger.info(f\"Creating/Rebuilding vector database at: {CHROMA_PERSIST_DIR_HIERARCHICAL}\")\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=final_chunks_for_db, # These chunks have the hierarchical metadata\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=CHROMA_PERSIST_DIR_HIERARCHICAL,\n",
    "            collection_name=CHROMA_COLLECTION_NAME_HIERARCHICAL\n",
    "        )\n",
    "        logger.info(f\"Vector database created/updated with {len(final_chunks_for_db)} chunks successfully.\")\n",
    "        logger.info(f\"ChromaDB collection count: {vector_db._collection.count()}\")\n",
    "\n",
    "        # Test a query (optional)\n",
    "        # if vector_db._collection.count() > 0:\n",
    "        #     test_query = \"digital forensics history\" # Example query\n",
    "        #     logger.info(f\"Testing DB with query: '{test_query}'\")\n",
    "        #     results = vector_db.similarity_search(test_query, k=2)\n",
    "        #     if results:\n",
    "        #         logger.info(f\"Found {len(results)} results for test query.\")\n",
    "        #         for doc in results:\n",
    "        #             logger.info(f\"  - Metadata: {doc.metadata}\")\n",
    "        #             logger.info(f\"    Content snippet: {doc.page_content[:100]}...\")\n",
    "        #     else:\n",
    "        #         logger.info(\"Test query returned no results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing embedding model or creating vector DB: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"No chunks were generated, so no vector database will be created.\")\n",
    "\n",
    "logger.info(\"Focused script for ToC extraction and DB creation finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee85de8",
   "metadata": {},
   "source": [
    "# Extract TOC from epub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e2e5763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "INFO: Found EPUB 2 (NCX) Table of Contents.\n",
      "INFO: Table of Contents extracted successfully.\n",
      "\n",
      "✅ Successfully wrote hierarchical ToC to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/epub_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# No need to ignore the XML warning, as we'll be parsing XML correctly.\n",
    "\n",
    "# Define keywords to ignore for a clean ToC.\n",
    "IGNORE_KEYWORDS = [\n",
    "    'note', 'tip', 'caution', 'list of illustrations', 'list of tables',\n",
    "    'copyright statement', 'hands-on project'\n",
    "]\n",
    "\n",
    "def parse_navpoint(navpoint, level=0):\n",
    "    \"\"\"\n",
    "    Recursively parses an NCX <navPoint> element and its children.\n",
    "    This is for the EPUB 2 format.\n",
    "    \"\"\"\n",
    "    # The title is in the <navLabel>/<text> tags\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    \n",
    "    # --- Filtering Logic ---\n",
    "    if not title or re.fullmatch(r'\\d+\\.?', title) or \\\n",
    "       any(title.lower().startswith(keyword) for keyword in IGNORE_KEYWORDS):\n",
    "        return None\n",
    "\n",
    "    node = {\n",
    "        \"level\": level,\n",
    "        \"title\": title,\n",
    "        \"children\": []\n",
    "    }\n",
    "\n",
    "    # Recursively process child <navPoint> elements\n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, level + 1)\n",
    "        if child_node:\n",
    "            node[\"children\"].append(child_node)\n",
    "    \n",
    "    return node\n",
    "\n",
    "def parse_li(li_element, level=0):\n",
    "    \"\"\"\n",
    "    Recursively parses an XHTML <li> element and its children.\n",
    "    This is for the EPUB 3 format.\n",
    "    \"\"\"\n",
    "    a_tag = li_element.find('a')\n",
    "\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "\n",
    "        if not title or re.fullmatch(r'\\d+\\.?', title) or \\\n",
    "           any(title.lower().startswith(keyword) for keyword in IGNORE_KEYWORDS):\n",
    "            return None\n",
    "\n",
    "        node = {\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        nested_ol = li_element.find('ol')\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, level + 1)\n",
    "                if child_node:\n",
    "                    node[\"children\"].append(child_node)\n",
    "        \n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_toc_as_json(epub_path, output_json_path):\n",
    "    \"\"\"\n",
    "    Extracts a clean, hierarchical ToC from an EPUB, supporting both\n",
    "    EPUB 2 (.ncx) and EPUB 3 (.xhtml) formats, and saves it as a JSON file.\n",
    "    \"\"\"\n",
    "    toc_data = []\n",
    "    try:\n",
    "        book = epub.read_epub(epub_path)\n",
    "        print(f\"Processing ToC for: {epub_path}\")\n",
    "\n",
    "        # Ebooklib gives us all navigation documents. We'll check each one.\n",
    "        for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "            soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "            \n",
    "            # --- CHECK 1: Is it an EPUB 2 (.ncx) file? ---\n",
    "            if nav_item.get_name().endswith('.ncx'):\n",
    "                print(\"INFO: Found EPUB 2 (NCX) Table of Contents.\")\n",
    "                # The main container in an NCX file is the <navMap>\n",
    "                navmap = soup.find('navMap')\n",
    "                if navmap:\n",
    "                    # We process the top-level <navPoint> tags\n",
    "                    for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                        node = parse_navpoint(navpoint)\n",
    "                        if node:\n",
    "                            toc_data.append(node)\n",
    "            \n",
    "            # --- CHECK 2: Is it an EPUB 3 (.xhtml) file? ---\n",
    "            else:\n",
    "                print(\"INFO: Found EPUB 3 (XHTML) Table of Contents.\")\n",
    "                # The main container is <nav epub:type=\"toc\">\n",
    "                toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "                if toc_nav:\n",
    "                    top_ol = toc_nav.find('ol')\n",
    "                    if top_ol:\n",
    "                        for li in top_ol.find_all('li', recursive=False):\n",
    "                            node = parse_li(li)\n",
    "                            if node:\n",
    "                                toc_data.append(node)\n",
    "\n",
    "            # If we successfully extracted a ToC, we can stop.\n",
    "            if toc_data:\n",
    "                print(\"INFO: Table of Contents extracted successfully.\")\n",
    "                break\n",
    "        \n",
    "        if not toc_data:\n",
    "            print(\"\\nWARNING: Finished processing, but no valid ToC data was extracted.\")\n",
    "        \n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n✅ Successfully wrote hierarchical ToC to: {output_json_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: The file was not found at {epub_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- Usage Example ---\n",
    "epub_file = \"/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "json_output_file = \"/home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/epub_table_of_contents.json\"\n",
    "\n",
    "extract_toc_as_json(epub_file, json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062b1cc4",
   "metadata": {},
   "source": [
    "# Extract TOC from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dad9953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: /home/sebas_dev_linux/projects/course_generator/data/books/(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\n",
      "INFO: Found 290 bookmark entries.\n",
      "\n",
      "✅ Successfully wrote PDF bookmarks to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/pdf_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # The PyMuPDF library\n",
    "import json\n",
    "\n",
    "def build_hierarchy(toc_list):\n",
    "    \"\"\"\n",
    "    Converts a flat list from PyMuPDF's get_toc() into a nested hierarchy.\n",
    "    \"\"\"\n",
    "    root = []\n",
    "    # This stack keeps track of the parent node at each level.\n",
    "    # The key is the level, the value is the node dictionary.\n",
    "    parent_stack = {0: {\"children\": root}}\n",
    "\n",
    "    for level, title, page in toc_list:\n",
    "        node = {\n",
    "            \"level\": level,\n",
    "            \"title\": title.strip(),\n",
    "            \"page\": page,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        # The parent of the current node is the last node seen at level-1\n",
    "        parent_node = parent_stack[level - 1]\n",
    "        parent_node[\"children\"].append(node)\n",
    "\n",
    "        # The current node becomes the new parent for any subsequent, deeper nodes\n",
    "        parent_stack[level] = node\n",
    "\n",
    "    return root\n",
    "\n",
    "def extract_pdf_bookmarks_to_json(pdf_path, output_json_path):\n",
    "    \"\"\"\n",
    "    Extracts the bookmarks (outline) from a PDF file, builds a hierarchy,\n",
    "    and saves it as a JSON file.\n",
    "    \"\"\"\n",
    "    print(f\"Processing PDF: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        \n",
    "        # get_toc() returns a flat list of bookmarks like: [[lvl, title, page], ...]\n",
    "        # For example: [[1, \"Chapter 1\", 10], [2, \"Section 1.1\", 12]]\n",
    "        toc = doc.get_toc()\n",
    "        \n",
    "        if not toc:\n",
    "            print(\"WARNING: This PDF has no embedded bookmarks (Table of Contents). Cannot extract structure.\")\n",
    "            # Create an empty JSON file\n",
    "            with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump([], f)\n",
    "            return\n",
    "\n",
    "        print(f\"INFO: Found {len(toc)} bookmark entries.\")\n",
    "\n",
    "        # Convert the flat list into a nested, hierarchical structure\n",
    "        hierarchical_toc = build_hierarchy(toc)\n",
    "\n",
    "        # Write the hierarchical data to the JSON file\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"\\n✅ Successfully wrote PDF bookmarks to: {output_json_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: The file was not found at {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- Usage Example ---\n",
    "# Replace this with the path to your PDF file\n",
    "pdf_file_path = \"/home/sebas_dev_linux/projects/course_generator/data/books/(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\" \n",
    "json_output_file = \"/home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/pdf_table_of_contents.json\"\n",
    "\n",
    "extract_pdf_bookmarks_to_json(pdf_file_path, json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0fb3253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 00:06:40,750 - INFO - Using book: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub and pre-extracted ToC: ./epub_table_of_contents.json\n",
      "2025-06-16 00:06:40,751 - INFO - Processing book '/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub' using pre-extracted ToC from './epub_table_of_contents.json'.\n",
      "2025-06-16 00:06:40,752 - INFO - Successfully loaded pre-extracted hierarchical ToC with 28 top-level entries.\n",
      "2025-06-16 00:06:42,598 - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-06-16 00:06:42,598 - INFO - NumExpr defaulting to 16 threads.\n",
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-06-16 00:06:47,405 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-16 00:06:47,405 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-16 00:06:50,700 - INFO - Loaded 11815 text elements from EPUB.\n",
      "2025-06-16 00:06:50,701 - INFO - Flattened and sorted ToC with 861 entries.\n",
      "2025-06-16 00:06:50,701 - WARNING - For EPUB, using element-based heuristic for assigning hierarchical metadata to chunks. The external ToC is primarily for output and less for precise text segmentation in this version.\n",
      "2025-06-16 00:06:50,856 - INFO - Total documents prepared for chunking: 11483\n",
      "2025-06-16 00:06:51,214 - INFO - Split into 11774 final chunks for vector store, inheriting hierarchical metadata.\n",
      "2025-06-16 00:06:51,224 - INFO - Pre-extracted Hierarchical Table of Contents was loaded and re-saved to: ./processed_book_table_of_contents.json\n",
      "2025-06-16 00:06:51,225 - INFO - Initializing Ollama embedding model...\n",
      "2025-06-16 00:06:51,233 - INFO - Creating/Rebuilding vector database at: ./chroma_db_book_toc_guided_chunks_v1\n",
      "2025-06-16 00:06:51,280 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-16 00:07:57,388 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-16 00:09:12,865 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-16 00:09:27,394 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-16 00:09:27,991 - INFO - Vector database created/updated with 11774 chunks and persisted to ./chroma_db_book_toc_guided_chunks_v1.\n",
      "2025-06-16 00:09:28,032 - INFO - ChromaDB collection count after reload: 11774\n",
      "2025-06-16 00:09:28,032 - INFO - Attempting test query: 'digital evidence'\n",
      "2025-06-16 00:09:28,053 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-16 00:09:28,058 - INFO - Found 2 results for test query 'digital evidence':\n",
      "2025-06-16 00:09:28,058 - INFO -   Result 1 Metadata: {'original_category': 'title', 'level_1_number_hierarchical': '5', 'level_1_title': 'Chapter 4. Processing Crime and Incident Scenes', 'level_2_title': 'Identifying Digital Evidence', 'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_2_number_hierarchical': '5.1'}\n",
      "2025-06-16 00:09:28,059 - INFO -   Result 2 Metadata: {'level_1_title': 'Chapter 4. Processing Crime and Incident Scenes', 'level_5_title': 'Processing and Handling Digital Evidence', 'source': 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub', 'level_2_number_hierarchical': '5.1', 'level_3_number_hierarchical': '5.1.1', 'level_1_number_hierarchical': '5', 'level_4_title': 'Understanding Rules of Evidence', 'level_3_title': 'Note', 'level_5_number_hierarchical': '5.1.1.1.39', 'original_category': 'title', 'level_2_title': 'Identifying Digital Evidence', 'level_4_number_hierarchical': '5.1.1.1'}\n",
      "2025-06-16 00:09:28,059 - INFO - Focused script for ToC-guided DB creation finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# --- LangChain & Pydantic Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field # Pydantic not directly used in this script but often in surrounding code\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader # For loading full text\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Setup Logging ---\n",
    "if not logging.getLogger().hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Determine book type and corresponding ToC path\n",
    "IS_EPUB = True # Set to False if processing a PDF\n",
    "\n",
    "if IS_EPUB:\n",
    "    BOOK_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "    # Assume you've run your ebooklib script and saved its output here:\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = \"./epub_table_of_contents.json\"\n",
    "else: # Example for PDF\n",
    "    BOOK_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/books/(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "    # Assume you've run your PyMuPDF script and saved its output here:\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = \"./pdf_table_of_contents.json\"\n",
    "\n",
    "# Define output path for the re-saved ToC\n",
    "OUTPUT_HIERARCHICAL_TOC_JSON = \"./processed_book_table_of_contents.json\"\n",
    "\n",
    "CHROMA_PERSIST_DIR_HIERARCHICAL = \"./chroma_db_book_toc_guided_chunks_v1\"\n",
    "CHROMA_COLLECTION_NAME_HIERARCHICAL = \"book_toc_guided_chunks_v1\"\n",
    "\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800  # Size of final chunks for ChromaDB\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- Helper: Clean metadata values for ChromaDB ---\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    if isinstance(value, list): return \", \".join(map(str, value))\n",
    "    elif isinstance(value, dict): return json.dumps(value)\n",
    "    elif isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    else: return str(value)\n",
    "\n",
    "# --- Core Function ---\n",
    "def process_book_with_extracted_toc(\n",
    "    book_path: str,\n",
    "    extracted_toc_path: str\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "    logger.info(f\"Processing book '{book_path}' using pre-extracted ToC from '{extracted_toc_path}'.\")\n",
    "\n",
    "    # 1. Load the pre-extracted hierarchical ToC\n",
    "    hierarchical_toc = []\n",
    "    try:\n",
    "        with open(extracted_toc_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        if not hierarchical_toc:\n",
    "            logger.error(f\"Pre-extracted ToC at '{extracted_toc_path}' is empty.\")\n",
    "            return [], [] # Return empty list for ToC as well\n",
    "        logger.info(f\"Successfully loaded pre-extracted hierarchical ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pre-extracted ToC JSON from '{extracted_toc_path}': {e}\", exc_info=True)\n",
    "        return [], [] # Return empty list for ToC as well\n",
    "\n",
    "    # 2. Load full book content\n",
    "    all_text_elements: List[Document] = []\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "    if file_extension == \".epub\":\n",
    "        loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            all_text_elements = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_text_elements)} text elements from EPUB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EPUB content for text: {e}\", exc_info=True); return [], hierarchical_toc\n",
    "    elif file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            all_text_elements = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_text_elements)} pages from PDF.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF content for text: {e}\", exc_info=True); return [], hierarchical_toc\n",
    "    else:\n",
    "        logger.error(f\"Unsupported book file format: {file_extension}\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    if not all_text_elements:\n",
    "        logger.error(\"No text elements loaded from the book.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    # Concatenating full book text is not used in the current metadata assignment logic,\n",
    "    # but kept here in case future refinements need it. Can be memory intensive.\n",
    "    # full_book_text = \"\\n\\n\".join([doc.page_content for doc in all_text_elements if doc.page_content])\n",
    "    # logger.info(f\"Full book text loaded (length: {len(full_book_text)} characters).\")\n",
    "\n",
    "    # 3. Prepare for assigning hierarchical metadata\n",
    "    \n",
    "    # Create a flat list of all ToC entries with their full path for easier lookup\n",
    "    flat_toc_with_paths: List[Dict[str, Any]] = []\n",
    "    def flatten_toc_recursive(nodes, parent_path_titles, parent_path_numbers):\n",
    "        for i, node in enumerate(nodes):\n",
    "            title = node.get(\"title\",\"\").strip()\n",
    "            level = node.get(\"level\", len(parent_path_titles) + 1) # Derive level if not present\n",
    "            page_start_val = node.get(\"page\") # Can be None or a number\n",
    "\n",
    "            current_path_titles = parent_path_titles + [title]\n",
    "            # current_path_numbers = parent_path_numbers + [str(node.get(\"number\", i + 1))] # Example numbering\n",
    "\n",
    "            flat_toc_with_paths.append({\n",
    "                \"path_titles\": current_path_titles,\n",
    "                # \"path_numbers\": current_path_numbers,\n",
    "                \"level\": level,\n",
    "                \"title\": title,\n",
    "                \"page_start\": page_start_val\n",
    "            })\n",
    "            \n",
    "            children = node.get(\"children\") or node.get(\"sections\") # Handle both common keys\n",
    "            if children:\n",
    "                flatten_toc_recursive(children, current_path_titles, []) # parent_path_numbers for more complex numbering\n",
    "\n",
    "    flatten_toc_recursive(hierarchical_toc, [], [])\n",
    "    \n",
    "    # Sort flat_toc by page number if page numbers are available and useful (mainly for PDFs)\n",
    "    # Ensure page_start is treated as an integer for sorting, with None handled.\n",
    "    if flat_toc_with_paths:\n",
    "        # Sorts items with None page_start (or page_start treated as -1) to the beginning.\n",
    "        flat_toc_with_paths.sort(key=lambda x: x.get(\"page_start\") if x.get(\"page_start\") is not None else -1)\n",
    "        logger.info(f\"Flattened and sorted ToC with {len(flat_toc_with_paths)} entries.\")\n",
    "\n",
    "\n",
    "    # Assign hierarchical metadata to each document from all_text_elements\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "    if file_extension == \".pdf\":\n",
    "        logger.info(\"Processing PDF: Assigning metadata based on ToC page ranges.\")\n",
    "        for page_doc in all_text_elements:\n",
    "            # PyPDFLoader's \"page\" metadata is 0-indexed. ToC page numbers are often 1-indexed.\n",
    "            page_num_0_indexed = page_doc.metadata.get(\"page\", 0) \n",
    "            page_num_1_indexed = page_num_0_indexed + 1\n",
    "            \n",
    "            active_toc_entry = None\n",
    "            # Find the ToC entry that this page belongs to\n",
    "            for toc_item in flat_toc_with_paths:\n",
    "                current_entry_page_start = toc_item.get(\"page_start\")\n",
    "                if current_entry_page_start is None: # Cannot use this ToC entry for page matching\n",
    "                    continue\n",
    "\n",
    "                # Find the start page of the *next* ToC item to define the end of the current section\n",
    "                next_entry_page_start_for_range = float('inf')\n",
    "                current_item_index = flat_toc_with_paths.index(toc_item)\n",
    "                for next_toc_item_idx in range(current_item_index + 1, len(flat_toc_with_paths)):\n",
    "                    potential_next_page_start = flat_toc_with_paths[next_toc_item_idx].get(\"page_start\")\n",
    "                    if potential_next_page_start is not None:\n",
    "                        next_entry_page_start_for_range = potential_next_page_start\n",
    "                        break \n",
    "                \n",
    "                if current_entry_page_start <= page_num_1_indexed < next_entry_page_start_for_range:\n",
    "                    active_toc_entry = toc_item\n",
    "                    break # Found the most specific ToC entry for this page\n",
    "            \n",
    "            metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "            if active_toc_entry:\n",
    "                for i, title in enumerate(active_toc_entry[\"path_titles\"]):\n",
    "                    metadata[f\"level_{i+1}_title\"] = title\n",
    "                # You could add other metadata from active_toc_entry if needed\n",
    "            else: # Page not covered by any specific ToC entry, or all ToC entries lacked page numbers\n",
    "                metadata[\"level_1_title\"] = \"Uncategorized PDF Content\"\n",
    "            \n",
    "            cleaned_metadata = {k: clean_metadata_for_chroma(v) for k,v in metadata.items() if v is not None}\n",
    "            final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=cleaned_metadata))\n",
    "\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.warning(\"For EPUB, using element-based heuristic for assigning hierarchical metadata to chunks. The external ToC is primarily for output and less for precise text segmentation in this version.\")\n",
    "        current_paths = {level: None for level in range(1, 6)} # Support up to 5 levels\n",
    "        current_numbers = {level: 0 for level in range(1, 6)}    \n",
    "        # More generic regex for titles/chapters/parts\n",
    "        title_regexes = [\n",
    "            (1, re.compile(r\"^(?:PART|BOOK)\\s+[IVXLCDM\\d]+\", re.IGNORECASE)),\n",
    "            (1, re.compile(r\"^CHAPTER\\s+\\d+\", re.IGNORECASE)),\n",
    "            (2, re.compile(r\"^\\d+\\.\\d+\\s+[^0-9.]\", re.IGNORECASE)), # e.g. 1.1 Section Title\n",
    "            (3, re.compile(r\"^\\d+\\.\\d+\\.\\d+\\s+[^0-9.]\", re.IGNORECASE)), # e.g. 1.1.1 Subsection\n",
    "        ]\n",
    "        ignore_title_prefixes = (\"Note:\", \"Tip:\", \"Figure\", \"Table\", \"Listing\", \"Caution:\", \"Example:\", \"Source:\", \"Credit:\")\n",
    "\n",
    "        for element_doc in all_text_elements:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            element_category = element_doc.metadata.get(\"category\", \"\").lower() # from Unstructured\n",
    "            \n",
    "            if not element_text: continue\n",
    "\n",
    "            is_potential_heading = (element_category == \"title\" or len(element_text.split()) < 15) and len(element_text) < 250 # Heuristic for titles\n",
    "            is_ignorable = any(element_text.lower().startswith(p.lower()) for p in ignore_title_prefixes)\n",
    "            \n",
    "            level_detected = 0\n",
    "            detected_title = \"\"\n",
    "\n",
    "            if is_potential_heading and not is_ignorable:\n",
    "                # Try to match known title patterns from ToC (if robust matching is feasible)\n",
    "                # Or use regex/heuristics as fallback\n",
    "                for lvl, rgx in title_regexes:\n",
    "                    match = rgx.match(element_text)\n",
    "                    if match:\n",
    "                        level_detected = lvl\n",
    "                        detected_title = element_text # Use the full matched text as title\n",
    "                        break\n",
    "                \n",
    "                # Simpler heuristic if regex doesn't match but it's categorized as title\n",
    "                if not level_detected and element_category == \"title\":\n",
    "                    # Basic hierarchy based on assumption of sequential titles\n",
    "                    if not current_paths[1]: level_detected = 1\n",
    "                    elif not current_paths[2]: level_detected = 2\n",
    "                    elif not current_paths[3]: level_detected = 3\n",
    "                    elif not current_paths[4]: level_detected = 4\n",
    "                    else: level_detected = 5 # Max depth for this heuristic\n",
    "                    if level_detected > 0: detected_title = element_text\n",
    "\n",
    "\n",
    "            if level_detected > 0 and detected_title:\n",
    "                for L_reset in range(level_detected + 1, 6): \n",
    "                    current_paths[L_reset], current_numbers[L_reset] = None, 0\n",
    "                current_numbers[level_detected] += 1\n",
    "                current_paths[level_detected] = detected_title.strip()\n",
    "            \n",
    "            doc_metadata = {\"source\": os.path.basename(book_path), \"original_category\": element_category}\n",
    "            # Add raw_text_snippet for debugging if needed\n",
    "            # doc_metadata[\"raw_text_snippet\"] = element_text[:100] \n",
    "            \n",
    "            has_hierarchy_info = False\n",
    "            for L_meta in range(1, 6):\n",
    "                if current_paths[L_meta]:\n",
    "                    doc_metadata[f\"level_{L_meta}_title\"] = current_paths[L_meta]\n",
    "                    # Construct hierarchical numbering like 1.2.1\n",
    "                    num_parts = [str(current_numbers[l_num_meta]) for l_num_meta in range(1, L_meta + 1) if current_paths[l_num_meta]]\n",
    "                    if num_parts:\n",
    "                        doc_metadata[f\"level_{L_meta}_number_hierarchical\"] = \".\".join(num_parts)\n",
    "                    has_hierarchy_info = True\n",
    "            \n",
    "            if not has_hierarchy_info and not final_documents_with_metadata: # First element, no hierarchy yet\n",
    "                 doc_metadata[\"level_1_title\"] = \"Book Introduction or Preamble\"\n",
    "\n",
    "            cleaned_doc_metadata = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata.items() if v is not None}\n",
    "            final_documents_with_metadata.append(Document(page_content=element_text, metadata=cleaned_doc_metadata))\n",
    "    else: \n",
    "        # Fallback for unsupported types, or if no specific processing was done\n",
    "        logger.warning(f\"No specific hierarchical processing for file type {file_extension}. Using basic document loading.\")\n",
    "        final_documents_with_metadata = all_text_elements # Each doc has minimal metadata\n",
    "\n",
    "    if not final_documents_with_metadata:\n",
    "        logger.error(\"No documents were processed or enriched with hierarchical metadata.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=len,\n",
    "        add_start_index=False \n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks for vector store, inheriting hierarchical metadata.\")\n",
    "    \n",
    "    # Log a sample of metadata from the first few chunks\n",
    "    for i, chunk in enumerate(final_chunks[:min(3, len(final_chunks))]):\n",
    "        logger.debug(f\"Sample Chunk {i} Metadata: {chunk.metadata}\")\n",
    "        \n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "\n",
    "# 1. Check if pre-extracted ToC JSON exists\n",
    "if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "    logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "    logger.error(\"Please run your EPUB or PDF ToC extraction script first and ensure the output is at this path.\")\n",
    "    book_hierarchical_toc_loaded = [] \n",
    "    final_chunks_for_db = []\n",
    "else:\n",
    "    # 2. Process the book using the pre-extracted ToC\n",
    "    logger.info(f\"Using book: {BOOK_PATH} and pre-extracted ToC: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "    final_chunks_for_db, book_hierarchical_toc_loaded = process_book_with_extracted_toc(\n",
    "        BOOK_PATH,\n",
    "        PRE_EXTRACTED_TOC_JSON_PATH\n",
    "    )\n",
    "\n",
    "# 3. Save the (loaded) Hierarchical Table of Contents\n",
    "if book_hierarchical_toc_loaded:\n",
    "    try:\n",
    "        with open(OUTPUT_HIERARCHICAL_TOC_JSON, 'w', encoding='utf-8') as f:\n",
    "            json.dump(book_hierarchical_toc_loaded, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Pre-extracted Hierarchical Table of Contents was loaded and re-saved to: {OUTPUT_HIERARCHICAL_TOC_JSON}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error re-saving hierarchical ToC: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"No hierarchical ToC was loaded (or loading failed), so nothing to re-save.\")\n",
    "\n",
    "\n",
    "# 4. Create and Persist the Vector Database\n",
    "if final_chunks_for_db:\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR_HIERARCHICAL):\n",
    "        logger.info(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR_HIERARCHICAL}\")\n",
    "        try:\n",
    "            shutil.rmtree(CHROMA_PERSIST_DIR_HIERARCHICAL)\n",
    "        except OSError as e:\n",
    "            logger.error(f\"Error deleting ChromaDB directory {CHROMA_PERSIST_DIR_HIERARCHICAL}: {e}\", exc_info=True)\n",
    "            # Potentially exit or handle more gracefully if deletion fails critically\n",
    "\n",
    "    logger.info(\"Initializing Ollama embedding model...\")\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        logger.info(f\"Creating/Rebuilding vector database at: {CHROMA_PERSIST_DIR_HIERARCHICAL}\")\n",
    "        \n",
    "        # Ensure metadata is clean one last time\n",
    "        for chunk in final_chunks_for_db:\n",
    "            chunk.metadata = {k: clean_metadata_for_chroma(v) for k, v in chunk.metadata.items()}\n",
    "\n",
    "        # Chroma.from_documents will create and persist the DB if persist_directory is provided\n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=final_chunks_for_db,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=CHROMA_PERSIST_DIR_HIERARCHICAL,\n",
    "            collection_name=CHROMA_COLLECTION_NAME_HIERARCHICAL\n",
    "        )\n",
    "        # REMOVE THE ERRONEOUS LINE: vector_db.persist() \n",
    "        logger.info(f\"Vector database created/updated with {len(final_chunks_for_db)} chunks and persisted to {CHROMA_PERSIST_DIR_HIERARCHICAL}.\")\n",
    "        \n",
    "        # Verify collection count from a new instance (more robust check)\n",
    "        # Ensure the embedding model is also passed when reloading an existing persisted DB\n",
    "        db_reloaded = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR_HIERARCHICAL, \n",
    "            embedding_function=embedding_model, \n",
    "            collection_name=CHROMA_COLLECTION_NAME_HIERARCHICAL\n",
    "        )\n",
    "        collection_count = db_reloaded._collection.count()\n",
    "        logger.info(f\"ChromaDB collection count after reload: {collection_count}\")\n",
    "\n",
    "        if collection_count > 0:\n",
    "            test_query = \"digital evidence\" if IS_EPUB else \"cryptography basics\"\n",
    "            logger.info(f\"Attempting test query: '{test_query}'\")\n",
    "            results = db_reloaded.similarity_search(test_query, k=2)\n",
    "            if results:\n",
    "                logger.info(f\"Found {len(results)} results for test query '{test_query}':\")\n",
    "                for doc_idx, doc in enumerate(results):\n",
    "                    logger.info(f\"  Result {doc_idx+1} Metadata: {doc.metadata}\")\n",
    "                    # logger.info(f\"  Result {doc_idx+1} Content Snippet: {doc.page_content[:200]}...\")\n",
    "            else: \n",
    "                logger.info(f\"Test query '{test_query}' returned no results.\")\n",
    "        else:\n",
    "            logger.warning(\"No documents in collection for test query.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing embedding model or creating/querying vector DB: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"No chunks were generated, so no vector database will be created.\")\n",
    "\n",
    "logger.info(\"Focused script for ToC-guided DB creation finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
