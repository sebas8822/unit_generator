{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9771e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION SUMMARY ---\n",
      "Processing Mode: EPUB\n",
      "Unit ID: ICT312\n",
      "Unit Outline Path: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "Book Path: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "Parsed UO Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "Parsed ToC Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n",
      "Vector DB Path: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "Vector DB Collection: book_toc_guided_chunks_epub_v2\n",
      "--- SETUP COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "EXTRACT_UO = False\n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0852ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a new cell after your imports, or within Cell 3 before the functions.\n",
    "# This code is based on the schema from your screenshot on page 4.\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# Define Pydantic models that match your JSON schema\n",
    "class UnitInformation(BaseModel):\n",
    "    unitCode: Optional[str] = None\n",
    "    unitName: Optional[str] = None\n",
    "    creditPoints: Optional[int] = None\n",
    "    unitRationale: Optional[str] = None\n",
    "    prerequisites: Optional[str] = None\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    taskName: str\n",
    "    description: str\n",
    "    dueWeek: Optional[str] = None\n",
    "    weightingPercent: Optional[int] = None\n",
    "    learningOutcomesAssessed: Optional[str] = None\n",
    "\n",
    "class WeeklyScheduleItem(BaseModel):\n",
    "    week: str\n",
    "    contentTopic: str\n",
    "    requiredReading: Optional[str] = None\n",
    "\n",
    "class ParsedUnitOutline(BaseModel):\n",
    "    unitInformation: UnitInformation\n",
    "    learningOutcomes: List[str]\n",
    "    assessments: List[Assessment]\n",
    "    weeklySchedule: List[WeeklyScheduleItem] \n",
    "    requiredReadings: List[str]\n",
    "    recommendedReadings: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline_robust(\n",
    "    input_filepath: str, \n",
    "    output_filepath: str, \n",
    "    prompt_template: str,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    logger.info(f\"Starting to robustly process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from file: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    client = ollama.Client(host=OLLAMA_HOST)\n",
    "    current_prompt = prompt_template.format(outline_text=outline_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{max_retries} to parse outline.\")\n",
    "        \n",
    "        try:\n",
    "            # Call the LLM\n",
    "            llm_output_str = call_ollama_with_retry(client, current_prompt)\n",
    "            \n",
    "            # Find the JSON blob in the response\n",
    "            json_blob = parse_llm_json_output(llm_output_str) # Your existing helper\n",
    "            if not json_blob:\n",
    "                raise ValueError(\"LLM did not return a parsable JSON object.\")\n",
    "\n",
    "            # *** THE KEY VALIDATION STEP ***\n",
    "            # Try to parse the dictionary into your Pydantic model.\n",
    "            # This will raise a `ValidationError` if keys are wrong, types are wrong, or fields are missing.\n",
    "            parsed_data = ParsedUnitOutline.model_validate(json_blob)\n",
    "            \n",
    "            # If successful, save the validated data and exit the loop\n",
    "            logger.info(\"Successfully validated JSON structure against Pydantic model.\")\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                # Use .model_dump_json() for clean, validated output\n",
    "                f.write(parsed_data.model_dump_json(indent=2)) \n",
    "\n",
    "            logger.info(f\"Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "            return # Exit function on success\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Validation failed on attempt {attempt + 1}. Error: {e}\")\n",
    "            # Formulate a new prompt with the error message for self-correction\n",
    "            error_feedback = (\n",
    "                f\"\\n\\nYour previous attempt failed. You MUST correct the following errors:\\n\"\n",
    "                f\"{e}\\n\\n\"\n",
    "                f\"Please regenerate the entire JSON object, ensuring it strictly adheres to the schema \"\n",
    "                f\"and corrects these specific errors. Do not change any key names.\"\n",
    "            )\n",
    "            current_prompt = current_prompt + error_feedback # Append the error to the prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch other errors like network issues from call_ollama_with_retry\n",
    "            logger.error(f\"An unexpected error occurred on attempt {attempt + 1}: {e}\", exc_info=True)\n",
    "            # You might want to wait before retrying for non-validation errors\n",
    "            time.sleep(5)\n",
    "\n",
    "    logger.error(f\"Failed to get valid structured data from the LLM after {max_retries} attempts.\")\n",
    "\n",
    "\n",
    "# --- In your execution block, call the new function ---\n",
    "# parse_and_save_outline(...) becomes:\n",
    "\n",
    "if EXTRACT_UO:\n",
    "    parse_and_save_outline_robust(\n",
    "        input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "        output_filepath=PARSED_UO_JSON_PATH,\n",
    "        prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or epub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EPUB ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "INFO: Found EPUB 2 (NCX) Table of Contents.\n",
      "✅ Successfully wrote EPUB ToC to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Book Table of Contents (ToC)\n",
    "# This cell extracts the ToC from the specified book (EPUB or PDF)\n",
    "# and saves it to the path defined in Cell 1.\n",
    "\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "\n",
    "# --- EPUB Extraction Logic ---\n",
    "def parse_navpoint(navpoint, level=0):\n",
    "    # (Your existing parse_navpoint function)\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    # Add filtering logic here if needed\n",
    "    node = {\"level\": level, \"title\": title, \"children\": []}\n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, level + 1)\n",
    "        if child_node: node[\"children\"].append(child_node)\n",
    "    return node\n",
    "\n",
    "def parse_li(li_element, level=0):\n",
    "    # (Your existing parse_li function)\n",
    "    a_tag = li_element.find('a')\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        # Add filtering logic here if needed\n",
    "        node = {\"level\": level, \"title\": title, \"children\": []}\n",
    "        nested_ol = li_element.find('ol')\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, level + 1)\n",
    "                if child_node: node[\"children\"].append(child_node)\n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_epub_toc(epub_path, output_json_path):\n",
    "    print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "    toc_data = []\n",
    "    book = epub.read_epub(epub_path)\n",
    "    for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "        soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "        if nav_item.get_name().endswith('.ncx'):\n",
    "            print(\"INFO: Found EPUB 2 (NCX) Table of Contents.\")\n",
    "            navmap = soup.find('navMap')\n",
    "            if navmap:\n",
    "                for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                    node = parse_navpoint(navpoint, level=0)\n",
    "                    if node: toc_data.append(node)\n",
    "        else:\n",
    "            print(\"INFO: Found EPUB 3 (XHTML) Table of Contents.\")\n",
    "            toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "            if toc_nav:\n",
    "                top_ol = toc_nav.find('ol')\n",
    "                if top_ol:\n",
    "                    for li in top_ol.find_all('li', recursive=False):\n",
    "                        node = parse_li(li, level=0)\n",
    "                        if node: toc_data.append(node)\n",
    "        if toc_data: break\n",
    "    \n",
    "    if toc_data:\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote EPUB ToC to: {output_json_path}\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# --- PDF Extraction Logic ---\n",
    "def build_pdf_hierarchy(toc_list):\n",
    "    \"\"\"\n",
    "    Builds a hierarchical structure from a flat ToC list from PyMuPDF.\n",
    "    MODIFIED: Normalizes levels to start at 0 for consistency with EPUB.\n",
    "    \"\"\"\n",
    "    root = []\n",
    "    # The parent_stack keys are now level-based, starting from -1 for the root's parent.\n",
    "    parent_stack = {-1: {\"children\": root}}\n",
    "\n",
    "    for level, title, page in toc_list:\n",
    "        # --- FIX: NORMALIZE LEVEL TO START AT 0 ---\n",
    "        # fitz/PyMuPDF ToC levels start at 1, so we subtract 1.\n",
    "        normalized_level = level - 1\n",
    "\n",
    "        node = {\n",
    "            \"level\": normalized_level,\n",
    "            \"title\": title.strip(),\n",
    "            \"page\": page,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        # Find the correct parent in the stack. The parent's level is one less than the current node's.\n",
    "        # This logic correctly places the node under its parent in the hierarchy.\n",
    "        parent_node = parent_stack[normalized_level - 1]\n",
    "        parent_node[\"children\"].append(node)\n",
    "\n",
    "        # Add the current node to the stack so it can be a parent for subsequent nodes.\n",
    "        parent_stack[normalized_level] = node\n",
    "\n",
    "    return root\n",
    "\n",
    "def extract_pdf_toc(pdf_path, output_json_path):\n",
    "    print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        if not toc:\n",
    "            print(\"❌ WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "            hierarchical_toc = []\n",
    "        else:\n",
    "            print(f\"INFO: Found {len(toc)} bookmark entries.\")\n",
    "            hierarchical_toc = build_pdf_hierarchy(toc)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote PDF ToC to: {output_json_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# --- Execute ToC Extraction ---\n",
    "if PROCESS_EPUB:\n",
    "    extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "else:\n",
    "    extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "effd9e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 20:57:57,274 - INFO - Processing book 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub' using ToC from 'ICT312_epub_table_of_contents.json'.\n",
      "2025-07-01 20:57:57,275 - INFO - Successfully loaded pre-extracted ToC with 28 top-level entries.\n",
      "2025-07-01 20:57:59,343 - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-07-01 20:57:59,344 - INFO - NumExpr defaulting to 16 threads.\n",
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-07-01 20:58:06,901 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-07-01 20:58:06,905 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-07-01 20:58:10,045 - INFO - Loaded 11815 text elements from EPUB.\n",
      "2025-07-01 20:58:10,046 - INFO - Flattened ToC and assigned sequential IDs to 877 entries.\n",
      "2025-07-01 20:58:10,047 - INFO - Assigning metadata to EPUB elements by matching ToC titles in text...\n",
      "2025-07-01 20:58:10,366 - INFO - Total documents prepared for chunking: 11483\n",
      "2025-07-01 20:58:10,530 - INFO - Split into 11774 final chunks, inheriting hierarchical metadata.\n",
      "2025-07-01 20:58:10,530 - INFO - Assigning sequential chunk_id to all final chunks...\n",
      "2025-07-01 20:58:10,532 - INFO - Assigned chunk_ids from 0 to 11773.\n",
      "2025-07-01 20:58:10,537 - INFO - Initializing embedding model 'nomic-embed-text' and creating new vector database...\n",
      "2025-07-01 20:58:10,568 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-07-01 20:59:20,442 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:00:32,720 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:00:45,373 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:00:45,884 - INFO - ✅ Vector DB created successfully at: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "2025-07-01 21:00:45,885 - INFO - ✅ Collection 'book_toc_guided_chunks_epub_v2' contains 11774 documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Hierarchical Vector Database (with Sequential ToC ID and Chunk ID)\n",
    "# This cell processes the book, enriches it with hierarchical and sequential metadata,\n",
    "# chunks it, and creates the final vector database.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper: Clean metadata values for ChromaDB ---\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "    if isinstance(value, list): return \", \".join(map(str, value))\n",
    "    if isinstance(value, dict): return json.dumps(value)\n",
    "    if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    return str(value)\n",
    "\n",
    "# --- Core Function to Process Book with Pre-extracted ToC ---\n",
    "def process_book_with_extracted_toc(\n",
    "    book_path: str,\n",
    "    extracted_toc_json_path: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "    logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "    # 1. Load the pre-extracted hierarchical ToC\n",
    "    try:\n",
    "        with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        if not hierarchical_toc:\n",
    "            logger.error(f\"Pre-extracted ToC at '{extracted_toc_json_path}' is empty or invalid.\")\n",
    "            return [], []\n",
    "        logger.info(f\"Successfully loaded pre-extracted ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pre-extracted ToC JSON: {e}\", exc_info=True)\n",
    "        return [], []\n",
    "\n",
    "    # 2. Load all text elements/pages from the book\n",
    "    all_raw_book_docs: List[Document] = []\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "    if file_extension == \".epub\":\n",
    "        loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} text elements from EPUB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EPUB content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    elif file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} pages from PDF.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    else:\n",
    "        logger.error(f\"Unsupported book file format: {file_extension}\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    if not all_raw_book_docs:\n",
    "        logger.error(\"No text elements/pages loaded from the book.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    # 3. Create enriched LangChain Documents by matching ToC to content\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "    # Flatten the ToC, AND add a unique sequential ID for sorting and validation.\n",
    "    flat_toc_entries: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def _add_ids_and_flatten_recursive(nodes: List[Dict[str, Any]], current_titles_path: List[str], counter: List[int]):\n",
    "        \"\"\"\n",
    "        Recursively traverses ToC nodes to flatten them and assign a unique, sequential toc_id.\n",
    "        \"\"\"\n",
    "        for node in nodes:\n",
    "            toc_id = counter[0]\n",
    "            counter[0] += 1\n",
    "            title = node.get(\"title\", \"\").strip()\n",
    "            if not title: continue\n",
    "            new_titles_path = current_titles_path + [title]\n",
    "            entry = {\n",
    "                \"titles_path\": new_titles_path,\n",
    "                \"level\": node.get(\"level\"),\n",
    "                \"full_title_for_matching\": title,\n",
    "                \"toc_id\": toc_id\n",
    "            }\n",
    "            if \"page\" in node: entry[\"page\"] = node[\"page\"]\n",
    "            flat_toc_entries.append(entry)\n",
    "            if node.get(\"children\"):\n",
    "                _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter)\n",
    "\n",
    "    toc_id_counter = [0]\n",
    "    _add_ids_and_flatten_recursive(hierarchical_toc, [], toc_id_counter)\n",
    "    logger.info(f\"Flattened ToC and assigned sequential IDs to {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "    # Logic for PDF metadata assignment\n",
    "    if file_extension == \".pdf\" and any(\"page\" in entry for entry in flat_toc_entries):\n",
    "        logger.info(\"Assigning metadata to PDF pages based on ToC page numbers...\")\n",
    "        flat_toc_entries.sort(key=lambda x: x.get(\"page\", -1) if x.get(\"page\") is not None else -1)\n",
    "        for page_doc in all_raw_book_docs:\n",
    "            page_num_0_indexed = page_doc.metadata.get(\"page\", -1)\n",
    "            page_num_1_indexed = page_num_0_indexed + 1\n",
    "            assigned_metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "            best_match_toc_entry = None\n",
    "            for toc_entry in flat_toc_entries:\n",
    "                toc_page = toc_entry.get(\"page\")\n",
    "                if toc_page is not None and toc_page <= page_num_1_indexed:\n",
    "                    if best_match_toc_entry is None or toc_page > best_match_toc_entry.get(\"page\", -1):\n",
    "                        best_match_toc_entry = toc_entry\n",
    "                elif toc_page is not None and toc_page > page_num_1_indexed:\n",
    "                    break\n",
    "            if best_match_toc_entry:\n",
    "                for i, title_in_path in enumerate(best_match_toc_entry[\"titles_path\"]):\n",
    "                    assigned_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                assigned_metadata['toc_id'] = best_match_toc_entry.get('toc_id')\n",
    "            else:\n",
    "                assigned_metadata[\"level_1_title\"] = \"Uncategorized PDF Page\"\n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in assigned_metadata.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=cleaned_meta))\n",
    "\n",
    "    # Logic for EPUB metadata assignment\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Assigning metadata to EPUB elements by matching ToC titles in text...\")\n",
    "        toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "        current_hierarchy_metadata = {}\n",
    "        for element_doc in all_raw_book_docs:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            if not element_text: continue\n",
    "            for toc_entry in toc_titles_for_search:\n",
    "                if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "                    current_hierarchy_metadata = {\"source\": os.path.basename(book_path)}\n",
    "                    for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "                        current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                    current_hierarchy_metadata['toc_id'] = toc_entry.get('toc_id')\n",
    "                    if \"page\" in toc_entry: current_hierarchy_metadata[\"epub_toc_page\"] = toc_entry[\"page\"]\n",
    "                    break\n",
    "            if not current_hierarchy_metadata:\n",
    "                doc_metadata_to_assign = {\"source\": os.path.basename(book_path), \"level_1_title\": \"EPUB Preamble\", \"toc_id\": -1}\n",
    "            else:\n",
    "                doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata_to_assign.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=element_text, metadata=cleaned_meta))\n",
    "    \n",
    "    else: # Fallback\n",
    "        final_documents_with_metadata = all_raw_book_docs\n",
    "\n",
    "    if not final_documents_with_metadata:\n",
    "        logger.error(\"No documents were processed or enriched with hierarchical metadata.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks, inheriting hierarchical metadata.\")\n",
    "    \n",
    "    # --- MODIFICATION START: Add a unique, sequential chunk_id to each chunk ---\n",
    "    logger.info(\"Assigning sequential chunk_id to all final chunks...\")\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        chunk.metadata['chunk_id'] = i\n",
    "    logger.info(f\"Assigned chunk_ids from 0 to {len(final_chunks) - 1}.\")\n",
    "    # --- MODIFICATION END ---\n",
    "\n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "# --- Main Execution Block for this Cell ---\n",
    "\n",
    "if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "    logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "    logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "else:\n",
    "    final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "        book_path=BOOK_PATH,\n",
    "        extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if final_chunks_for_db:\n",
    "        if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "            logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "            shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "        logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=final_chunks_for_db,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        reloaded_db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model, collection_name=CHROMA_COLLECTION_NAME)\n",
    "        count = reloaded_db._collection.count()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        logger.info(f\"✅ Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "        logger.info(f\"✅ Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        logger.error(\"❌ Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d38d",
   "metadata": {},
   "source": [
    "### Full Database Health & Hierarchy Diagnostic Report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9902b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 22:02:40,404 - INFO - Connecting to the vector database...\n",
      "2025-07-01 22:02:40,421 - INFO - Successfully connected to the database.\n",
      "2025-07-01 22:02:40,458 - INFO - Retrieving metadata for all 11774 chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "               Full Database Health & Hierarchy Diagnostic Report               \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 22:02:41,229 - INFO - Successfully retrieved all metadata.\n",
      "2025-07-01 22:02:41,229 - INFO - Reconstructing hierarchy from chunk metadata...\n",
      "2025-07-01 22:02:41,239 - INFO - Hierarchy reconstruction complete.\n",
      "2025-07-01 22:02:41,241 - INFO - Verifying chunk order and reassembling content for a random ToC section.\n",
      "2025-07-01 22:02:41,241 - INFO - Selected random section for testing: 'Chapter 4. Processing Crime and Incident Scenes -> Collecting Evidence in Private-Sector Incident Scenes' (toc_id: 147)\n",
      "2025-07-01 22:02:41,249 - INFO - Retrieved 24 document chunks for toc_id 147.\n",
      "2025-07-01 22:02:41,249 - INFO - ✅ TEST PASSED: Chunk IDs for the section are sequential and content is reassembled.\n",
      "2025-07-01 22:02:41,249 - WARNING - Found 21 chunks MISSING a valid 'toc_id'. Check 'Orphaned' sections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                  Reconstructed Hierarchy Report (Book Order)                   \n",
      "--------------------------------------------------------------------------------\n",
      "|-- Preface [ID: 3] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "|-- Introduction [ID: 4] (Total Chuck in branch: 73, Direct Chunk: 73)\n",
      "|-- About the Authors [ID: 5] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "|-- Acknowledgments [ID: 6] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "|-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: 7] (Total Chuck in branch: 4566, Direct Chunk: 23)\n",
      "  |-- An Overview of Digital Forensics [ID: 9] (Total Chuck in branch: 60, Direct Chunk: 18)\n",
      "    |-- Digital Forensics and Other Related Disciplines [ID: 10] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- A Brief History of Digital Forensics [ID: 11] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Understanding Case Law [ID: 12] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Developing Digital Forensics Resources [ID: 13] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Preparing for Digital Investigations [ID: 14] (Total Chuck in branch: 84, Direct Chunk: 5)\n",
      "    |-- Understanding Law Enforcement Agency Investigations [ID: 15] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Following Legal Processes [ID: 16] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Understanding Private-Sector Investigations [ID: 17] (Total Chuck in branch: 56, Direct Chunk: 3)\n",
      "      |-- Establishing Company Policies [ID: 18] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- Displaying Warning Banners [ID: 19] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "      |-- Designating an Authorized Requester [ID: 20] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "      |-- Conducting Security Investigations [ID: 21] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "      |-- Distinguishing Personal and Company Property [ID: 22] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "  |-- Maintaining Professional Conduct [ID: 23] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Preparing a Digital Forensics Investigation [ID: 24] (Total Chuck in branch: 97, Direct Chunk: 4)\n",
      "    |-- An Overview of a Computer Crime [ID: 25] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- An Overview of a Company Policy Violation [ID: 26] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Taking a Systematic Approach [ID: 27] (Total Chuck in branch: 77, Direct Chunk: 16)\n",
      "      |-- Assessing the Case [ID: 28] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "      |-- Planning Your Investigation [ID: 29] (Total Chuck in branch: 41, Direct Chunk: 41)\n",
      "      |-- Securing Your Evidence [ID: 30] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Procedures for Private-Sector High-Tech Investigations [ID: 31] (Total Chuck in branch: 124, Direct Chunk: 2)\n",
      "    |-- Employee Termination Cases [ID: 32] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Internet Abuse Investigations [ID: 33] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "    |-- E-mail Abuse Investigations [ID: 34] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "    |-- Attorney-Client Privilege Investigations [ID: 35] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "    |-- Industrial Espionage Investigations [ID: 36] (Total Chuck in branch: 52, Direct Chunk: 41)\n",
      "      |-- Interviews and Interrogations in High-Tech Investigations [ID: 37] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Understanding Data Recovery Workstations and Software [ID: 38] (Total Chuck in branch: 37, Direct Chunk: 18)\n",
      "    |-- Setting Up Your Workstation for Digital Forensics [ID: 39] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "  |-- Conducting an Investigation [ID: 40] (Total Chuck in branch: 109, Direct Chunk: 8)\n",
      "    |-- Gathering the Evidence [ID: 41] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Understanding Bit-stream Copies [ID: 42] (Total Chuck in branch: 8, Direct Chunk: 6)\n",
      "      |-- Acquiring an Image of Evidence Media [ID: 43] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Analyzing Your Digital Evidence [ID: 44] (Total Chuck in branch: 48, Direct Chunk: 44)\n",
      "      |-- Some Additional Features of Autopsy [ID: 45] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Completing the Case [ID: 46] (Total Chuck in branch: 22, Direct Chunk: 12)\n",
      "      |-- Autopsy’s Report Generator [ID: 47] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Critiquing the Case [ID: 48] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Chapter Review [ID: inf] (Total Chuck in branch: 4022, Direct Chunk: 0)\n",
      "    |-- Chapter Summary [ID: 50] (Total Chuck in branch: 211, Direct Chunk: 211)\n",
      "    |-- Key Terms [ID: 51] (Total Chuck in branch: 309, Direct Chunk: 309)\n",
      "    |-- Review Questions [ID: 52] (Total Chuck in branch: 1785, Direct Chunk: 1785)\n",
      "    |-- Hands-On Projects [ID: 53] (Total Chuck in branch: 1527, Direct Chunk: 1527)\n",
      "    |-- Case Projects [ID: 54] (Total Chuck in branch: 190, Direct Chunk: 190)\n",
      "|-- Chapter 2. The Investigator’s Office and Laboratory [ID: 55] (Total Chuck in branch: 331, Direct Chunk: 22)\n",
      "  |-- Understanding Forensics Lab Accreditation Requirements [ID: 57] (Total Chuck in branch: 86, Direct Chunk: 7)\n",
      "    |-- Identifying Duties of the Lab Manager and Staff [ID: 58] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Lab Budget Planning [ID: 59] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Acquiring Certification and Training [ID: 60] (Total Chuck in branch: 54, Direct Chunk: 4)\n",
      "      |-- International Association of Computer Investigative Specialists [ID: 61] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "      |-- ISC2 Certified Cyber Forensics Professional [ID: 62] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- High Tech Crime Network [ID: 63] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "      |-- EnCase Certified Examiner Certification [ID: 64] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- AccessData Certified Examiner [ID: 65] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Other Training and Certifications [ID: 66] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "  |-- Determining the Physical Requirements for a Digital Forensics Lab [ID: 67] (Total Chuck in branch: 68, Direct Chunk: 3)\n",
      "    |-- Identifying Lab Security Needs [ID: 68] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Conducting High-Risk Investigations [ID: 69] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Using Evidence Containers [ID: 70] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "    |-- Overseeing Facility Maintenance [ID: 71] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Considering Physical Security Needs [ID: 72] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Auditing a Digital Forensics Lab [ID: 73] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Determining Floor Plans for Digital Forensics Labs [ID: 74] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Selecting a Basic Forensic Workstation [ID: 75] (Total Chuck in branch: 51, Direct Chunk: 2)\n",
      "    |-- Selecting Workstations for a Lab [ID: 76] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Selecting Workstations for Private-Sector Labs [ID: 77] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Stocking Hardware Peripherals [ID: 78] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Maintaining Operating Systems and Software Inventories [ID: 79] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Using a Disaster Recovery Plan [ID: 80] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Planning for Equipment Upgrades [ID: 81] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Building a Business Case for Developing a Forensics Lab [ID: 82] (Total Chuck in branch: 104, Direct Chunk: 11)\n",
      "    |-- Preparing a Business Case for a Digital Forensics Lab [ID: 83] (Total Chuck in branch: 93, Direct Chunk: 2)\n",
      "      |-- Justification [ID: 84] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Budget Development [ID: 85] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Facility Cost [ID: 86] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "      |-- Hardware Requirements [ID: 87] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "      |-- Software Requirements [ID: 88] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "      |-- Miscellaneous Budget Needs [ID: 89] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Approval and Acquisition [ID: 90] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Implementation [ID: 91] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Acceptance Testing [ID: 92] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Correction for Acceptance [ID: 93] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Production [ID: 94] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "|-- Chapter 3. Data Acquisition [ID: 101] (Total Chuck in branch: 390, Direct Chunk: 28)\n",
      "  |-- Understanding Storage Formats for Digital Evidence [ID: 103] (Total Chuck in branch: 31, Direct Chunk: 5)\n",
      "    |-- Raw Format [ID: 104] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Proprietary Formats [ID: 105] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "    |-- Advanced Forensic Format [ID: 106] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Determining the Best Acquisition Method [ID: 107] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "  |-- Contingency Planning for Image Acquisitions [ID: 108] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Using Acquisition Tools [ID: 109] (Total Chuck in branch: 173, Direct Chunk: 5)\n",
      "    |-- Mini-WinFE Boot CDs and USB Drives [ID: 110] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Acquiring Data with a Linux Boot CD [ID: 111] (Total Chuck in branch: 113, Direct Chunk: 5)\n",
      "      |-- Using Linux Live CD Distributions [ID: 112] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "      |-- Preparing a Target Drive for Acquisition in Linux [ID: 113] (Total Chuck in branch: 45, Direct Chunk: 45)\n",
      "      |-- Acquiring Data with dd in Linux [ID: 114] (Total Chuck in branch: 32, Direct Chunk: 32)\n",
      "      |-- Acquiring Data with dcfldd in Linux [ID: 115] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Capturing an Image with AccessData FTK Imager Lite [ID: 116] (Total Chuck in branch: 46, Direct Chunk: 46)\n",
      "  |-- Validating Data Acquisitions [ID: 117] (Total Chuck in branch: 32, Direct Chunk: 5)\n",
      "    |-- Linux Validation Methods [ID: 118] (Total Chuck in branch: 21, Direct Chunk: 3)\n",
      "      |-- Validating dd-Acquired Data [ID: 119] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "      |-- Validating dcfldd-Acquired Data [ID: 120] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Windows Validation Methods [ID: 121] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Performing RAID Data Acquisitions [ID: 122] (Total Chuck in branch: 30, Direct Chunk: 2)\n",
      "    |-- Understanding RAID [ID: 123] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Acquiring RAID Disks [ID: 124] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "  |-- Using Remote Network Acquisition Tools [ID: 125] (Total Chuck in branch: 39, Direct Chunk: 5)\n",
      "    |-- Remote Acquisition with ProDiscover [ID: 126] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Remote Acquisition with EnCase Enterprise [ID: 127] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Remote Acquisition with R-Tools R-Studio [ID: 128] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Remote Acquisition with WetStone US-LATT PRO [ID: 129] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Remote Acquisition with F-Response [ID: 130] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Using Other Forensics Acquisition Tools [ID: 131] (Total Chuck in branch: 27, Direct Chunk: 2)\n",
      "    |-- PassMark Software ImageUSB [ID: 132] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- ASR Data SMART [ID: 133] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Runtime Software [ID: 134] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- ILookIX IXImager [ID: 135] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- SourceForge [ID: 136] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "|-- Chapter 4. Processing Crime and Incident Scenes [ID: 143] (Total Chuck in branch: 413, Direct Chunk: 29)\n",
      "  |-- Identifying Digital Evidence [ID: 145] (Total Chuck in branch: 76, Direct Chunk: 13)\n",
      "    |-- Understanding Rules of Evidence [ID: 146] (Total Chuck in branch: 63, Direct Chunk: 63)\n",
      "  |-- Collecting Evidence in Private-Sector Incident Scenes [ID: 147] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "  |-- Processing Law Enforcement Crime Scenes [ID: 148] (Total Chuck in branch: 24, Direct Chunk: 6)\n",
      "    |-- Understanding Concepts and Terms Used in Warrants [ID: 149] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "  |-- Preparing for a Search [ID: 150] (Total Chuck in branch: 40, Direct Chunk: 2)\n",
      "    |-- Identifying the Nature of the Case [ID: 151] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Identifying the Type of OS or Digital Device [ID: 152] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Determining Whether You Can Seize Computers and Digital Devices [ID: 153] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Getting a Detailed Description of the Location [ID: 154] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Determining Who Is in Charge [ID: 155] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Using Additional Technical Expertise [ID: 156] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Determining the Tools You Need [ID: 157] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "    |-- Preparing the Investigation Team [ID: 158] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Securing a Digital Incident or Crime Scene [ID: 159] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Seizing Digital Evidence at the Scene [ID: 160] (Total Chuck in branch: 72, Direct Chunk: 4)\n",
      "    |-- Preparing to Acquire Digital Evidence [ID: 161] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Processing Incident or Crime Scenes [ID: 162] (Total Chuck in branch: 34, Direct Chunk: 34)\n",
      "    |-- Processing Data Centers with RAID Systems [ID: 163] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Using a Technical Advisor [ID: 164] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Documenting Evidence in the Lab [ID: 165] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Processing and Handling Digital Evidence [ID: 166] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Storing Digital Evidence [ID: 167] (Total Chuck in branch: 18, Direct Chunk: 7)\n",
      "    |-- Evidence Retention and Media Storage Needs [ID: 168] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Documenting Evidence [ID: 169] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Obtaining a Digital Hash [ID: 170] (Total Chuck in branch: 42, Direct Chunk: 42)\n",
      "  |-- Reviewing a Case [ID: 171] (Total Chuck in branch: 79, Direct Chunk: 8)\n",
      "    |-- Sample Civil Investigation [ID: 172] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "    |-- An Example of a Criminal Investigation [ID: 173] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Reviewing Background Information for a Case [ID: 174] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Planning the Investigation [ID: 175] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Conducting the Investigation: Acquiring Evidence with OSForensics [ID: 176] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "|-- Chapter 5. Working with Windows and CLI Systems [ID: 183] (Total Chuck in branch: 471, Direct Chunk: 22)\n",
      "  |-- Understanding File Systems [ID: 185] (Total Chuck in branch: 33, Direct Chunk: 3)\n",
      "    |-- Understanding the Boot Sequence [ID: 186] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Understanding Disk Drives [ID: 187] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Solid-State Storage Devices [ID: 188] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Exploring Microsoft File Structures [ID: 189] (Total Chuck in branch: 71, Direct Chunk: 5)\n",
      "    |-- Disk Partitions [ID: 190] (Total Chuck in branch: 39, Direct Chunk: 39)\n",
      "    |-- Examining FAT Disks [ID: 191] (Total Chuck in branch: 27, Direct Chunk: 24)\n",
      "      |-- Deleting FAT Files [ID: 192] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Examining NTFS Disks [ID: 193] (Total Chuck in branch: 168, Direct Chunk: 14)\n",
      "    |-- NTFS System Files [ID: 194] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- MFT and File Attributes [ID: 195] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- MFT Structures for File Data [ID: 196] (Total Chuck in branch: 69, Direct Chunk: 3)\n",
      "      |-- MFT Header Fields [ID: 197] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "      |-- Attribute 0x10: Standard Information [ID: 198] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Attribute 0x30: File Name [ID: 199] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "      |-- Attribute 0x40: Object_ID [ID: 200] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "      |-- Attribute 0x80: Data for a Resident File [ID: 201] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Attribute 0x80: Data for a Nonresident File [ID: 202] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Interpreting a Data Run [ID: 203] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- NTFS Alternate Data Streams [ID: 204] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- NTFS Compressed Files [ID: 205] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- NTFS Encrypting File System [ID: 206] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- EFS Recovery Key Agent [ID: 207] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Deleting NTFS Files [ID: 208] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Resilient File System [ID: 209] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Understanding Whole Disk Encryption [ID: 210] (Total Chuck in branch: 26, Direct Chunk: 11)\n",
      "    |-- Examining Microsoft BitLocker [ID: 211] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Examining Third-Party Disk Encryption Tools [ID: 212] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Understanding the Windows Registry [ID: 213] (Total Chuck in branch: 56, Direct Chunk: 9)\n",
      "    |-- Exploring the Organization of the Windows Registry [ID: 214] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Examining the Windows Registry [ID: 215] (Total Chuck in branch: 29, Direct Chunk: 29)\n",
      "  |-- Understanding Microsoft Startup Tasks [ID: 216] (Total Chuck in branch: 47, Direct Chunk: 3)\n",
      "    |-- Startup in Windows 7, Windows 8, and Windows 10 [ID: 217] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Startup in Windows NT and Later [ID: 218] (Total Chuck in branch: 39, Direct Chunk: 10)\n",
      "      |-- Startup Files for Windows Vista [ID: 219] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Startup Files for Windows XP [ID: 220] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "      |-- Windows XP System Files [ID: 221] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Contamination Concerns with Windows XP [ID: 222] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "  |-- Understanding Virtual Machines [ID: 223] (Total Chuck in branch: 48, Direct Chunk: 10)\n",
      "    |-- Creating a Virtual Machine [ID: 224] (Total Chuck in branch: 38, Direct Chunk: 38)\n",
      "|-- Chapter 6. Current Digital Forensics Tools [ID: 231] (Total Chuck in branch: 315, Direct Chunk: 22)\n",
      "  |-- Evaluating Digital Forensics Tool Needs [ID: 233] (Total Chuck in branch: 184, Direct Chunk: 10)\n",
      "    |-- Types of Digital Forensics Tools [ID: 234] (Total Chuck in branch: 11, Direct Chunk: 4)\n",
      "      |-- Hardware Forensics Tools [ID: 235] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Software Forensics Tools [ID: 236] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Tasks Performed by Digital Forensics Tools [ID: 237] (Total Chuck in branch: 153, Direct Chunk: 3)\n",
      "      |-- Acquisition [ID: 238] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "      |-- Validation and Verification [ID: 239] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "      |-- Extraction [ID: 240] (Total Chuck in branch: 25, Direct Chunk: 25)\n",
      "      |-- Reconstruction [ID: 241] (Total Chuck in branch: 66, Direct Chunk: 66)\n",
      "      |-- Reporting [ID: 242] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "    |-- Tool Comparisons [ID: 243] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Other Considerations for Tools [ID: 244] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Digital Forensics Software Tools [ID: 245] (Total Chuck in branch: 41, Direct Chunk: 4)\n",
      "    |-- Command-Line Forensics Tools [ID: 246] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Linux Forensics Tools [ID: 247] (Total Chuck in branch: 19, Direct Chunk: 4)\n",
      "      |-- Smart [ID: 248] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Helix 3 [ID: 249] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Kali Linux [ID: 250] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Autopsy and Sleuth Kit [ID: 251] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Forcepoint Threat Protection [ID: 252] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Other GUI Forensics Tools [ID: 253] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Digital Forensics Hardware Tools [ID: 254] (Total Chuck in branch: 38, Direct Chunk: 3)\n",
      "    |-- Forensic Workstations [ID: 255] (Total Chuck in branch: 13, Direct Chunk: 7)\n",
      "      |-- Building Your Own Workstation [ID: 256] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Using a Write-Blocker [ID: 257] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "    |-- Recommendations for a Forensic Workstation [ID: 258] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "  |-- Validating and Testing Forensics Software [ID: 259] (Total Chuck in branch: 30, Direct Chunk: 2)\n",
      "    |-- Using National Institute of Standards and Technology Tools [ID: 260] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Using Validation Protocols [ID: 261] (Total Chuck in branch: 15, Direct Chunk: 6)\n",
      "      |-- Digital Forensics Examination Protocol [ID: 262] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- Digital Forensics Tool Upgrade Protocol [ID: 263] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "|-- Chapter 7. Linux and Macintosh File Systems [ID: 270] (Total Chuck in branch: 274, Direct Chunk: 17)\n",
      "  |-- Examining Linux File Structures [ID: 272] (Total Chuck in branch: 131, Direct Chunk: 77)\n",
      "    |-- File Structures in Ext4 [ID: 273] (Total Chuck in branch: 54, Direct Chunk: 8)\n",
      "      |-- Inodes [ID: 274] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "      |-- Hard Links and Symbolic Links [ID: 275] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "  |-- Understanding Macintosh File Structures [ID: 276] (Total Chuck in branch: 58, Direct Chunk: 6)\n",
      "    |-- An Overview of Mac File Structures [ID: 277] (Total Chuck in branch: 23, Direct Chunk: 23)\n",
      "    |-- Forensics Procedures in Mac [ID: 278] (Total Chuck in branch: 29, Direct Chunk: 18)\n",
      "      |-- Acquisition Methods in macOS [ID: 279] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Using Linux Forensics Tools [ID: 280] (Total Chuck in branch: 68, Direct Chunk: 5)\n",
      "    |-- Installing Sleuth Kit and Autopsy [ID: 281] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "    |-- Examining a Case with Sleuth Kit and Autopsy [ID: 282] (Total Chuck in branch: 42, Direct Chunk: 42)\n",
      "|-- Chapter 8. Recovering Graphics Files [ID: 289] (Total Chuck in branch: 240, Direct Chunk: 19)\n",
      "  |-- Recognizing a Graphics File [ID: 291] (Total Chuck in branch: 54, Direct Chunk: 4)\n",
      "    |-- Understanding Bitmap and Raster Images [ID: 292] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Understanding Vector Graphics [ID: 293] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Understanding Metafile Graphics [ID: 294] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Understanding Graphics File Formats [ID: 295] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Understanding Digital Photograph File Formats [ID: 296] (Total Chuck in branch: 19, Direct Chunk: 2)\n",
      "      |-- Examining the Raw File Format [ID: 297] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- Examining the Exchangeable Image File Format [ID: 298] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "  |-- Understanding Data Compression [ID: 299] (Total Chuck in branch: 101, Direct Chunk: 2)\n",
      "    |-- Lossless and Lossy Compression [ID: 300] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Locating and Recovering Graphics Files [ID: 301] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Identifying Graphics File Fragments [ID: 302] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Repairing Damaged Headers [ID: 303] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Searching for and Carving Data from Unallocated Space [ID: 304] (Total Chuck in branch: 39, Direct Chunk: 9)\n",
      "      |-- Planning Your Examination [ID: 305] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Searching for and Recovering Digital Photograph Evidence [ID: 306] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "    |-- Rebuilding File Headers [ID: 307] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "    |-- Reconstructing File Fragments [ID: 308] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "  |-- Identifying Unknown File Formats [ID: 309] (Total Chuck in branch: 47, Direct Chunk: 14)\n",
      "    |-- Analyzing Graphics File Headers [ID: 310] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Tools for Viewing Images [ID: 311] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Understanding Steganography in Graphics Files [ID: 312] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "    |-- Using Steganalysis Tools [ID: 313] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Understanding Copyright Issues with Graphics [ID: 314] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "|-- Chapter 9. Digital Forensics Analysis and Validation [ID: 321] (Total Chuck in branch: 248, Direct Chunk: 16)\n",
      "  |-- Determining What Data to Collect and Analyze [ID: 323] (Total Chuck in branch: 99, Direct Chunk: 6)\n",
      "    |-- Approaching Digital Forensics Cases [ID: 324] (Total Chuck in branch: 35, Direct Chunk: 30)\n",
      "      |-- Refining and Modifying the Investigation Plan [ID: 325] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Using Autopsy to Validate Data [ID: 326] (Total Chuck in branch: 23, Direct Chunk: 8)\n",
      "      |-- Installing NSRL Hashes in Autopsy [ID: 327] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Collecting Hash Values in Autopsy [ID: 328] (Total Chuck in branch: 35, Direct Chunk: 35)\n",
      "  |-- Validating Forensic Data [ID: 329] (Total Chuck in branch: 51, Direct Chunk: 3)\n",
      "    |-- Validating with Hexadecimal Editors [ID: 330] (Total Chuck in branch: 31, Direct Chunk: 28)\n",
      "      |-- Using Hash Values to Discriminate Data [ID: 331] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Validating with Digital Forensics Tools [ID: 332] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "  |-- Addressing Data-Hiding Techniques [ID: 333] (Total Chuck in branch: 82, Direct Chunk: 2)\n",
      "    |-- Hiding Files by Using the OS [ID: 334] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Hiding Partitions [ID: 335] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Marking Bad Clusters [ID: 336] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Bit-Shifting [ID: 337] (Total Chuck in branch: 34, Direct Chunk: 34)\n",
      "    |-- Understanding Steganalysis Methods [ID: 338] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Examining Encrypted Files [ID: 339] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Recovering Passwords [ID: 340] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "|-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: 347] (Total Chuck in branch: 287, Direct Chunk: 17)\n",
      "  |-- An Overview of Virtual Machine Forensics [ID: 349] (Total Chuck in branch: 176, Direct Chunk: 7)\n",
      "    |-- Type 2 Hypervisors [ID: 350] (Total Chuck in branch: 32, Direct Chunk: 6)\n",
      "      |-- Parallels Desktop [ID: 351] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- KVM [ID: 352] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Microsoft Hyper-V [ID: 353] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- VMware Workstation and Workstation Player [ID: 354] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "      |-- VirtualBox [ID: 355] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Conducting an Investigation with Type 2 Hypervisors [ID: 356] (Total Chuck in branch: 103, Direct Chunk: 70)\n",
      "      |-- Other VM Examination Methods [ID: 357] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "      |-- Using VMs as Forensics Tools [ID: 358] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Working with Type 1 Hypervisors [ID: 359] (Total Chuck in branch: 34, Direct Chunk: 34)\n",
      "  |-- Performing Live Acquisitions [ID: 360] (Total Chuck in branch: 18, Direct Chunk: 15)\n",
      "    |-- Performing a Live Acquisition in Windows [ID: 361] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Network Forensics Overview [ID: 362] (Total Chuck in branch: 76, Direct Chunk: 4)\n",
      "    |-- The Need for Established Procedures [ID: 363] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Securing a Network [ID: 364] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Developing Procedures for Network Forensics [ID: 365] (Total Chuck in branch: 41, Direct Chunk: 8)\n",
      "      |-- Reviewing Network Logs [ID: 366] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "      |-- Using Network Tools [ID: 367] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "      |-- Using Packet Analyzers [ID: 368] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Investigating Virtual Networks [ID: 369] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Examining the Honeynet Project [ID: 370] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "|-- Chapter 11. E-mail and Social Media Investigations [ID: 377] (Total Chuck in branch: 302, Direct Chunk: 20)\n",
      "  |-- Exploring the Role of E-mail in Investigations [ID: 379] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Exploring the Roles of the Client and Server in E-mail [ID: 380] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Investigating E-mail Crimes and Violations [ID: 381] (Total Chuck in branch: 101, Direct Chunk: 4)\n",
      "    |-- Understanding Forensic Linguistics [ID: 382] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Examining E-mail Messages [ID: 383] (Total Chuck in branch: 28, Direct Chunk: 8)\n",
      "      |-- Copying an E-mail Message [ID: 384] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "    |-- Viewing E-mail Headers [ID: 385] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "    |-- Examining E-mail Headers [ID: 386] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Examining Additional E-mail Files [ID: 387] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Tracing an E-mail Message [ID: 388] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- Using Network E-mail Logs [ID: 389] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "  |-- Understanding E-mail Servers [ID: 390] (Total Chuck in branch: 33, Direct Chunk: 13)\n",
      "    |-- Examining UNIX E-mail Server Logs [ID: 391] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Examining Microsoft E-mail Server Logs [ID: 392] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Using Specialized E-mail Forensics Tools [ID: 393] (Total Chuck in branch: 91, Direct Chunk: 22)\n",
      "    |-- Using Magnet AXIOM to Recover E-mail [ID: 394] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Using a Hex Editor to Carve E-mail Messages [ID: 395] (Total Chuck in branch: 41, Direct Chunk: 41)\n",
      "    |-- Recovering Outlook Files [ID: 396] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "    |-- E-mail Case Studies [ID: 397] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "  |-- Applying Digital Forensics Methods to Social Media Communications [ID: 398] (Total Chuck in branch: 36, Direct Chunk: 23)\n",
      "    |-- Forensics Tools for Social Media Investigations [ID: 399] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "|-- Chapter 12. Mobile Device Forensics and the Internet of Anything [ID: 406] (Total Chuck in branch: 169, Direct Chunk: 8)\n",
      "  |-- Understanding Mobile Device Forensics [ID: 408] (Total Chuck in branch: 65, Direct Chunk: 19)\n",
      "    |-- Mobile Phone Basics [ID: 409] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "    |-- Inside Mobile Devices [ID: 410] (Total Chuck in branch: 22, Direct Chunk: 11)\n",
      "      |-- SIM Cards [ID: 411] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "  |-- Understanding Acquisition Procedures for Mobile Devices [ID: 412] (Total Chuck in branch: 75, Direct Chunk: 30)\n",
      "    |-- Mobile Forensics Equipment [ID: 413] (Total Chuck in branch: 30, Direct Chunk: 6)\n",
      "      |-- SIM Card Readers [ID: 414] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "      |-- Mobile Phone Forensics Tools and Methods [ID: 415] (Total Chuck in branch: 17, Direct Chunk: 17)\n",
      "    |-- Using Mobile Forensics Tools [ID: 416] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "  |-- Understanding Forensics in the Internet of Anything [ID: 417] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "|-- Chapter 13. Cloud Forensics [ID: 424] (Total Chuck in branch: 290, Direct Chunk: 20)\n",
      "  |-- An Overview of Cloud Computing [ID: 426] (Total Chuck in branch: 42, Direct Chunk: 2)\n",
      "    |-- History of the Cloud [ID: 427] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- Cloud Service Levels and Deployment Methods [ID: 428] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "    |-- Cloud Vendors [ID: 429] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Basic Concepts of Cloud Forensics [ID: 430] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Legal Challenges in Cloud Forensics [ID: 431] (Total Chuck in branch: 56, Direct Chunk: 2)\n",
      "    |-- Service Level Agreements [ID: 432] (Total Chuck in branch: 25, Direct Chunk: 17)\n",
      "      |-- Policies, Standards, and Guidelines for CSPs [ID: 433] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "      |-- CSP Processes and Procedures [ID: 434] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Jurisdiction Issues [ID: 435] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- Accessing Evidence in the Cloud [ID: 436] (Total Chuck in branch: 23, Direct Chunk: 3)\n",
      "      |-- Search Warrants [ID: 437] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "      |-- Subpoenas and Court Orders [ID: 438] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "  |-- Technical Challenges in Cloud Forensics [ID: 439] (Total Chuck in branch: 33, Direct Chunk: 5)\n",
      "    |-- Architecture [ID: 440] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Analysis of Cloud Forensic Data [ID: 441] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Anti-Forensics [ID: 442] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Incident First Responders [ID: 443] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Role Management [ID: 444] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Standards and Training [ID: 445] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Acquisitions in the Cloud [ID: 446] (Total Chuck in branch: 21, Direct Chunk: 8)\n",
      "    |-- Encryption in the Cloud [ID: 447] (Total Chuck in branch: 13, Direct Chunk: 13)\n",
      "  |-- Conducting a Cloud Investigation [ID: 448] (Total Chuck in branch: 105, Direct Chunk: 2)\n",
      "    |-- Investigating CSPs [ID: 449] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Investigating Cloud Customers [ID: 450] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Understanding Prefetch Files [ID: 451] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Examining Stored Cloud Data on a PC [ID: 452] (Total Chuck in branch: 63, Direct Chunk: 5)\n",
      "      |-- Dropbox [ID: 453] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "      |-- Google Drive [ID: 454] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "      |-- OneDrive [ID: 455] (Total Chuck in branch: 29, Direct Chunk: 29)\n",
      "    |-- Windows Prefetch Artifacts [ID: 456] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "  |-- Tools for Cloud Forensics [ID: 457] (Total Chuck in branch: 13, Direct Chunk: 5)\n",
      "    |-- Forensic Open-Stack Tools [ID: 458] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "    |-- F-Response for the Cloud [ID: 459] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "    |-- Magnet AXIOM Cloud [ID: 460] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "|-- Chapter 14. Report Writing for High-Tech Investigations [ID: 467] (Total Chuck in branch: 263, Direct Chunk: 16)\n",
      "  |-- Understanding the Importance of Reports [ID: 469] (Total Chuck in branch: 31, Direct Chunk: 19)\n",
      "    |-- Limiting a Report to Specifics [ID: 470] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Types of Reports [ID: 471] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "  |-- Guidelines for Writing Reports [ID: 472] (Total Chuck in branch: 138, Direct Chunk: 19)\n",
      "    |-- What to Include in Written Preliminary Reports [ID: 473] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Report Structure [ID: 474] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Writing Reports Clearly [ID: 475] (Total Chuck in branch: 17, Direct Chunk: 8)\n",
      "      |-- Considering Writing Style [ID: 476] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Including Signposts [ID: 477] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Designing the Layout and Presentation of Reports [ID: 478] (Total Chuck in branch: 85, Direct Chunk: 44)\n",
      "      |-- Providing Supporting Material [ID: 479] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Formatting Consistently [ID: 480] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Explaining Examination and Data Collection Methods [ID: 481] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Including Calculations [ID: 482] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Providing for Uncertainty and Error Analysis [ID: 483] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "      |-- Explaining Results and Conclusions [ID: 484] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "      |-- Providing References [ID: 485] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "      |-- Including Appendixes [ID: 486] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Generating Report Findings with Forensics Software Tools [ID: 487] (Total Chuck in branch: 78, Direct Chunk: 2)\n",
      "    |-- Using Autopsy to Generate Reports [ID: 488] (Total Chuck in branch: 76, Direct Chunk: 76)\n",
      "|-- Chapter 15. Expert Testimony in Digital Investigations [ID: 495] (Total Chuck in branch: 329, Direct Chunk: 14)\n",
      "  |-- Preparing for Testimony [ID: 497] (Total Chuck in branch: 62, Direct Chunk: 15)\n",
      "    |-- Documenting and Preparing Evidence [ID: 498] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Reviewing Your Role as a Consulting Expert or an Expert Witness [ID: 499] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Creating and Maintaining Your CV [ID: 500] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "    |-- Preparing Technical Definitions [ID: 501] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Preparing to Deal with the News Media [ID: 502] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Testifying in Court [ID: 503] (Total Chuck in branch: 155, Direct Chunk: 2)\n",
      "    |-- Understanding the Trial Process [ID: 504] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "    |-- Providing Qualifications for Your Testimony [ID: 505] (Total Chuck in branch: 59, Direct Chunk: 59)\n",
      "    |-- General Guidelines on Testifying [ID: 506] (Total Chuck in branch: 47, Direct Chunk: 27)\n",
      "      |-- Using Graphics During Testimony [ID: 507] (Total Chuck in branch: 10, Direct Chunk: 10)\n",
      "      |-- Avoiding Testimony Problems [ID: 508] (Total Chuck in branch: 7, Direct Chunk: 7)\n",
      "      |-- Understanding Prosecutorial Misconduct [ID: 509] (Total Chuck in branch: 3, Direct Chunk: 3)\n",
      "    |-- Testifying During Direct Examination [ID: 510] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "    |-- Testifying During Cross-Examination [ID: 511] (Total Chuck in branch: 25, Direct Chunk: 25)\n",
      "  |-- Preparing for a Deposition or Hearing [ID: 512] (Total Chuck in branch: 33, Direct Chunk: 6)\n",
      "    |-- Guidelines for Testifying at Depositions [ID: 513] (Total Chuck in branch: 19, Direct Chunk: 10)\n",
      "      |-- Recognizing Deposition Problems [ID: 514] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Guidelines for Testifying at Hearings [ID: 515] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "  |-- Preparing Forensics Evidence for Testimony [ID: 516] (Total Chuck in branch: 65, Direct Chunk: 34)\n",
      "    |-- Preparing a Defense of Your Evidence-Collection Methods [ID: 517] (Total Chuck in branch: 31, Direct Chunk: 31)\n",
      "|-- Chapter 16. Ethics for the Expert Witness [ID: 524] (Total Chuck in branch: 310, Direct Chunk: 13)\n",
      "  |-- Applying Ethics and Codes to Expert Witnesses [ID: 526] (Total Chuck in branch: 58, Direct Chunk: 11)\n",
      "    |-- Forensics Examiners’ Roles in Testifying [ID: 527] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- Considerations in Disqualification [ID: 528] (Total Chuck in branch: 22, Direct Chunk: 22)\n",
      "    |-- Traps for Unwary Experts [ID: 529] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "    |-- Determining Admissibility of Evidence [ID: 530] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- Organizations with Codes of Ethics [ID: 531] (Total Chuck in branch: 26, Direct Chunk: 2)\n",
      "    |-- International Society of Forensic Computer Examiners [ID: 532] (Total Chuck in branch: 11, Direct Chunk: 11)\n",
      "    |-- International High Technology Crime Investigation Association [ID: 533] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "    |-- American Bar Association [ID: 535] (Total Chuck in branch: 5, Direct Chunk: 5)\n",
      "    |-- American Psychological Association [ID: 536] (Total Chuck in branch: 2, Direct Chunk: 2)\n",
      "  |-- Ethical Difficulties in Expert Testimony [ID: 537] (Total Chuck in branch: 19, Direct Chunk: 6)\n",
      "    |-- Ethical Responsibilities Owed to You [ID: 538] (Total Chuck in branch: 9, Direct Chunk: 9)\n",
      "    |-- Standard Forensics Tools and Tools You Create [ID: 539] (Total Chuck in branch: 4, Direct Chunk: 4)\n",
      "  |-- An Ethics Exercise [ID: 540] (Total Chuck in branch: 194, Direct Chunk: 4)\n",
      "    |-- Performing a Cursory Exam of a Forensic Image [ID: 541] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "    |-- Performing a Detailed Exam of a Forensic Image [ID: 542] (Total Chuck in branch: 33, Direct Chunk: 33)\n",
      "    |-- Performing the Exam [ID: 543] (Total Chuck in branch: 76, Direct Chunk: 2)\n",
      "      |-- Preparing for an Examination [ID: 544] (Total Chuck in branch: 74, Direct Chunk: 74)\n",
      "    |-- Interpreting Attribute 0x80 Data Runs [ID: 545] (Total Chuck in branch: 44, Direct Chunk: 2)\n",
      "      |-- Finding Attribute 0x80 an MFT Record [ID: 546] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "      |-- Configuring Data Interpreter Options in WinHex [ID: 547] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "      |-- Calculating Data Runs [ID: 548] (Total Chuck in branch: 15, Direct Chunk: 15)\n",
      "    |-- Carving Data Run Clusters Manually [ID: 549] (Total Chuck in branch: 19, Direct Chunk: 19)\n",
      "|-- Lab Manual for Guide to Computer Forensics and Investigations [ID: 556] (Total Chuck in branch: 2165, Direct Chunk: 1)\n",
      "  |-- Chapter 12. Mobile Device Forensics [ID: 792] (Total Chuck in branch: 13, Direct Chunk: 10)\n",
      "    |-- Lab 12.1. Examining Cell Phone Storage Devices [ID: 794] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 12.2. Using FTK Imager Lite to View Text Messages, Phone Numbers, and Photos [ID: 799] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 12.3. Using Autopsy to Search Cloud Backups of Mobile Devices [ID: 804] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: inf] (Total Chuck in branch: 1673, Direct Chunk: 0)\n",
      "    |-- Lab 1.1. Installing Autopsy for Windows [ID: 560] (Total Chuck in branch: 1670, Direct Chunk: 1)\n",
      "      |-- Objectives [ID: 561] (Total Chuck in branch: 702, Direct Chunk: 345)\n",
      "        |-- Materials Required [ID: 562] (Total Chuck in branch: 357, Direct Chunk: 357)\n",
      "      |-- Activity [ID: 563] (Total Chuck in branch: 967, Direct Chunk: 967)\n",
      "    |-- Lab 1.2. Downloading FTK Imager Lite [ID: 565] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 1.3. Downloading WinHex [ID: 570] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 1.4. Using Autopsy for Windows [ID: 575] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 2. The Investigator’s Office and Laboratory [ID: inf] (Total Chuck in branch: 5, Direct Chunk: 0)\n",
      "    |-- Lab 2.1. Wiping a USB Drive Securely [ID: 582] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.2. Using Directory Snoop to Image a USB Drive [ID: 587] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.3. Converting a Raw Image to an .E01 Image [ID: 592] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.4. Imaging Evidence with FTK Imager Lite [ID: 597] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 2.5. Viewing Images in FTK Imager Lite [ID: 602] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 3. Data Acquisition [ID: inf] (Total Chuck in branch: 70, Direct Chunk: 0)\n",
      "    |-- Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive [ID: 609] (Total Chuck in branch: 67, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 66, Direct Chunk: 0)\n",
      "        |-- Creating a DEFT Zero Boot CD [ID: 613] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Creating a Bootable USB DEFT Zero Drive [ID: 614] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Learning DEFT Zero Features [ID: 615] (Total Chuck in branch: 38, Direct Chunk: 38)\n",
      "    |-- Lab 3.2. Examining a FAT Image [ID: 617] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 3.3. Examining an NTFS Image [ID: 622] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 3.4. Examining an HFS+ Image [ID: 627] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 4. Processing Crime and Incident Scenes [ID: inf] (Total Chuck in branch: 36, Direct Chunk: 0)\n",
      "    |-- Lab 4.1. Creating a Mini-WinFE Boot CD [ID: 634] (Total Chuck in branch: 33, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 32, Direct Chunk: 0)\n",
      "        |-- Setting Up Mini-WinFE [ID: 638] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "        |-- Creating a Mini-WinFE ISO Image [ID: 639] (Total Chuck in branch: 24, Direct Chunk: 24)\n",
      "    |-- Lab 4.2. Using Mini-WinFE to Boot and Image a Windows Computer [ID: 641] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 4.3. Testing the Mini-WinFE Write-Protection Feature [ID: 646] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 4.4. Creating an Image with Guymager [ID: 651] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 5. Working with Windows and CLI Systems [ID: inf] (Total Chuck in branch: 4, Direct Chunk: 0)\n",
      "    |-- Lab 5.1. Using DART to Export Windows Registry Files [ID: 658] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 5.2. Examining the SAM Hive [ID: 663] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 5.3. Examining the SYSTEM Hive [ID: 668] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 5.4. Examining the ntuser.dat Registry File [ID: 673] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 6. Current Digital Forensics Tools [ID: inf] (Total Chuck in branch: 79, Direct Chunk: 0)\n",
      "    |-- Lab 6.1. Using Autopsy 4.7.0 to Search an Image File [ID: 680] (Total Chuck in branch: 41, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 40, Direct Chunk: 0)\n",
      "        |-- Installing Autopsy 4.7.0 [ID: 684] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Searching E-mail in Autopsy 4.7.0 [ID: 685] (Total Chuck in branch: 28, Direct Chunk: 28)\n",
      "    |-- Lab 6.2. Using OSForensics to Search an Image of a Hard Drive [ID: 687] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex [ID: 692] (Total Chuck in branch: 37, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 36, Direct Chunk: 0)\n",
      "        |-- Testing an Image File in Autopsy 4.3.0 [ID: 696] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "        |-- Examining Image Files in WinHex [ID: 697] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "  |-- Chapter 7. Linux and Macintosh File Systems [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 7.1. Using Autopsy to Process a Mac OS X Image [ID: 701] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 7.2. Using Autopsy to Process a Mac OS 9 Image [ID: 706] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 7.3. Using Autopsy to Process a Linux Image [ID: 711] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 8. Recovering Graphics Files [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 8.1. Using Autopsy to Analyze Multimedia Files [ID: 718] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 8.2. Using OSForensics to Analyze Multimedia Files [ID: 723] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 8.3. Using WinHex to Analyze Multimedia Files [ID: 728] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 9. Digital Forensics Analysis and Validation [ID: inf] (Total Chuck in branch: 9, Direct Chunk: 0)\n",
      "    |-- Lab 9.1. Using Autopsy to Search for Keywords in an Image [ID: 735] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 9.2. Validating File Hash Values with FTK Imager Lite [ID: 740] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 9.3. Validating File Hash Values with WinHex [ID: 745] (Total Chuck in branch: 7, Direct Chunk: 1)\n",
      "      |-- Objectives [ID: inf] (Total Chuck in branch: 6, Direct Chunk: 0)\n",
      "        |-- Materials Required: [ID: 747] (Total Chuck in branch: 6, Direct Chunk: 6)\n",
      "  |-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: inf] (Total Chuck in branch: 143, Direct Chunk: 0)\n",
      "    |-- Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine [ID: 752] (Total Chuck in branch: 41, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 40, Direct Chunk: 0)\n",
      "        |-- Installing MD5 Hashes in Autopsy [ID: 756] (Total Chuck in branch: 8, Direct Chunk: 8)\n",
      "        |-- Analyzing a Windows Image Containing a Virtual Machine [ID: 757] (Total Chuck in branch: 32, Direct Chunk: 32)\n",
      "    |-- Lab 10.2. Conducting a Live Acquisition [ID: 759] (Total Chuck in branch: 45, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 44, Direct Chunk: 0)\n",
      "        |-- Installing Tools for Live Acquisitions [ID: 763] (Total Chuck in branch: 16, Direct Chunk: 16)\n",
      "        |-- Exploring Tools for Live Acquisitions [ID: 764] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Capturing Data in a Live Acquisition [ID: 765] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "    |-- Lab 10.3. Using Kali Linux for Network Forensics [ID: 767] (Total Chuck in branch: 57, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 56, Direct Chunk: 0)\n",
      "        |-- Installing Kali Linux [ID: 771] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "        |-- Mounting Drives in Kali Linux [ID: 772] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Identifying Open Ports and Making a Screen Capture [ID: 773] (Total Chuck in branch: 18, Direct Chunk: 18)\n",
      "  |-- Chapter 11. E-mail and Social Media Investigations [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 11.1. Using OSForensics to Search for E-mails and Mailboxes [ID: 777] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 11.2. Using Autopsy to Search for E-mails and Mailboxes [ID: 782] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 11.3. Finding Google Searches and Multiple E-mail Accounts [ID: 787] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 13. Cloud Forensics [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 13.1. Examining Dropbox Cloud Storage [ID: 811] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 13.2. Examining Google Drive Cloud Storage [ID: 816] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 13.3. Examining OneDrive Cloud Storage [ID: 821] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 14. Report Writing for High-Tech Investigations [ID: inf] (Total Chuck in branch: 3, Direct Chunk: 0)\n",
      "    |-- Lab 14.1. Investigating Corporate Espionage [ID: 828] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 14.2. Adding Evidence to a Case [ID: 833] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 14.3. Preparing a Report [ID: 838] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "  |-- Chapter 15. Expert Testimony in Digital Investigations [ID: inf] (Total Chuck in branch: 44, Direct Chunk: 0)\n",
      "    |-- Lab 15.1. Conducting a Preliminary Investigation [ID: 845] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 15.2. Investigating an Arsonist [ID: 850] (Total Chuck in branch: 1, Direct Chunk: 1)\n",
      "    |-- Lab 15.3. Recovering a Password from Password-Protected Files [ID: 855] (Total Chuck in branch: 42, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 41, Direct Chunk: 0)\n",
      "        |-- Verifying the Existence of a Warning Banner [ID: 859] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Recovering a Password from Password-Protected Files [ID: 860] (Total Chuck in branch: 29, Direct Chunk: 29)\n",
      "  |-- Chapter 16. Ethics for the Expert Witness [ID: inf] (Total Chuck in branch: 73, Direct Chunk: 0)\n",
      "    |-- Lab 16.1. Rebuilding an MFT Record from a Corrupt Image [ID: 864] (Total Chuck in branch: 73, Direct Chunk: 1)\n",
      "      |-- Activity [ID: inf] (Total Chuck in branch: 72, Direct Chunk: 0)\n",
      "        |-- Creating a Duplicate Forensic Image [ID: 868] (Total Chuck in branch: 14, Direct Chunk: 14)\n",
      "        |-- Determining the Offset Byte Address of the Corrupt MFT Record [ID: 869] (Total Chuck in branch: 12, Direct Chunk: 12)\n",
      "        |-- Copying the Corrected MFT Record [ID: 870] (Total Chuck in branch: 20, Direct Chunk: 20)\n",
      "        |-- Extracting Additional Evidence [ID: 871] (Total Chuck in branch: 26, Direct Chunk: 26)\n",
      "|-- Appendix A. Certification Test References [ID: 873] (Total Chuck in branch: 56, Direct Chunk: 56)\n",
      "|-- Appendix B. Digital Forensics References [ID: 874] (Total Chuck in branch: 109, Direct Chunk: 109)\n",
      "|-- Appendix C. Digital Forensics Lab Considerations [ID: 875] (Total Chuck in branch: 58, Direct Chunk: 58)\n",
      "|-- Appendix D. Legacy File System and Forensics Tools [ID: 876] (Total Chuck in branch: 59, Direct Chunk: 59)\n",
      "|-- EPUB Preamble [ID: inf] (Total Chuck in branch: 21, Direct Chunk: 21)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Chunk Sequence & Content Integrity Test                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "------------------------- CONTENT PREVIEW -------------------------\n",
      "Title: Collecting Evidence in Private-Sector Incident Scenes [toc_id: 147]\n",
      "Chunk IDs: [1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960]\n",
      "----------------------------------------------------------------------\n",
      "Collecting Evidence in Private-Sector Incident Scenes\n",
      "Private-sector organizations include small to medium businesses, large corporations, and non-government organizations (NGOs), which might get funding from the government or other agencies. In the United States, NGOs and similar agencies must comply with state public disclosure and federal Freedom of Information Act (FOIA) laws and make certain documents available as public records. State public disclosure laws define state public records as open and available for inspection. For example, divorces recorded in a public office, such as a courthouse, become matters of public record unless a judge orders the documents sealed. Anyone can request a copy of a public divorce decree. Figure 4-3 shows an excerpt of a public disclosure law for the state of Idaho.\n",
      "State public disclosure laws apply to state records, but the FOIA allows citizens to request copies of public documents created by federal agencies. The FOIA was originally enacted in the 1960s, and several subsequent amendments have broadened its laws. Some Web sites now provide copies of publicly accessible records for a fee.\n",
      "ISPs and other communication companies make up a special category of private-sector businesses. ISPs can investigate computer abuse committed by their employees but not by customers. They must preserve customer privacy, especially when dealing with e-mail. However, federal regulations related to the Homeland Security Act and the PATRIOT Act of 2001 have redefined how ISPs and large organizations operate and maintain their records. ISPs and other communication companies can be called on to investigate customers’ activities that are deemed to create an emergency situation. An emergency situation under the PATRIOT Act is defined as the immediate risk of death or personal injury, such as finding a bomb threat in an e-mail.\n",
      "As recent events have shown, the government monitors e-mails for the occurrence of keywords. Incidents such as the Edward Snowden case have made public the amount of electronic surveillance done by the U.S. government and the governments of other countries. Some provisions of these federal regulations have been revised over the past few years, so you should stay abreast of their implications. For example, in March 2017, the U.S. Congress voted to allow ISPs to sell customers’ browsing histories without their explicit permission. (See www.congress.gov/bill/115th-congress/senate-joint-resolution/34 for more details.)\n",
      "Investigating and controlling computer incident scenes in private-sector environments is much easier than in crime scenes. In the private sector, the incident scene is often a workplace, such as a contained office or manufacturing area, where a policy violation is being investigated. Everything from the computers used to violate a company policy to the surrounding facility is under a controlled authority—that is, company management. Typically, businesses have inventory databases of computer hardware and software. Having access to these databases and knowing what applications are on suspected computers help identify the forensics tools needed to analyze a policy violation and the best way to conduct the analysis. For example, companies might have a preferred Web browser, such as Microsoft\n",
      "conduct the analysis. For example, companies might have a preferred Web browser, such as Microsoft Internet Explorer, Microsoft Edge, Mozilla Firefox, or Google Chrome. Knowing which browser a suspect used helps you develop standard examination procedures to identify data downloaded to the suspect’s workstation.\n",
      "To investigate employees suspected of improper use of company digital assets, a company policy statement about misuse of digital assets allows private-sector investigators to conduct covert surveillance with little or no cause and access company computer systems and digital devices without a warrant, which is an advantage. Law enforcement investigators can’t do the same, however, without sufficient reason for a warrant.\n",
      "However, if a company doesn’t display a warning banner or publish a policy stating that it reserves the right to inspect digital assets at will, employees have an expectation of privacy (as explained in Chapter 1). When an employee is being investigated, this expected privacy prevents the employer from legally conducting an intrusive investigation. A well-defined company policy, therefore, should state that an employer has the right to examine, inspect, or access any company-owned digital assets. If a company issues a policy statement to all employees, the employer can investigate digital assets at will without any privacy right restrictions; this practice might violate the privacy laws of countries in the EU, for example. As a standard practice, companies should use both warning banners\n",
      "countries in the EU, for example. As a standard practice, companies should use both warning banners and policy statements. For example, if an incident is escalated to a criminal complaint, prosecutors prefer showing juries warning banners instead of policy manuals. A warning banner leaves a much stronger impression on a jury.\n",
      "In addition to making sure a company has a policy statement or a warning banner, private-sector investigators should know under what circumstances they can examine an employee’s computer. With a policy statement, an employer can freely initiate any inquiry necessary to protect the company or organization. However, organizations must also have a well-defined process describing when an investigation can be initiated. At a minimum, most company policies require that employers have a “reasonable suspicion” that a law or policy is being violated. For example, if a policy states that employees can’t use company computers for outside business and a supervisor notices a change in work behavior that could indicate an employee is violating this rule, generally it’s enough to warrant an\n",
      "that could indicate an employee is violating this rule, generally it’s enough to warrant an investigation. However, some countries require notifying employees that they’re being investigated if they’re suspected of criminal behavior at work.\n",
      "If a private-sector investigator finds that an employee is committing or has committed a crime, the employer can file a criminal complaint with the police. Some businesses, such as banks, have a regulatory requirement to report crimes. In the United States, the employer must turn over all evidence to the police for prosecution. If this evidence had been collected by a law enforcement officer, it would require a warrant, which would be difficult to get without sufficient probable cause. In “Processing Law Enforcement Crime Scenes” later in this chapter, you learn more about probable cause and how it applies to a criminal investigation.\n",
      "Employers are usually interested in enforcing company policy, not seeking out and prosecuting employees, so typically they approve digital investigations only to identify employees who are misusing company assets. Private-sector investigators are, therefore, concerned mainly with protecting company assets, such as intellectual property. Finding evidence of a criminal act during an investigation escalates the investigation from an internal civil matter to an external criminal complaint. In some situations, such as the discovery of child pornography, the company or its agents must notify law enforcement immediately.\n",
      "If you discover evidence of a crime during a company policy investigation, first determine whether the incident meets the elements of criminal law. You might have to consult with your organization’s attorney to determine whether the situation is a potential crime. Next, inform management of the incident; they might have other concerns, such as protecting confidential business data that could be included with the criminal evidence (called “commingled data”). In this case, coordinate with management and the organization’s attorney to determine the best way to protect commingled data. After you submit evidence containing sensitive information to the police, it becomes public record. Public record laws do include exceptions for protecting sensitive company information; ultimately, however, a\n",
      "laws do include exceptions for protecting sensitive company information; ultimately, however, a judge decides what to protect.\n",
      "After you discover illegal activity and document and report the crime, stop your investigation to make sure you don’t violate Fourth Amendment restrictions on obtaining evidence. If the information you supply is specific enough to meet the criteria for a search warrant, the police are responsible for obtaining a warrant that requests any new evidence. If you follow police instructions to gather additional evidence without a search warrant after you have reported the crime, you run the risk of becoming an agent of law enforcement. Instead, consult with your organization’s attorney on how to respond to a police request for information. The police and prosecutor should issue a subpoena for any additional new evidence, which minimizes your exposure to potential civil liability. In addition,\n",
      "additional new evidence, which minimizes your exposure to potential civil liability. In addition, you should keep all documentation of evidence collected to investigate an internal company policy violation. Later in this section, you learn more about using affidavits in an internal investigation.\n",
      "One example of a company policy violation involves employees observing another employee accessing pornographic Web sites. If your organization’s policy requires you to determine whether any evidence supports this accusation, you could start by extracting log file data from the proxy server (used to connect a company LAN to the Internet) and conducting a forensic examination of the subject’s computer. Suppose that during your examination, you find adult and child pornography. Further examination of the subject’s hard disk reveals that the employee has been collecting child pornography in separate folders on his workstation’s hard drive. In the United States, possessing child pornography is a crime under federal and state criminal statutes. These situations aren’t uncommon and make life\n",
      "a crime under federal and state criminal statutes. These situations aren’t uncommon and make life difficult for investigators who don’t want to be guilty of possession of this contraband on their forensic workstations.\n",
      "You survey the remaining content of the subject’s drive and find that he’s a lead engineer for the team developing your company’s latest high-tech bicycle. He placed the child pornography images in a subfolder where the bicycle plans are stored. By doing so, he has commingled contraband with the company’s confidential design plans for the bicycle. Your discovery poses two problems in dealing with this contraband evidence. First, you must report the crime to the police; all U.S. states and most countries have legal and moral codes when evidence of sexual exploitation of children is found. Second, you must also protect sensitive company information. Letting the high-tech bicycle plans become part of the criminal evidence might make it public record, and the design work will then be available\n",
      "of the criminal evidence might make it public record, and the design work will then be available to competitors. Your first step is to ask your organization’s attorney how to deal with the commingled contraband data and sensitive design plans.\n",
      "Your next step is to work with the attorney to write an affidavit confirming your findings. The attorney should indicate in the affidavit that the evidence is commingled with company secrets, and releasing the information will be detrimental to the company’s financial health. When the affidavit is completed, you sign it before a notary, and then deliver the affidavit and the recovered evidence with log files to the police, where you make a criminal complaint. At the same time, the attorney goes to court and requests that all evidence recovered from the hard disk that’s not related to the complaint and is a company trade secret be protected from public viewing. You and the attorney have reported the crime and taken steps to protect the sensitive data.\n",
      "Now suppose the detective assigned to the case calls you. In the evidence you’ve turned over to the police, the detective notices that the suspect is collecting most of his contraband from e-mail attachments. The prosecutor needs you to collect more evidence to determine whether the suspect is transmitting contraband pictures to other potential suspects. The detective realizes that collecting more evidence might make you an agent of law enforcement and violate the employee’s Fourth Amendment rights, so she writes an affidavit for a search warrant, ensuring that any subsequent instructions to you are legal. Before collecting any additional information, you wait until you or your organization’s attorney gets a subpoena, search warrant, or other court order.\n",
      "----------------------- END CONTENT PREVIEW -----------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Diagnostic Summary                               \n",
      "--------------------------------------------------------------------------------\n",
      "Total Chunks in DB: 11774\n",
      "\n",
      "================================================================================\n",
      "                              Diagnostic Complete                               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.1: Full Database Health & Hierarchy Diagnostic Report (V5 - with Content Preview)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# You might need to install pandas if you haven't already\n",
    "try:\n",
    "    import pandas as pd\n",
    "    pandas_available = True\n",
    "except ImportError:\n",
    "    pandas_available = False\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def count_total_chunks(node: Dict) -> int:\n",
    "    \"\"\"Recursively counts all chunks in a node and its children.\"\"\"\n",
    "    total = node.get('_chunks', 0)\n",
    "    for child_node in node.get('_children', {}).values():\n",
    "        total += count_total_chunks(child_node)\n",
    "    return total\n",
    "\n",
    "def print_hierarchy_report(node: Dict, indent_level: int = 0):\n",
    "    \"\"\"\n",
    "    Recursively prints the reconstructed hierarchy, sorting by sequential ToC ID.\n",
    "    \"\"\"\n",
    "    sorted_children = sorted(\n",
    "        node.get('_children', {}).items(),\n",
    "        key=lambda item: item[1].get('_toc_id', float('inf'))\n",
    "    )\n",
    "    \n",
    "    for title, child_node in sorted_children:\n",
    "        prefix = \"  \" * indent_level + \"|-- \"\n",
    "        total_chunks_in_branch = count_total_chunks(child_node)\n",
    "        direct_chunks = child_node.get('_chunks', 0)\n",
    "        toc_id = child_node.get('_toc_id', 'N/A')\n",
    "        print(f\"{prefix}{title} [ID: {toc_id}] (Total Chuck in branch: {total_chunks_in_branch}, Direct Chunk: {direct_chunks})\")\n",
    "        print_hierarchy_report(child_node, indent_level + 1)\n",
    "\n",
    "def find_testable_sections(node: Dict, path: str, testable_list: List):\n",
    "    \"\"\"\n",
    "    Recursively find sections with a decent number of \"direct\" chunks to test sequence on.\n",
    "    \"\"\"\n",
    "    if node.get('_chunks', 0) > 10 and not node.get('_children'):\n",
    "        testable_list.append({\n",
    "            \"path\": path,\n",
    "            \"toc_id\": node.get('_toc_id'),\n",
    "            \"chunk_count\": node.get('_chunks')\n",
    "        })\n",
    "\n",
    "    for title, child_node in node.get('_children', {}).items():\n",
    "        new_path = f\"{path} -> {title}\" if path else title\n",
    "        find_testable_sections(child_node, new_path, testable_list)\n",
    "\n",
    "\n",
    "# --- MODIFIED TEST FUNCTION ---\n",
    "def verify_chunk_sequence_and_content(vector_store: Chroma, hierarchy_tree: Dict):\n",
    "    \"\"\"\n",
    "    Selects a random ToC section, verifies chunk sequence, and displays the reassembled content.\n",
    "    \"\"\"\n",
    "    print_header(\"Chunk Sequence & Content Integrity Test\", char=\"-\")\n",
    "    logger.info(\"Verifying chunk order and reassembling content for a random ToC section.\")\n",
    "    \n",
    "    # 1. Find a good section to test\n",
    "    testable_sections = []\n",
    "    find_testable_sections(hierarchy_tree, \"\", testable_sections)\n",
    "    \n",
    "    if not testable_sections:\n",
    "        logger.warning(\"Could not find a suitable section with enough chunks to test. Skipping content test.\")\n",
    "        return\n",
    "\n",
    "    random_section = random.choice(testable_sections)\n",
    "    test_toc_id = random_section['toc_id']\n",
    "    section_title = random_section['path'].split(' -> ')[-1]\n",
    "    \n",
    "    logger.info(f\"Selected random section for testing: '{random_section['path']}' (toc_id: {test_toc_id})\")\n",
    "\n",
    "    # 2. Retrieve all documents (content + metadata) for that toc_id\n",
    "    try:\n",
    "        # Use .get() to retrieve full documents, not just similarity search\n",
    "        retrieved_data = vector_store.get(\n",
    "            where={\"toc_id\": test_toc_id},\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "        \n",
    "        # Combine metadatas and documents into LangChain Document objects\n",
    "        docs = [Document(page_content=doc, metadata=meta) for doc, meta in zip(retrieved_data['documents'], retrieved_data['metadatas'])]\n",
    "\n",
    "        logger.info(f\"Retrieved {len(docs)} document chunks for toc_id {test_toc_id}.\")\n",
    "\n",
    "        if len(docs) < 1:\n",
    "            logger.warning(\"No chunks found in the selected section. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # 3. Sort the documents by chunk_id\n",
    "        # Handle cases where chunk_id might be missing for robustness\n",
    "        docs.sort(key=lambda d: d.metadata.get('chunk_id', -1))\n",
    "        \n",
    "        chunk_ids = [d.metadata.get('chunk_id') for d in docs]\n",
    "        if None in chunk_ids:\n",
    "            logger.error(\"TEST FAILED: Some retrieved chunks are missing a 'chunk_id'.\")\n",
    "            return\n",
    "\n",
    "        # 4. Verify sequence\n",
    "        is_sequential = all(chunk_ids[i] == chunk_ids[i-1] + 1 for i in range(1, len(chunk_ids)))\n",
    "        \n",
    "        # 5. Reassemble and print content\n",
    "        full_content = \"\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "        print(\"\\n\" + \"-\"*25 + \" CONTENT PREVIEW \" + \"-\"*25)\n",
    "        print(f\"Title: {section_title} [toc_id: {test_toc_id}]\")\n",
    "        print(f\"Chunk IDs: {chunk_ids}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(full_content)\n",
    "        print(\"-\" * 23 + \" END CONTENT PREVIEW \" + \"-\"*23 + \"\\n\")\n",
    "        \n",
    "        if is_sequential:\n",
    "            logger.info(\"✅ TEST PASSED: Chunk IDs for the section are sequential and content is reassembled.\")\n",
    "        else:\n",
    "            logger.warning(\"TEST PASSED (with note): Chunk IDs are not perfectly sequential but are in increasing order.\")\n",
    "            logger.warning(\"This is acceptable. Sorting by chunk_id successfully restored narrative order.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"TEST FAILED: An error occurred during chunk sequence verification: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- MAIN DIAGNOSTIC FUNCTION ---\n",
    "def run_full_diagnostics():\n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain components not installed. Skipping diagnostics.\")\n",
    "        return\n",
    "    if not pandas_available:\n",
    "        logger.warning(\"Pandas not installed. Some reports may not be available.\")\n",
    "\n",
    "    print_header(\"Full Database Health & Hierarchy Diagnostic Report\")\n",
    "\n",
    "    # 1. Connect to the Database\n",
    "    logger.info(\"Connecting to the vector database...\")\n",
    "    if not os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        logger.error(f\"FATAL: Chroma DB directory not found at {CHROMA_PERSIST_DIR}.\")\n",
    "        return\n",
    "\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    logger.info(\"Successfully connected to the database.\")\n",
    "\n",
    "    # 2. Retrieve ALL Metadata\n",
    "    total_docs = vector_store._collection.count()\n",
    "    if total_docs == 0:\n",
    "        logger.warning(\"Database is empty. No diagnostics to run.\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Retrieving metadata for all {total_docs} chunks...\")\n",
    "    metadatas = vector_store.get(limit=total_docs, include=[\"metadatas\"])['metadatas']\n",
    "    logger.info(\"Successfully retrieved all metadata.\")\n",
    "    \n",
    "    # 3. Reconstruct the Hierarchy Tree\n",
    "    logger.info(\"Reconstructing hierarchy from chunk metadata...\")\n",
    "    hierarchy_tree = {'_children': {}}\n",
    "    chunks_without_id = 0\n",
    "\n",
    "    for meta in metadatas:\n",
    "        toc_id = meta.get('toc_id')\n",
    "        if toc_id is None or toc_id == -1:\n",
    "            chunks_without_id += 1\n",
    "            node_title = meta.get('level_1_title', 'Orphaned Chunks')\n",
    "            if node_title not in hierarchy_tree['_children']:\n",
    "                 hierarchy_tree['_children'][node_title] = {'_children': {}, '_chunks': 0, '_toc_id': float('inf')}\n",
    "            hierarchy_tree['_children'][node_title]['_chunks'] += 1\n",
    "            continue\n",
    "        \n",
    "        current_node = hierarchy_tree\n",
    "        for level in range(1, 7):\n",
    "            level_key = f'level_{level}_title'\n",
    "            title = meta.get(level_key)\n",
    "            if not title: break\n",
    "            if title not in current_node['_children']:\n",
    "                current_node['_children'][title] = {'_children': {}, '_chunks': 0, '_toc_id': float('inf')}\n",
    "            current_node = current_node['_children'][title]\n",
    "\n",
    "        current_node['_chunks'] += 1\n",
    "        current_node['_toc_id'] = min(current_node['_toc_id'], toc_id)\n",
    "        \n",
    "    logger.info(\"Hierarchy reconstruction complete.\")\n",
    "\n",
    "    # 4. Print Hierarchy Report\n",
    "    print_header(\"Reconstructed Hierarchy Report (Book Order)\", char=\"-\")\n",
    "    print_hierarchy_report(hierarchy_tree)\n",
    "        \n",
    "    # 5. Run Chunk Sequence and Content Test\n",
    "    verify_chunk_sequence_and_content(vector_store, hierarchy_tree)\n",
    "    \n",
    "    # 6. Final Summary\n",
    "    print_header(\"Diagnostic Summary\", char=\"-\")\n",
    "    print(f\"Total Chunks in DB: {total_docs}\")\n",
    "    \n",
    "    if chunks_without_id > 0:\n",
    "        logger.warning(f\"Found {chunks_without_id} chunks MISSING a valid 'toc_id'. Check 'Orphaned' sections.\")\n",
    "    else:\n",
    "        logger.info(\"All chunks contain valid 'toc_id' metadata. Sequential integrity is maintained.\")\n",
    "\n",
    "    print_header(\"Diagnostic Complete\")\n",
    "\n",
    "# --- Execute Diagnostics ---\n",
    "if 'CHROMA_PERSIST_DIR' in locals() and langchain_available:\n",
    "    run_full_diagnostics()\n",
    "else:\n",
    "    logger.error(\"Skipping diagnostics: Global variables not defined or LangChain not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "Require Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf3ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 21:02:48,736 - INFO - Connecting to DB and initializing components...\n",
      "2025-07-01 21:02:48,746 - INFO - Goal: Confirm the database is live and contains thematically relevant content.\n",
      "2025-07-01 21:02:48,746 - INFO - Strategy: Perform a simple similarity search using the course's 'unitName'.\n",
      "2025-07-01 21:02:48,747 - INFO - Action: Searching for query: 'Digital Forensic'...\n",
      "2025-07-01 21:02:48,814 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:02:48,818 - INFO - Verification: Check if at least one document was returned.\n",
      "2025-07-01 21:02:48,818 - INFO - ✅ Result: TEST 1 PASSED. The database is online and responsive.\n",
      "2025-07-01 21:02:48,819 - INFO - Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\n",
      "2025-07-01 21:02:48,819 - INFO - Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\n",
      "2025-07-01 21:02:48,820 - INFO -   - Selected random deep section: Chapter 6. Current Digital Forensics Tools -> Digital Forensics Hardware Tools -> Forensic Workstations\n",
      "2025-07-01 21:02:48,820 - INFO - Action: Performing a similarity search with a highly specific '$and' filter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         Database Verification Process                          \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                            Test 1: Basic Retrieval                             \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Digital Forensic'\n",
      "--> Found 1 results. Displaying top 1:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'An Overview of Digital Forensics...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 156,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\",\n",
      "  \"toc_id\": 9,\n",
      "  \"level_2_title\": \"An Overview of Digital Forensics\"\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                        Test 2: Deep Hierarchy Retrieval                        \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 21:02:48,953 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:02:48,965 - INFO - Verification: Check if the precisely filtered query returned any documents.\n",
      "2025-07-01 21:02:48,965 - INFO - ✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\n",
      "2025-07-01 21:02:48,966 - INFO - Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\n",
      "2025-07-01 21:02:48,966 - INFO - Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\n",
      "2025-07-01 21:02:48,967 - INFO -   - Selected random week: Week Week 9 - 'Email and Social Media.'\n",
      "2025-07-01 21:02:48,967 - INFO -   - Extracted required chapter number(s): ['2019', '978', '1', '337', '56894', '4', '11']\n",
      "2025-07-01 21:02:48,970 - INFO -   - Mapped to top-level ToC entries: ['Chapter 11. E-mail and Social Media Investigations', 'Chapter 4. Processing Crime and Incident Scenes', 'Chapter 1. Understanding the Digital Forensics Profession and Investigations']\n",
      "2025-07-01 21:02:48,970 - INFO - Action: Searching for the weekly topic, filtered by the mapped chapter(s).\n",
      "2025-07-01 21:02:49,080 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:02:49,099 - INFO - Verification: Check if at least one returned document is from the correct chapter.\n",
      "2025-07-01 21:02:49,100 - INFO - ✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\n",
      "2025-07-01 21:02:49,101 - INFO - Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\n",
      "2025-07-01 21:02:49,101 - INFO - Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\n",
      "2025-07-01 21:02:49,102 - INFO - Action: Performing similarity search for topic: 'Current Computer Forensics Tools.' to get a set of chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Forensic Workstations'\n",
      "FILTER: {\n",
      "  \"$and\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 6. Current Digital Forensics Tools\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_2_title\": {\n",
      "        \"$eq\": \"Digital Forensics Hardware Tools\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_3_title\": {\n",
      "        \"$eq\": \"Forensic Workstations\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> Found 1 results. Displaying top 1:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Forensic Workstations...'\n",
      "  Metadata: {\n",
      "  \"level_3_title\": \"Forensic Workstations\",\n",
      "  \"level_2_title\": \"Digital Forensics Hardware Tools\",\n",
      "  \"toc_id\": 255,\n",
      "  \"level_1_title\": \"Chapter 6. Current Digital Forensics Tools\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 3311\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Test 3: Advanced Unit Outline Alignment                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Email and Social Media.'\n",
      "FILTER: {\n",
      "  \"$or\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 11. E-mail and Social Media Investigations\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 4. Processing Crime and Incident Scenes\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> Found 5 results. Displaying top 3:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Chapter 11. E-mail and Social Media Investigations...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 5378,\n",
      "  \"toc_id\": 377,\n",
      "  \"level_1_title\": \"Chapter 11. E-mail and Social Media Investigations\"\n",
      "}\n",
      "\n",
      "[ RESULT 2 ]\n",
      "  Content : 'Chapter 11. E-mail and Social Media Investigations...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 10484,\n",
      "  \"level_1_title\": \"Chapter 11. E-mail and Social Media Investigations\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 377\n",
      "}\n",
      "\n",
      "[ RESULT 3 ]\n",
      "  Content : 'Social media can contain a lot of information, including the following:...'\n",
      "  Metadata: {\n",
      "  \"level_2_title\": \"Applying Digital Forensics Methods to Social Media Communications\",\n",
      "  \"chunk_id\": 5636,\n",
      "  \"toc_id\": 398,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_1_title\": \"Chapter 11. E-mail and Social Media Investigations\"\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Test 4: Content Sequence Verification                      \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 21:02:49,217 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-01 21:02:49,220 - INFO -   - Retrieved and sorted chunk IDs: [49, 3138, 3141, 3160, 3164, 3166, 3267, 3271, 3308, 9541]\n",
      "2025-07-01 21:02:49,221 - INFO - Verification: Check if the sorted list of chunk_ids is strictly increasing.\n",
      "2025-07-01 21:02:49,221 - INFO - ✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Current Computer Forensics Tools.'\n",
      "--> Found 10 results. Displaying top 3:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Chapter 6. Current Digital Forensics Tools...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 231,\n",
      "  \"level_1_title\": \"Chapter 6. Current Digital Forensics Tools\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 3138\n",
      "}\n",
      "\n",
      "[ RESULT 2 ]\n",
      "  Content : 'Chapter 6. Current Digital Forensics Tools...'\n",
      "  Metadata: {\n",
      "  \"level_1_title\": \"Chapter 6. Current Digital Forensics Tools\",\n",
      "  \"chunk_id\": 9541,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 231\n",
      "}\n",
      "\n",
      "[ RESULT 3 ]\n",
      "  Content : 'Software Forensics Tools...'\n",
      "  Metadata: {\n",
      "  \"level_3_title\": \"Types of Digital Forensics Tools\",\n",
      "  \"level_4_title\": \"Software Forensics Tools\",\n",
      "  \"level_1_title\": \"Chapter 6. Current Digital Forensics Tools\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"level_2_title\": \"Evaluating Digital Forensics Tool Needs\",\n",
      "  \"toc_id\": 236,\n",
      "  \"chunk_id\": 3166\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                              Verification Summary                              \n",
      "================================================================================\n",
      "Total Tests Run: 4\n",
      "✅ Passed: 4\n",
      "❌ Failed: 0\n",
      "\n",
      "================================================================================\n",
      "                             Verification Complete                              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Vector Database (Final Version with Rich Diagnostic Output)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "def print_results(query_text: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Richly prints query results, showing the query, filter, and retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*10 + \" DIAGNOSTIC: RETRIEVAL RESULTS \" + \"-\"*10)\n",
    "    print(f\"QUERY: '{query_text}'\")\n",
    "    if where_filter:\n",
    "        print(f\"FILTER: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"--> No documents were retrieved for this query and filter.\")\n",
    "        print(\"-\" * 55)\n",
    "        return\n",
    "        \n",
    "    print(f\"--> Found {len(results)} results. Displaying top {min(len(results), 3)}:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n[ RESULT {i+1} ]\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"  Content : '{content_preview[:200]}...'\")\n",
    "        print(f\"  Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "\n",
    "# --- HELPER FUNCTIONS FOR FINDING DATA (UNCHANGED) ---\n",
    "def find_deep_entry(nodes: List[Dict], current_path: List[str] = []) -> Optional[Tuple[Dict, List[str]]]:\n",
    "    shuffled_nodes = random.sample(nodes, len(nodes))\n",
    "    for node in shuffled_nodes:\n",
    "        if node.get('level', 0) >= 2 and node.get('children'): return node, current_path + [node['title']]\n",
    "        if node.get('children'):\n",
    "            path = current_path + [node['title']]\n",
    "            deep_entry = find_deep_entry(node['children'], path)\n",
    "            if deep_entry: return deep_entry\n",
    "    return None\n",
    "\n",
    "def find_chapter_title_by_number(toc_data: List[Dict], chap_num: int) -> Optional[List[str]]:\n",
    "    def search_nodes(nodes, num, current_path):\n",
    "        for node in nodes:\n",
    "            path = current_path + [node['title']]\n",
    "            if re.match(rf\"(Chapter\\s)?{num}[.:\\s]\", node.get('title', ''), re.IGNORECASE): return path\n",
    "            if node.get('children'):\n",
    "                found_path = search_nodes(node['children'], num, path)\n",
    "                if found_path: return found_path\n",
    "        return None\n",
    "    return search_nodes(toc_data, chap_num, [])\n",
    "\n",
    "\n",
    "# --- ENHANCED TEST CASES with DIAGNOSTIC OUTPUT ---\n",
    "\n",
    "def basic_retrieval_test(db, outline):\n",
    "    print_header(\"Test 1: Basic Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm the database is live and contains thematically relevant content.\")\n",
    "        logger.info(\"Strategy: Perform a simple similarity search using the course's 'unitName'.\")\n",
    "        query_text = outline.get(\"unitInformation\", {}).get(\"unitName\", \"introduction\")\n",
    "        \n",
    "        logger.info(f\"Action: Searching for query: '{query_text}'...\")\n",
    "        results = db.similarity_search(query_text, k=1)\n",
    "        \n",
    "        print_results(query_text, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one document was returned.\")\n",
    "        assert len(results) > 0, \"Basic retrieval query returned no results.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 1 PASSED. The database is online and responsive.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 1 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def deep_hierarchy_test(db, toc):\n",
    "    print_header(\"Test 2: Deep Hierarchy Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\")\n",
    "        logger.info(\"Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\")\n",
    "        deep_entry_result = find_deep_entry(toc)\n",
    "        assert deep_entry_result, \"Could not find a suitable deep entry (level >= 2) to test.\"\n",
    "        node, path = deep_entry_result\n",
    "        query = node['title']\n",
    "        \n",
    "        logger.info(f\"  - Selected random deep section: {' -> '.join(path)}\")\n",
    "        conditions = [{f\"level_{i+1}_title\": {\"$eq\": title}} for i, title in enumerate(path)]\n",
    "        w_filter = {\"$and\": conditions}\n",
    "        \n",
    "        logger.info(\"Action: Performing a similarity search with a highly specific '$and' filter.\")\n",
    "        results = db.similarity_search(query, k=1, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if the precisely filtered query returned any documents.\")\n",
    "        assert len(results) > 0, \"Deeply filtered query returned no results.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 2 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def advanced_alignment_test(db, outline, toc):\n",
    "    print_header(\"Test 3: Advanced Unit Outline Alignment\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\")\n",
    "        logger.info(\"Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\")\n",
    "        week_to_test = random.choice(outline['weeklySchedule'])\n",
    "        logger.info(f\"  - Selected random week: Week {week_to_test['week']} - '{week_to_test['contentTopic']}'\")\n",
    "\n",
    "        reading = week_to_test.get('requiredReading', '')\n",
    "        chap_nums_str = re.findall(r'\\d+', reading)\n",
    "        assert chap_nums_str, f\"Could not find chapter numbers in required reading: '{reading}'\"\n",
    "        logger.info(f\"  - Extracted required chapter number(s): {chap_nums_str}\")\n",
    "\n",
    "        chapter_paths = [find_chapter_title_by_number(toc, int(n)) for n in chap_nums_str]\n",
    "        chapter_paths = [path for path in chapter_paths if path is not None]\n",
    "        assert chapter_paths, f\"Could not map chapter numbers {chap_nums_str} to a valid ToC path.\"\n",
    "        \n",
    "        level_1_titles = list(set([path[0] for path in chapter_paths]))\n",
    "        logger.info(f\"  - Mapped to top-level ToC entries: {level_1_titles}\")\n",
    "\n",
    "        or_filter = [{\"level_1_title\": {\"$eq\": title}} for title in level_1_titles]\n",
    "        w_filter = {\"$or\": or_filter} if len(or_filter) > 1 else or_filter[0]\n",
    "        query = week_to_test['contentTopic']\n",
    "        \n",
    "        logger.info(\"Action: Searching for the weekly topic, filtered by the mapped chapter(s).\")\n",
    "        results = db.similarity_search(query, k=5, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one returned document is from the correct chapter.\")\n",
    "        assert len(results) > 0, \"Alignment query returned no results for the correct section/chapter.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 3 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def content_sequence_test(db, outline):\n",
    "    print_header(\"Test 4: Content Sequence Verification\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\")\n",
    "        logger.info(\"Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\")\n",
    "        topic_query = random.choice(outline['weeklySchedule'])['contentTopic']\n",
    "        \n",
    "        logger.info(f\"Action: Performing similarity search for topic: '{topic_query}' to get a set of chunks.\")\n",
    "        results = db.similarity_search(topic_query, k=10)\n",
    "        \n",
    "        print_results(topic_query, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        docs_with_id = [doc for doc in results if 'chunk_id' in doc.metadata]\n",
    "        assert len(docs_with_id) > 3, \"Fewer than 4 retrieved chunks have a 'chunk_id' to test.\"\n",
    "        \n",
    "        chunk_ids = [doc.metadata['chunk_id'] for doc in docs_with_id]\n",
    "        sorted_ids = sorted(chunk_ids)\n",
    "        \n",
    "        logger.info(f\"  - Retrieved and sorted chunk IDs: {sorted_ids}\")\n",
    "        logger.info(\"Verification: Check if the sorted list of chunk_ids is strictly increasing.\")\n",
    "        is_ordered = all(sorted_ids[i] >= sorted_ids[i-1] for i in range(1, len(sorted_ids)))\n",
    "        assert is_ordered, \"The retrieved chunks' chunk_ids are not in ascending order when sorted.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 4 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    \n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain libraries not found. Aborting tests.\")\n",
    "        return\n",
    "\n",
    "    required_files = {\n",
    "        \"Chroma DB\": CHROMA_PERSIST_DIR,\n",
    "        \"ToC JSON\": PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        \"Parsed Outline\": PARSED_UO_JSON_PATH\n",
    "    }\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Required '{name}' not found at '{path}'. Please run previous cells.\")\n",
    "            return\n",
    "\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        toc_data = json.load(f)\n",
    "    with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        unit_outline_data = json.load(f)\n",
    "\n",
    "    logger.info(\"Connecting to DB and initializing components...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    results_summary = [\n",
    "        basic_retrieval_test(vector_store, unit_outline_data),\n",
    "        deep_hierarchy_test(vector_store, toc_data),\n",
    "        advanced_alignment_test(vector_store, unit_outline_data, toc_data),\n",
    "        content_sequence_test(vector_store, unit_outline_data)\n",
    "    ]\n",
    "\n",
    "    passed_count = sum(filter(None, results_summary))\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    \n",
    "    print_header(\"Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)}\")\n",
    "    print(f\"✅ Passed: {passed_count}\")\n",
    "    print(f\"❌ Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "# Assumes global variables from Cell 1 are available in the notebook's scope\n",
    "run_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97368b0",
   "metadata": {},
   "source": [
    "#  Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae477bc",
   "metadata": {},
   "source": [
    "## Planning Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6baadd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 11:07:25,436 - INFO - --- Initializing Data-Driven Planning Agent Test ---\n",
      "2025-07-02 11:07:25,438 - INFO - Connecting to ChromaDB for the Planning Agent...\n",
      "2025-07-02 11:07:25,448 - INFO - Database connection successful.\n",
      "2025-07-02 11:07:25,449 - INFO - Loading configuration files...\n",
      "2025-07-02 11:07:25,451 - INFO - Configuration files loaded.\n",
      "2025-07-02 11:07:25,452 - INFO - Data-Driven PlanningAgent initialized successfully.\n",
      "2025-07-02 11:07:25,453 - INFO - --> Explicitly testing planning logic for Week 7\n",
      "2025-07-02 11:07:25,508 - INFO - Partitioning strategy: Splitting sub-topics from 2 chapter(s) across 4 decks.\n",
      "2025-07-02 11:07:25,508 - INFO - --- Planning Deck 1/4 | Topics: ['Examining Linux File Structures'] | Weight: 131 chunks | Slide Budget: 6 ---\n",
      "2025-07-02 11:07:25,508 - INFO - --- Planning Deck 2/4 | Topics: ['Understanding Data Compression', 'Understanding Copyright Issues with Graphics'] | Weight: 120 chunks | Slide Budget: 6 ---\n",
      "2025-07-02 11:07:25,509 - INFO - --- Planning Deck 3/4 | Topics: ['Using Linux Forensics Tools', 'Identifying Unknown File Formats'] | Weight: 115 chunks | Slide Budget: 6 ---\n",
      "2025-07-02 11:07:25,509 - INFO - --- Planning Deck 4/4 | Topics: ['Understanding Macintosh File Structures', 'Recognizing a Graphics File'] | Weight: 112 chunks | Slide Budget: 5 ---\n",
      "2025-07-02 11:07:25,511 - INFO - \n",
      "Successfully saved content plan for Week 7 to: /home/sebas_dev_linux/projects/course_generator/generated_plans/ICT312_Week7_plan.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                                Planning Week 7                                 \n",
      "********************************************************************************\n",
      "\n",
      "--- Generated Content Plan (Hierarchical & Partitioned) ---\n",
      "{\n",
      "  \"week\": 7,\n",
      "  \"overall_topic\": \"Linux Boot Processes and File Systems. Recovering Graphics Files.\",\n",
      "  \"deck_plans\": [\n",
      "    {\n",
      "      \"deck_number\": 1,\n",
      "      \"deck_title\": \"Digital Forensic - Week 7, Lecture 1\",\n",
      "      \"session_content\": [\n",
      "        {\n",
      "          \"title\": \"Examining Linux File Structures\",\n",
      "          \"toc_id\": 272,\n",
      "          \"chunk_count\": 77,\n",
      "          \"total_chunks_in_branch\": 131,\n",
      "          \"slides_allocated\": 6,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"File Structures in Ext4\",\n",
      "              \"toc_id\": 273,\n",
      "              \"chunk_count\": 8,\n",
      "              \"total_chunks_in_branch\": 54,\n",
      "              \"slides_allocated\": 6,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Inodes\",\n",
      "                  \"toc_id\": 274,\n",
      "                  \"chunk_count\": 22,\n",
      "                  \"total_chunks_in_branch\": 22,\n",
      "                  \"slides_allocated\": 3,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Hard Links and Symbolic Links\",\n",
      "                  \"toc_id\": 275,\n",
      "                  \"chunk_count\": 24,\n",
      "                  \"total_chunks_in_branch\": 24,\n",
      "                  \"slides_allocated\": 3,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"deck_number\": 2,\n",
      "      \"deck_title\": \"Digital Forensic - Week 7, Lecture 2\",\n",
      "      \"session_content\": [\n",
      "        {\n",
      "          \"title\": \"Understanding Data Compression\",\n",
      "          \"toc_id\": 299,\n",
      "          \"chunk_count\": 2,\n",
      "          \"total_chunks_in_branch\": 101,\n",
      "          \"slides_allocated\": 3,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Lossless and Lossy Compression\",\n",
      "              \"toc_id\": 300,\n",
      "              \"chunk_count\": 8,\n",
      "              \"total_chunks_in_branch\": 8,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Locating and Recovering Graphics Files\",\n",
      "              \"toc_id\": 301,\n",
      "              \"chunk_count\": 6,\n",
      "              \"total_chunks_in_branch\": 6,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Identifying Graphics File Fragments\",\n",
      "              \"toc_id\": 302,\n",
      "              \"chunk_count\": 3,\n",
      "              \"total_chunks_in_branch\": 3,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Repairing Damaged Headers\",\n",
      "              \"toc_id\": 303,\n",
      "              \"chunk_count\": 6,\n",
      "              \"total_chunks_in_branch\": 6,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Searching for and Carving Data from Unallocated Space\",\n",
      "              \"toc_id\": 304,\n",
      "              \"chunk_count\": 9,\n",
      "              \"total_chunks_in_branch\": 39,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Planning Your Examination\",\n",
      "                  \"toc_id\": 305,\n",
      "                  \"chunk_count\": 4,\n",
      "                  \"total_chunks_in_branch\": 4,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Searching for and Recovering Digital Photograph Evidence\",\n",
      "                  \"toc_id\": 306,\n",
      "                  \"chunk_count\": 26,\n",
      "                  \"total_chunks_in_branch\": 26,\n",
      "                  \"slides_allocated\": 1,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Rebuilding File Headers\",\n",
      "              \"toc_id\": 307,\n",
      "              \"chunk_count\": 22,\n",
      "              \"total_chunks_in_branch\": 22,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Reconstructing File Fragments\",\n",
      "              \"toc_id\": 308,\n",
      "              \"chunk_count\": 15,\n",
      "              \"total_chunks_in_branch\": 15,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Understanding Copyright Issues with Graphics\",\n",
      "          \"toc_id\": 314,\n",
      "          \"chunk_count\": 19,\n",
      "          \"total_chunks_in_branch\": 19,\n",
      "          \"slides_allocated\": 1,\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"deck_number\": 3,\n",
      "      \"deck_title\": \"Digital Forensic - Week 7, Lecture 3\",\n",
      "      \"session_content\": [\n",
      "        {\n",
      "          \"title\": \"Using Linux Forensics Tools\",\n",
      "          \"toc_id\": 280,\n",
      "          \"chunk_count\": 5,\n",
      "          \"total_chunks_in_branch\": 68,\n",
      "          \"slides_allocated\": 4,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Installing Sleuth Kit and Autopsy\",\n",
      "              \"toc_id\": 281,\n",
      "              \"chunk_count\": 21,\n",
      "              \"total_chunks_in_branch\": 21,\n",
      "              \"slides_allocated\": 2,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Examining a Case with Sleuth Kit and Autopsy\",\n",
      "              \"toc_id\": 282,\n",
      "              \"chunk_count\": 42,\n",
      "              \"total_chunks_in_branch\": 42,\n",
      "              \"slides_allocated\": 2,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Identifying Unknown File Formats\",\n",
      "          \"toc_id\": 309,\n",
      "          \"chunk_count\": 14,\n",
      "          \"total_chunks_in_branch\": 47,\n",
      "          \"slides_allocated\": 1,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Analyzing Graphics File Headers\",\n",
      "              \"toc_id\": 310,\n",
      "              \"chunk_count\": 5,\n",
      "              \"total_chunks_in_branch\": 5,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Tools for Viewing Images\",\n",
      "              \"toc_id\": 311,\n",
      "              \"chunk_count\": 5,\n",
      "              \"total_chunks_in_branch\": 5,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Steganography in Graphics Files\",\n",
      "              \"toc_id\": 312,\n",
      "              \"chunk_count\": 16,\n",
      "              \"total_chunks_in_branch\": 16,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Using Steganalysis Tools\",\n",
      "              \"toc_id\": 313,\n",
      "              \"chunk_count\": 7,\n",
      "              \"total_chunks_in_branch\": 7,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"deck_number\": 4,\n",
      "      \"deck_title\": \"Digital Forensic - Week 7, Lecture 4\",\n",
      "      \"session_content\": [\n",
      "        {\n",
      "          \"title\": \"Understanding Macintosh File Structures\",\n",
      "          \"toc_id\": 276,\n",
      "          \"chunk_count\": 6,\n",
      "          \"total_chunks_in_branch\": 58,\n",
      "          \"slides_allocated\": 3,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"An Overview of Mac File Structures\",\n",
      "              \"toc_id\": 277,\n",
      "              \"chunk_count\": 23,\n",
      "              \"total_chunks_in_branch\": 23,\n",
      "              \"slides_allocated\": 2,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Forensics Procedures in Mac\",\n",
      "              \"toc_id\": 278,\n",
      "              \"chunk_count\": 18,\n",
      "              \"total_chunks_in_branch\": 29,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Acquisition Methods in macOS\",\n",
      "                  \"toc_id\": 279,\n",
      "                  \"chunk_count\": 11,\n",
      "                  \"total_chunks_in_branch\": 11,\n",
      "                  \"slides_allocated\": 1,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Recognizing a Graphics File\",\n",
      "          \"toc_id\": 291,\n",
      "          \"chunk_count\": 4,\n",
      "          \"total_chunks_in_branch\": 54,\n",
      "          \"slides_allocated\": 2,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Understanding Bitmap and Raster Images\",\n",
      "              \"toc_id\": 292,\n",
      "              \"chunk_count\": 13,\n",
      "              \"total_chunks_in_branch\": 13,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Vector Graphics\",\n",
      "              \"toc_id\": 293,\n",
      "              \"chunk_count\": 2,\n",
      "              \"total_chunks_in_branch\": 2,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Metafile Graphics\",\n",
      "              \"toc_id\": 294,\n",
      "              \"chunk_count\": 2,\n",
      "              \"total_chunks_in_branch\": 2,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Graphics File Formats\",\n",
      "              \"toc_id\": 295,\n",
      "              \"chunk_count\": 14,\n",
      "              \"total_chunks_in_branch\": 14,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Digital Photograph File Formats\",\n",
      "              \"toc_id\": 296,\n",
      "              \"chunk_count\": 2,\n",
      "              \"total_chunks_in_branch\": 19,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Examining the Raw File Format\",\n",
      "                  \"toc_id\": 297,\n",
      "                  \"chunk_count\": 5,\n",
      "                  \"total_chunks_in_branch\": 5,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Examining the Exchangeable Image File Format\",\n",
      "                  \"toc_id\": 298,\n",
      "                  \"chunk_count\": 12,\n",
      "                  \"total_chunks_in_branch\": 12,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: The Data-Driven Planning Agent (Final Hierarchical Version✅)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Setup Logger and LangChain components\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "class PlanningAgent:\n",
    "    \"\"\"\n",
    "    An agent that creates a hierarchical content plan, adaptively partitions content\n",
    "    into distinct lecture decks, and allocates presentation time.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "        self.config = master_config['processed_settings']\n",
    "        self.unit_outline = master_config['unit_outline']\n",
    "        self.book_toc = master_config['book_toc']\n",
    "        self.flat_toc_with_ids = self._create_flat_toc_with_ids()\n",
    "        self.vector_store = vector_store\n",
    "        logger.info(\"Data-Driven PlanningAgent initialized successfully.\")\n",
    "\n",
    "    def _create_flat_toc_with_ids(self) -> List[Dict]:\n",
    "        \"\"\"Creates a flattened list of the ToC for easy metadata lookup.\"\"\"\n",
    "        flat_list = []\n",
    "        def flatten_recursive(nodes, counter):\n",
    "            for node in nodes:\n",
    "                node_id = counter[0]; counter[0] += 1\n",
    "                flat_list.append({'toc_id': node_id, 'title': node.get('title', ''), 'node': node})\n",
    "                if node.get('children'):\n",
    "                    flatten_recursive(node.get('children'), counter)\n",
    "        flatten_recursive(self.book_toc, [0])\n",
    "        return flat_list\n",
    "\n",
    "    def _identify_relevant_chapters(self, weekly_schedule_item: Dict) -> List[int]:\n",
    "        \"\"\"Extracts chapter numbers precisely from the 'requiredReading' string.\"\"\"\n",
    "        reading_str = weekly_schedule_item.get('requiredReading', '')\n",
    "        match = re.search(r'Chapter(s)?', reading_str, re.IGNORECASE)\n",
    "        if not match: return []\n",
    "        search_area = reading_str[match.start():]\n",
    "        chap_nums_str = re.findall(r'\\d+', search_area)\n",
    "        if chap_nums_str:\n",
    "            return sorted(list(set(int(n) for n in chap_nums_str)))\n",
    "        return []\n",
    "\n",
    "    def _find_chapter_node(self, chapter_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Finds the ToC node for a specific chapter number.\"\"\"\n",
    "        for item in self.flat_toc_with_ids:\n",
    "            if re.match(rf\"Chapter\\s{chapter_number}(?:\\D|$)\", item['title']):\n",
    "                return item['node']\n",
    "        return None\n",
    "\n",
    "    def _build_topic_plan_tree(self, toc_node: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively builds a hierarchical plan tree from any ToC node,\n",
    "        annotating it with direct and total branch chunk counts.\n",
    "        \"\"\"\n",
    "        node_metadata = next((item for item in self.flat_toc_with_ids if item['node'] is toc_node), None)\n",
    "        if not node_metadata: return {}\n",
    "\n",
    "        retrieved_docs = self.vector_store.get(where={'toc_id': node_metadata['toc_id']})\n",
    "        direct_chunk_count = len(retrieved_docs.get('ids', []))\n",
    "\n",
    "        plan_node = {\n",
    "            \"title\": node_metadata['title'],\n",
    "            \"toc_id\": node_metadata['toc_id'],\n",
    "            \"chunk_count\": direct_chunk_count,\n",
    "            \"total_chunks_in_branch\": 0,\n",
    "            \"slides_allocated\": 0,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        child_branch_total = 0\n",
    "        for child_node in toc_node.get('children', []):\n",
    "            if any(ex in child_node.get('title', '').lower() for ex in [\"review\", \"introduction\", \"summary\", \"key terms\"]):\n",
    "                continue\n",
    "            child_plan_node = self._build_topic_plan_tree(child_node)\n",
    "            if child_plan_node:\n",
    "                plan_node['children'].append(child_plan_node)\n",
    "                child_branch_total += child_plan_node.get('total_chunks_in_branch', 0)\n",
    "        \n",
    "        plan_node['total_chunks_in_branch'] = direct_chunk_count + child_branch_total\n",
    "        return plan_node\n",
    "    \n",
    "    def _allocate_slides_to_tree(self, plan_tree: Dict, content_slides_budget: int):\n",
    "        \"\"\"Performs a two-pass safety-net allocation on a hierarchical plan tree.\"\"\"\n",
    "        leaf_nodes = []\n",
    "        def find_leaves(node):\n",
    "            if not node.get('children'):\n",
    "                leaf_nodes.append(node)\n",
    "            for child in node.get('children', []):\n",
    "                find_leaves(child)\n",
    "        find_leaves(plan_tree)\n",
    "\n",
    "        if not leaf_nodes or content_slides_budget <= 0: return plan_tree\n",
    "\n",
    "        # Pass 1: Safety Net\n",
    "        slides_per_topic = 1 if content_slides_budget >= len(leaf_nodes) else 0\n",
    "        for node in leaf_nodes:\n",
    "            node['slides_allocated'] = slides_per_topic\n",
    "        \n",
    "        remaining_budget = content_slides_budget - (len(leaf_nodes) * slides_per_topic)\n",
    "\n",
    "        # Pass 2: Proportional Distribution\n",
    "        if remaining_budget > 0:\n",
    "            total_leaf_chunks = sum(node['chunk_count'] for node in leaf_nodes)\n",
    "            if total_leaf_chunks > 0:\n",
    "                # Distribute remaining slides based on chunk weight\n",
    "                for node in leaf_nodes:\n",
    "                    node['slides_allocated'] += round((node['chunk_count'] / total_leaf_chunks) * remaining_budget)\n",
    "        \n",
    "        # Pass 3: Sum totals upwards\n",
    "        def sum_slides_upwards(node):\n",
    "            if not node.get('children'):\n",
    "                return node['slides_allocated']\n",
    "            node['slides_allocated'] = sum(sum_slides_upwards(child) for child in node['children'])\n",
    "            return node['slides_allocated']\n",
    "        sum_slides_upwards(plan_tree)\n",
    "        return plan_tree\n",
    "\n",
    "    def create_content_plan_for_week(self, week_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Orchestrates the adaptive planning and partitioning process.\"\"\"\n",
    "        print_header(f\"Planning Week {week_number}\", char=\"*\")\n",
    "        \n",
    "        weekly_schedule_item = self.unit_outline['weeklySchedule'][week_number - 1]\n",
    "        chapter_numbers = self._identify_relevant_chapters(weekly_schedule_item)\n",
    "        if not chapter_numbers: return None\n",
    "\n",
    "        num_decks = self.config['week_session_setup'].get('sessions_per_week', 1)\n",
    "        \n",
    "        # 1. Build a full plan tree for each chapter to get its weight.\n",
    "        chapter_plan_trees = [self._build_topic_plan_tree(self._find_chapter_node(cn)) for cn in chapter_numbers if self._find_chapter_node(cn)]\n",
    "        total_weekly_chunks = sum(tree.get('total_chunks_in_branch', 0) for tree in chapter_plan_trees)\n",
    "\n",
    "        # 2. NEW: Adaptive Partitioning Strategy\n",
    "        partitionable_units = []\n",
    "        num_chapters = len(chapter_plan_trees)\n",
    "        \n",
    "        if num_chapters >= num_decks:\n",
    "            logger.info(f\"Partitioning strategy: Distributing {num_chapters} whole chapters across {num_decks} decks.\")\n",
    "            partitionable_units = chapter_plan_trees\n",
    "        else:\n",
    "            logger.info(f\"Partitioning strategy: Splitting sub-topics from {num_chapters} chapter(s) across {num_decks} decks.\")\n",
    "            for chapter_tree in chapter_plan_trees:\n",
    "                partitionable_units.extend(chapter_tree.get('children', []))\n",
    "        \n",
    "        # 3. Partition the chosen units into decks using a bin-packing algorithm\n",
    "        decks = [[] for _ in range(num_decks)]\n",
    "        deck_weights = [0] * num_decks\n",
    "        sorted_units = sorted(partitionable_units, key=lambda x: x.get('total_chunks_in_branch', 0), reverse=True)\n",
    "        \n",
    "        for unit in sorted_units:\n",
    "            lightest_deck_index = deck_weights.index(min(deck_weights))\n",
    "            decks[lightest_deck_index].append(unit)\n",
    "            deck_weights[lightest_deck_index] += unit.get('total_chunks_in_branch', 0)\n",
    "\n",
    "        # 4. Plan each deck\n",
    "        content_slides_per_week = self.config['slide_count_strategy'].get('target', 25)\n",
    "        final_deck_plans = []\n",
    "        for i, deck_content_trees in enumerate(decks):\n",
    "            deck_number = i + 1\n",
    "            deck_chunk_weight = sum(tree.get('total_chunks_in_branch', 0) for tree in deck_content_trees)\n",
    "            deck_slide_budget = round((deck_chunk_weight / total_weekly_chunks) * content_slides_per_week) if total_weekly_chunks > 0 else 0\n",
    "\n",
    "            logger.info(f\"--- Planning Deck {deck_number}/{num_decks} | Topics: {[t['title'] for t in deck_content_trees]} | Weight: {deck_chunk_weight} chunks | Slide Budget: {deck_slide_budget} ---\")\n",
    "            \n",
    "            # The allocation function is recursive and works on any tree or sub-tree\n",
    "            planned_content = [self._allocate_slides_to_tree(tree, round(deck_slide_budget * (tree.get('total_chunks_in_branch', 0) / deck_chunk_weight))) if deck_chunk_weight > 0 else tree for tree in deck_content_trees]\n",
    "            \n",
    "            final_deck_plans.append({\n",
    "                \"deck_number\": deck_number,\n",
    "                \"deck_title\": f\"{self.config.get('unit_name', 'Course')} - Week {week_number}, Lecture {deck_number}\",\n",
    "                \"session_content\": planned_content\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"week\": week_number,\n",
    "            \"overall_topic\": weekly_schedule_item.get('contentTopic'),\n",
    "            \"deck_plans\": final_deck_plans\n",
    "        }\n",
    "\n",
    "# --- EXECUTION BLOCK (No Changes Needed Here) ---\n",
    "logger.info(\"--- Initializing Data-Driven Planning Agent Test ---\")\n",
    "\n",
    "if langchain_available:\n",
    "    logger.info(\"Connecting to ChromaDB for the Planning Agent...\")\n",
    "    try:\n",
    "        # NOTE: Ensure these global variables are defined in your notebook from Cell 1\n",
    "        # CHROMA_PERSIST_DIR, EMBEDDING_MODEL_OLLAMA, CHROMA_COLLECTION_NAME\n",
    "        # CONFIG_DIR, PRE_EXTRACTED_TOC_JSON_PATH, PARSED_UO_JSON_PATH, PROJECT_BASE_DIR\n",
    "        \n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        logger.info(\"Database connection successful.\")\n",
    "\n",
    "        logger.info(\"Loading configuration files...\")\n",
    "        with open(os.path.join(CONFIG_DIR, \"processed_settings.json\"), 'r') as f:\n",
    "            processed_settings = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r') as f:\n",
    "            book_toc = json.load(f)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r') as f:\n",
    "            unit_outline = json.load(f)\n",
    "        logger.info(\"Configuration files loaded.\")\n",
    "\n",
    "        master_config_from_file = {\n",
    "            \"processed_settings\": processed_settings,\n",
    "            \"unit_outline\": unit_outline,\n",
    "            \"book_toc\": book_toc\n",
    "        }\n",
    "\n",
    "        planning_agent = PlanningAgent(master_config_from_file, vector_store=vector_store)\n",
    "\n",
    "        WEEK_TO_TEST = 7 # Changed to Week 6 to match your example\n",
    "        logger.info(f\"--> Explicitly testing planning logic for Week {WEEK_TO_TEST}\")\n",
    "        content_plan = planning_agent.create_content_plan_for_week(WEEK_TO_TEST)\n",
    "\n",
    "        if content_plan:\n",
    "            print(\"\\n--- Generated Content Plan (Hierarchical & Partitioned) ---\")\n",
    "            print(json.dumps(content_plan, indent=2))\n",
    "\n",
    "            PLAN_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_plans\")\n",
    "            os.makedirs(PLAN_OUTPUT_DIR, exist_ok=True)\n",
    "            plan_filename = f\"{processed_settings.get('course_id', 'COURSE')}_Week{WEEK_TO_TEST}_plan.json\"\n",
    "            plan_filepath = os.path.join(PLAN_OUTPUT_DIR, plan_filename)\n",
    "            with open(plan_filepath, 'w') as f:\n",
    "                json.dump(content_plan, f, indent=2)\n",
    "            logger.info(f\"\\nSuccessfully saved content plan for Week {WEEK_TO_TEST} to: {plan_filepath}\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to generate content plan for Week {WEEK_TO_TEST}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the planning process: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.error(\"LangChain/Chroma libraries not found. Cannot run the Planning Agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 10:25:21,794 - INFO - --- Initializing Data-Driven Planning Agent Test ---\n",
      "2025-07-02 10:25:21,795 - INFO - Connecting to ChromaDB for the Planning Agent...\n",
      "2025-07-02 10:25:21,806 - INFO - Database connection successful.\n",
      "2025-07-02 10:25:21,808 - INFO - Data-Driven PlanningAgent initialized successfully.\n",
      "2025-07-02 10:25:21,809 - INFO - --> Explicitly testing planning logic for Week 6\n",
      "2025-07-02 10:25:21,810 - INFO - --- Planning Session 1 (Chapter 6) ---\n",
      "2025-07-02 10:25:21,851 - INFO - Allocating a budget of 25 slides across 22 granular topics.\n",
      "2025-07-02 10:25:21,852 - INFO - \n",
      "Successfully saved content plan for Week 6 to: /home/sebas_dev_linux/projects/course_generator/generated_plans/ICT312_Week6_plan.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                                Planning Week 6                                 \n",
      "********************************************************************************\n",
      "\n",
      "********************************************************************************\n",
      "                     Plan for Week 6 Generated Successfully                     \n",
      "********************************************************************************\n",
      "\n",
      "--- Generated Content Plan (Data-Driven Allocation) ---\n",
      "{\n",
      "  \"week\": 6,\n",
      "  \"overall_topic\": \"Current Computer Forensics Tools.\",\n",
      "  \"sessions\": [\n",
      "    {\n",
      "      \"session_number\": 1,\n",
      "      \"session_topic\": \"Chapter 6. Current Digital Forensics Tools\",\n",
      "      \"topics_to_cover\": {\n",
      "        \"title\": \"Chapter 6. Current Digital Forensics Tools\",\n",
      "        \"toc_id\": 231,\n",
      "        \"chunk_count\": 22,\n",
      "        \"total_chunks_in_branch\": 315,\n",
      "        \"slides_allocated\": 23,\n",
      "        \"children\": [\n",
      "          {\n",
      "            \"title\": \"Evaluating Digital Forensics Tool Needs\",\n",
      "            \"toc_id\": 233,\n",
      "            \"chunk_count\": 10,\n",
      "            \"total_chunks_in_branch\": 184,\n",
      "            \"slides_allocated\": 10,\n",
      "            \"children\": [\n",
      "              {\n",
      "                \"title\": \"Types of Digital Forensics Tools\",\n",
      "                \"toc_id\": 234,\n",
      "                \"chunk_count\": 4,\n",
      "                \"total_chunks_in_branch\": 11,\n",
      "                \"slides_allocated\": 2,\n",
      "                \"children\": [\n",
      "                  {\n",
      "                    \"title\": \"Hardware Forensics Tools\",\n",
      "                    \"toc_id\": 235,\n",
      "                    \"chunk_count\": 2,\n",
      "                    \"total_chunks_in_branch\": 2,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Software Forensics Tools\",\n",
      "                    \"toc_id\": 236,\n",
      "                    \"chunk_count\": 5,\n",
      "                    \"total_chunks_in_branch\": 5,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Tasks Performed by Digital Forensics Tools\",\n",
      "                \"toc_id\": 237,\n",
      "                \"chunk_count\": 3,\n",
      "                \"total_chunks_in_branch\": 153,\n",
      "                \"slides_allocated\": 6,\n",
      "                \"children\": [\n",
      "                  {\n",
      "                    \"title\": \"Acquisition\",\n",
      "                    \"toc_id\": 238,\n",
      "                    \"chunk_count\": 22,\n",
      "                    \"total_chunks_in_branch\": 22,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Validation and Verification\",\n",
      "                    \"toc_id\": 239,\n",
      "                    \"chunk_count\": 14,\n",
      "                    \"total_chunks_in_branch\": 14,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Extraction\",\n",
      "                    \"toc_id\": 240,\n",
      "                    \"chunk_count\": 25,\n",
      "                    \"total_chunks_in_branch\": 25,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Reconstruction\",\n",
      "                    \"toc_id\": 241,\n",
      "                    \"chunk_count\": 66,\n",
      "                    \"total_chunks_in_branch\": 66,\n",
      "                    \"slides_allocated\": 2,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Reporting\",\n",
      "                    \"toc_id\": 242,\n",
      "                    \"chunk_count\": 23,\n",
      "                    \"total_chunks_in_branch\": 23,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Tool Comparisons\",\n",
      "                \"toc_id\": 243,\n",
      "                \"chunk_count\": 6,\n",
      "                \"total_chunks_in_branch\": 6,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Other Considerations for Tools\",\n",
      "                \"toc_id\": 244,\n",
      "                \"chunk_count\": 4,\n",
      "                \"total_chunks_in_branch\": 4,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"title\": \"Digital Forensics Software Tools\",\n",
      "            \"toc_id\": 245,\n",
      "            \"chunk_count\": 4,\n",
      "            \"total_chunks_in_branch\": 41,\n",
      "            \"slides_allocated\": 7,\n",
      "            \"children\": [\n",
      "              {\n",
      "                \"title\": \"Command-Line Forensics Tools\",\n",
      "                \"toc_id\": 246,\n",
      "                \"chunk_count\": 14,\n",
      "                \"total_chunks_in_branch\": 14,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Linux Forensics Tools\",\n",
      "                \"toc_id\": 247,\n",
      "                \"chunk_count\": 4,\n",
      "                \"total_chunks_in_branch\": 19,\n",
      "                \"slides_allocated\": 5,\n",
      "                \"children\": [\n",
      "                  {\n",
      "                    \"title\": \"Smart\",\n",
      "                    \"toc_id\": 248,\n",
      "                    \"chunk_count\": 4,\n",
      "                    \"total_chunks_in_branch\": 4,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Helix 3\",\n",
      "                    \"toc_id\": 249,\n",
      "                    \"chunk_count\": 3,\n",
      "                    \"total_chunks_in_branch\": 3,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Kali Linux\",\n",
      "                    \"toc_id\": 250,\n",
      "                    \"chunk_count\": 2,\n",
      "                    \"total_chunks_in_branch\": 2,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Autopsy and Sleuth Kit\",\n",
      "                    \"toc_id\": 251,\n",
      "                    \"chunk_count\": 4,\n",
      "                    \"total_chunks_in_branch\": 4,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Forcepoint Threat Protection\",\n",
      "                    \"toc_id\": 252,\n",
      "                    \"chunk_count\": 2,\n",
      "                    \"total_chunks_in_branch\": 2,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Other GUI Forensics Tools\",\n",
      "                \"toc_id\": 253,\n",
      "                \"chunk_count\": 4,\n",
      "                \"total_chunks_in_branch\": 4,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"title\": \"Digital Forensics Hardware Tools\",\n",
      "            \"toc_id\": 254,\n",
      "            \"chunk_count\": 3,\n",
      "            \"total_chunks_in_branch\": 38,\n",
      "            \"slides_allocated\": 3,\n",
      "            \"children\": [\n",
      "              {\n",
      "                \"title\": \"Forensic Workstations\",\n",
      "                \"toc_id\": 255,\n",
      "                \"chunk_count\": 7,\n",
      "                \"total_chunks_in_branch\": 13,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": [\n",
      "                  {\n",
      "                    \"title\": \"Building Your Own Workstation\",\n",
      "                    \"toc_id\": 256,\n",
      "                    \"chunk_count\": 6,\n",
      "                    \"total_chunks_in_branch\": 6,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Using a Write-Blocker\",\n",
      "                \"toc_id\": 257,\n",
      "                \"chunk_count\": 17,\n",
      "                \"total_chunks_in_branch\": 17,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Recommendations for a Forensic Workstation\",\n",
      "                \"toc_id\": 258,\n",
      "                \"chunk_count\": 5,\n",
      "                \"total_chunks_in_branch\": 5,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"title\": \"Validating and Testing Forensics Software\",\n",
      "            \"toc_id\": 259,\n",
      "            \"chunk_count\": 2,\n",
      "            \"total_chunks_in_branch\": 30,\n",
      "            \"slides_allocated\": 3,\n",
      "            \"children\": [\n",
      "              {\n",
      "                \"title\": \"Using National Institute of Standards and Technology Tools\",\n",
      "                \"toc_id\": 260,\n",
      "                \"chunk_count\": 13,\n",
      "                \"total_chunks_in_branch\": 13,\n",
      "                \"slides_allocated\": 1,\n",
      "                \"children\": []\n",
      "              },\n",
      "              {\n",
      "                \"title\": \"Using Validation Protocols\",\n",
      "                \"toc_id\": 261,\n",
      "                \"chunk_count\": 6,\n",
      "                \"total_chunks_in_branch\": 15,\n",
      "                \"slides_allocated\": 2,\n",
      "                \"children\": [\n",
      "                  {\n",
      "                    \"title\": \"Digital Forensics Examination Protocol\",\n",
      "                    \"toc_id\": 262,\n",
      "                    \"chunk_count\": 5,\n",
      "                    \"total_chunks_in_branch\": 5,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  },\n",
      "                  {\n",
      "                    \"title\": \"Digital Forensics Tool Upgrade Protocol\",\n",
      "                    \"toc_id\": 263,\n",
      "                    \"chunk_count\": 4,\n",
      "                    \"total_chunks_in_branch\": 4,\n",
      "                    \"slides_allocated\": 1,\n",
      "                    \"children\": []\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: The Data-Driven Planning Agent (Old version🔴)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Setup Logger and LangChain components\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "class PlanningAgent:\n",
    "    \"\"\"\n",
    "    An agent that creates a content plan driven by the syllabus and uses\n",
    "    data-driven metrics (chunk counts) to allocate presentation time.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "        self.config = master_config['processed_settings']\n",
    "        self.unit_outline = master_config['unit_outline']\n",
    "        self.book_toc = master_config['book_toc']\n",
    "        self.flat_toc_with_ids = self._create_flat_toc_with_ids()\n",
    "        # The agent now requires access to the vector store to get chunk counts\n",
    "        self.vector_store = vector_store\n",
    "        logger.info(\"Data-Driven PlanningAgent initialized successfully.\")\n",
    "\n",
    "    def _create_flat_toc_with_ids(self) -> List[Dict]:\n",
    "        \"\"\"Creates a flattened list of the ToC with a unique sequential ID for each node.\"\"\"\n",
    "        flat_list = []\n",
    "        def flatten_recursive(nodes, counter):\n",
    "            for node in nodes:\n",
    "                node_id = counter[0]; counter[0] += 1\n",
    "                flat_list.append({'toc_id': node_id, 'title': node['title'], 'node': node})\n",
    "                if node.get('children'):\n",
    "                    flatten_recursive(node.get('children'), counter)\n",
    "        flatten_recursive(self.book_toc, [0])\n",
    "        return flat_list\n",
    "\n",
    "    def _identify_relevant_chapters(self, weekly_schedule_item: Dict) -> List[int]:\n",
    "        \"\"\"Extracts chapter numbers precisely from the 'requiredReading' string.\"\"\"\n",
    "        reading_str = weekly_schedule_item.get('requiredReading', '')\n",
    "        match = re.search(r'Chapter(s)?', reading_str, re.IGNORECASE)\n",
    "        if not match: return []\n",
    "        search_area = reading_str[match.start():]\n",
    "        chap_nums_str = re.findall(r'\\d+', search_area)\n",
    "        if chap_nums_str:\n",
    "            return sorted(list(set(int(n) for n in chap_nums_str)))\n",
    "        return []\n",
    "\n",
    "    def _find_chapter_node(self, chapter_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Accurately finds the ToC node for a specific chapter number.\"\"\"\n",
    "        for item in self.flat_toc_with_ids:\n",
    "            title = item['title']\n",
    "            if re.match(rf\"Chapter\\s{chapter_number}(?:\\D|$)\", title):\n",
    "                return item['node']\n",
    "        return None\n",
    "\n",
    "    def _get_chapter_sub_topics(self, chapter_node: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recursively traverses a chapter node to find all teachable sub-topics (nodes that have no children).\n",
    "        This ensures we plan based on the most granular topics available.\n",
    "        \"\"\"\n",
    "        sub_topics = []\n",
    "        \n",
    "        def find_leaf_nodes_recursive(node: Dict):\n",
    "            # A \"leaf\" or \"teachable topic\" is a node that has no further sub-topics (children).\n",
    "            is_leaf_node = not node.get('children')\n",
    "\n",
    "            # Exclude high-level container topics that are not granular enough to teach directly.\n",
    "            # You can customize this list.\n",
    "            excluded_titles = [\"review\", \"introduction\", \"summary\", \"key terms\"]\n",
    "            title_lower = node.get('title', '').lower()\n",
    "            if any(excluded in title_lower for excluded in excluded_titles):\n",
    "                return\n",
    "\n",
    "            if is_leaf_node:\n",
    "                # Find the full metadata for this node from our flattened ToC list\n",
    "                matched_item = next((item for item in self.flat_toc_with_ids if item['node'] is node), None)\n",
    "                if matched_item:\n",
    "                    sub_topics.append({\n",
    "                        \"title\": matched_item['title'], \n",
    "                        \"toc_id\": matched_item['toc_id']\n",
    "                    })\n",
    "            else:\n",
    "                # If it's not a leaf, continue diving into its children.\n",
    "                for child_node in node.get('children', []):\n",
    "                    find_leaf_nodes_recursive(child_node)\n",
    "\n",
    "        # Start the recursive search from the main chapter node.\n",
    "        find_leaf_nodes_recursive(chapter_node)\n",
    "        \n",
    "        return sub_topics\n",
    "\n",
    "    def _build_topic_plan_tree(self, chapter_node: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively builds a hierarchical plan tree from a ToC chapter node,\n",
    "        annotating each node with its direct and total branch chunk counts.\n",
    "        \"\"\"\n",
    "        # Find the full metadata for this node from our flattened ToC list\n",
    "        node_metadata = next((item for item in self.flat_toc_with_ids if item['node'] is chapter_node), None)\n",
    "        if not node_metadata:\n",
    "            return {}\n",
    "\n",
    "        # Get the chunk count for THIS specific level (Direct Chunks)\n",
    "        retrieved_docs = self.vector_store.get(where={'toc_id': node_metadata['toc_id']})\n",
    "        direct_chunk_count = len(retrieved_docs.get('ids', []))\n",
    "\n",
    "        plan_node = {\n",
    "            \"title\": node_metadata['title'],\n",
    "            \"toc_id\": node_metadata['toc_id'],\n",
    "            \"chunk_count\": direct_chunk_count,\n",
    "            \"total_chunks_in_branch\": 0, # Initialize the new field\n",
    "            \"slides_allocated\": 0,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        # --- RECURSION and SUMMATION LOGIC ---\n",
    "        \n",
    "        # 1. Recursively build all children for the current node\n",
    "        child_branch_total = 0\n",
    "        for child_node in chapter_node.get('children', []):\n",
    "            # Exclude non-teachable summary sections\n",
    "            title_lower = child_node.get('title', '').lower()\n",
    "            if any(excluded in title_lower for excluded in [\"review\", \"introduction\", \"summary\", \"key terms\"]):\n",
    "                continue\n",
    "                \n",
    "            child_plan_node = self._build_topic_plan_tree(child_node)\n",
    "            if child_plan_node:\n",
    "                plan_node['children'].append(child_plan_node)\n",
    "                # 2. Sum the total branch count from the returned children\n",
    "                child_branch_total += child_plan_node.get('total_chunks_in_branch', 0)\n",
    "        \n",
    "        # 3. The total for this node is its direct chunks + the sum of its children's branches.\n",
    "        plan_node['total_chunks_in_branch'] = direct_chunk_count + child_branch_total\n",
    "        \n",
    "        return plan_node\n",
    "    \n",
    "    def _allocate_slides_to_tree(self, plan_tree: Dict, content_slides_budget: int):\n",
    "        \"\"\"\n",
    "        Performs a two-pass \"safety net\" allocation on a hierarchical plan tree.\n",
    "        \"\"\"\n",
    "        leaf_nodes = []\n",
    "        def find_leaves(node):\n",
    "            if not node.get('children'):\n",
    "                leaf_nodes.append(node)\n",
    "            for child in node.get('children', []):\n",
    "                find_leaves(child)\n",
    "        \n",
    "        find_leaves(plan_tree)\n",
    "\n",
    "        if not leaf_nodes:\n",
    "            return plan_tree # No teachable topics found\n",
    "\n",
    "        logger.info(f\"Allocating a budget of {content_slides_budget} slides across {len(leaf_nodes)} granular topics.\")\n",
    "        \n",
    "        # --- Pass 1: Safety Net Allocation ---\n",
    "        # Give every leaf topic 1 slide to ensure it's covered.\n",
    "        for node in leaf_nodes:\n",
    "            node['slides_allocated'] = 1\n",
    "        \n",
    "        remaining_budget = content_slides_budget - len(leaf_nodes)\n",
    "\n",
    "        # --- Pass 2: Proportional Distribution of Remainder ---\n",
    "        if remaining_budget > 0:\n",
    "            # Calculate total chunks ONLY among the leaf nodes that will receive more slides\n",
    "            total_leaf_chunks = sum(node['chunk_count'] for node in leaf_nodes)\n",
    "\n",
    "            if total_leaf_chunks > 0:\n",
    "                for node in leaf_nodes:\n",
    "                    proportion = node['chunk_count'] / total_leaf_chunks\n",
    "                    additional_slides = round(proportion * remaining_budget)\n",
    "                    node['slides_allocated'] += additional_slides\n",
    "        \n",
    "        # --- Pass 3: Sum totals up the tree ---\n",
    "        def sum_slides_upwards(node):\n",
    "            if not node.get('children'):\n",
    "                return node['slides_allocated']\n",
    "            \n",
    "            child_slide_total = sum(sum_slides_upwards(child) for child in node['children'])\n",
    "            node['slides_allocated'] = child_slide_total\n",
    "            return child_slide_total\n",
    "\n",
    "        sum_slides_upwards(plan_tree)\n",
    "        return plan_tree\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --- NEW DATA-DRIVEN ALLOCATION METHOD ---\n",
    "    def _allocate_slides_by_chunk_count(self, topics: List[Dict], \n",
    "                                    content_slides_budget: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Allocates slides proportionally based on the number of chunks \n",
    "        associated with each topic.\n",
    "        \"\"\"\n",
    "        if not self.vector_store:\n",
    "            logger.error(\"Vector store not available. Cannot perform chunk count allocation. Falling back to even distribution.\")\n",
    "            # Fallback logic: Distribute slides evenly if DB is not available\n",
    "            num_topics = len(topics)\n",
    "            if num_topics == 0: return []\n",
    "            slides_per_topic = content_slides_budget // num_topics\n",
    "            for topic in topics:\n",
    "                topic['chunk_count'] = 0\n",
    "                topic['slides_allocated'] = slides_per_topic\n",
    "            return topics\n",
    "\n",
    "        logger.info(f\"Allocating a budget of {content_slides_budget} content slides across {len(topics)} topics...\")\n",
    "    \n",
    "        # Step 1: Get chunk counts (your existing logic here is now correct)\n",
    "        total_chunks_in_session = 0\n",
    "        for topic in topics:\n",
    "            retrieved_docs = self.vector_store.get(where={'toc_id': topic['toc_id']})\n",
    "            count = len(retrieved_docs.get('ids', []))\n",
    "            topic['chunk_count'] = count\n",
    "            total_chunks_in_session += count\n",
    "            \n",
    "        logger.info(f\"Found a total of {total_chunks_in_session} chunks for this session's topics.\")\n",
    "\n",
    "        if total_chunks_in_session == 0:\n",
    "            logger.warning(\"No chunks found for any topics. Distributing slides evenly.\")\n",
    "            slides_per_topic = content_slides_budget // len(topics) if topics else 0\n",
    "            for topic in topics:\n",
    "                topic['slides_allocated'] = slides_per_topic\n",
    "            return topics\n",
    "\n",
    "        # Step 2: Allocate slides proportionally using the content-only budget\n",
    "        slides_allocated_so_far = 0\n",
    "        for topic in topics:\n",
    "            proportion = topic['chunk_count'] / total_chunks_in_session\n",
    "            # Use round() for a more balanced initial allocation instead of floor()\n",
    "            topic['slides_allocated'] = round(proportion * content_slides_budget)\n",
    "            slides_allocated_so_far += topic['slides_allocated']\n",
    "\n",
    "        # Step 3: Distribute remainder/deficit to topics with the most chunks to match the budget\n",
    "        # This handles cases where rounding causes a mismatch with the budget.\n",
    "        difference = content_slides_budget - slides_allocated_so_far\n",
    "        \n",
    "        sorted_topics = sorted(topics, key=lambda x: x.get('chunk_count', 0), reverse=True)\n",
    "        \n",
    "        # Adjust slide counts up or down to meet the exact budget\n",
    "        for i in range(abs(difference)):\n",
    "            # If we allocated too many, subtract from the smallest topics first by reversing the sort\n",
    "            if difference < 0:\n",
    "                topic_to_adjust = sorted_topics[-(i + 1)]\n",
    "                # Don't let a slide count go below 1 if it has chunks\n",
    "                if topic_to_adjust['slides_allocated'] > 1:\n",
    "                    topic_to_adjust['slides_allocated'] -= 1\n",
    "            else: # If we allocated too few, add to the largest topics\n",
    "                topic_to_adjust = sorted_topics[i % len(sorted_topics)]\n",
    "                topic_to_adjust['slides_allocated'] += 1\n",
    "                \n",
    "        return topics\n",
    "\n",
    "    def create_content_plan_for_week(self, week_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Orchestrates the new hierarchical planning process for a single week.\"\"\"\n",
    "        print_header(f\"Planning Week {week_number}\", char=\"*\")\n",
    "        \n",
    "        # Get weekly data and chapter numbers (no change here)\n",
    "        weekly_schedule_item = self.unit_outline['weeklySchedule'][week_number - 1]\n",
    "        chapter_numbers = self._identify_relevant_chapters(weekly_schedule_item)\n",
    "        if not chapter_numbers:\n",
    "            logger.error(\"No valid chapter numbers found. Aborting.\")\n",
    "            return None\n",
    "\n",
    "        content_slides_per_week = self.config['slide_count_strategy'].get('target', 25)\n",
    "        slides_per_session = content_slides_per_week // len(chapter_numbers) if chapter_numbers else content_slides_per_week\n",
    "\n",
    "        final_sessions_plan = []\n",
    "        for i, chap_num in enumerate(chapter_numbers):\n",
    "            logger.info(f\"--- Planning Session {i+1} (Chapter {chap_num}) ---\")\n",
    "            chapter_node = self._find_chapter_node(chap_num)\n",
    "            if not chapter_node: continue\n",
    "\n",
    "            # 1. Build the hierarchical plan tree for the chapter\n",
    "            plan_tree = self._build_topic_plan_tree(chapter_node)\n",
    "            \n",
    "            # 2. Allocate slides to the hierarchical tree\n",
    "            plan_with_slides = self._allocate_slides_to_tree(plan_tree, slides_per_session)\n",
    "\n",
    "            final_sessions_plan.append({\n",
    "                \"session_number\": i + 1,\n",
    "                # The 'session_topic' is the root of our plan\n",
    "                \"session_topic\": plan_with_slides['title'],\n",
    "                # 'topics_to_cover' is now the entire hierarchical tree\n",
    "                \"topics_to_cover\": plan_with_slides\n",
    "            })\n",
    "\n",
    "        if not final_sessions_plan: return None\n",
    "        \n",
    "        week_plan = {\n",
    "            \"week\": week_number,\n",
    "            \"overall_topic\": weekly_schedule_item.get('contentTopic'),\n",
    "            \"sessions\": final_sessions_plan\n",
    "        }\n",
    "\n",
    "        print_header(f\"Plan for Week {week_number} Generated Successfully\", char=\"*\")\n",
    "        return week_plan\n",
    "\n",
    "# --- EXECUTION BLOCK ---\n",
    "logger.info(\"--- Initializing Data-Driven Planning Agent Test ---\")\n",
    "\n",
    "# Check if LangChain components are available before proceeding\n",
    "if langchain_available:\n",
    "\n",
    "    # 1. --- Connect to the Database FIRST ---\n",
    "    # The agent requires a live vector store connection to count chunks.\n",
    "    logger.info(\"Connecting to ChromaDB for the Planning Agent...\")\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    logger.info(\"Database connection successful.\")\n",
    "\n",
    "    # 2. --- Load all configuration files ---\n",
    "    # This part of your code is correct.\n",
    "    with open(os.path.join(CONFIG_DIR, \"processed_settings.json\"), 'r') as f:\n",
    "        processed_settings = json.load(f)\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r') as f:\n",
    "        book_toc = json.load(f)\n",
    "    with open(PARSED_UO_JSON_PATH, 'r') as f:\n",
    "        unit_outline = json.load(f)\n",
    "\n",
    "    master_config_from_file = {\n",
    "        \"processed_settings\": processed_settings,\n",
    "        \"unit_outline\": unit_outline,\n",
    "        \"book_toc\": book_toc\n",
    "    }\n",
    "\n",
    "    # 3. --- Initialize the agent, PASSING the vector store connection ---\n",
    "    # This is the crucial step.\n",
    "    planning_agent = PlanningAgent(master_config_from_file, vector_store=vector_store)\n",
    "\n",
    "    # 4. --- Run the test ---\n",
    "    WEEK_TO_TEST = 6\n",
    "    logger.info(f\"--> Explicitly testing planning logic for Week {WEEK_TO_TEST}\")\n",
    "    content_plan = planning_agent.create_content_plan_for_week(WEEK_TO_TEST)\n",
    "\n",
    "    # 5. --- Save and print the output ---\n",
    "    if content_plan:\n",
    "        print(\"\\n--- Generated Content Plan (Data-Driven Allocation) ---\")\n",
    "        print(json.dumps(content_plan, indent=2))\n",
    "\n",
    "        # Save the plan to a file\n",
    "        PLAN_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_plans\")\n",
    "        os.makedirs(PLAN_OUTPUT_DIR, exist_ok=True)\n",
    "        plan_filename = f\"{processed_settings['course_id']}_Week{WEEK_TO_TEST}_plan.json\"\n",
    "        plan_filepath = os.path.join(PLAN_OUTPUT_DIR, plan_filename)\n",
    "        with open(plan_filepath, 'w') as f:\n",
    "            json.dump(content_plan, f, indent=2)\n",
    "        logger.info(f\"\\nSuccessfully saved content plan for Week {WEEK_TO_TEST} to: {plan_filepath}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to generate content plan for Week {WEEK_TO_TEST}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d7393",
   "metadata": {},
   "source": [
    "Next steps in the plan\n",
    "- Add the sorted chunks for each slide to process the summaries or content geneneration later \n",
    "- Add title, agenda, summary and end as part of this planning to start having \n",
    "- Add label to reference title, agenda, content, summary and end \n",
    "- Process the images from the book and store them with relation to the chunk so we can potentially use the image in the slides \n",
    "- Process unit outlines and store them with good labels for phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52692ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 21:38:19,164 - INFO - --- Starting Diagnostic Comparison Dashboard for Week 7 ---\n",
      "2025-07-01 21:38:19,165 - INFO - Successfully loaded all raw data files.\n",
      "2025-07-01 21:38:19,166 - INFO - Data-Driven PlanningAgent initialized successfully.\n",
      "2025-07-01 21:38:19,166 - INFO - --- Planning Session 1 (Chapter 7) ---\n",
      "2025-07-01 21:38:19,167 - ERROR - Vector store not available. Cannot perform chunk count allocation.\n",
      "2025-07-01 21:38:19,167 - INFO - --- Planning Session 2 (Chapter 8) ---\n",
      "2025-07-01 21:38:19,167 - ERROR - Vector store not available. Cannot perform chunk count allocation.\n",
      "2025-07-01 21:38:19,168 - INFO - Connecting to ChromaDB to verify chunk counts...\n",
      "2025-07-01 21:38:19,177 - INFO - Successfully connected to ChromaDB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC DASHBOARD: Ground Truth: Weekly Schedule from Unit Outline\n",
      "================================================================================\n",
      "--- Input for Week 7 ---\n",
      "{\n",
      "  \"week\": \"Week 7\",\n",
      "  \"contentTopic\": \"Linux Boot Processes and File Systems. Recovering Graphics Files.\",\n",
      "  \"requiredReading\": \"Nelson, Phillips, Steuart,\\u00a0Guide to Computer Forensics and Investigations, Sixth Edition, Cengage Learning 2019, ISBN:978-1-337-56894-4 Chapters 7 &8\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC DASHBOARD: Generated Plan\n",
      "================================================================================\n",
      "\n",
      "********************************************************************************\n",
      "                                Planning Week 7                                 \n",
      "********************************************************************************\n",
      "\n",
      "********************************************************************************\n",
      "                     Plan for Week 7 Generated Successfully                     \n",
      "********************************************************************************\n",
      "{\n",
      "  \"week\": 7,\n",
      "  \"overall_topic\": \"Linux Boot Processes and File Systems. Recovering Graphics Files.\",\n",
      "  \"sessions\": [\n",
      "    {\n",
      "      \"session_number\": 1,\n",
      "      \"session_topic\": \"Chapter 7. Linux and Macintosh File Systems\",\n",
      "      \"topics_to_cover\": [\n",
      "        {\n",
      "          \"title\": \"Examining Linux File Structures\",\n",
      "          \"toc_id\": 272\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Understanding Macintosh File Structures\",\n",
      "          \"toc_id\": 276\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Using Linux Forensics Tools\",\n",
      "          \"toc_id\": 280\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"session_number\": 2,\n",
      "      \"session_topic\": \"Chapter 8. Recovering Graphics Files\",\n",
      "      \"topics_to_cover\": [\n",
      "        {\n",
      "          \"title\": \"Recognizing a Graphics File\",\n",
      "          \"toc_id\": 291\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Understanding Data Compression\",\n",
      "          \"toc_id\": 299\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Identifying Unknown File Formats\",\n",
      "          \"toc_id\": 309\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Understanding Copyright Issues with Graphics\",\n",
      "          \"toc_id\": 314\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC DASHBOARD: Ground Truth: Relevant Section(s) from book_toc.json\n",
      "================================================================================\n",
      "\n",
      "--- ToC for 'Chapter 7. Linux and Macintosh File Systems' ---\n",
      "{\n",
      "  \"level\": 0,\n",
      "  \"title\": \"Chapter 7. Linux and Macintosh File Systems\",\n",
      "  \"children\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Chapter Introduction\",\n",
      "      \"children\": []\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Examining Linux File Structures\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"File Structures in Ext4\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Inodes\",\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Hard Links and Symbolic Links\",\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Understanding Macintosh File Structures\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"An Overview of Mac File Structures\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Forensics Procedures in Mac\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Acquisition Methods in macOS\",\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Using Linux Forensics Tools\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Installing Sleuth Kit and Autopsy\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Examining a Case with Sleuth Kit and Autopsy\",\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Chapter Review\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Chapter Summary\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Key Terms\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Review Questions\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Hands-On Projects\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Case Projects\",\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- ToC for 'Chapter 8. Recovering Graphics Files' ---\n",
      "{\n",
      "  \"level\": 0,\n",
      "  \"title\": \"Chapter 8. Recovering Graphics Files\",\n",
      "  \"children\": [\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Chapter Introduction\",\n",
      "      \"children\": []\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Recognizing a Graphics File\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Understanding Bitmap and Raster Images\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Understanding Vector Graphics\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Understanding Metafile Graphics\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Understanding Graphics File Formats\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Understanding Digital Photograph File Formats\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Examining the Raw File Format\",\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Examining the Exchangeable Image File Format\",\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Understanding Data Compression\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Lossless and Lossy Compression\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Locating and Recovering Graphics Files\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Identifying Graphics File Fragments\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Repairing Damaged Headers\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Searching for and Carving Data from Unallocated Space\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Planning Your Examination\",\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"level\": 3,\n",
      "              \"title\": \"Searching for and Recovering Digital Photograph Evidence\",\n",
      "              \"children\": []\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Rebuilding File Headers\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Reconstructing File Fragments\",\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Identifying Unknown File Formats\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Analyzing Graphics File Headers\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Tools for Viewing Images\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Understanding Steganography in Graphics Files\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Using Steganalysis Tools\",\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Understanding Copyright Issues with Graphics\",\n",
      "      \"children\": []\n",
      "    },\n",
      "    {\n",
      "      \"level\": 1,\n",
      "      \"title\": \"Chapter Review\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Chapter Summary\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Key Terms\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Review Questions\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Hands-On Projects\",\n",
      "          \"children\": []\n",
      "        },\n",
      "        {\n",
      "          \"level\": 2,\n",
      "          \"title\": \"Case Projects\",\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC DASHBOARD: Vector Database Verification\n",
      "================================================================================\n",
      "\n",
      "Verifying chunk counts for Session 1: 'Chapter 7. Linux and Macintosh File Systems'\n",
      "----------------------------------------------------------------------\n",
      "Topic Title                                             | ToC ID | Chunk Count\n",
      "----------------------------------------------------------------------\n",
      "Examining Linux File Structures                         | 272    | 77\n",
      "Understanding Macintosh File Structures                 | 276    | 6\n",
      "Using Linux Forensics Tools                             | 280    | 5\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Verifying chunk counts for Session 2: 'Chapter 8. Recovering Graphics Files'\n",
      "----------------------------------------------------------------------\n",
      "Topic Title                                             | ToC ID | Chunk Count\n",
      "----------------------------------------------------------------------\n",
      "Recognizing a Graphics File                             | 291    | 4\n",
      "Understanding Data Compression                          | 299    | 2\n",
      "Identifying Unknown File Formats                        | 309    | 14\n",
      "Understanding Copyright Issues with Graphics            | 314    | 19\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC DASHBOARD COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8.1: Diagnostic Comparison Dashboard for Planning Agent (Final Version)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Ensure the PlanningAgent class from Cell 8 is available in memory\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "def print_diag_header(text: str):\n",
    "    \"\"\"Prints a formatted header for a diagnostic section.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"DIAGNOSTIC DASHBOARD: {text}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# This helper is not strictly needed for the diagnostic but kept for completeness\n",
    "def find_node_by_title(nodes: List[Dict], title: str) -> Optional[Dict]:\n",
    "    \"\"\"Recursively finds a node in the ToC by its exact title.\"\"\"\n",
    "    for node in nodes:\n",
    "        if node.get('title') == title:\n",
    "            return node\n",
    "        if node.get('children'):\n",
    "            found = find_node_by_title(node.get('children', []), title)\n",
    "            if found:\n",
    "                return found\n",
    "    return None\n",
    "\n",
    "# --- Main Diagnostic Function ---\n",
    "def run_comparison_diagnostic(week_to_test: int):\n",
    "    logger.info(f\"--- Starting Diagnostic Comparison Dashboard for Week {week_to_test} ---\")\n",
    "    \n",
    "    # 1. --- Load All necessary data ---\n",
    "    try:\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r') as f:\n",
    "            book_toc_data = json.load(f)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r') as f:\n",
    "            unit_outline_data = json.load(f)\n",
    "        with open(os.path.join(CONFIG_DIR, \"processed_settings.json\"), 'r') as f:\n",
    "            processed_settings = json.load(f)\n",
    "        \n",
    "        master_config = {\n",
    "            \"processed_settings\": processed_settings,\n",
    "            \"unit_outline\": unit_outline_data,\n",
    "            \"book_toc\": book_toc_data\n",
    "        }\n",
    "        logger.info(\"Successfully loaded all raw data files.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load initial files. Error: {e}\")\n",
    "        return\n",
    "        \n",
    "    # --- MODIFICATION: Show the initial input from the Unit Outline ---\n",
    "    print_diag_header(\"Ground Truth: Weekly Schedule from Unit Outline\")\n",
    "    try:\n",
    "        weekly_item = unit_outline_data['weeklySchedule'][week_to_test - 1]\n",
    "        print(f\"--- Input for Week {week_to_test} ---\")\n",
    "        print(json.dumps(weekly_item, indent=2))\n",
    "    except IndexError:\n",
    "        logger.error(f\"Week {week_to_test} not found in Unit Outline. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 2. --- Generate the Plan using the Agent ---\n",
    "    print_diag_header(\"Generated Plan\")\n",
    "    agent = PlanningAgent(master_config)\n",
    "    generated_plan = agent.create_content_plan_for_week(week_to_test)\n",
    "    \n",
    "    if not generated_plan:\n",
    "        logger.error(\"Planning Agent failed to generate a plan. Aborting diagnostic.\")\n",
    "        return\n",
    "        \n",
    "    print(json.dumps(generated_plan, indent=2))\n",
    "\n",
    "    # 3. --- Extract the relevant slice from the ground-truth ToC ---\n",
    "    print_diag_header(\"Ground Truth: Relevant Section(s) from book_toc.json\")\n",
    "    chapter_numbers = agent._identify_relevant_chapters(weekly_item)\n",
    "    \n",
    "    if not chapter_numbers:\n",
    "        logger.error(\"Could not identify chapters to create ToC slice.\")\n",
    "    else:\n",
    "        for chap_num in chapter_numbers:\n",
    "            chapter_node = agent._find_chapter_node(chap_num)\n",
    "            if chapter_node:\n",
    "                print(f\"\\n--- ToC for '{chapter_node['title']}' ---\")\n",
    "                print(json.dumps(chapter_node, indent=2))\n",
    "            else:\n",
    "                print(f\"\\nCould not find Chapter {chap_num} in ToC.\")\n",
    "\n",
    "\n",
    "    # 4. --- Verify Plan against Vector Database ---\n",
    "    print_diag_header(\"Vector Database Verification\")\n",
    "    if not langchain_available:\n",
    "        logger.warning(\"LangChain/Chroma libraries not available. Skipping Vector DB verification.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        logger.info(\"Connecting to ChromaDB to verify chunk counts...\")\n",
    "        embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        logger.info(\"Successfully connected to ChromaDB.\")\n",
    "        \n",
    "        # This handles both single and multi-session plans\n",
    "        for session in generated_plan.get('sessions', []):\n",
    "            topics_to_verify = session.get('topics_to_cover', [])\n",
    "            print(f\"\\nVerifying chunk counts for Session {session['session_number']}: '{session['session_topic']}'\")\n",
    "            print(\"-\" * 70)\n",
    "            print(f\"{'Topic Title':<55} | {'ToC ID'} | {'Chunk Count'}\")\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "            for topic in topics_to_verify:\n",
    "                toc_id = topic['toc_id']\n",
    "                title = topic['title']\n",
    "                retrieved = vector_store._collection.get(where={\"toc_id\": toc_id})\n",
    "                count = len(retrieved['ids'])\n",
    "                \n",
    "                title_short = (title[:50] + '...') if len(title) > 53 else title\n",
    "                print(f\"{title_short:<55} | {toc_id:<6} | {count}\")\n",
    "            \n",
    "            print(\"-\" * 70)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during Vector DB verification: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "# --- Execute the Diagnostic Dashboard ---\n",
    "WEEK_TO_TEST = 7 # Set the week you want to diagnose here\n",
    "run_comparison_diagnostic(WEEK_TO_TEST)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSTIC DASHBOARD COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df028a46",
   "metadata": {},
   "source": [
    "Next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d072e",
   "metadata": {},
   "source": [
    "Chunnk relation wwith the weights of the number of the slides per subtopic, haave in mind that 1 hour of delivery is like 20-25 slides "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0be385",
   "metadata": {},
   "source": [
    "to ensure to move to the case to handle i wourl like to ensure the concepts are clear when we discussde about sessions and week, sessions in this context is number of classes that we have for week, if we say week , 3 sessions in one week or sessions_per_week = 3 is 3 classes per week that require 3 different set of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74abd4c",
   "metadata": {},
   "source": [
    "https://youtu.be/6xcCwlDx6f8?si=7QxFyzuNVppHBQ-c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d8be7",
   "metadata": {},
   "source": [
    "## Configuration and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459d53b",
   "metadata": {},
   "source": [
    "**Description:**  \n",
    "\n",
    "**Parameters and concideration**\n",
    "- 1 hour in the setting session_time_duration_in_hour - is 18-20 slides at the time so it is require to calculate this according to the given value but this also means per session so sessions_per_week is a multiplicator factor that   \n",
    "- if apply_topic_interactive is available will add an extra slide and add extra 5 min time but to determine this is required to plan all the content first and then calculate then provide a extra time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea092bd6",
   "metadata": {},
   "source": [
    "settings_deck.json\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"teaching_flow_id\": \"apply_topic_interactive\",\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"target\": 0\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"session_time_duration_in_hour\": 1.5\n",
    "  },\n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": \"all\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75a04010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 11:05:53,200 - INFO - Loading all necessary configuration and data files...\n",
      "2025-07-02 11:05:53,201 - INFO - All files loaded successfully.\n",
      "2025-07-02 11:05:53,202 - INFO - Pre-processing settings_deck for definitive plan...\n",
      "2025-07-02 11:05:53,202 - INFO -   - Set 'course_id' from Unit Outline: ICT312\n",
      "2025-07-02 11:05:53,202 - INFO -   - Set 'unit_name' from Unit Outline: Digital Forensic\n",
      "2025-07-02 11:05:53,202 - INFO -   - Using 'teaching_flow_id' from test override: 'apply_topic_interactive'\n",
      "2025-07-02 11:05:53,203 - INFO -   - Using 'sessions_per_week' from test override: 4\n",
      "2025-07-02 11:05:53,203 - INFO -   - Using 'distribution_strategy' from test override: 'even'\n",
      "2025-07-02 11:05:53,204 - INFO -   - 'generation_scope' is a specific list from test override: [7]\n",
      "2025-07-02 11:05:53,204 - INFO - Saving the final processed configuration to: /home/sebas_dev_linux/projects/course_generator/configs/processed_settings.json\n",
      "2025-07-02 11:05:53,205 - INFO - Master configuration object is ready for the generation pipeline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                       Configuration and Scoping Process                        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                             Configuration Complete                             \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Preview of Processed Settings ---\n",
      "{\n",
      "  \"course_id\": \"ICT312\",\n",
      "  \"unit_name\": \"Digital Forensic\",\n",
      "  \"teaching_flow_id\": \"apply_topic_interactive\",\n",
      "  \"slide_count_strategy\": {\n",
      "    \"method\": \"per_week\",\n",
      "    \"target\": 25\n",
      "  },\n",
      "  \"week_session_setup\": {\n",
      "    \"sessions_per_week\": 4,\n",
      "    \"distribution_strategy\": \"even\"\n",
      "  },\n",
      "  \"generation_scope\": {\n",
      "    \"weeks\": [\n",
      "      7\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Number of weeks to generate: 1\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Configuration and Scoping for Content Generation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. DEFINE FILE PATHS AND GLOBAL TEST SETTINGS ---\n",
    "# Assumes these variables are loaded from a previous setup cell (like Cell 1)\n",
    "# For demonstration, we define them here. Replace with your actual global vars.\n",
    "# PROJECT_BASE_DIR = \"/path/to/your/project\"\n",
    "# PARSED_UO_JSON_PATH = os.path.join(PROJECT_BASE_DIR, \"Parse_data/Parse_UO/ICT312_Digital_Forensic_Final_parsed.json\")\n",
    "# PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(PROJECT_BASE_DIR, \"Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\")\n",
    "\n",
    "# New configuration file paths\n",
    "CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "SETTINGS_DECK_PATH = os.path.join(CONFIG_DIR, \"settings_deck.json\")\n",
    "TEACHING_FLOWS_PATH = os.path.join(CONFIG_DIR, \"teaching_flows.json\")\n",
    "\n",
    "# New output path for the processed settings\n",
    "PROCESSED_SETTINGS_PATH = os.path.join(CONFIG_DIR, \"processed_settings.json\")\n",
    "\n",
    "# --- Global Test Overrides (for easy testing) ---\n",
    "# To test a specific week, change this from \"all\" to a list, e.g., [7]\n",
    "# To test a different flow, change the teaching_flow_id.\n",
    "\n",
    "TEST_OVERRIDE_WEEKS = [7] # e.g., [7] or [1, 2, 3] or \"all\"\n",
    "TEST_OVERRIDE_FLOW_ID = \"apply_topic_interactive\" # or \"standard_lecture\"\n",
    "TEST_OVERRIDE_SESSIONS_PER_WEEK = 4 # e.g., 1 or 2\n",
    "TEST_OVERRIDE_DISTRIBUTION_STRATEGY = \"even\" # 'even', 'front_load', 'end_load'\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "\n",
    "def process_and_load_configurations():\n",
    "    \"\"\"\n",
    "    Loads all configuration files, processes them to create a definitive plan,\n",
    "    and returns a master configuration object.\n",
    "    \"\"\"\n",
    "    print_header(\"Configuration and Scoping Process\", char=\"-\")\n",
    "    \n",
    "    # --- 2. LOAD ALL INPUT FILES ---\n",
    "    logger.info(\"Loading all necessary configuration and data files...\")\n",
    "    try:\n",
    "        with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            unit_outline = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            book_toc = json.load(f)\n",
    "        with open(SETTINGS_DECK_PATH, 'r', encoding='utf-8') as f:\n",
    "            settings_deck = json.load(f)\n",
    "        with open(TEACHING_FLOWS_PATH, 'r', encoding='utf-8') as f:\n",
    "            teaching_flows = json.load(f)\n",
    "        logger.info(\"All files loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"FATAL: A required configuration file was not found: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. PRE-PROCESS AND REFINE SETTINGS ---\n",
    "    logger.info(\"Pre-processing settings_deck for definitive plan...\")\n",
    "    \n",
    "    # Create a deep copy to avoid modifying the original object\n",
    "    processed_settings = json.loads(json.dumps(settings_deck))\n",
    "\n",
    "    # a. Smartly set Course ID and Unit Name from the Unit Outline\n",
    "    unit_info = unit_outline.get(\"unitInformation\", {})\n",
    "    course_id = unit_info.get(\"unitCode\", \"UNKNOWN_COURSE\")\n",
    "    unit_name = unit_info.get(\"unitName\", \"Unknown Unit Name\")\n",
    "    \n",
    "    processed_settings['course_id'] = course_id\n",
    "    processed_settings['unit_name'] = unit_name # <-- NEW: Add unit_name\n",
    "    \n",
    "    logger.info(f\"  - Set 'course_id' from Unit Outline: {course_id}\")\n",
    "    logger.info(f\"  - Set 'unit_name' from Unit Outline: {unit_name}\")\n",
    "\n",
    "    # b. Apply test overrides for easier development\n",
    "    processed_settings['teaching_flow_id'] = TEST_OVERRIDE_FLOW_ID\n",
    "    logger.info(f\"  - Using 'teaching_flow_id' from test override: '{TEST_OVERRIDE_FLOW_ID}'\")\n",
    "    \n",
    "    processed_settings['week_session_setup']['sessions_per_week'] = TEST_OVERRIDE_SESSIONS_PER_WEEK\n",
    "    logger.info(f\"  - Using 'sessions_per_week' from test override: {TEST_OVERRIDE_SESSIONS_PER_WEEK}\")\n",
    "    \n",
    "    processed_settings['week_session_setup']['distribution_strategy'] = TEST_OVERRIDE_DISTRIBUTION_STRATEGY\n",
    "    logger.info(f\"  - Using 'distribution_strategy' from test override: '{TEST_OVERRIDE_DISTRIBUTION_STRATEGY}'\")\n",
    "    \n",
    "    # c. Resolve the generation scope (which weeks to generate)\n",
    "    scope = TEST_OVERRIDE_WEEKS\n",
    "    if scope == \"all\":\n",
    "        num_weeks = len(unit_outline.get('weeklySchedule', []))\n",
    "        if num_weeks == 0:\n",
    "            logger.error(\"Unit Outline 'weeklySchedule' is empty. Cannot determine number of weeks.\")\n",
    "            return None\n",
    "        final_scope = list(range(1, num_weeks + 1))\n",
    "        logger.info(f\"  - 'generation_scope' is 'all'. Resolved to {num_weeks} weeks: {final_scope}\")\n",
    "    elif isinstance(scope, list) and all(isinstance(i, int) for i in scope):\n",
    "        final_scope = scope\n",
    "        logger.info(f\"  - 'generation_scope' is a specific list from test override: {final_scope}\")\n",
    "    else:\n",
    "        logger.error(f\"Invalid 'generation_scope' setting: {scope}. Must be 'all' or a list of integers.\")\n",
    "        return None\n",
    "        \n",
    "    processed_settings['generation_scope']['weeks'] = final_scope\n",
    "    \n",
    "    # --- 4. ASSEMBLE & SAVE FINAL CONFIGURATION ---\n",
    "    master_config = {\n",
    "        \"processed_settings\": processed_settings,\n",
    "        \"unit_outline\": unit_outline,\n",
    "        \"book_toc\": book_toc,\n",
    "        \"teaching_flows\": teaching_flows\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Saving the final processed configuration to: {PROCESSED_SETTINGS_PATH}\")\n",
    "    os.makedirs(os.path.dirname(PROCESSED_SETTINGS_PATH), exist_ok=True)\n",
    "    with open(PROCESSED_SETTINGS_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_settings, f, indent=2)\n",
    "    \n",
    "    print_header(\"Configuration Complete\", char=\"-\")\n",
    "    logger.info(\"Master configuration object is ready for the generation pipeline.\")\n",
    "    \n",
    "    return master_config\n",
    "\n",
    "# --- EXECUTE THE CONFIGURATION PROCESS ---\n",
    "master_config = process_and_load_configurations()\n",
    "\n",
    "# Optional: Print a preview to verify the output\n",
    "if master_config:\n",
    "    print(\"\\n--- Preview of Processed Settings ---\")\n",
    "    print(json.dumps(master_config['processed_settings'], indent=2))\n",
    "    print(f\"\\nNumber of weeks to generate: {len(master_config['processed_settings']['generation_scope']['weeks'])}\")\n",
    "    print(\"-----------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
