{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9771e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION SUMMARY ---\n",
      "Processing Mode: EPUB\n",
      "Unit ID: ICT312\n",
      "Unit Outline Path: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "Book Path: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "Parsed UO Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "Parsed ToC Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n",
      "Vector DB Path: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub_v2\n",
      "Vector DB Collection: book_toc_guided_chunks_epub_v2\n",
      "--- SETUP COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}_v2\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200383d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 23:10:54,968 - INFO - Starting to process Unit Outline: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "2025-06-24 23:10:54,996 - INFO - Calling Ollama model 'qwen3:8b'...\n",
      "2025-06-24 23:11:54,472 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-24 23:11:54,476 - INFO - ‚úÖ Successfully parsed and saved Unit Outline to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline(input_filepath: str, output_filepath: str, prompt_template: str):\n",
    "    logger.info(f\"Starting to process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "\n",
    "        prompt = prompt_template.format(outline_text=outline_text)\n",
    "        client = ollama.Client(host=OLLAMA_HOST)\n",
    "        llm_output = call_ollama_with_retry(client, prompt)\n",
    "        parsed_data = parse_llm_json_output(llm_output)\n",
    "\n",
    "        if parsed_data:\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(parsed_data, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"‚úÖ Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "        else:\n",
    "            logger.error(\"‚ùå Failed to get valid structured data from the LLM.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during parsing: {e}\", exc_info=True)\n",
    "\n",
    "# --- Execute Parsing ---\n",
    "# Uses variables from Cell 1\n",
    "parse_and_save_outline(\n",
    "    input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "    output_filepath=PARSED_UO_JSON_PATH,\n",
    "    prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or epub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\n",
      "INFO: Found 290 bookmark entries.\n",
      "‚úÖ Successfully wrote PDF ToC to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT311_pdf_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Book Table of Contents (ToC)\n",
    "# This cell extracts the ToC from the specified book (EPUB or PDF)\n",
    "# and saves it to the path defined in Cell 1.\n",
    "\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "\n",
    "# --- EPUB Extraction Logic ---\n",
    "def parse_navpoint(navpoint, level=0):\n",
    "    # (Your existing parse_navpoint function)\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    # Add filtering logic here if needed\n",
    "    node = {\"level\": level, \"title\": title, \"children\": []}\n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, level + 1)\n",
    "        if child_node: node[\"children\"].append(child_node)\n",
    "    return node\n",
    "\n",
    "def parse_li(li_element, level=0):\n",
    "    # (Your existing parse_li function)\n",
    "    a_tag = li_element.find('a')\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        # Add filtering logic here if needed\n",
    "        node = {\"level\": level, \"title\": title, \"children\": []}\n",
    "        nested_ol = li_element.find('ol')\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, level + 1)\n",
    "                if child_node: node[\"children\"].append(child_node)\n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_epub_toc(epub_path, output_json_path):\n",
    "    print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "    toc_data = []\n",
    "    book = epub.read_epub(epub_path)\n",
    "    for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "        soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "        if nav_item.get_name().endswith('.ncx'):\n",
    "            print(\"INFO: Found EPUB 2 (NCX) Table of Contents.\")\n",
    "            navmap = soup.find('navMap')\n",
    "            if navmap:\n",
    "                for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                    node = parse_navpoint(navpoint)\n",
    "                    if node: toc_data.append(node)\n",
    "        else:\n",
    "            print(\"INFO: Found EPUB 3 (XHTML) Table of Contents.\")\n",
    "            toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "            if toc_nav:\n",
    "                top_ol = toc_nav.find('ol')\n",
    "                if top_ol:\n",
    "                    for li in top_ol.find_all('li', recursive=False):\n",
    "                        node = parse_li(li)\n",
    "                        if node: toc_data.append(node)\n",
    "        if toc_data: break\n",
    "    \n",
    "    if toc_data:\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Successfully wrote EPUB ToC to: {output_json_path}\")\n",
    "    else:\n",
    "        print(\"‚ùå WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# --- PDF Extraction Logic ---\n",
    "def build_pdf_hierarchy(toc_list):\n",
    "    \"\"\"\n",
    "    Builds a hierarchical structure from a flat ToC list from PyMuPDF.\n",
    "    MODIFIED: Normalizes levels to start at 0 for consistency with EPUB.\n",
    "    \"\"\"\n",
    "    root = []\n",
    "    # The parent_stack keys are now level-based, starting from -1 for the root's parent.\n",
    "    parent_stack = {-1: {\"children\": root}}\n",
    "\n",
    "    for level, title, page in toc_list:\n",
    "        # --- FIX: NORMALIZE LEVEL TO START AT 0 ---\n",
    "        # fitz/PyMuPDF ToC levels start at 1, so we subtract 1.\n",
    "        normalized_level = level - 1\n",
    "\n",
    "        node = {\n",
    "            \"level\": normalized_level,\n",
    "            \"title\": title.strip(),\n",
    "            \"page\": page,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        # Find the correct parent in the stack. The parent's level is one less than the current node's.\n",
    "        # This logic correctly places the node under its parent in the hierarchy.\n",
    "        parent_node = parent_stack[normalized_level - 1]\n",
    "        parent_node[\"children\"].append(node)\n",
    "\n",
    "        # Add the current node to the stack so it can be a parent for subsequent nodes.\n",
    "        parent_stack[normalized_level] = node\n",
    "\n",
    "    return root\n",
    "\n",
    "def extract_pdf_toc(pdf_path, output_json_path):\n",
    "    print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        if not toc:\n",
    "            print(\"‚ùå WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "            hierarchical_toc = []\n",
    "        else:\n",
    "            print(f\"INFO: Found {len(toc)} bookmark entries.\")\n",
    "            hierarchical_toc = build_pdf_hierarchy(toc)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Successfully wrote PDF ToC to: {output_json_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# --- Execute ToC Extraction ---\n",
    "if PROCESS_EPUB:\n",
    "    extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "else:\n",
    "    extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cd59ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:34:39,134 - INFO - Processing book '(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf' using ToC from 'ICT311_pdf_table_of_contents.json'.\n",
      "2025-06-19 17:34:39,135 - INFO - Successfully loaded pre-extracted ToC with 16 top-level entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:34:50,405 - INFO - Loaded 649 pages from PDF.\n",
      "2025-06-19 17:34:50,405 - INFO - Flattened ToC into 290 entries for matching.\n",
      "2025-06-19 17:34:50,406 - INFO - Assigning metadata to PDF pages based on ToC page numbers...\n",
      "2025-06-19 17:34:50,415 - INFO - Total documents prepared for chunking: 649\n",
      "2025-06-19 17:34:50,443 - INFO - Split into 2353 final chunks, inheriting hierarchical metadata.\n",
      "2025-06-19 17:34:50,448 - INFO - Initializing embedding model 'nomic-embed-text' and creating new vector database...\n",
      "2025-06-19 17:34:50,463 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-19 17:35:19,272 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 17:35:22,588 - INFO - ‚úÖ Vector DB created successfully at: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_pdf_v2\n",
      "2025-06-19 17:35:22,588 - INFO - ‚úÖ Collection 'book_toc_guided_chunks_pdf_v2' contains 2353 documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create Hierarchical Vector Database\n",
    "# This cell processes the book, enriches it with the hierarchical ToC,\n",
    "# chunks it, and creates the final vector database.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper: Clean metadata values for ChromaDB ---\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "    if isinstance(value, list): return \", \".join(map(str, value))\n",
    "    if isinstance(value, dict): return json.dumps(value)\n",
    "    if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "    return str(value)\n",
    "\n",
    "# --- Core Function to Process Book with Pre-extracted ToC ---\n",
    "def process_book_with_extracted_toc(\n",
    "    book_path: str,\n",
    "    extracted_toc_json_path: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "    logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "    # 1. Load the pre-extracted hierarchical ToC\n",
    "    try:\n",
    "        with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        if not hierarchical_toc:\n",
    "            logger.error(f\"Pre-extracted ToC at '{extracted_toc_json_path}' is empty or invalid.\")\n",
    "            return [], []\n",
    "        logger.info(f\"Successfully loaded pre-extracted ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading pre-extracted ToC JSON: {e}\", exc_info=True)\n",
    "        return [], []\n",
    "\n",
    "    # 2. Load all text elements/pages from the book\n",
    "    all_raw_book_docs: List[Document] = []\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "    if file_extension == \".epub\":\n",
    "        loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} text elements from EPUB.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading EPUB content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    elif file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            all_raw_book_docs = loader.load()\n",
    "            logger.info(f\"Loaded {len(all_raw_book_docs)} pages from PDF.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF content: {e}\", exc_info=True)\n",
    "            return [], hierarchical_toc\n",
    "    else:\n",
    "        logger.error(f\"Unsupported book file format: {file_extension}\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    if not all_raw_book_docs:\n",
    "        logger.error(\"No text elements/pages loaded from the book.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    # 3. Create enriched LangChain Documents by matching ToC to content\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "    # Flatten the ToC for easier iteration and path tracking\n",
    "    flat_toc_entries: List[Dict[str, Any]] = []\n",
    "    def _flatten_toc_recursive(nodes: List[Dict[str,Any]], current_titles_path: List[str]):\n",
    "        for node in nodes:\n",
    "            title = node.get(\"title\",\"\").strip()\n",
    "            if not title: continue\n",
    "\n",
    "            new_titles_path = current_titles_path + [title]\n",
    "            entry = {\n",
    "                \"titles_path\": new_titles_path,\n",
    "                \"level\": node.get(\"level\"),\n",
    "                \"full_title_for_matching\": title\n",
    "            }\n",
    "            if \"page\" in node: entry[\"page\"] = node[\"page\"]\n",
    "            flat_toc_entries.append(entry)\n",
    "\n",
    "            if node.get(\"children\"):\n",
    "                _flatten_toc_recursive(node.get(\"children\", []), new_titles_path)\n",
    "    \n",
    "    _flatten_toc_recursive(hierarchical_toc, [])\n",
    "    logger.info(f\"Flattened ToC into {len(flat_toc_entries)} entries for matching.\")\n",
    "\n",
    "    # Logic for PDF metadata assignment\n",
    "    if file_extension == \".pdf\" and any(\"page\" in entry for entry in flat_toc_entries):\n",
    "        logger.info(\"Assigning metadata to PDF pages based on ToC page numbers...\")\n",
    "        flat_toc_entries.sort(key=lambda x: x.get(\"page\", -1) if x.get(\"page\") is not None else -1)\n",
    "        \n",
    "        for page_doc in all_raw_book_docs:\n",
    "            page_num_0_indexed = page_doc.metadata.get(\"page\", -1)\n",
    "            page_num_1_indexed = page_num_0_indexed + 1\n",
    "            \n",
    "            assigned_metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "            best_match_toc_entry = None\n",
    "\n",
    "            for toc_entry in flat_toc_entries:\n",
    "                toc_page = toc_entry.get(\"page\")\n",
    "                if toc_page is not None and toc_page <= page_num_1_indexed:\n",
    "                    if best_match_toc_entry is None or toc_page > best_match_toc_entry.get(\"page\", -1):\n",
    "                        best_match_toc_entry = toc_entry\n",
    "                elif toc_page is not None and toc_page > page_num_1_indexed:\n",
    "                    break\n",
    "            \n",
    "            if best_match_toc_entry:\n",
    "                for i, title_in_path in enumerate(best_match_toc_entry[\"titles_path\"]):\n",
    "                    assigned_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "            else:\n",
    "                assigned_metadata[\"level_1_title\"] = \"Uncategorized PDF Page\"\n",
    "            \n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k,v in assigned_metadata.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=cleaned_meta))\n",
    "\n",
    "    # Logic for EPUB metadata assignment\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Assigning metadata to EPUB elements by matching ToC titles in text...\")\n",
    "        toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "        current_hierarchy_metadata = {}\n",
    "\n",
    "        for element_doc in all_raw_book_docs:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            if not element_text: continue\n",
    "\n",
    "            found_new_heading = False\n",
    "            for toc_entry in toc_titles_for_search:\n",
    "                if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "                    current_hierarchy_metadata = {\"source\": os.path.basename(book_path)}\n",
    "                    for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "                        current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "                    if \"page\" in toc_entry: current_hierarchy_metadata[\"epub_toc_page\"] = toc_entry[\"page\"]\n",
    "                    found_new_heading = True\n",
    "                    break\n",
    "            \n",
    "            doc_metadata_to_assign = current_hierarchy_metadata.copy() if current_hierarchy_metadata else {\"source\": os.path.basename(book_path), \"level_1_title\": \"EPUB Preamble\"}\n",
    "            \n",
    "            cleaned_meta = {k: clean_metadata_for_chroma(v) for k,v in doc_metadata_to_assign.items()}\n",
    "            final_documents_with_metadata.append(Document(page_content=element_text, metadata=cleaned_meta))\n",
    "    \n",
    "    else: # Fallback for unsupported types or logic failure\n",
    "        final_documents_with_metadata = all_raw_book_docs\n",
    "\n",
    "    if not final_documents_with_metadata:\n",
    "        logger.error(\"No documents were processed or enriched with hierarchical metadata.\")\n",
    "        return [], hierarchical_toc\n",
    "\n",
    "    logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks, inheriting hierarchical metadata.\")\n",
    "    \n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "# --- Main Execution Block for this Cell ---\n",
    "\n",
    "# Use the global variables defined in Cell 1\n",
    "if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "    logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "    logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "else:\n",
    "    # Process the book to get the chunks\n",
    "    final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "        book_path=BOOK_PATH,\n",
    "        extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if final_chunks_for_db:\n",
    "        # Delete old DB if it exists\n",
    "        if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "            logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "            shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "        # Create and persist the new vector database\n",
    "        logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=final_chunks_for_db,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        # Verify creation\n",
    "        reloaded_db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model, collection_name=CHROMA_COLLECTION_NAME)\n",
    "        count = reloaded_db._collection.count()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        logger.info(f\"‚úÖ Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "        logger.info(f\"‚úÖ Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        logger.error(\"‚ùå Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "### Verification Test Strategy\n",
    "The script automatically validates the vector database by performing four dynamic tests that increase in complexity, moving from a general health check to specific application-level requirements.\n",
    "\n",
    "Basic Retrieval Test:\n",
    "- Goal: Confirm the database is live and its content is broadly relevant to the course subject.\n",
    "- Method: It performs a simple search using the course's unitName (e.g., \"Digital Forensic\") extracted from the unit outline.\n",
    "- Success means: The database is online, and the ingested content is thematically correct.\n",
    "\n",
    "Deep Hierarchy Test:\n",
    "- Goal: Verify the structural integrity of the metadata, ensuring text is correctly tagged with its full, multi-level context (e.g., Part -> Chapter -> Section).\n",
    "- Method: It randomly picks a deeply nested sub-section from the Table of Contents and performs a search that is filtered to match that exact hierarchical path.\n",
    "- Success means: The data ingestion process is correctly assigning detailed, nested parentage to all text chunks.\n",
    "\n",
    "Advanced Unit Outline Alignment Test:\n",
    "- Goal: Ensure the system can correctly map a weekly syllabus topic to the right chapter(s) in the book, adapting to different ToC structures (e.g., flat chapters vs. chapters inside \"Parts\").\n",
    "- Method: It randomly selects a week, finds all required chapter numbers from the reading list, and dynamically determines the correct metadata level to check. It then verifies that a search for the weekly topic retrieves chunks belonging to the correct chapters.\n",
    "- Success means: The database is directly useful for its primary purpose: linking the course structure to the source textbook reliably.\n",
    "\n",
    "Content Sequence Test (PDF-only):\n",
    "- Goal: Check if retrieved content can be re-ordered chronologically to form a coherent narrative.\n",
    "- Method: It retrieves multiple chunks for a random topic, sorts them using the page_number metadata, and verifies the page numbers are in ascending order.\n",
    "- Success means: The database contains the necessary metadata to reconstruct the original flow of the book's content, which is crucial for generating logical summaries or lecture material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eccec103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:37:06,572 - INFO - Initializing embedding model 'nomic-embed-text'...\n",
      "2025-06-19 17:37:06,586 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-19 17:37:06,642 - INFO - ‚ÑπÔ∏è Description: Checks if a query using the Unit Name retrieves any content.\n",
      "2025-06-19 17:37:06,643 - INFO - Using Unit Name for basic query: 'Applied Cryptography'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         Database Verification Process                          \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                       Test 1: Basic Retrieval (Dynamic)                        \n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 17:37:06,788 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 17:37:06,807 - INFO - ‚úÖ TEST PASSED\n",
      "2025-06-19 17:37:06,808 - INFO - ‚ÑπÔ∏è Description: Checks if a deep sub-section is tagged with its full path.\n",
      "2025-06-19 17:37:06,883 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 17:37:06,886 - INFO - ‚úÖ TEST PASSED\n",
      "2025-06-19 17:37:06,887 - INFO - ‚ÑπÔ∏è Description: Checks if a weekly topic maps to its required chapter(s).\n",
      "2025-06-19 17:37:06,887 - INFO - Testing alignment for Week 6: 'Number Theory and Cryptographic Hardness Assumptions'\n",
      "2025-06-19 17:37:06,915 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 17:37:06,921 - INFO - ‚úÖ TEST PASSED\n",
      "2025-06-19 17:37:06,921 - INFO - ‚ÑπÔ∏è Description: Checks if retrieved chunks can be ordered by page number.\n",
      "2025-06-19 17:37:06,921 - INFO - Testing content sequence for topic: 'Introduction and Classical Cryptography'\n",
      "2025-06-19 17:37:06,937 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 17:37:06,940 - INFO - ‚úÖ Page numbers are in correct ascending order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query: 'Applied Cryptography'\n",
      "\n",
      "‚úÖ Found 1 results. Displaying top 1:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: 'Introduction to Modern  Cryptography...'\n",
      "Metadata: {\n",
      "  \"level_1_title\": \"Half Title\",\n",
      "  \"page_number\": 2,\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                        Test 2: Deep Hierarchy Retrieval                        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Query: '2: Perfectly Secret Encryption'\n",
      "üìÑ Filter: {\n",
      "  \"$and\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"I: Introduction and Classical Cryptography\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_2_title\": {\n",
      "        \"$eq\": \"2: Perfectly Secret Encryption\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "‚úÖ Found 1 results. Displaying top 1:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: 'Chapter 2 Perfectly Secret Encryption In the previous chapter we presented some historical encryption schemes and showed that they can be broken easil...'\n",
      "Metadata: {\n",
      "  \"level_1_title\": \"I: Introduction and Classical Cryptography\",\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\",\n",
      "  \"page_number\": 46,\n",
      "  \"level_2_title\": \"2: Perfectly Secret Encryption\"\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Test 3: Advanced Unit Outline Alignment                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Query: 'Number Theory and Cryptographic Hardness Assumptions'\n",
      "üìÑ Filter: {\n",
      "  \"level_1_title\": {\n",
      "    \"$eq\": \"III: Public-Key (Asymmetric) Cryptography\"\n",
      "  }\n",
      "}\n",
      "\n",
      "‚úÖ Found 5 results. Displaying top 3:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: 'Chapter 9 Number Theory and Cryptographic Hardness Assumptions Modern cryptosystems are invariably based on an assumption that some problem is hard. I...'\n",
      "Metadata: {\n",
      "  \"page_number\": 328,\n",
      "  \"level_1_title\": \"III: Public-Key (Asymmetric) Cryptography\",\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\",\n",
      "  \"level_2_title\": \"9: Number Theory and Cryptographic Hardness Assumptions\"\n",
      "}\n",
      "\n",
      "--- Result 2 ---\n",
      "Content: 'Number Theory and Cryptographic Hardness Assumptions 359 there is a collision; i.e., x Ã∏= x‚Ä≤ and Hs(x) = Hs(x‚Ä≤). Lemma 9.65 implies that whenever ther...'\n",
      "Metadata: {\n",
      "  \"level_3_title\": \"References and Additional Reading\",\n",
      "  \"page_number\": 382,\n",
      "  \"level_1_title\": \"III: Public-Key (Asymmetric) Cryptography\",\n",
      "  \"level_2_title\": \"9: Number Theory and Cryptographic Hardness Assumptions\",\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
      "}\n",
      "\n",
      "--- Result 3 ---\n",
      "Content: 'Number Theory and Cryptographic Hardness Assumptions 355 9.4.1 One-Way Functions and Permutations One-way functions are the minimal cryptographic prim...'\n",
      "Metadata: {\n",
      "  \"level_2_title\": \"9: Number Theory and Cryptographic Hardness Assumptions\",\n",
      "  \"level_1_title\": \"III: Public-Key (Asymmetric) Cryptography\",\n",
      "  \"page_number\": 378,\n",
      "  \"level_3_title\": \"9.4 *Cryptographic Applications\",\n",
      "  \"level_4_title\": \"9.4.1 One-Way Functions and Permutations\",\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                Test 4: Content Sequence Verification (PDF-only)                \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üîç Query: 'Introduction and Classical Cryptography'\n",
      "\n",
      "‚úÖ Found 10 results. Displaying top 3:\n",
      "\n",
      "--- Result 1 ---\n",
      "Content: 'Part I Introduction and Classical Cryptography...'\n",
      "Metadata: {\n",
      "  \"level_1_title\": \"I: Introduction and Classical Cryptography\",\n",
      "  \"page_number\": 22,\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
      "}\n",
      "\n",
      "--- Result 2 ---\n",
      "Content: 'Introduction to Modern  Cryptography...'\n",
      "Metadata: {\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\",\n",
      "  \"level_1_title\": \"Half Title\",\n",
      "  \"page_number\": 2\n",
      "}\n",
      "\n",
      "--- Result 3 ---\n",
      "Content: '(or lack thereof) and the mathematical assumptions underlying those proofs. It is not our intention for readers to become experts‚Äîor to be able to de-...'\n",
      "Metadata: {\n",
      "  \"level_2_title\": \"1: Introduction\",\n",
      "  \"level_3_title\": \"1.2 The Setting of Private-Key Encryption\",\n",
      "  \"source\": \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\",\n",
      "  \"level_1_title\": \"I: Introduction and Classical Cryptography\",\n",
      "  \"page_number\": 25\n",
      "}\n",
      "Retrieved and sorted page numbers: [2, 8, 16, 16, 19, 22, 25, 25, 25, 641]\n",
      "\n",
      "================================================================================\n",
      "                              Verification Summary                              \n",
      "================================================================================\n",
      "Total Tests Run: 4\n",
      "‚úÖ Passed: 4\n",
      "‚ùå Failed: 0\n",
      "\n",
      "================================================================================\n",
      "                             Verification Complete                              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Vector Database (Final Version for All ToC Structures)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS (UNCHANGED) ---\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def print_results(query_text: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    print(f\"\\nüîç Query: '{query_text}'\")\n",
    "    if where_filter:\n",
    "        print(f\"üìÑ Filter: {json.dumps(where_filter, indent=2)}\")\n",
    "    if not results:\n",
    "        print(\"\\n‚ùå No results found for this query and filter.\")\n",
    "        return\n",
    "    print(f\"\\n‚úÖ Found {len(results)} results. Displaying top {min(len(results), 3)}:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n--- Result {i+1} ---\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"Content: '{content_preview[:150]}...'\")\n",
    "        print(f\"Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "\n",
    "def find_deep_entry(nodes: List[Dict], current_path: List[str] = []) -> Optional[Tuple[Dict, List[str]]]:\n",
    "    shuffled_nodes = random.sample(nodes, len(nodes))\n",
    "    for node in shuffled_nodes:\n",
    "        if node.get('level', 0) >= 2 and node.get('children'): return node, current_path + [node['title']]\n",
    "        if node.get('children'):\n",
    "            path = current_path + [node['title']]\n",
    "            deep_entry = find_deep_entry(node['children'], path)\n",
    "            if deep_entry: return deep_entry\n",
    "    return None\n",
    "\n",
    "def find_chapter_title_by_number(toc_data: List[Dict], chap_num: int) -> Optional[List[str]]:\n",
    "    def search_nodes(nodes, num, current_path):\n",
    "        for node in nodes:\n",
    "            path = current_path + [node['title']]\n",
    "            # This regex handles \"Chapter 1\", \"1: Title\", \"1. Title\" etc.\n",
    "            if re.match(rf\"(Chapter\\s)?{num}[.:\\s]\", node.get('title', ''), re.IGNORECASE):\n",
    "                return path\n",
    "            if node.get('children'):\n",
    "                found_path = search_nodes(node['children'], num, path)\n",
    "                if found_path: return found_path\n",
    "        return None\n",
    "    return search_nodes(toc_data, chap_num, [])\n",
    "\n",
    "# --- TEST CASE FUNCTIONS ---\n",
    "\n",
    "def advanced_alignment_test(db, outline, toc):\n",
    "    print_header(\"Test 3: Advanced Unit Outline Alignment\", char=\"-\")\n",
    "    logger.info(\"‚ÑπÔ∏è Description: Checks if a weekly topic maps to its required chapter(s).\")\n",
    "    try:\n",
    "        week_to_test = random.choice(outline['weeklySchedule'])\n",
    "        logger.info(f\"Testing alignment for Week {week_to_test['week']}: '{week_to_test['contentTopic']}'\")\n",
    "        \n",
    "        reading = week_to_test.get('requiredReading', '')\n",
    "        chap_nums = re.findall(r'\\d+', reading)\n",
    "        assert chap_nums, f\"Could not find chapter numbers in reading: '{reading}'\"\n",
    "        \n",
    "        chapter_paths = [find_chapter_title_by_number(toc, int(n)) for n in chap_nums]\n",
    "        chapter_paths = [path for path in chapter_paths if path is not None]\n",
    "        assert chapter_paths, f\"Could not map any chapter numbers {chap_nums} to a title in the ToC.\"\n",
    "\n",
    "        # --- FIX IS HERE: This logic now handles both flat and nested ToCs ---\n",
    "        \n",
    "        # The chapter title is always the LAST element in the path.\n",
    "        expected_chapter_titles = [path[-1] for path in chapter_paths]\n",
    "        \n",
    "        # The metadata level where the chapter title is stored depends on the path's length.\n",
    "        # e.g., if path is ['Chapter 3...'], length is 1, so it's in 'level_1_title'.\n",
    "        # e.g., if path is ['Part I...', 'Chapter 3...'], length is 2, so it's in 'level_2_title'.\n",
    "        chapter_level = len(chapter_paths[0])\n",
    "        chapter_metadata_key = f\"level_{chapter_level}_title\"\n",
    "\n",
    "        # The filter should be based on the FIRST element of the path (the top-level entry).\n",
    "        level_1_titles = list(set([path[0] for path in chapter_paths]))\n",
    "        \n",
    "        query = week_to_test['contentTopic']\n",
    "        or_filter = [{\"level_1_title\": {\"$eq\": title}} for title in level_1_titles]\n",
    "        w_filter = {\"$or\": or_filter} if len(or_filter) > 1 else or_filter[0]\n",
    "        \n",
    "        results = db.similarity_search(query, k=5, filter=w_filter)\n",
    "        print_results(query, results, w_filter)\n",
    "        assert len(results) > 0, \"Alignment query returned no results for the correct section/chapter.\"\n",
    "        \n",
    "        # Check if any of the retrieved documents have the correct Chapter title at the correct level.\n",
    "        is_match_found = any(\n",
    "            doc.metadata.get(chapter_metadata_key) in expected_chapter_titles for doc in results\n",
    "        )\n",
    "        assert is_match_found, f\"None of the top results had the expected chapter title in metadata field '{chapter_metadata_key}'.\"\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "        logger.info(\"‚úÖ TEST PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå TEST FAILED: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "# (The other test functions remain the same)\n",
    "def basic_retrieval_test(db, outline):\n",
    "   \n",
    "    print_header(\"Test 1: Basic Retrieval (Dynamic)\", char=\"-\")\n",
    "    logger.info(\"‚ÑπÔ∏è Description: Checks if a query using the Unit Name retrieves any content.\")\n",
    "    try:\n",
    "        query_text = outline.get(\"unitInformation\", {}).get(\"unitName\", \"introduction\")\n",
    "        logger.info(f\"Using Unit Name for basic query: '{query_text}'\")\n",
    "        results = db.similarity_search(query_text, k=1)\n",
    "        print_results(query_text, results)\n",
    "        assert len(results) > 0, f\"Basic retrieval for '{query_text}' should return at least one chunk.\"\n",
    "        logger.info(\"‚úÖ TEST PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå TEST FAILED: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def deep_hierarchy_test(db, toc):\n",
    "    \n",
    "    print_header(\"Test 2: Deep Hierarchy Retrieval\", char=\"-\")\n",
    "    logger.info(\"‚ÑπÔ∏è Description: Checks if a deep sub-section is tagged with its full path.\")\n",
    "    try:\n",
    "        deep_entry_result = find_deep_entry(toc)\n",
    "        assert deep_entry_result, \"Could not find a deep entry (level >= 2) in the ToC to test.\"\n",
    "        node, path = deep_entry_result\n",
    "        query = node['title']\n",
    "        conditions = [{f\"level_{i+1}_title\": {\"$eq\": title}} for i, title in enumerate(path)]\n",
    "        w_filter = {\"$and\": conditions}\n",
    "        results = db.similarity_search(query, k=1, filter=w_filter)\n",
    "        print_results(query, results, w_filter)\n",
    "        assert len(results) > 0, \"Deeply filtered query returned no results.\"\n",
    "        logger.info(\"‚úÖ TEST PASSED\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå TEST FAILED: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def sequential_content_test(db, outline):\n",
    " \n",
    "    print_header(\"Test 4: Content Sequence Verification (PDF-only)\", char=\"-\")\n",
    "    logger.info(\"‚ÑπÔ∏è Description: Checks if retrieved chunks can be ordered by page number.\")\n",
    "    try:\n",
    "        topic_query = random.choice(outline['weeklySchedule'])['contentTopic']\n",
    "        logger.info(f\"Testing content sequence for topic: '{topic_query}'\")\n",
    "        results = db.similarity_search(topic_query, k=10)\n",
    "        docs_with_pages = [doc for doc in results if doc.metadata.get(\"page_number\") is not None]\n",
    "        assert len(docs_with_pages) > 3, \"Fewer than 4 retrieved chunks have page numbers.\"\n",
    "        docs_with_pages.sort(key=lambda x: x.metadata[\"page_number\"])\n",
    "        page_numbers = [doc.metadata[\"page_number\"] for doc in docs_with_pages]\n",
    "        print_results(topic_query, results)\n",
    "        print(f\"Retrieved and sorted page numbers: {page_numbers}\")\n",
    "        is_sorted = all(page_numbers[i] <= page_numbers[i+1] for i in range(len(page_numbers)-1))\n",
    "        assert is_sorted, \"The retrieved chunks' page numbers are not in ascending order.\"\n",
    "        logger.info(\"‚úÖ Page numbers are in correct ascending order.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå TEST FAILED: {e}\", exc_info=True)\n",
    "        return False\n",
    "        \n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    if not langchain_available: logger.error(\"LangChain libraries not found.\"); return\n",
    "    if not all([os.path.exists(p) for p in [CHROMA_PERSIST_DIR, PRE_EXTRACTED_TOC_JSON_PATH]]):\n",
    "        logger.error(\"‚ùå Required file/directory is missing. Run Cells 4 and 5 first.\"); return\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: toc_data = json.load(f)\n",
    "    if not os.path.exists(PARSED_UO_JSON_PATH):\n",
    "        logger.error(f\"‚ùå Parsed Unit Outline not found. Cannot run all dynamic tests.\"); return\n",
    "    with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f: unit_outline_data = json.load(f)\n",
    "\n",
    "    logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}'...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embeddings, collection_name=CHROMA_COLLECTION_NAME)\n",
    "    \n",
    "    # Run tests and count results\n",
    "    results_summary = []\n",
    "    results_summary.append(basic_retrieval_test(vector_store, unit_outline_data))\n",
    "    results_summary.append(deep_hierarchy_test(vector_store, toc_data))\n",
    "    results_summary.append(advanced_alignment_test(vector_store, unit_outline_data, toc_data))\n",
    "    \n",
    "    if not PROCESS_EPUB: # Global variable from Cell 1\n",
    "        results_summary.append(sequential_content_test(vector_store, unit_outline_data))\n",
    "    else:\n",
    "        logger.info(\"\\n‚ÑπÔ∏è Skipping Content Sequence test (for PDF-based sources only).\")\n",
    "\n",
    "    # Final Report\n",
    "    passed_count = sum(results_summary)\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    print_header(\"Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)}\")\n",
    "    print(f\"‚úÖ Passed: {passed_count}\")\n",
    "    print(f\"‚ùå Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "run_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
