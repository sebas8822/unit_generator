{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192046b1",
   "metadata": {},
   "source": [
    "# Set up Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9771e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION SUMMARY ---\n",
      "Processing Mode: EPUB\n",
      "Unit ID: ICT312\n",
      "Unit Outline Path: /home/sebas_dev_linux/projects/course_generator/data/UO/ICT312 Digital Forensic_Final.docx\n",
      "Book Path: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "Parsed UO Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "Parsed ToC Output Path: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n",
      "Vector DB Path: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "Vector DB Collection: book_toc_guided_chunks_epub_v2\n",
      "--- SETUP COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from PIL import Image \n",
    "import io\n",
    "from docx import Document\n",
    "import pdfplumber\n",
    "import ollama\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "import json\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. CORE SETTINGS ---\n",
    "# Set this to True for EPUB, False for PDF. This controls the entire notebook's flow.\n",
    "PROCESS_EPUB = True # for EPUB\n",
    "# PROCESS_EPUB = False # for PDF\n",
    "\n",
    "# --- 2. INPUT FILE NAMES ---\n",
    "# The name of the Unit Outline file (e.g., DOCX, PDF)\n",
    "UNIT_OUTLINE_FILENAME = \"ICT312 Digital Forensic_Final.docx\" # epub\n",
    "# UNIT_OUTLINE_FILENAME = \"ICT311 Applied Cryptography.docx\" # pdf\n",
    "\n",
    "EXTRACT_UO = False \n",
    "\n",
    "# The names of the book files\n",
    "EPUB_BOOK_FILENAME = \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "PDF_BOOK_FILENAME = \"(Chapman & Hall_CRC Cryptography and Network Security Series) Jonathan Katz, Yehuda Lindell - Introduction to Modern Cryptography-CRC Press (2020).pdf\"\n",
    "\n",
    "# --- 3. DIRECTORY STRUCTURE ---\n",
    "# Define the base path to your project to avoid hardcoding long paths everywhere\n",
    "PROJECT_BASE_DIR = \"/home/sebas_dev_linux/projects/course_generator\"\n",
    "\n",
    "# Define subdirectories relative to the base path\n",
    "DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"data\")\n",
    "PARSE_DATA_DIR = os.path.join(PROJECT_BASE_DIR, \"Parse_data\")\n",
    "\n",
    "# Construct full paths for clarity\n",
    "INPUT_UO_DIR = os.path.join(DATA_DIR, \"UO\")\n",
    "INPUT_BOOKS_DIR = os.path.join(DATA_DIR, \"books\")\n",
    "OUTPUT_PARSED_UO_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_UO\")\n",
    "OUTPUT_PARSED_TOC_DIR = os.path.join(PARSE_DATA_DIR, \"Parse_TOC_books\")\n",
    "OUTPUT_DB_DIR = os.path.join(DATA_DIR, \"DataBase_Chroma\")\n",
    "OUTPUT_IMAGES_DIR = os.path.join(PROJECT_BASE_DIR, \"extracted_images\")\n",
    "os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# --- 4. LLM & EMBEDDING CONFIGURATION ---\n",
    "LLM_PROVIDER = \"ollama\"  # Can be \"ollama\", \"openai\", \"gemini\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen3:8b\" # \"qwen3:8b\", #\"mistral:latest\"\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# --- 5. DYNAMICALLY GENERATED PATHS & IDs (DO NOT EDIT THIS SECTION) ---\n",
    "# This section uses the settings above to create all the necessary variables for later cells.\n",
    "\n",
    "# Extract Unit ID from the filename\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def extract_uo_id_from_filename(filename: str) -> str:\n",
    "    match = re.match(r'^[A-Z]+\\d+', os.path.basename(filename))\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    raise ValueError(f\"Could not extract a valid Unit ID from filename: '{filename}'\")\n",
    "\n",
    "try:\n",
    "    UNIT_ID = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    UNIT_ID = \"UNKNOWN_ID\"\n",
    "\n",
    "# Full path to the unit outline file\n",
    "FULL_PATH_UNIT_OUTLINE = os.path.join(INPUT_UO_DIR, UNIT_OUTLINE_FILENAME)\n",
    "\n",
    "# Determine which book and output paths to use based on the PROCESS_EPUB flag\n",
    "if PROCESS_EPUB:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, EPUB_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_epub_table_of_contents.json\")\n",
    "else:\n",
    "    BOOK_PATH = os.path.join(INPUT_BOOKS_DIR, PDF_BOOK_FILENAME)\n",
    "    PRE_EXTRACTED_TOC_JSON_PATH = os.path.join(OUTPUT_PARSED_TOC_DIR, f\"{UNIT_ID}_pdf_table_of_contents.json\")\n",
    "\n",
    "# Define paths for the vector database\n",
    "file_type_suffix = 'epub' if PROCESS_EPUB else 'pdf'\n",
    "CHROMA_PERSIST_DIR = os.path.join(OUTPUT_DB_DIR, f\"chroma_db_toc_guided_chunks_{file_type_suffix}\")\n",
    "CHROMA_COLLECTION_NAME = f\"book_toc_guided_chunks_{file_type_suffix}_v2\"\n",
    "\n",
    "# Define path for the parsed unit outline\n",
    "PARSED_UO_JSON_PATH = os.path.join(OUTPUT_PARSED_UO_DIR, f\"{os.path.splitext(UNIT_OUTLINE_FILENAME)[0]}_parsed.json\")\n",
    "\n",
    "# --- Sanity Check Printout ---\n",
    "print(\"--- CONFIGURATION SUMMARY ---\")\n",
    "print(f\"Processing Mode: {'EPUB' if PROCESS_EPUB else 'PDF'}\")\n",
    "print(f\"Unit ID: {UNIT_ID}\")\n",
    "print(f\"Unit Outline Path: {FULL_PATH_UNIT_OUTLINE}\")\n",
    "print(f\"Book Path: {BOOK_PATH}\")\n",
    "print(f\"Parsed UO Output Path: {PARSED_UO_JSON_PATH}\")\n",
    "print(f\"Parsed ToC Output Path: {PRE_EXTRACTED_TOC_JSON_PATH}\")\n",
    "print(f\"Vector DB Path: {CHROMA_PERSIST_DIR}\")\n",
    "print(f\"Vector DB Collection: {CHROMA_COLLECTION_NAME}\")\n",
    "print(\"--- SETUP COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ae41c",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e0137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert academic assistant tasked with parsing a university unit outline document and extracting key information into a structured JSON format.\n",
    "\n",
    "The input will be the raw text content of a unit outline. Your goal is to identify and extract the following details and structure them precisely as specified in the JSON schema below. Note: do not change any key name\n",
    "\n",
    "**JSON Output Schema:**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"unitInformation\": {{\n",
    "    \"unitCode\": \"string | null\",\n",
    "    \"unitName\": \"string | null\",\n",
    "    \"creditPoints\": \"integer | null\",\n",
    "    \"unitRationale\": \"string | null\",\n",
    "    \"prerequisites\": \"string | null\"\n",
    "  }},\n",
    "  \"learningOutcomes\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"assessments\": [\n",
    "    {{\n",
    "      \"taskName\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"dueWeek\": \"string | null\",\n",
    "      \"weightingPercent\": \"integer | null\",\n",
    "      \"learningOutcomesAssessed\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"weeklySchedule\": [\n",
    "    {{\n",
    "      \"week\": \"string\",\n",
    "      \"contentTopic\": \"string\",\n",
    "      \"requiredReading\": \"string | null\"\n",
    "    }}\n",
    "  ],\n",
    "  \"requiredReadings\": [\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"recommendedReadings\": [\n",
    "    \"string\"\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Instructions for Extraction:\n",
    "Unit Information: Locate Unit Code, Unit Name, Credit Points. Capture 'Unit Overview / Rationale' as unitRationale. Identify prerequisites.\n",
    "Learning Outcomes: Extract each learning outcome statement.\n",
    "Assessments: Each task as an object. Capture full task name, description, Due Week, Weighting % (number), and Learning Outcomes Assessed.\n",
    "weeklySchedule: Each week as an object. Capture Week, contentTopic, and requiredReading.\n",
    "Required and Recommended Readings: List full text for each.\n",
    "**Important Considerations for the LLM**:\n",
    "Pay close attention to headings and table structures.\n",
    "If information is missing, use null for string/integer fields, or an empty list [] for array fields.\n",
    "Do no change keys in the template given\n",
    "Ensure the output is ONLY the JSON object, starting with {{{{ and ending with }}}}. No explanations or conversational text before or after the JSON. \n",
    "Now, parse the following unit outline text:\n",
    "--- UNIT_OUTLINE_TEXT_START ---\n",
    "{outline_text}\n",
    "--- UNIT_OUTLINE_TEXT_END ---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0852ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a new cell after your imports, or within Cell 3 before the functions.\n",
    "# This code is based on the schema from your screenshot on page 4.\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "# Define Pydantic models that match your JSON schema\n",
    "class UnitInformation(BaseModel):\n",
    "    unitCode: Optional[str] = None\n",
    "    unitName: Optional[str] = None\n",
    "    creditPoints: Optional[int] = None\n",
    "    unitRationale: Optional[str] = None\n",
    "    prerequisites: Optional[str] = None\n",
    "\n",
    "class Assessment(BaseModel):\n",
    "    taskName: str\n",
    "    description: str\n",
    "    dueWeek: Optional[str] = None\n",
    "    weightingPercent: Optional[int] = None\n",
    "    learningOutcomesAssessed: Optional[str] = None\n",
    "\n",
    "class WeeklyScheduleItem(BaseModel):\n",
    "    week: str\n",
    "    contentTopic: str\n",
    "    requiredReading: Optional[str] = None\n",
    "\n",
    "class ParsedUnitOutline(BaseModel):\n",
    "    unitInformation: UnitInformation\n",
    "    learningOutcomes: List[str]\n",
    "    assessments: List[Assessment]\n",
    "    weeklySchedule: List[WeeklyScheduleItem] \n",
    "    requiredReadings: List[str]\n",
    "    recommendedReadings: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490df6",
   "metadata": {},
   "source": [
    "# Extrac Unit outline details to process following steps - output raw json with UO details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200383d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Parse Unit Outline\n",
    "\n",
    "\n",
    "# --- Helper Functions for Parsing ---\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    _, ext = os.path.splitext(filepath.lower())\n",
    "    if ext == '.docx':\n",
    "        doc = Document(filepath)\n",
    "        full_text = [p.text for p in doc.paragraphs]\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                full_text.append(\" | \".join(cell.text for cell in row.cells))\n",
    "        return '\\n'.join(full_text)\n",
    "    elif ext == '.pdf':\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported file type: {ext}\")\n",
    "\n",
    "def parse_llm_json_output(content: str) -> dict:\n",
    "    try:\n",
    "        match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if not match: return None\n",
    "        return json.loads(match.group(0))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=2, max=10))\n",
    "def call_ollama_with_retry(client, prompt):\n",
    "    logger.info(f\"Calling Ollama model '{OLLAMA_MODEL}'...\")\n",
    "    response = client.chat(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        format=\"json\",\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "    if not response or 'message' not in response or not response['message'].get('content'):\n",
    "        raise ValueError(\"Ollama returned an empty or invalid response.\")\n",
    "    return response['message']['content']\n",
    "\n",
    "# --- Main Orchestration Function for this Cell ---\n",
    "def parse_and_save_outline_robust(\n",
    "    input_filepath: str, \n",
    "    output_filepath: str, \n",
    "    prompt_template: str,\n",
    "    max_retries: int = 3\n",
    "):\n",
    "    logger.info(f\"Starting to robustly process Unit Outline: {input_filepath}\")\n",
    "    \n",
    "    if not os.path.exists(input_filepath):\n",
    "        logger.error(f\"Input file not found: {input_filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        outline_text = extract_text_from_file(input_filepath)\n",
    "        if not outline_text.strip():\n",
    "            logger.error(\"Extracted text is empty. Aborting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to extract text from file: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    client = ollama.Client(host=OLLAMA_HOST)\n",
    "    current_prompt = prompt_template.format(outline_text=outline_text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        logger.info(f\"Attempt {attempt + 1}/{max_retries} to parse outline.\")\n",
    "        \n",
    "        try:\n",
    "            # Call the LLM\n",
    "            llm_output_str = call_ollama_with_retry(client, current_prompt)\n",
    "            \n",
    "            # Find the JSON blob in the response\n",
    "            json_blob = parse_llm_json_output(llm_output_str) # Your existing helper\n",
    "            if not json_blob:\n",
    "                raise ValueError(\"LLM did not return a parsable JSON object.\")\n",
    "\n",
    "            # *** THE KEY VALIDATION STEP ***\n",
    "            # Try to parse the dictionary into your Pydantic model.\n",
    "            # This will raise a `ValidationError` if keys are wrong, types are wrong, or fields are missing.\n",
    "            parsed_data = ParsedUnitOutline.model_validate(json_blob)\n",
    "            \n",
    "            # If successful, save the validated data and exit the loop\n",
    "            logger.info(\"Successfully validated JSON structure against Pydantic model.\")\n",
    "            os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "                # Use .model_dump_json() for clean, validated output\n",
    "                f.write(parsed_data.model_dump_json(indent=2)) \n",
    "\n",
    "            logger.info(f\"Successfully parsed and saved Unit Outline to: {output_filepath}\")\n",
    "            return # Exit function on success\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Validation failed on attempt {attempt + 1}. Error: {e}\")\n",
    "            # Formulate a new prompt with the error message for self-correction\n",
    "            error_feedback = (\n",
    "                f\"\\n\\nYour previous attempt failed. You MUST correct the following errors:\\n\"\n",
    "                f\"{e}\\n\\n\"\n",
    "                f\"Please regenerate the entire JSON object, ensuring it strictly adheres to the schema \"\n",
    "                f\"and corrects these specific errors. Do not change any key names.\"\n",
    "            )\n",
    "            current_prompt = current_prompt + error_feedback # Append the error to the prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Catch other errors like network issues from call_ollama_with_retry\n",
    "            logger.error(f\"An unexpected error occurred on attempt {attempt + 1}: {e}\", exc_info=True)\n",
    "            # You might want to wait before retrying for non-validation errors\n",
    "            time.sleep(5)\n",
    "\n",
    "    logger.error(f\"Failed to get valid structured data from the LLM after {max_retries} attempts.\")\n",
    "\n",
    "\n",
    "# --- In your execution block, call the new function ---\n",
    "# parse_and_save_outline(...) becomes:\n",
    "\n",
    "if EXTRACT_UO:\n",
    "    parse_and_save_outline_robust(\n",
    "        input_filepath=FULL_PATH_UNIT_OUTLINE,\n",
    "        output_filepath=PARSED_UO_JSON_PATH,\n",
    "        prompt_template=UNIT_OUTLINE_SYSTEM_PROMPT_TEMPLATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38c82",
   "metadata": {},
   "source": [
    "# Extract TOC from epub or epub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c3959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EPUB ToC for: /home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\n",
      "INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\n",
      "INFO: Annotating ToC with full title paths...\n",
      "✅ Successfully wrote EPUB ToC with IDs, links, and paths to: /home/sebas_dev_linux/projects/course_generator/Parse_data/Parse_TOC_books/ICT312_epub_table_of_contents.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract Book Table of Contents (ToC) with Pre-assigned IDs, Links, and Full Title Paths\n",
    "\n",
    "from ebooklib import epub, ITEM_NAVIGATION\n",
    "from bs4 import BeautifulSoup\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import urllib.parse # Needed to clean up links\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS \n",
    "# ==============================================================================\n",
    "\n",
    "def clean_epub_href(href: str) -> str:\n",
    "    \"\"\"Removes URL fragments and decodes URL-encoded characters.\"\"\"\n",
    "    if not href: return \"\"\n",
    "    cleaned_href = href.split('#')[0]\n",
    "    return urllib.parse.unquote(cleaned_href)\n",
    "\n",
    "# --- NEW: Helper to add full title paths to any ToC hierarchy ---\n",
    "def _add_paths_to_hierarchy(nodes: List[Dict], current_path: List[str] = []):\n",
    "    \"\"\"\n",
    "    Recursively traverses a list of ToC nodes and adds a 'titles_path'\n",
    "    key to each node, containing the full list of titles from the root.\n",
    "    \"\"\"\n",
    "    for node in nodes:\n",
    "        # Construct the new path for the current node\n",
    "        new_path = current_path + [node['title']]\n",
    "        node['titles_path'] = new_path\n",
    "        \n",
    "        # Recurse into the children with the updated path\n",
    "        if node.get('children'):\n",
    "            _add_paths_to_hierarchy(node['children'], new_path)\n",
    "\n",
    "# --- EPUB Extraction Logic ---\n",
    "def parse_navpoint(navpoint: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 2 navPoints and assigns a toc_id and link_filename.\"\"\"\n",
    "    title = navpoint.navLabel.text.strip()\n",
    "    if not title: return None\n",
    "    \n",
    "    content_tag = navpoint.find('content', recursive=False)\n",
    "    link_filename = clean_epub_href(content_tag['src']) if content_tag else \"\"\n",
    "    \n",
    "    node = {\n",
    "        \"level\": level,\n",
    "        \"toc_id\": counter[0],\n",
    "        \"title\": title,\n",
    "        \"link_filename\": link_filename,\n",
    "        \"children\": []\n",
    "    }\n",
    "    counter[0] += 1\n",
    "    \n",
    "    for child_navpoint in navpoint.find_all('navPoint', recursive=False):\n",
    "        child_node = parse_navpoint(child_navpoint, counter, level + 1)\n",
    "        if child_node: node[\"children\"].append(child_node)\n",
    "        \n",
    "    return node\n",
    "\n",
    "def parse_li(li_element: BeautifulSoup, counter: List[int], level: int = 0) -> Dict:\n",
    "    \"\"\"Recursively parses EPUB 3 <li> elements and assigns a toc_id and link_filename.\"\"\"\n",
    "    a_tag = li_element.find('a', recursive=False)\n",
    "    if a_tag:\n",
    "        title = a_tag.get_text(strip=True)\n",
    "        if not title: return None\n",
    "        \n",
    "        link_filename = clean_epub_href(a_tag.get('href'))\n",
    "        \n",
    "        node = {\n",
    "            \"level\": level,\n",
    "            \"toc_id\": counter[0],\n",
    "            \"title\": title,\n",
    "            \"link_filename\": link_filename,\n",
    "            \"children\": []\n",
    "        }\n",
    "        counter[0] += 1\n",
    "        \n",
    "        nested_ol = li_element.find('ol', recursive=False)\n",
    "        if nested_ol:\n",
    "            for sub_li in nested_ol.find_all('li', recursive=False):\n",
    "                child_node = parse_li(sub_li, counter, level + 1)\n",
    "                if child_node: node[\"children\"].append(child_node)\n",
    "        return node\n",
    "    return None\n",
    "\n",
    "def extract_epub_toc(epub_path, output_json_path):\n",
    "    print(f\"Processing EPUB ToC for: {epub_path}\")\n",
    "    toc_data = []\n",
    "    book = epub.read_epub(epub_path)\n",
    "    id_counter = [1]\n",
    "    \n",
    "    for nav_item in book.get_items_of_type(ITEM_NAVIGATION):\n",
    "        soup = BeautifulSoup(nav_item.get_content(), 'xml')\n",
    "        if nav_item.get_name().endswith('.ncx'):\n",
    "            print(\"INFO: Found EPUB 2 (NCX) Table of Contents. Parsing...\")\n",
    "            navmap = soup.find('navMap')\n",
    "            if navmap:\n",
    "                for navpoint in navmap.find_all('navPoint', recursive=False):\n",
    "                    node = parse_navpoint(navpoint, id_counter, level=0)\n",
    "                    if node: toc_data.append(node)\n",
    "        else: # Assumes EPUB 3\n",
    "            print(\"INFO: Found EPUB 3 (XHTML) Table of Contents. Parsing...\")\n",
    "            toc_nav = soup.select_one('nav[epub|type=\"toc\"]')\n",
    "            if toc_nav:\n",
    "                top_ol = toc_nav.find('ol', recursive=False)\n",
    "                if top_ol:\n",
    "                    for li in top_ol.find_all('li', recursive=False):\n",
    "                        node = parse_li(li, id_counter, level=0)\n",
    "                        if node: toc_data.append(node)\n",
    "        if toc_data: break\n",
    "    \n",
    "    if toc_data:\n",
    "        # --- MODIFICATION: Add the full title paths ---\n",
    "        print(\"INFO: Annotating ToC with full title paths...\")\n",
    "        _add_paths_to_hierarchy(toc_data)\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote EPUB ToC with IDs, links, and paths to: {output_json_path}\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: No ToC data extracted from EPUB.\")\n",
    "\n",
    "# --- PDF Extraction Logic ---\n",
    "def build_pdf_hierarchy_with_ids(toc_list: List) -> List[Dict]:\n",
    "    root = []\n",
    "    parent_stack = {-1: {\"children\": root}}\n",
    "    id_counter = [1]\n",
    "    for level, title, page in toc_list:\n",
    "        normalized_level = level - 1\n",
    "        node = {\"level\": normalized_level, \"toc_id\": id_counter[0], \"title\": title.strip(), \"page\": page, \"children\": []}\n",
    "        id_counter[0] += 1\n",
    "        parent_node = parent_stack.get(normalized_level - 1)\n",
    "        if parent_node: parent_node[\"children\"].append(node)\n",
    "        parent_stack[normalized_level] = node\n",
    "    return root\n",
    "\n",
    "def extract_pdf_toc(pdf_path, output_json_path):\n",
    "    print(f\"Processing PDF ToC for: {pdf_path}\")\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        toc = doc.get_toc()\n",
    "        hierarchical_toc = []\n",
    "        if not toc: \n",
    "            print(\"❌ WARNING: This PDF has no embedded bookmarks (ToC).\")\n",
    "        else:\n",
    "            print(f\"INFO: Found {len(toc)} bookmark entries. Building hierarchy...\")\n",
    "            hierarchical_toc = build_pdf_hierarchy_with_ids(toc)\n",
    "            # --- MODIFICATION: Add the full title paths ---\n",
    "            print(\"INFO: Annotating ToC with full title paths...\")\n",
    "            _add_paths_to_hierarchy(hierarchical_toc)\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ Successfully wrote PDF ToC with assigned IDs and paths to: {output_json_path}\")\n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred during PDF ToC extraction: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "# This uses the global variables defined in your setup cell (Cell 1)\n",
    "if PROCESS_EPUB:\n",
    "    extract_epub_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)\n",
    "else:\n",
    "    extract_pdf_toc(BOOK_PATH, PRE_EXTRACTED_TOC_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9df11d",
   "metadata": {},
   "source": [
    "# Hirachical DB base on TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736bbb0",
   "metadata": {},
   "source": [
    "## Process Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e505df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5.a: Create Hierarchical Vector Database (with Sequential ToC ID and Chunk ID)\n",
    "# # This cell processes the book, enriches it with hierarchical and sequential metadata,\n",
    "# # chunks it, and creates the final vector database.\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import logging\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "# from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # Setup Logger for this cell\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # --- Helper: Clean metadata values for ChromaDB ---\n",
    "# def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "#     \"\"\"Sanitizes metadata values to be compatible with ChromaDB.\"\"\"\n",
    "#     if isinstance(value, list): return \", \".join(map(str, value))\n",
    "#     if isinstance(value, dict): return json.dumps(value)\n",
    "#     if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "#     return str(value)\n",
    "\n",
    "# # --- Core Function to Process Book with Pre-extracted ToC ---\n",
    "# def process_book_with_extracted_toc(\n",
    "#     book_path: str,\n",
    "#     extracted_toc_json_path: str,\n",
    "#     chunk_size: int,\n",
    "#     chunk_overlap: int\n",
    "# ) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "#     logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "#     # 1. Load the pre-extracted hierarchical ToC\n",
    "#     try:\n",
    "#         with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "#             hierarchical_toc = json.load(f)\n",
    "#         if not hierarchical_toc:\n",
    "#             logger.error(f\"Pre-extracted ToC at '{extracted_toc_json_path}' is empty or invalid.\")\n",
    "#             return [], []\n",
    "#         logger.info(f\"Successfully loaded pre-extracted ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error loading pre-extracted ToC JSON: {e}\", exc_info=True)\n",
    "#         return [], []\n",
    "\n",
    "#     # 2. Load all text elements/pages from the book\n",
    "#     all_raw_book_docs: List[Document] = []\n",
    "#     _, file_extension = os.path.splitext(book_path.lower())\n",
    "\n",
    "#     if file_extension == \".epub\":\n",
    "#         loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "#         try:\n",
    "#             all_raw_book_docs = loader.load()\n",
    "#             logger.info(f\"Loaded {len(all_raw_book_docs)} text elements from EPUB.\")\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error loading EPUB content: {e}\", exc_info=True)\n",
    "#             return [], hierarchical_toc\n",
    "#     elif file_extension == \".pdf\":\n",
    "#         loader = PyPDFLoader(book_path)\n",
    "#         try:\n",
    "#             all_raw_book_docs = loader.load()\n",
    "#             logger.info(f\"Loaded {len(all_raw_book_docs)} pages from PDF.\")\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error loading PDF content: {e}\", exc_info=True)\n",
    "#             return [], hierarchical_toc\n",
    "#     else:\n",
    "#         logger.error(f\"Unsupported book file format: {file_extension}\")\n",
    "#         return [], hierarchical_toc\n",
    "\n",
    "#     if not all_raw_book_docs:\n",
    "#         logger.error(\"No text elements/pages loaded from the book.\")\n",
    "#         return [], hierarchical_toc\n",
    "\n",
    "#     # 3. Create enriched LangChain Documents by matching ToC to content\n",
    "#     final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "#     # Flatten the ToC, AND add a unique sequential ID for sorting and validation.\n",
    "#     flat_toc_entries: List[Dict[str, Any]] = []\n",
    "    \n",
    "#     def _add_ids_and_flatten_recursive(nodes: List[Dict[str, Any]], current_titles_path: List[str], counter: List[int]):\n",
    "#         \"\"\"\n",
    "#         Recursively traverses ToC nodes to flatten them and assign a unique, sequential toc_id.\n",
    "#         \"\"\"\n",
    "#         for node in nodes:\n",
    "#             toc_id = counter[0]\n",
    "#             counter[0] += 1\n",
    "#             title = node.get(\"title\", \"\").strip()\n",
    "#             if not title: continue\n",
    "#             new_titles_path = current_titles_path + [title]\n",
    "#             entry = {\n",
    "#                 \"titles_path\": new_titles_path,\n",
    "#                 \"level\": node.get(\"level\"),\n",
    "#                 \"full_title_for_matching\": title,\n",
    "#                 \"toc_id\": toc_id\n",
    "#             }\n",
    "#             if \"page\" in node: entry[\"page\"] = node[\"page\"]\n",
    "#             flat_toc_entries.append(entry)\n",
    "#             if node.get(\"children\"):\n",
    "#                 _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter)\n",
    "\n",
    "#     toc_id_counter = [0]\n",
    "#     _add_ids_and_flatten_recursive(hierarchical_toc, [], toc_id_counter)\n",
    "#     logger.info(f\"Flattened ToC and assigned sequential IDs to {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "#     # Logic for PDF metadata assignment\n",
    "#     if file_extension == \".pdf\" and any(\"page\" in entry for entry in flat_toc_entries):\n",
    "#         logger.info(\"Assigning metadata to PDF pages based on ToC page numbers...\")\n",
    "#         flat_toc_entries.sort(key=lambda x: x.get(\"page\", -1) if x.get(\"page\") is not None else -1)\n",
    "#         for page_doc in all_raw_book_docs:\n",
    "#             page_num_0_indexed = page_doc.metadata.get(\"page\", -1)\n",
    "#             page_num_1_indexed = page_num_0_indexed + 1\n",
    "#             assigned_metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "#             best_match_toc_entry = None\n",
    "#             for toc_entry in flat_toc_entries:\n",
    "#                 toc_page = toc_entry.get(\"page\")\n",
    "#                 if toc_page is not None and toc_page <= page_num_1_indexed:\n",
    "#                     if best_match_toc_entry is None or toc_page > best_match_toc_entry.get(\"page\", -1):\n",
    "#                         best_match_toc_entry = toc_entry\n",
    "#                 elif toc_page is not None and toc_page > page_num_1_indexed:\n",
    "#                     break\n",
    "#             if best_match_toc_entry:\n",
    "#                 for i, title_in_path in enumerate(best_match_toc_entry[\"titles_path\"]):\n",
    "#                     assigned_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "#                 assigned_metadata['toc_id'] = best_match_toc_entry.get('toc_id')\n",
    "#             else:\n",
    "#                 assigned_metadata[\"level_1_title\"] = \"Uncategorized PDF Page\"\n",
    "#             cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in assigned_metadata.items()}\n",
    "#             final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=cleaned_meta))\n",
    "\n",
    "#     # Logic for EPUB metadata assignment\n",
    "#     elif file_extension == \".epub\":\n",
    "#         logger.info(\"Assigning metadata to EPUB elements by matching ToC titles in text...\")\n",
    "#         toc_titles_for_search = [entry for entry in flat_toc_entries if entry.get(\"full_title_for_matching\")]\n",
    "#         current_hierarchy_metadata = {}\n",
    "#         for element_doc in all_raw_book_docs:\n",
    "#             element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "#             if not element_text: continue\n",
    "#             for toc_entry in toc_titles_for_search:\n",
    "#                 if element_text == toc_entry[\"full_title_for_matching\"]:\n",
    "#                     current_hierarchy_metadata = {\"source\": os.path.basename(book_path)}\n",
    "#                     for i, title_in_path in enumerate(toc_entry[\"titles_path\"]):\n",
    "#                         current_hierarchy_metadata[f\"level_{i+1}_title\"] = title_in_path\n",
    "#                     current_hierarchy_metadata['toc_id'] = toc_entry.get('toc_id')\n",
    "#                     if \"page\" in toc_entry: current_hierarchy_metadata[\"epub_toc_page\"] = toc_entry[\"page\"]\n",
    "#                     break\n",
    "#             if not current_hierarchy_metadata:\n",
    "#                 doc_metadata_to_assign = {\"source\": os.path.basename(book_path), \"level_1_title\": \"EPUB Preamble\", \"toc_id\": -1}\n",
    "#             else:\n",
    "#                 doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "#             cleaned_meta = {k: clean_metadata_for_chroma(v) for k, v in doc_metadata_to_assign.items()}\n",
    "#             final_documents_with_metadata.append(Document(page_content=element_text, metadata=cleaned_meta))\n",
    "    \n",
    "#     else: # Fallback\n",
    "#         final_documents_with_metadata = all_raw_book_docs\n",
    "\n",
    "#     if not final_documents_with_metadata:\n",
    "#         logger.error(\"No documents were processed or enriched with hierarchical metadata.\")\n",
    "#         return [], hierarchical_toc\n",
    "\n",
    "#     logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    \n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         length_function=len\n",
    "#     )\n",
    "#     final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "#     logger.info(f\"Split into {len(final_chunks)} final chunks, inheriting hierarchical metadata.\")\n",
    "    \n",
    "#     # --- MODIFICATION START: Add a unique, sequential chunk_id to each chunk ---\n",
    "#     logger.info(\"Assigning sequential chunk_id to all final chunks...\")\n",
    "#     for i, chunk in enumerate(final_chunks):\n",
    "#         chunk.metadata['chunk_id'] = i\n",
    "#     logger.info(f\"Assigned chunk_ids from 0 to {len(final_chunks) - 1}.\")\n",
    "#     # --- MODIFICATION END ---\n",
    "\n",
    "#     return final_chunks, hierarchical_toc\n",
    "\n",
    "# # --- Main Execution Block for this Cell ---\n",
    "\n",
    "# if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "#     logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "#     logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "# else:\n",
    "#     final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "#         book_path=BOOK_PATH,\n",
    "#         extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "#         chunk_size=CHUNK_SIZE,\n",
    "#         chunk_overlap=CHUNK_OVERLAP\n",
    "#     )\n",
    "\n",
    "#     if final_chunks_for_db:\n",
    "#         if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "#             logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "#             shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "#         logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "#         embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "#         vector_db = Chroma.from_documents(\n",
    "#             documents=final_chunks_for_db,\n",
    "#             embedding=embedding_model,\n",
    "#             persist_directory=CHROMA_PERSIST_DIR,\n",
    "#             collection_name=CHROMA_COLLECTION_NAME\n",
    "#         )\n",
    "        \n",
    "#         reloaded_db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model, collection_name=CHROMA_COLLECTION_NAME)\n",
    "#         count = reloaded_db._collection.count()\n",
    "        \n",
    "#         print(\"-\" * 50)\n",
    "#         logger.info(f\"✅ Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "#         logger.info(f\"✅ Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "#         print(\"-\" * 50)\n",
    "#     else:\n",
    "#         logger.error(\"❌ Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a003021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5.b: Create Hierarchical Vector Database (V10 - ToC-First Method)\n",
    "# # This cell uses the pre-tagged ToC from Cell 4 as the source of truth\n",
    "# # to process the book, enrich text, and create the final vector database.\n",
    "\n",
    "# # --- Core Imports ---\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import logging\n",
    "# from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# # --- LangChain and Data Loading Imports ---\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "# from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# # --- Imports for EPUB and PDF Processing ---\n",
    "# from ebooklib import epub, ITEM_DOCUMENT\n",
    "# from bs4 import BeautifulSoup\n",
    "# import fitz  # PyMuPDF\n",
    "\n",
    "# # --- Logger Setup ---\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 1. HELPER FUNCTIONS\n",
    "# # ==============================================================================\n",
    "# # The previous helper functions (clean_metadata_for_chroma, extract_images_*)\n",
    "# # are still needed and can be copied from the previous answer. For brevity,\n",
    "# # only the new/modified helpers are shown in full here.\n",
    "\n",
    "# def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "#     if isinstance(value, (list, dict, set)):\n",
    "#         if isinstance(value, set): value = sorted(list(value))\n",
    "#         return json.dumps(value)\n",
    "#     if isinstance(value, (str, int, float, bool)) or value is None: return value\n",
    "#     return str(value)\n",
    "\n",
    "# def extract_images_from_epub(epub_path: str, output_dir: str, unit_id: str) -> Dict[str, List[str]]:\n",
    "#     logger.info(f\"Extracting images from EPUB: {os.path.basename(epub_path)}\")\n",
    "#     image_map: Dict[str, List[str]] = {}\n",
    "#     book_image_dir = os.path.join(output_dir, f\"{unit_id}_epub_images\")\n",
    "#     os.makedirs(book_image_dir, exist_ok=True)\n",
    "#     book = epub.read_epub(epub_path)\n",
    "#     text_files = [item for item in book.get_items_of_type(ITEM_DOCUMENT)]\n",
    "#     for item in book.get_items_of_type(ITEM_DOCUMENT):\n",
    "#         source_filename = os.path.basename(item.get_name())\n",
    "#         content = item.get_content().decode('utf-8', 'ignore')\n",
    "#         for image_item in book.get_items_of_type('image'):\n",
    "#             img_internal_path = image_item.get_name()\n",
    "#             if img_internal_path in content:\n",
    "#                 if source_filename not in image_map: image_map[source_filename] = []\n",
    "#                 img_filename = os.path.basename(img_internal_path)\n",
    "#                 image_path = os.path.join(book_image_dir, img_filename)\n",
    "#                 if not os.path.exists(image_path):\n",
    "#                     with open(image_path, \"wb\") as f: f.write(image_item.get_content())\n",
    "#                 if image_path not in image_map[source_filename]: image_map[source_filename].append(image_path)\n",
    "#     total_images = sum(len(v) for v in image_map.values())\n",
    "#     logger.info(f\"Extracted {total_images} total images to '{book_image_dir}'\")\n",
    "#     return image_map\n",
    "    \n",
    "# def flatten_toc_with_paths(nodes: List[Dict], current_path: List[str] = []) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Flattens the hierarchical ToC and adds the full 'titles_path' to each entry.\n",
    "#     \"\"\"\n",
    "#     flat_list = []\n",
    "#     for node in nodes:\n",
    "#         new_path = current_path + [node['title']]\n",
    "#         # Create a new entry to avoid modifying the original node\n",
    "#         flat_entry = node.copy()\n",
    "#         flat_entry['titles_path'] = new_path\n",
    "        \n",
    "#         # Add the entry itself (without its children) to the list\n",
    "#         children = flat_entry.pop('children', [])\n",
    "#         flat_list.append(flat_entry)\n",
    "        \n",
    "#         # Recursively process the children\n",
    "#         if children:\n",
    "#             flat_list.extend(flatten_toc_with_paths(children, new_path))\n",
    "            \n",
    "#     return flat_list\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 2. CORE ORCHESTRATION FUNCTION\n",
    "# # ==============================================================================\n",
    "\n",
    "# def process_book_with_extracted_toc(\n",
    "#     book_path: str,\n",
    "#     extracted_toc_json_path: str,\n",
    "#     chunk_size: int,\n",
    "#     chunk_overlap: int\n",
    "# ) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "\n",
    "#     logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "#     # --- Step 1: Load ToC with Pre-assigned IDs ---\n",
    "#     try:\n",
    "#         with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "#             hierarchical_toc = json.load(f)\n",
    "#         logger.info(\"Successfully loaded pre-extracted ToC with assigned IDs.\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"FATAL: Error loading ToC JSON: {e}\", exc_info=True)\n",
    "#         return [], []\n",
    "\n",
    "#     # --- Step 2: Create a Flattened ToC and a Title-based Lookup ---\n",
    "#     flat_toc = flatten_toc_with_paths(hierarchical_toc)\n",
    "#     toc_lookup = {entry['title'].strip().lower(): entry for entry in flat_toc}\n",
    "#     logger.info(f\"Created a flattened ToC with {len(flat_toc)} entries for matching.\")\n",
    "\n",
    "#     # --- Step 3: Extract Images (if any) ---\n",
    "#     file_extension = os.path.splitext(book_path.lower())[1]\n",
    "#     image_map = {}\n",
    "#     if file_extension == \".epub\":\n",
    "#         unit_id = extract_uo_id_from_filename(UNIT_OUTLINE_FILENAME)\n",
    "#         image_map = extract_images_from_epub(book_path, OUTPUT_IMAGES_DIR, unit_id)\n",
    "#     # PDF image extraction would go here if needed\n",
    "\n",
    "#     # --- Step 4: Create Enriched Documents by Matching Content to ToC ---\n",
    "#     final_documents_with_metadata: List[Document] = []\n",
    "#     if file_extension == \".epub\":\n",
    "#         book = epub.read_epub(book_path)\n",
    "#         current_metadata = {\"source\": os.path.basename(book_path), \"toc_id\": -1, \"level_1_title\": \"Preamble\"}\n",
    "        \n",
    "#         for item in book.get_items_of_type(ITEM_DOCUMENT):\n",
    "#             source_filename = os.path.basename(item.get_name())\n",
    "#             soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
    "            \n",
    "#             for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li']):\n",
    "#                 text = element.get_text().strip()\n",
    "#                 if not text:\n",
    "#                     continue\n",
    "\n",
    "#                 # Check if this element's text is a heading in our ToC\n",
    "#                 normalized_text = text.lower()\n",
    "#                 if normalized_text in toc_lookup:\n",
    "#                     # It's a heading, update the current context\n",
    "#                     toc_entry = toc_lookup[normalized_text]\n",
    "#                     current_metadata = {\"source\": os.path.basename(book_path)}\n",
    "#                     for i, title in enumerate(toc_entry['titles_path']):\n",
    "#                         current_metadata[f\"level_{i+1}_title\"] = title\n",
    "#                     current_metadata['toc_id'] = toc_entry['toc_id']\n",
    "#                     logger.info(f\"Context updated to: '{' -> '.join(toc_entry['titles_path'])}' [ID: {toc_entry['toc_id']}]\")\n",
    "                \n",
    "#                 # Tag the document with the current metadata\n",
    "#                 doc_meta = current_metadata.copy()\n",
    "#                 if source_filename in image_map:\n",
    "#                     doc_meta.setdefault('image_paths', []).extend(p for p in image_map[source_filename] if p not in doc_meta.get('image_paths', []))\n",
    "                \n",
    "#                 final_documents_with_metadata.append(Document(page_content=text, metadata=doc_meta))\n",
    "\n",
    "#     # --- Step 5: Finalize and Chunk ---\n",
    "#     logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "\n",
    "#     logger.info(\"Sanitizing metadata and chunking documents...\")\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    \n",
    "#     for doc in final_documents_with_metadata:\n",
    "#         doc.metadata = {k: clean_metadata_for_chroma(v) for k, v in doc.metadata.items()}\n",
    "        \n",
    "#     final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    \n",
    "#     logger.info(f\"Split into {len(final_chunks)} final chunks and assigning chunk_id...\")\n",
    "#     for i, chunk in enumerate(final_chunks):\n",
    "#         chunk.metadata['chunk_id'] = i\n",
    "\n",
    "#     return final_chunks, hierarchical_toc\n",
    "\n",
    "# # ==============================================================================\n",
    "# # 3. MAIN EXECUTION BLOCK\n",
    "# # ==============================================================================\n",
    "# if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "#     logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "#     logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "# else:\n",
    "#     final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "#         book_path=BOOK_PATH,\n",
    "#         extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "#         chunk_size=CHUNK_SIZE,\n",
    "#         chunk_overlap=CHUNK_OVERLAP\n",
    "#     )\n",
    "#     if final_chunks_for_db:\n",
    "#         if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "#             logger.warning(f\"Deleting existing ChromaDB directory: '{CHROMA_PERSIST_DIR}'\")\n",
    "#             shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "        \n",
    "#         logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "#         embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "#         vector_db = Chroma.from_documents(\n",
    "#             documents=final_chunks_for_db,\n",
    "#             embedding=embedding_model,\n",
    "#             persist_directory=CHROMA_PERSIST_DIR,\n",
    "#             collection_name=CHROMA_COLLECTION_NAME\n",
    "#         )\n",
    "#         count = vector_db._collection.count()\n",
    "#         print(\"-\" * 50)\n",
    "#         logger.info(f\"Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "#         logger.info(f\"Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "#         print(\"-\" * 50)\n",
    "#     else:\n",
    "#         logger.error(\"Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184c54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00948820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:46:37,295 - INFO - Processing book 'Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub' using ToC from 'ICT312_epub_table_of_contents.json'.\n",
      "2025-07-06 00:46:37,297 - INFO - Successfully loaded pre-extracted ToC.\n",
      "2025-07-06 00:46:39,109 - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-07-06 00:46:39,109 - INFO - NumExpr defaulting to 16 threads.\n",
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-07-06 00:46:44,336 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-07-06 00:46:44,337 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-07-06 00:46:47,770 - INFO - Loaded 11815 raw documents from source.\n",
      "2025-07-06 00:46:47,771 - INFO - Flattened ToC into 877 entries.\n",
      "2025-07-06 00:46:47,772 - INFO - Assigning metadata to EPUB elements using robust normalized matching...\n",
      "2025-07-06 00:46:47,774 - INFO - Context updated to: 'Preface' [ID: 3]\n",
      "2025-07-06 00:46:47,775 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Introduction' [ID: 557]\n",
      "2025-07-06 00:46:47,776 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,777 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,777 - INFO - Context updated to: 'About the Authors' [ID: 5]\n",
      "2025-07-06 00:46:47,778 - INFO - Context updated to: 'Acknowledgments' [ID: 6]\n",
      "2025-07-06 00:46:47,779 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 1. Understanding the Digital Forensics Profession and Investigations' [ID: 558]\n",
      "2025-07-06 00:46:47,779 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> An Overview of Digital Forensics' [ID: 9]\n",
      "2025-07-06 00:46:47,780 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> An Overview of Digital Forensics -> Digital Forensics and Other Related Disciplines' [ID: 10]\n",
      "2025-07-06 00:46:47,781 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> An Overview of Digital Forensics -> A Brief History of Digital Forensics' [ID: 11]\n",
      "2025-07-06 00:46:47,781 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> An Overview of Digital Forensics -> Understanding Case Law' [ID: 12]\n",
      "2025-07-06 00:46:47,782 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> An Overview of Digital Forensics -> Developing Digital Forensics Resources' [ID: 13]\n",
      "2025-07-06 00:46:47,783 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations' [ID: 14]\n",
      "2025-07-06 00:46:47,783 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Law Enforcement Agency Investigations' [ID: 15]\n",
      "2025-07-06 00:46:47,784 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Following Legal Processes' [ID: 16]\n",
      "2025-07-06 00:46:47,784 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Private-Sector Investigations' [ID: 17]\n",
      "2025-07-06 00:46:47,785 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Private-Sector Investigations -> Establishing Company Policies' [ID: 18]\n",
      "2025-07-06 00:46:47,785 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Private-Sector Investigations -> Displaying Warning Banners' [ID: 19]\n",
      "2025-07-06 00:46:47,786 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Private-Sector Investigations -> Designating an Authorized Requester' [ID: 20]\n",
      "2025-07-06 00:46:47,787 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Private-Sector Investigations -> Conducting Security Investigations' [ID: 21]\n",
      "2025-07-06 00:46:47,787 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing for Digital Investigations -> Understanding Private-Sector Investigations -> Distinguishing Personal and Company Property' [ID: 22]\n",
      "2025-07-06 00:46:47,788 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Maintaining Professional Conduct' [ID: 23]\n",
      "2025-07-06 00:46:47,788 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation' [ID: 24]\n",
      "2025-07-06 00:46:47,789 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> An Overview of a Computer Crime' [ID: 25]\n",
      "2025-07-06 00:46:47,789 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> An Overview of a Company Policy Violation' [ID: 26]\n",
      "2025-07-06 00:46:47,790 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> Taking a Systematic Approach' [ID: 27]\n",
      "2025-07-06 00:46:47,790 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> Taking a Systematic Approach -> Assessing the Case' [ID: 28]\n",
      "2025-07-06 00:46:47,791 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> Taking a Systematic Approach -> Planning Your Investigation' [ID: 29]\n",
      "2025-07-06 00:46:47,792 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Preparing a Digital Forensics Investigation -> Taking a Systematic Approach -> Securing Your Evidence' [ID: 30]\n",
      "2025-07-06 00:46:47,792 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations' [ID: 31]\n",
      "2025-07-06 00:46:47,793 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations -> Employee Termination Cases' [ID: 32]\n",
      "2025-07-06 00:46:47,793 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations -> Internet Abuse Investigations' [ID: 33]\n",
      "2025-07-06 00:46:47,793 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations -> E-mail Abuse Investigations' [ID: 34]\n",
      "2025-07-06 00:46:47,794 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations -> Attorney-Client Privilege Investigations' [ID: 35]\n",
      "2025-07-06 00:46:47,795 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations -> Industrial Espionage Investigations' [ID: 36]\n",
      "2025-07-06 00:46:47,795 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Procedures for Private-Sector High-Tech Investigations -> Industrial Espionage Investigations -> Interviews and Interrogations in High-Tech Investigations' [ID: 37]\n",
      "2025-07-06 00:46:47,796 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Understanding Data Recovery Workstations and Software' [ID: 38]\n",
      "2025-07-06 00:46:47,796 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Understanding Data Recovery Workstations and Software -> Setting Up Your Workstation for Digital Forensics' [ID: 39]\n",
      "2025-07-06 00:46:47,797 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation' [ID: 40]\n",
      "2025-07-06 00:46:47,797 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Gathering the Evidence' [ID: 41]\n",
      "2025-07-06 00:46:47,798 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Understanding Bit-stream Copies' [ID: 42]\n",
      "2025-07-06 00:46:47,798 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Understanding Bit-stream Copies -> Acquiring an Image of Evidence Media' [ID: 43]\n",
      "2025-07-06 00:46:47,799 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Analyzing Your Digital Evidence' [ID: 44]\n",
      "2025-07-06 00:46:47,799 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Analyzing Your Digital Evidence -> Some Additional Features of Autopsy' [ID: 45]\n",
      "2025-07-06 00:46:47,800 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Completing the Case' [ID: 46]\n",
      "2025-07-06 00:46:47,800 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Completing the Case -> Autopsy’s Report Generator' [ID: 47]\n",
      "2025-07-06 00:46:47,801 - INFO - Context updated to: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Conducting an Investigation -> Critiquing the Case' [ID: 48]\n",
      "2025-07-06 00:46:47,802 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,803 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,803 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,804 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,805 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,806 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory' [ID: 580]\n",
      "2025-07-06 00:46:47,806 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements' [ID: 57]\n",
      "2025-07-06 00:46:47,807 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Identifying Duties of the Lab Manager and Staff' [ID: 58]\n",
      "2025-07-06 00:46:47,807 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Lab Budget Planning' [ID: 59]\n",
      "2025-07-06 00:46:47,808 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Acquiring Certification and Training' [ID: 60]\n",
      "2025-07-06 00:46:47,808 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics -> International Association of Computer Investigative Specialists' [ID: 534]\n",
      "2025-07-06 00:46:47,809 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Acquiring Certification and Training -> ISC2 Certified Cyber Forensics Professional' [ID: 62]\n",
      "2025-07-06 00:46:47,809 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Acquiring Certification and Training -> High Tech Crime Network' [ID: 63]\n",
      "2025-07-06 00:46:47,809 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Acquiring Certification and Training -> EnCase Certified Examiner Certification' [ID: 64]\n",
      "2025-07-06 00:46:47,810 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Acquiring Certification and Training -> AccessData Certified Examiner' [ID: 65]\n",
      "2025-07-06 00:46:47,810 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Understanding Forensics Lab Accreditation Requirements -> Acquiring Certification and Training -> Other Training and Certifications' [ID: 66]\n",
      "2025-07-06 00:46:47,811 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab' [ID: 67]\n",
      "2025-07-06 00:46:47,811 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Identifying Lab Security Needs' [ID: 68]\n",
      "2025-07-06 00:46:47,812 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Conducting High-Risk Investigations' [ID: 69]\n",
      "2025-07-06 00:46:47,813 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Using Evidence Containers' [ID: 70]\n",
      "2025-07-06 00:46:47,813 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Overseeing Facility Maintenance' [ID: 71]\n",
      "2025-07-06 00:46:47,815 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Considering Physical Security Needs' [ID: 72]\n",
      "2025-07-06 00:46:47,816 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Auditing a Digital Forensics Lab' [ID: 73]\n",
      "2025-07-06 00:46:47,817 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Determining the Physical Requirements for a Digital Forensics Lab -> Determining Floor Plans for Digital Forensics Labs' [ID: 74]\n",
      "2025-07-06 00:46:47,817 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation' [ID: 75]\n",
      "2025-07-06 00:46:47,817 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation -> Selecting Workstations for a Lab' [ID: 76]\n",
      "2025-07-06 00:46:47,818 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation -> Selecting Workstations for Private-Sector Labs' [ID: 77]\n",
      "2025-07-06 00:46:47,818 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation -> Stocking Hardware Peripherals' [ID: 78]\n",
      "2025-07-06 00:46:47,819 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation -> Maintaining Operating Systems and Software Inventories' [ID: 79]\n",
      "2025-07-06 00:46:47,820 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation -> Using a Disaster Recovery Plan' [ID: 80]\n",
      "2025-07-06 00:46:47,821 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Selecting a Basic Forensic Workstation -> Planning for Equipment Upgrades' [ID: 81]\n",
      "2025-07-06 00:46:47,821 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab' [ID: 82]\n",
      "2025-07-06 00:46:47,822 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab' [ID: 83]\n",
      "2025-07-06 00:46:47,822 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Justification' [ID: 84]\n",
      "2025-07-06 00:46:47,823 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Budget Development' [ID: 85]\n",
      "2025-07-06 00:46:47,823 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Facility Cost' [ID: 86]\n",
      "2025-07-06 00:46:47,823 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Hardware Requirements' [ID: 87]\n",
      "2025-07-06 00:46:47,824 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Software Requirements' [ID: 88]\n",
      "2025-07-06 00:46:47,825 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Miscellaneous Budget Needs' [ID: 89]\n",
      "2025-07-06 00:46:47,825 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Approval and Acquisition' [ID: 90]\n",
      "2025-07-06 00:46:47,826 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Implementation' [ID: 91]\n",
      "2025-07-06 00:46:47,826 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Acceptance Testing' [ID: 92]\n",
      "2025-07-06 00:46:47,827 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Correction for Acceptance' [ID: 93]\n",
      "2025-07-06 00:46:47,828 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Production' [ID: 94]\n",
      "2025-07-06 00:46:47,828 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,829 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,829 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,830 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,830 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,831 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition' [ID: 607]\n",
      "2025-07-06 00:46:47,831 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Understanding Storage Formats for Digital Evidence' [ID: 103]\n",
      "2025-07-06 00:46:47,832 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Understanding Storage Formats for Digital Evidence -> Raw Format' [ID: 104]\n",
      "2025-07-06 00:46:47,832 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Understanding Storage Formats for Digital Evidence -> Proprietary Formats' [ID: 105]\n",
      "2025-07-06 00:46:47,833 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Understanding Storage Formats for Digital Evidence -> Advanced Forensic Format' [ID: 106]\n",
      "2025-07-06 00:46:47,833 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Determining the Best Acquisition Method' [ID: 107]\n",
      "2025-07-06 00:46:47,834 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Contingency Planning for Image Acquisitions' [ID: 108]\n",
      "2025-07-06 00:46:47,834 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools' [ID: 109]\n",
      "2025-07-06 00:46:47,835 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Mini-WinFE Boot CDs and USB Drives' [ID: 110]\n",
      "2025-07-06 00:46:47,835 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Acquiring Data with a Linux Boot CD' [ID: 111]\n",
      "2025-07-06 00:46:47,836 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Acquiring Data with a Linux Boot CD -> Using Linux Live CD Distributions' [ID: 112]\n",
      "2025-07-06 00:46:47,837 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Acquiring Data with a Linux Boot CD -> Preparing a Target Drive for Acquisition in Linux' [ID: 113]\n",
      "2025-07-06 00:46:47,838 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Acquiring Data with a Linux Boot CD -> Acquiring Data with dd in Linux' [ID: 114]\n",
      "2025-07-06 00:46:47,838 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Acquiring Data with a Linux Boot CD -> Acquiring Data with dcfldd in Linux' [ID: 115]\n",
      "2025-07-06 00:46:47,839 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Acquisition Tools -> Capturing an Image with AccessData FTK Imager Lite' [ID: 116]\n",
      "2025-07-06 00:46:47,840 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Validating Data Acquisitions' [ID: 117]\n",
      "2025-07-06 00:46:47,840 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Validating Data Acquisitions -> Linux Validation Methods' [ID: 118]\n",
      "2025-07-06 00:46:47,841 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Validating Data Acquisitions -> Linux Validation Methods -> Validating dd-Acquired Data' [ID: 119]\n",
      "2025-07-06 00:46:47,842 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Validating Data Acquisitions -> Linux Validation Methods -> Validating dcfldd-Acquired Data' [ID: 120]\n",
      "2025-07-06 00:46:47,843 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Validating Data Acquisitions -> Windows Validation Methods' [ID: 121]\n",
      "2025-07-06 00:46:47,843 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Performing RAID Data Acquisitions' [ID: 122]\n",
      "2025-07-06 00:46:47,844 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Performing RAID Data Acquisitions -> Understanding RAID' [ID: 123]\n",
      "2025-07-06 00:46:47,845 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Performing RAID Data Acquisitions -> Acquiring RAID Disks' [ID: 124]\n",
      "2025-07-06 00:46:47,845 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools -> Runtime Software' [ID: 134]\n",
      "2025-07-06 00:46:47,846 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Remote Network Acquisition Tools' [ID: 125]\n",
      "2025-07-06 00:46:47,846 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Remote Network Acquisition Tools -> Remote Acquisition with ProDiscover' [ID: 126]\n",
      "2025-07-06 00:46:47,847 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Remote Network Acquisition Tools -> Remote Acquisition with EnCase Enterprise' [ID: 127]\n",
      "2025-07-06 00:46:47,847 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Remote Network Acquisition Tools -> Remote Acquisition with R-Tools R-Studio' [ID: 128]\n",
      "2025-07-06 00:46:47,848 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Remote Network Acquisition Tools -> Remote Acquisition with WetStone US-LATT PRO' [ID: 129]\n",
      "2025-07-06 00:46:47,849 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Remote Network Acquisition Tools -> Remote Acquisition with F-Response' [ID: 130]\n",
      "2025-07-06 00:46:47,849 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools' [ID: 131]\n",
      "2025-07-06 00:46:47,850 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools -> PassMark Software ImageUSB' [ID: 132]\n",
      "2025-07-06 00:46:47,850 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools -> ASR Data SMART' [ID: 133]\n",
      "2025-07-06 00:46:47,851 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools -> Runtime Software' [ID: 134]\n",
      "2025-07-06 00:46:47,851 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools -> ILookIX IXImager' [ID: 135]\n",
      "2025-07-06 00:46:47,852 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Using Other Forensics Acquisition Tools -> SourceForge' [ID: 136]\n",
      "2025-07-06 00:46:47,852 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,853 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,854 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,854 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,855 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,855 - INFO - Context updated to: 'Chapter 3. Data Acquisition -> Understanding Storage Formats for Digital Evidence -> Raw Format' [ID: 104]\n",
      "2025-07-06 00:46:47,856 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes' [ID: 632]\n",
      "2025-07-06 00:46:47,856 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Identifying Digital Evidence' [ID: 145]\n",
      "2025-07-06 00:46:47,857 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Identifying Digital Evidence -> Understanding Rules of Evidence' [ID: 146]\n",
      "2025-07-06 00:46:47,858 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Collecting Evidence in Private-Sector Incident Scenes' [ID: 147]\n",
      "2025-07-06 00:46:47,858 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Processing Law Enforcement Crime Scenes' [ID: 148]\n",
      "2025-07-06 00:46:47,859 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Processing Law Enforcement Crime Scenes -> Understanding Concepts and Terms Used in Warrants' [ID: 149]\n",
      "2025-07-06 00:46:47,860 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search' [ID: 150]\n",
      "2025-07-06 00:46:47,860 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Identifying the Nature of the Case' [ID: 151]\n",
      "2025-07-06 00:46:47,860 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Identifying the Type of OS or Digital Device' [ID: 152]\n",
      "2025-07-06 00:46:47,861 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Determining Whether You Can Seize Computers and Digital Devices' [ID: 153]\n",
      "2025-07-06 00:46:47,861 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Getting a Detailed Description of the Location' [ID: 154]\n",
      "2025-07-06 00:46:47,862 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Determining Who Is in Charge' [ID: 155]\n",
      "2025-07-06 00:46:47,862 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Using Additional Technical Expertise' [ID: 156]\n",
      "2025-07-06 00:46:47,863 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Determining the Tools You Need' [ID: 157]\n",
      "2025-07-06 00:46:47,864 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Preparing for a Search -> Preparing the Investigation Team' [ID: 158]\n",
      "2025-07-06 00:46:47,865 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Securing a Digital Incident or Crime Scene' [ID: 159]\n",
      "2025-07-06 00:46:47,865 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene' [ID: 160]\n",
      "2025-07-06 00:46:47,866 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene -> Preparing to Acquire Digital Evidence' [ID: 161]\n",
      "2025-07-06 00:46:47,866 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene -> Processing Incident or Crime Scenes' [ID: 162]\n",
      "2025-07-06 00:46:47,867 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene -> Processing Data Centers with RAID Systems' [ID: 163]\n",
      "2025-07-06 00:46:47,867 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene -> Using a Technical Advisor' [ID: 164]\n",
      "2025-07-06 00:46:47,868 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene -> Documenting Evidence in the Lab' [ID: 165]\n",
      "2025-07-06 00:46:47,868 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Seizing Digital Evidence at the Scene -> Processing and Handling Digital Evidence' [ID: 166]\n",
      "2025-07-06 00:46:47,869 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Storing Digital Evidence' [ID: 167]\n",
      "2025-07-06 00:46:47,869 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Storing Digital Evidence -> Evidence Retention and Media Storage Needs' [ID: 168]\n",
      "2025-07-06 00:46:47,869 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Storing Digital Evidence -> Documenting Evidence' [ID: 169]\n",
      "2025-07-06 00:46:47,870 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Obtaining a Digital Hash' [ID: 170]\n",
      "2025-07-06 00:46:47,871 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Reviewing a Case' [ID: 171]\n",
      "2025-07-06 00:46:47,872 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Reviewing a Case -> Sample Civil Investigation' [ID: 172]\n",
      "2025-07-06 00:46:47,872 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Reviewing a Case -> An Example of a Criminal Investigation' [ID: 173]\n",
      "2025-07-06 00:46:47,873 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Reviewing a Case -> Reviewing Background Information for a Case' [ID: 174]\n",
      "2025-07-06 00:46:47,874 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Reviewing a Case -> Planning the Investigation' [ID: 175]\n",
      "2025-07-06 00:46:47,875 - INFO - Context updated to: 'Chapter 4. Processing Crime and Incident Scenes -> Reviewing a Case -> Conducting the Investigation: Acquiring Evidence with OSForensics' [ID: 176]\n",
      "2025-07-06 00:46:47,876 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,877 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,878 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,878 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,879 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,880 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 5. Working with Windows and CLI Systems' [ID: 656]\n",
      "2025-07-06 00:46:47,881 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding File Systems' [ID: 185]\n",
      "2025-07-06 00:46:47,882 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding File Systems -> Understanding the Boot Sequence' [ID: 186]\n",
      "2025-07-06 00:46:47,882 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding File Systems -> Understanding Disk Drives' [ID: 187]\n",
      "2025-07-06 00:46:47,883 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding File Systems -> Solid-State Storage Devices' [ID: 188]\n",
      "2025-07-06 00:46:47,883 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Exploring Microsoft File Structures' [ID: 189]\n",
      "2025-07-06 00:46:47,884 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Exploring Microsoft File Structures -> Disk Partitions' [ID: 190]\n",
      "2025-07-06 00:46:47,885 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Exploring Microsoft File Structures -> Examining FAT Disks' [ID: 191]\n",
      "2025-07-06 00:46:47,886 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Exploring Microsoft File Structures -> Examining FAT Disks -> Deleting FAT Files' [ID: 192]\n",
      "2025-07-06 00:46:47,886 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks' [ID: 193]\n",
      "2025-07-06 00:46:47,887 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> NTFS System Files' [ID: 194]\n",
      "2025-07-06 00:46:47,887 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT and File Attributes' [ID: 195]\n",
      "2025-07-06 00:46:47,888 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data' [ID: 196]\n",
      "2025-07-06 00:46:47,888 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> MFT Header Fields' [ID: 197]\n",
      "2025-07-06 00:46:47,889 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> Attribute 0x10: Standard Information' [ID: 198]\n",
      "2025-07-06 00:46:47,890 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> Attribute 0x30: File Name' [ID: 199]\n",
      "2025-07-06 00:46:47,890 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> Attribute 0x40: Object_ID' [ID: 200]\n",
      "2025-07-06 00:46:47,891 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> Attribute 0x80: Data for a Resident File' [ID: 201]\n",
      "2025-07-06 00:46:47,891 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> Attribute 0x80: Data for a Nonresident File' [ID: 202]\n",
      "2025-07-06 00:46:47,891 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> MFT Structures for File Data -> Interpreting a Data Run' [ID: 203]\n",
      "2025-07-06 00:46:47,892 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> NTFS Alternate Data Streams' [ID: 204]\n",
      "2025-07-06 00:46:47,892 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> NTFS Compressed Files' [ID: 205]\n",
      "2025-07-06 00:46:47,893 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> NTFS Encrypting File System' [ID: 206]\n",
      "2025-07-06 00:46:47,893 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> EFS Recovery Key Agent' [ID: 207]\n",
      "2025-07-06 00:46:47,894 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> Deleting NTFS Files' [ID: 208]\n",
      "2025-07-06 00:46:47,895 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Examining NTFS Disks -> Resilient File System' [ID: 209]\n",
      "2025-07-06 00:46:47,895 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Whole Disk Encryption' [ID: 210]\n",
      "2025-07-06 00:46:47,896 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Whole Disk Encryption -> Examining Microsoft BitLocker' [ID: 211]\n",
      "2025-07-06 00:46:47,896 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Whole Disk Encryption -> Examining Third-Party Disk Encryption Tools' [ID: 212]\n",
      "2025-07-06 00:46:47,897 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding the Windows Registry' [ID: 213]\n",
      "2025-07-06 00:46:47,897 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding the Windows Registry -> Exploring the Organization of the Windows Registry' [ID: 214]\n",
      "2025-07-06 00:46:47,898 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding the Windows Registry -> Examining the Windows Registry' [ID: 215]\n",
      "2025-07-06 00:46:47,898 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks' [ID: 216]\n",
      "2025-07-06 00:46:47,899 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks -> Startup in Windows 7, Windows 8, and Windows 10' [ID: 217]\n",
      "2025-07-06 00:46:47,899 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks -> Startup in Windows NT and Later' [ID: 218]\n",
      "2025-07-06 00:46:47,899 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks -> Startup in Windows NT and Later -> Startup Files for Windows Vista' [ID: 219]\n",
      "2025-07-06 00:46:47,900 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks -> Startup in Windows NT and Later -> Startup Files for Windows XP' [ID: 220]\n",
      "2025-07-06 00:46:47,900 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks -> Startup in Windows NT and Later -> Windows XP System Files' [ID: 221]\n",
      "2025-07-06 00:46:47,901 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Microsoft Startup Tasks -> Startup in Windows NT and Later -> Contamination Concerns with Windows XP' [ID: 222]\n",
      "2025-07-06 00:46:47,902 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Virtual Machines' [ID: 223]\n",
      "2025-07-06 00:46:47,903 - INFO - Context updated to: 'Chapter 5. Working with Windows and CLI Systems -> Understanding Virtual Machines -> Creating a Virtual Machine' [ID: 224]\n",
      "2025-07-06 00:46:47,904 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,904 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,905 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,906 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,906 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,907 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools' [ID: 678]\n",
      "2025-07-06 00:46:47,907 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs' [ID: 233]\n",
      "2025-07-06 00:46:47,908 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Types of Digital Forensics Tools' [ID: 234]\n",
      "2025-07-06 00:46:47,909 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Types of Digital Forensics Tools -> Hardware Forensics Tools' [ID: 235]\n",
      "2025-07-06 00:46:47,909 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Types of Digital Forensics Tools -> Software Forensics Tools' [ID: 236]\n",
      "2025-07-06 00:46:47,909 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools' [ID: 237]\n",
      "2025-07-06 00:46:47,910 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Acquisition' [ID: 238]\n",
      "2025-07-06 00:46:47,910 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Validation and Verification' [ID: 239]\n",
      "2025-07-06 00:46:47,911 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Extraction' [ID: 240]\n",
      "2025-07-06 00:46:47,911 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Reconstruction' [ID: 241]\n",
      "2025-07-06 00:46:47,911 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Reporting' [ID: 242]\n",
      "2025-07-06 00:46:47,912 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Acquisition' [ID: 238]\n",
      "2025-07-06 00:46:47,913 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Validation and Verification' [ID: 239]\n",
      "2025-07-06 00:46:47,913 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Extraction' [ID: 240]\n",
      "2025-07-06 00:46:47,914 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Reconstruction' [ID: 241]\n",
      "2025-07-06 00:46:47,915 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Reporting' [ID: 242]\n",
      "2025-07-06 00:46:47,915 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tool Comparisons' [ID: 243]\n",
      "2025-07-06 00:46:47,916 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Other Considerations for Tools' [ID: 244]\n",
      "2025-07-06 00:46:47,917 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools' [ID: 245]\n",
      "2025-07-06 00:46:47,917 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Command-Line Forensics Tools' [ID: 246]\n",
      "2025-07-06 00:46:47,918 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools' [ID: 247]\n",
      "2025-07-06 00:46:47,918 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools -> Smart' [ID: 248]\n",
      "2025-07-06 00:46:47,919 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools -> Helix 3' [ID: 249]\n",
      "2025-07-06 00:46:47,919 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools -> Kali Linux' [ID: 250]\n",
      "2025-07-06 00:46:47,920 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools -> Autopsy and Sleuth Kit' [ID: 251]\n",
      "2025-07-06 00:46:47,920 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools -> Forcepoint Threat Protection' [ID: 252]\n",
      "2025-07-06 00:46:47,920 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Other GUI Forensics Tools' [ID: 253]\n",
      "2025-07-06 00:46:47,921 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Hardware Tools' [ID: 254]\n",
      "2025-07-06 00:46:47,921 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Hardware Tools -> Forensic Workstations' [ID: 255]\n",
      "2025-07-06 00:46:47,922 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Hardware Tools -> Forensic Workstations -> Building Your Own Workstation' [ID: 256]\n",
      "2025-07-06 00:46:47,922 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Hardware Tools -> Using a Write-Blocker' [ID: 257]\n",
      "2025-07-06 00:46:47,923 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Hardware Tools -> Recommendations for a Forensic Workstation' [ID: 258]\n",
      "2025-07-06 00:46:47,924 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Validating and Testing Forensics Software' [ID: 259]\n",
      "2025-07-06 00:46:47,924 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Validating and Testing Forensics Software -> Using National Institute of Standards and Technology Tools' [ID: 260]\n",
      "2025-07-06 00:46:47,925 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Validating and Testing Forensics Software -> Using Validation Protocols' [ID: 261]\n",
      "2025-07-06 00:46:47,925 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Validating and Testing Forensics Software -> Using Validation Protocols -> Digital Forensics Examination Protocol' [ID: 262]\n",
      "2025-07-06 00:46:47,926 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Validating and Testing Forensics Software -> Using Validation Protocols -> Digital Forensics Tool Upgrade Protocol' [ID: 263]\n",
      "2025-07-06 00:46:47,926 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,927 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,928 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,929 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Validation and Verification' [ID: 239]\n",
      "2025-07-06 00:46:47,929 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Acquisition' [ID: 238]\n",
      "2025-07-06 00:46:47,929 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Extraction' [ID: 240]\n",
      "2025-07-06 00:46:47,930 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Reconstruction' [ID: 241]\n",
      "2025-07-06 00:46:47,931 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Reconstructing File Fragments' [ID: 308]\n",
      "2025-07-06 00:46:47,931 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,932 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,933 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 7. Linux and Macintosh File Systems' [ID: 699]\n",
      "2025-07-06 00:46:47,933 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Examining Linux File Structures' [ID: 272]\n",
      "2025-07-06 00:46:47,935 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Examining Linux File Structures -> File Structures in Ext4' [ID: 273]\n",
      "2025-07-06 00:46:47,935 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Examining Linux File Structures -> File Structures in Ext4 -> Inodes' [ID: 274]\n",
      "2025-07-06 00:46:47,936 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Examining Linux File Structures -> File Structures in Ext4 -> Hard Links and Symbolic Links' [ID: 275]\n",
      "2025-07-06 00:46:47,936 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Understanding Macintosh File Structures' [ID: 276]\n",
      "2025-07-06 00:46:47,937 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Understanding Macintosh File Structures -> An Overview of Mac File Structures' [ID: 277]\n",
      "2025-07-06 00:46:47,937 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Understanding Macintosh File Structures -> Forensics Procedures in Mac' [ID: 278]\n",
      "2025-07-06 00:46:47,938 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Understanding Macintosh File Structures -> Forensics Procedures in Mac -> Acquisition Methods in macOS' [ID: 279]\n",
      "2025-07-06 00:46:47,939 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Using Linux Forensics Tools' [ID: 280]\n",
      "2025-07-06 00:46:47,940 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Using Linux Forensics Tools -> Installing Sleuth Kit and Autopsy' [ID: 281]\n",
      "2025-07-06 00:46:47,941 - INFO - Context updated to: 'Chapter 7. Linux and Macintosh File Systems -> Using Linux Forensics Tools -> Examining a Case with Sleuth Kit and Autopsy' [ID: 282]\n",
      "2025-07-06 00:46:47,941 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,942 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,943 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,943 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,944 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,944 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 8. Recovering Graphics Files' [ID: 716]\n",
      "2025-07-06 00:46:47,945 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File' [ID: 291]\n",
      "2025-07-06 00:46:47,945 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Bitmap and Raster Images' [ID: 292]\n",
      "2025-07-06 00:46:47,946 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Vector Graphics' [ID: 293]\n",
      "2025-07-06 00:46:47,946 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Metafile Graphics' [ID: 294]\n",
      "2025-07-06 00:46:47,946 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Graphics File Formats' [ID: 295]\n",
      "2025-07-06 00:46:47,948 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Digital Photograph File Formats' [ID: 296]\n",
      "2025-07-06 00:46:47,948 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Digital Photograph File Formats -> Examining the Raw File Format' [ID: 297]\n",
      "2025-07-06 00:46:47,949 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Recognizing a Graphics File -> Understanding Digital Photograph File Formats -> Examining the Exchangeable Image File Format' [ID: 298]\n",
      "2025-07-06 00:46:47,949 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression' [ID: 299]\n",
      "2025-07-06 00:46:47,950 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Lossless and Lossy Compression' [ID: 300]\n",
      "2025-07-06 00:46:47,950 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Locating and Recovering Graphics Files' [ID: 301]\n",
      "2025-07-06 00:46:47,951 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Identifying Graphics File Fragments' [ID: 302]\n",
      "2025-07-06 00:46:47,952 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Repairing Damaged Headers' [ID: 303]\n",
      "2025-07-06 00:46:47,952 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Searching for and Carving Data from Unallocated Space' [ID: 304]\n",
      "2025-07-06 00:46:47,953 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Searching for and Carving Data from Unallocated Space -> Planning Your Examination' [ID: 305]\n",
      "2025-07-06 00:46:47,953 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Searching for and Carving Data from Unallocated Space -> Searching for and Recovering Digital Photograph Evidence' [ID: 306]\n",
      "2025-07-06 00:46:47,954 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Rebuilding File Headers' [ID: 307]\n",
      "2025-07-06 00:46:47,954 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Data Compression -> Reconstructing File Fragments' [ID: 308]\n",
      "2025-07-06 00:46:47,955 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Identifying Unknown File Formats' [ID: 309]\n",
      "2025-07-06 00:46:47,956 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Identifying Unknown File Formats -> Analyzing Graphics File Headers' [ID: 310]\n",
      "2025-07-06 00:46:47,956 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Identifying Unknown File Formats -> Tools for Viewing Images' [ID: 311]\n",
      "2025-07-06 00:46:47,957 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Identifying Unknown File Formats -> Understanding Steganography in Graphics Files' [ID: 312]\n",
      "2025-07-06 00:46:47,957 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Identifying Unknown File Formats -> Using Steganalysis Tools' [ID: 313]\n",
      "2025-07-06 00:46:47,958 - INFO - Context updated to: 'Chapter 8. Recovering Graphics Files -> Understanding Copyright Issues with Graphics' [ID: 314]\n",
      "2025-07-06 00:46:47,958 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,959 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,959 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,960 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,961 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,961 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 9. Digital Forensics Analysis and Validation' [ID: 733]\n",
      "2025-07-06 00:46:47,962 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Determining What Data to Collect and Analyze' [ID: 323]\n",
      "2025-07-06 00:46:47,963 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Determining What Data to Collect and Analyze -> Approaching Digital Forensics Cases' [ID: 324]\n",
      "2025-07-06 00:46:47,963 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Determining What Data to Collect and Analyze -> Approaching Digital Forensics Cases -> Refining and Modifying the Investigation Plan' [ID: 325]\n",
      "2025-07-06 00:46:47,964 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Determining What Data to Collect and Analyze -> Using Autopsy to Validate Data' [ID: 326]\n",
      "2025-07-06 00:46:47,965 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Determining What Data to Collect and Analyze -> Using Autopsy to Validate Data -> Installing NSRL Hashes in Autopsy' [ID: 327]\n",
      "2025-07-06 00:46:47,966 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Determining What Data to Collect and Analyze -> Collecting Hash Values in Autopsy' [ID: 328]\n",
      "2025-07-06 00:46:47,967 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Validating Forensic Data' [ID: 329]\n",
      "2025-07-06 00:46:47,967 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Validating Forensic Data -> Validating with Hexadecimal Editors' [ID: 330]\n",
      "2025-07-06 00:46:47,968 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Validating Forensic Data -> Validating with Hexadecimal Editors -> Using Hash Values to Discriminate Data' [ID: 331]\n",
      "2025-07-06 00:46:47,968 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Validating Forensic Data -> Validating with Digital Forensics Tools' [ID: 332]\n",
      "2025-07-06 00:46:47,969 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques' [ID: 333]\n",
      "2025-07-06 00:46:47,969 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Hiding Files by Using the OS' [ID: 334]\n",
      "2025-07-06 00:46:47,970 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Hiding Partitions' [ID: 335]\n",
      "2025-07-06 00:46:47,970 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Marking Bad Clusters' [ID: 336]\n",
      "2025-07-06 00:46:47,971 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Bit-Shifting' [ID: 337]\n",
      "2025-07-06 00:46:47,971 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Understanding Steganalysis Methods' [ID: 338]\n",
      "2025-07-06 00:46:47,972 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Examining Encrypted Files' [ID: 339]\n",
      "2025-07-06 00:46:47,972 - INFO - Context updated to: 'Chapter 9. Digital Forensics Analysis and Validation -> Addressing Data-Hiding Techniques -> Recovering Passwords' [ID: 340]\n",
      "2025-07-06 00:46:47,973 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,973 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,973 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,974 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Digital Forensics Software Tools -> Linux Forensics Tools -> Smart' [ID: 248]\n",
      "2025-07-06 00:46:47,974 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,975 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,976 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics' [ID: 750]\n",
      "2025-07-06 00:46:47,977 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics' [ID: 349]\n",
      "2025-07-06 00:46:47,978 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors' [ID: 350]\n",
      "2025-07-06 00:46:47,978 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors -> Parallels Desktop' [ID: 351]\n",
      "2025-07-06 00:46:47,979 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors -> KVM' [ID: 352]\n",
      "2025-07-06 00:46:47,979 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors -> Microsoft Hyper-V' [ID: 353]\n",
      "2025-07-06 00:46:47,979 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors -> VMware Workstation and Workstation Player' [ID: 354]\n",
      "2025-07-06 00:46:47,980 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors -> VirtualBox' [ID: 355]\n",
      "2025-07-06 00:46:47,981 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Conducting an Investigation with Type 2 Hypervisors' [ID: 356]\n",
      "2025-07-06 00:46:47,982 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Conducting an Investigation with Type 2 Hypervisors -> Other VM Examination Methods' [ID: 357]\n",
      "2025-07-06 00:46:47,983 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Conducting an Investigation with Type 2 Hypervisors -> Using VMs as Forensics Tools' [ID: 358]\n",
      "2025-07-06 00:46:47,983 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Working with Type 1 Hypervisors' [ID: 359]\n",
      "2025-07-06 00:46:47,984 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Performing Live Acquisitions' [ID: 360]\n",
      "2025-07-06 00:46:47,984 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Performing Live Acquisitions -> Performing a Live Acquisition in Windows' [ID: 361]\n",
      "2025-07-06 00:46:47,985 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview' [ID: 362]\n",
      "2025-07-06 00:46:47,985 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> The Need for Established Procedures' [ID: 363]\n",
      "2025-07-06 00:46:47,985 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Securing a Network' [ID: 364]\n",
      "2025-07-06 00:46:47,986 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Developing Procedures for Network Forensics' [ID: 365]\n",
      "2025-07-06 00:46:47,987 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Developing Procedures for Network Forensics -> Reviewing Network Logs' [ID: 366]\n",
      "2025-07-06 00:46:47,988 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Developing Procedures for Network Forensics -> Using Network Tools' [ID: 367]\n",
      "2025-07-06 00:46:47,988 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Developing Procedures for Network Forensics -> Using Packet Analyzers' [ID: 368]\n",
      "2025-07-06 00:46:47,989 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Investigating Virtual Networks' [ID: 369]\n",
      "2025-07-06 00:46:47,989 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Network Forensics Overview -> Examining the Honeynet Project' [ID: 370]\n",
      "2025-07-06 00:46:47,990 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:47,991 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:47,992 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:47,992 - INFO - Context updated to: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> An Overview of Virtual Machine Forensics -> Type 2 Hypervisors' [ID: 350]\n",
      "2025-07-06 00:46:47,994 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:47,995 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:47,995 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 11. E-mail and Social Media Investigations' [ID: 775]\n",
      "2025-07-06 00:46:47,996 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Exploring the Role of E-mail in Investigations' [ID: 379]\n",
      "2025-07-06 00:46:47,996 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Exploring the Roles of the Client and Server in E-mail' [ID: 380]\n",
      "2025-07-06 00:46:47,997 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations' [ID: 381]\n",
      "2025-07-06 00:46:47,997 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Understanding Forensic Linguistics' [ID: 382]\n",
      "2025-07-06 00:46:47,998 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Examining E-mail Messages' [ID: 383]\n",
      "2025-07-06 00:46:47,998 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Examining E-mail Messages -> Copying an E-mail Message' [ID: 384]\n",
      "2025-07-06 00:46:47,999 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Viewing E-mail Headers' [ID: 385]\n",
      "2025-07-06 00:46:48,000 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Examining E-mail Headers' [ID: 386]\n",
      "2025-07-06 00:46:48,001 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Examining Additional E-mail Files' [ID: 387]\n",
      "2025-07-06 00:46:48,001 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Tracing an E-mail Message' [ID: 388]\n",
      "2025-07-06 00:46:48,003 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Investigating E-mail Crimes and Violations -> Using Network E-mail Logs' [ID: 389]\n",
      "2025-07-06 00:46:48,004 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Understanding E-mail Servers' [ID: 390]\n",
      "2025-07-06 00:46:48,005 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Understanding E-mail Servers -> Examining UNIX E-mail Server Logs' [ID: 391]\n",
      "2025-07-06 00:46:48,005 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Understanding E-mail Servers -> Examining Microsoft E-mail Server Logs' [ID: 392]\n",
      "2025-07-06 00:46:48,007 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Using Specialized E-mail Forensics Tools' [ID: 393]\n",
      "2025-07-06 00:46:48,008 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Using Specialized E-mail Forensics Tools -> Using Magnet AXIOM to Recover E-mail' [ID: 394]\n",
      "2025-07-06 00:46:48,008 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Using Specialized E-mail Forensics Tools -> Using a Hex Editor to Carve E-mail Messages' [ID: 395]\n",
      "2025-07-06 00:46:48,009 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Using Specialized E-mail Forensics Tools -> Recovering Outlook Files' [ID: 396]\n",
      "2025-07-06 00:46:48,010 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Using Specialized E-mail Forensics Tools -> E-mail Case Studies' [ID: 397]\n",
      "2025-07-06 00:46:48,011 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Applying Digital Forensics Methods to Social Media Communications' [ID: 398]\n",
      "2025-07-06 00:46:48,012 - INFO - Context updated to: 'Chapter 11. E-mail and Social Media Investigations -> Applying Digital Forensics Methods to Social Media Communications -> Forensics Tools for Social Media Investigations' [ID: 399]\n",
      "2025-07-06 00:46:48,013 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:48,013 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:48,014 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,015 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:48,016 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:48,017 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything' [ID: 406]\n",
      "2025-07-06 00:46:48,017 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Mobile Device Forensics' [ID: 408]\n",
      "2025-07-06 00:46:48,018 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Mobile Device Forensics -> Mobile Phone Basics' [ID: 409]\n",
      "2025-07-06 00:46:48,018 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Mobile Device Forensics -> Inside Mobile Devices' [ID: 410]\n",
      "2025-07-06 00:46:48,019 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Mobile Device Forensics -> Inside Mobile Devices -> SIM Cards' [ID: 411]\n",
      "2025-07-06 00:46:48,019 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Acquisition Procedures for Mobile Devices' [ID: 412]\n",
      "2025-07-06 00:46:48,020 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Acquisition Procedures for Mobile Devices -> Mobile Forensics Equipment' [ID: 413]\n",
      "2025-07-06 00:46:48,021 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Acquisition Procedures for Mobile Devices -> Mobile Forensics Equipment -> SIM Card Readers' [ID: 414]\n",
      "2025-07-06 00:46:48,021 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Acquisition Procedures for Mobile Devices -> Mobile Forensics Equipment -> Mobile Phone Forensics Tools and Methods' [ID: 415]\n",
      "2025-07-06 00:46:48,022 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Acquisition Procedures for Mobile Devices -> Using Mobile Forensics Tools' [ID: 416]\n",
      "2025-07-06 00:46:48,023 - INFO - Context updated to: 'Chapter 12. Mobile Device Forensics and the Internet of Anything -> Understanding Forensics in the Internet of Anything' [ID: 417]\n",
      "2025-07-06 00:46:48,024 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:48,024 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:48,025 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,025 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:48,027 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:48,027 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 13. Cloud Forensics' [ID: 809]\n",
      "2025-07-06 00:46:48,027 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> An Overview of Cloud Computing' [ID: 426]\n",
      "2025-07-06 00:46:48,028 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> An Overview of Cloud Computing -> History of the Cloud' [ID: 427]\n",
      "2025-07-06 00:46:48,029 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> An Overview of Cloud Computing -> Cloud Service Levels and Deployment Methods' [ID: 428]\n",
      "2025-07-06 00:46:48,030 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> An Overview of Cloud Computing -> Cloud Vendors' [ID: 429]\n",
      "2025-07-06 00:46:48,031 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> An Overview of Cloud Computing -> Basic Concepts of Cloud Forensics' [ID: 430]\n",
      "2025-07-06 00:46:48,031 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics' [ID: 431]\n",
      "2025-07-06 00:46:48,032 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Service Level Agreements' [ID: 432]\n",
      "2025-07-06 00:46:48,032 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Service Level Agreements -> Policies, Standards, and Guidelines for CSPs' [ID: 433]\n",
      "2025-07-06 00:46:48,033 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Service Level Agreements -> CSP Processes and Procedures' [ID: 434]\n",
      "2025-07-06 00:46:48,033 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Jurisdiction Issues' [ID: 435]\n",
      "2025-07-06 00:46:48,034 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Accessing Evidence in the Cloud' [ID: 436]\n",
      "2025-07-06 00:46:48,034 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Accessing Evidence in the Cloud -> Search Warrants' [ID: 437]\n",
      "2025-07-06 00:46:48,035 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Accessing Evidence in the Cloud -> Subpoenas and Court Orders' [ID: 438]\n",
      "2025-07-06 00:46:48,035 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics' [ID: 439]\n",
      "2025-07-06 00:46:48,035 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Architecture' [ID: 440]\n",
      "2025-07-06 00:46:48,036 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Analysis of Cloud Forensic Data' [ID: 441]\n",
      "2025-07-06 00:46:48,036 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Anti-Forensics' [ID: 442]\n",
      "2025-07-06 00:46:48,037 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Incident First Responders' [ID: 443]\n",
      "2025-07-06 00:46:48,038 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Role Management' [ID: 444]\n",
      "2025-07-06 00:46:48,038 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Standards and Training' [ID: 445]\n",
      "2025-07-06 00:46:48,039 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Architecture' [ID: 440]\n",
      "2025-07-06 00:46:48,039 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Analysis of Cloud Forensic Data' [ID: 441]\n",
      "2025-07-06 00:46:48,040 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Anti-Forensics' [ID: 442]\n",
      "2025-07-06 00:46:48,040 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Incident First Responders' [ID: 443]\n",
      "2025-07-06 00:46:48,041 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Role Management' [ID: 444]\n",
      "2025-07-06 00:46:48,041 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Technical Challenges in Cloud Forensics -> Standards and Training' [ID: 445]\n",
      "2025-07-06 00:46:48,042 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Acquisitions in the Cloud' [ID: 446]\n",
      "2025-07-06 00:46:48,042 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Acquisitions in the Cloud -> Encryption in the Cloud' [ID: 447]\n",
      "2025-07-06 00:46:48,043 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation' [ID: 448]\n",
      "2025-07-06 00:46:48,043 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Investigating CSPs' [ID: 449]\n",
      "2025-07-06 00:46:48,044 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Investigating Cloud Customers' [ID: 450]\n",
      "2025-07-06 00:46:48,044 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Understanding Prefetch Files' [ID: 451]\n",
      "2025-07-06 00:46:48,045 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Examining Stored Cloud Data on a PC' [ID: 452]\n",
      "2025-07-06 00:46:48,045 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Examining Stored Cloud Data on a PC -> Dropbox' [ID: 453]\n",
      "2025-07-06 00:46:48,046 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Examining Stored Cloud Data on a PC -> Google Drive' [ID: 454]\n",
      "2025-07-06 00:46:48,046 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Examining Stored Cloud Data on a PC -> OneDrive' [ID: 455]\n",
      "2025-07-06 00:46:48,047 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Windows Prefetch Artifacts' [ID: 456]\n",
      "2025-07-06 00:46:48,047 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Tools for Cloud Forensics' [ID: 457]\n",
      "2025-07-06 00:46:48,048 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Tools for Cloud Forensics -> Forensic Open-Stack Tools' [ID: 458]\n",
      "2025-07-06 00:46:48,048 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Tools for Cloud Forensics -> F-Response for the Cloud' [ID: 459]\n",
      "2025-07-06 00:46:48,048 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Tools for Cloud Forensics -> Magnet AXIOM Cloud' [ID: 460]\n",
      "2025-07-06 00:46:48,049 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:48,049 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:48,050 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,050 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Legal Challenges in Cloud Forensics -> Accessing Evidence in the Cloud -> Search Warrants' [ID: 437]\n",
      "2025-07-06 00:46:48,050 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:48,052 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:48,053 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 14. Report Writing for High-Tech Investigations' [ID: 826]\n",
      "2025-07-06 00:46:48,053 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Understanding the Importance of Reports' [ID: 469]\n",
      "2025-07-06 00:46:48,054 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Understanding the Importance of Reports -> Limiting a Report to Specifics' [ID: 470]\n",
      "2025-07-06 00:46:48,054 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Understanding the Importance of Reports -> Types of Reports' [ID: 471]\n",
      "2025-07-06 00:46:48,055 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports' [ID: 472]\n",
      "2025-07-06 00:46:48,056 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> What to Include in Written Preliminary Reports' [ID: 473]\n",
      "2025-07-06 00:46:48,057 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Report Structure' [ID: 474]\n",
      "2025-07-06 00:46:48,057 - INFO - Context updated to: 'Acknowledgments' [ID: 6]\n",
      "2025-07-06 00:46:48,058 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Writing Reports Clearly' [ID: 475]\n",
      "2025-07-06 00:46:48,059 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Writing Reports Clearly -> Considering Writing Style' [ID: 476]\n",
      "2025-07-06 00:46:48,059 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Writing Reports Clearly -> Including Signposts' [ID: 477]\n",
      "2025-07-06 00:46:48,059 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports' [ID: 478]\n",
      "2025-07-06 00:46:48,061 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Providing Supporting Material' [ID: 479]\n",
      "2025-07-06 00:46:48,061 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Formatting Consistently' [ID: 480]\n",
      "2025-07-06 00:46:48,061 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Explaining Examination and Data Collection Methods' [ID: 481]\n",
      "2025-07-06 00:46:48,062 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Including Calculations' [ID: 482]\n",
      "2025-07-06 00:46:48,062 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Providing for Uncertainty and Error Analysis' [ID: 483]\n",
      "2025-07-06 00:46:48,063 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Explaining Results and Conclusions' [ID: 484]\n",
      "2025-07-06 00:46:48,063 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Providing References' [ID: 485]\n",
      "2025-07-06 00:46:48,064 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Guidelines for Writing Reports -> Designing the Layout and Presentation of Reports -> Including Appendixes' [ID: 486]\n",
      "2025-07-06 00:46:48,064 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Generating Report Findings with Forensics Software Tools' [ID: 487]\n",
      "2025-07-06 00:46:48,064 - INFO - Context updated to: 'Chapter 14. Report Writing for High-Tech Investigations -> Generating Report Findings with Forensics Software Tools -> Using Autopsy to Generate Reports' [ID: 488]\n",
      "2025-07-06 00:46:48,065 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:48,066 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:48,067 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,068 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:48,069 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:48,070 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations' [ID: 843]\n",
      "2025-07-06 00:46:48,070 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for Testimony' [ID: 497]\n",
      "2025-07-06 00:46:48,070 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for Testimony -> Documenting and Preparing Evidence' [ID: 498]\n",
      "2025-07-06 00:46:48,071 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for Testimony -> Reviewing Your Role as a Consulting Expert or an Expert Witness' [ID: 499]\n",
      "2025-07-06 00:46:48,072 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for Testimony -> Creating and Maintaining Your CV' [ID: 500]\n",
      "2025-07-06 00:46:48,072 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for Testimony -> Preparing Technical Definitions' [ID: 501]\n",
      "2025-07-06 00:46:48,072 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for Testimony -> Preparing to Deal with the News Media' [ID: 502]\n",
      "2025-07-06 00:46:48,073 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court' [ID: 503]\n",
      "2025-07-06 00:46:48,074 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> Understanding the Trial Process' [ID: 504]\n",
      "2025-07-06 00:46:48,075 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> Providing Qualifications for Your Testimony' [ID: 505]\n",
      "2025-07-06 00:46:48,075 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> General Guidelines on Testifying' [ID: 506]\n",
      "2025-07-06 00:46:48,076 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> General Guidelines on Testifying -> Using Graphics During Testimony' [ID: 507]\n",
      "2025-07-06 00:46:48,076 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> General Guidelines on Testifying -> Avoiding Testimony Problems' [ID: 508]\n",
      "2025-07-06 00:46:48,077 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> General Guidelines on Testifying -> Understanding Prosecutorial Misconduct' [ID: 509]\n",
      "2025-07-06 00:46:48,078 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> Testifying During Direct Examination' [ID: 510]\n",
      "2025-07-06 00:46:48,079 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Testifying in Court -> Testifying During Cross-Examination' [ID: 511]\n",
      "2025-07-06 00:46:48,080 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for a Deposition or Hearing' [ID: 512]\n",
      "2025-07-06 00:46:48,080 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for a Deposition or Hearing -> Guidelines for Testifying at Depositions' [ID: 513]\n",
      "2025-07-06 00:46:48,081 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for a Deposition or Hearing -> Guidelines for Testifying at Depositions -> Recognizing Deposition Problems' [ID: 514]\n",
      "2025-07-06 00:46:48,081 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing for a Deposition or Hearing -> Guidelines for Testifying at Hearings' [ID: 515]\n",
      "2025-07-06 00:46:48,082 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing Forensics Evidence for Testimony' [ID: 516]\n",
      "2025-07-06 00:46:48,083 - INFO - Context updated to: 'Chapter 15. Expert Testimony in Digital Investigations -> Preparing Forensics Evidence for Testimony -> Preparing a Defense of Your Evidence-Collection Methods' [ID: 517]\n",
      "2025-07-06 00:46:48,083 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:48,084 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:48,084 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,085 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:48,087 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:48,088 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness' [ID: 862]\n",
      "2025-07-06 00:46:48,088 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Applying Ethics and Codes to Expert Witnesses' [ID: 526]\n",
      "2025-07-06 00:46:48,089 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Applying Ethics and Codes to Expert Witnesses -> Forensics Examiners’ Roles in Testifying' [ID: 527]\n",
      "2025-07-06 00:46:48,089 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Applying Ethics and Codes to Expert Witnesses -> Considerations in Disqualification' [ID: 528]\n",
      "2025-07-06 00:46:48,090 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Applying Ethics and Codes to Expert Witnesses -> Traps for Unwary Experts' [ID: 529]\n",
      "2025-07-06 00:46:48,090 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Applying Ethics and Codes to Expert Witnesses -> Determining Admissibility of Evidence' [ID: 530]\n",
      "2025-07-06 00:46:48,090 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics' [ID: 531]\n",
      "2025-07-06 00:46:48,091 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics -> International Society of Forensic Computer Examiners' [ID: 532]\n",
      "2025-07-06 00:46:48,092 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics -> International High Technology Crime Investigation Association' [ID: 533]\n",
      "2025-07-06 00:46:48,092 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics -> International Association of Computer Investigative Specialists' [ID: 534]\n",
      "2025-07-06 00:46:48,092 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics -> American Bar Association' [ID: 535]\n",
      "2025-07-06 00:46:48,093 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Organizations with Codes of Ethics -> American Psychological Association' [ID: 536]\n",
      "2025-07-06 00:46:48,093 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Ethical Difficulties in Expert Testimony' [ID: 537]\n",
      "2025-07-06 00:46:48,094 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Ethical Difficulties in Expert Testimony -> Ethical Responsibilities Owed to You' [ID: 538]\n",
      "2025-07-06 00:46:48,094 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Ethical Difficulties in Expert Testimony -> Standard Forensics Tools and Tools You Create' [ID: 539]\n",
      "2025-07-06 00:46:48,095 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise' [ID: 540]\n",
      "2025-07-06 00:46:48,095 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Performing a Cursory Exam of a Forensic Image' [ID: 541]\n",
      "2025-07-06 00:46:48,095 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Performing a Detailed Exam of a Forensic Image' [ID: 542]\n",
      "2025-07-06 00:46:48,096 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Performing the Exam' [ID: 543]\n",
      "2025-07-06 00:46:48,096 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Performing the Exam -> Preparing for an Examination' [ID: 544]\n",
      "2025-07-06 00:46:48,097 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Interpreting Attribute 0x80 Data Runs' [ID: 545]\n",
      "2025-07-06 00:46:48,097 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Interpreting Attribute 0x80 Data Runs -> Finding Attribute 0x80 an MFT Record' [ID: 546]\n",
      "2025-07-06 00:46:48,098 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Interpreting Attribute 0x80 Data Runs -> Configuring Data Interpreter Options in WinHex' [ID: 547]\n",
      "2025-07-06 00:46:48,098 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Interpreting Attribute 0x80 Data Runs -> Calculating Data Runs' [ID: 548]\n",
      "2025-07-06 00:46:48,098 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> An Ethics Exercise -> Carving Data Run Clusters Manually' [ID: 549]\n",
      "2025-07-06 00:46:48,099 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Chapter Summary' [ID: 551]\n",
      "2025-07-06 00:46:48,099 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Key Terms' [ID: 552]\n",
      "2025-07-06 00:46:48,100 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,100 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,101 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Hands-On Projects' [ID: 554]\n",
      "2025-07-06 00:46:48,102 - INFO - Context updated to: 'Chapter 16. Ethics for the Expert Witness -> Chapter Review -> Case Projects' [ID: 555]\n",
      "2025-07-06 00:46:48,103 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations' [ID: 556]\n",
      "2025-07-06 00:46:48,105 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Introduction' [ID: 557]\n",
      "2025-07-06 00:46:48,106 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Hardware Requirements' [ID: 87]\n",
      "2025-07-06 00:46:48,106 - INFO - Context updated to: 'Chapter 2. The Investigator’s Office and Laboratory -> Building a Business Case for Developing a Forensics Lab -> Preparing a Business Case for a Digital Forensics Lab -> Software Requirements' [ID: 88]\n",
      "2025-07-06 00:46:48,107 - INFO - Context updated to: 'Acknowledgments' [ID: 6]\n",
      "2025-07-06 00:46:48,108 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 1. Understanding the Digital Forensics Profession and Investigations' [ID: 558]\n",
      "2025-07-06 00:46:48,109 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Lab 1.1. Installing Autopsy for Windows' [ID: 560]\n",
      "2025-07-06 00:46:48,109 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,110 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,110 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,111 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,111 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Lab 1.2. Downloading FTK Imager Lite' [ID: 565]\n",
      "2025-07-06 00:46:48,111 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,112 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,112 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,113 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,114 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Lab 1.3. Downloading WinHex' [ID: 570]\n",
      "2025-07-06 00:46:48,114 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,115 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,115 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,115 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,116 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 1. Understanding the Digital Forensics Profession and Investigations -> Lab 1.4. Using Autopsy for Windows' [ID: 575]\n",
      "2025-07-06 00:46:48,117 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,117 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,118 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,118 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,119 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory' [ID: 580]\n",
      "2025-07-06 00:46:48,119 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory -> Lab 2.1. Wiping a USB Drive Securely' [ID: 582]\n",
      "2025-07-06 00:46:48,119 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,120 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,121 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,121 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,122 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory -> Lab 2.2. Using Directory Snoop to Image a USB Drive' [ID: 587]\n",
      "2025-07-06 00:46:48,122 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,122 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,123 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,123 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,124 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory -> Lab 2.3. Converting a Raw Image to an .E01 Image' [ID: 592]\n",
      "2025-07-06 00:46:48,125 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,125 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,126 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,126 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,127 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory -> Lab 2.4. Imaging Evidence with FTK Imager Lite' [ID: 597]\n",
      "2025-07-06 00:46:48,127 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,128 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,130 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,131 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,132 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 2. The Investigator’s Office and Laboratory -> Lab 2.5. Viewing Images in FTK Imager Lite' [ID: 602]\n",
      "2025-07-06 00:46:48,132 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,133 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,134 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,134 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,135 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition' [ID: 607]\n",
      "2025-07-06 00:46:48,135 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive' [ID: 609]\n",
      "2025-07-06 00:46:48,135 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,136 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,137 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,137 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive -> Activity -> Creating a DEFT Zero Boot CD' [ID: 613]\n",
      "2025-07-06 00:46:48,138 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive -> Activity -> Creating a Bootable USB DEFT Zero Drive' [ID: 614]\n",
      "2025-07-06 00:46:48,138 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive -> Activity -> Learning DEFT Zero Features' [ID: 615]\n",
      "2025-07-06 00:46:48,139 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,140 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.2. Examining a FAT Image' [ID: 617]\n",
      "2025-07-06 00:46:48,140 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,141 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,141 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,142 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,143 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.3. Examining an NTFS Image' [ID: 622]\n",
      "2025-07-06 00:46:48,143 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,144 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,144 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,145 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,145 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 3. Data Acquisition -> Lab 3.4. Examining an HFS+ Image' [ID: 627]\n",
      "2025-07-06 00:46:48,146 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,146 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,147 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,148 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,148 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes' [ID: 632]\n",
      "2025-07-06 00:46:48,149 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes -> Lab 4.1. Creating a Mini-WinFE Boot CD' [ID: 634]\n",
      "2025-07-06 00:46:48,149 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,150 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,150 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,150 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes -> Lab 4.1. Creating a Mini-WinFE Boot CD -> Activity -> Setting Up Mini-WinFE' [ID: 638]\n",
      "2025-07-06 00:46:48,151 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes -> Lab 4.1. Creating a Mini-WinFE Boot CD -> Activity -> Creating a Mini-WinFE ISO Image' [ID: 639]\n",
      "2025-07-06 00:46:48,153 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,153 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes -> Lab 4.2. Using Mini-WinFE to Boot and Image a Windows Computer' [ID: 641]\n",
      "2025-07-06 00:46:48,154 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,154 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,155 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,156 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,156 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes -> Lab 4.3. Testing the Mini-WinFE Write-Protection Feature' [ID: 646]\n",
      "2025-07-06 00:46:48,156 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,157 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,157 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,158 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,158 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 4. Processing Crime and Incident Scenes -> Lab 4.4. Creating an Image with Guymager' [ID: 651]\n",
      "2025-07-06 00:46:48,158 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,159 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,159 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,161 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,161 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 5. Working with Windows and CLI Systems' [ID: 656]\n",
      "2025-07-06 00:46:48,162 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 5. Working with Windows and CLI Systems -> Lab 5.1. Using DART to Export Windows Registry Files' [ID: 658]\n",
      "2025-07-06 00:46:48,162 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,162 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,163 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,163 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,163 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 5. Working with Windows and CLI Systems -> Lab 5.2. Examining the SAM Hive' [ID: 663]\n",
      "2025-07-06 00:46:48,164 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,164 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,165 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,165 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,166 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 5. Working with Windows and CLI Systems -> Lab 5.3. Examining the SYSTEM Hive' [ID: 668]\n",
      "2025-07-06 00:46:48,168 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,168 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,169 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,170 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,170 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 5. Working with Windows and CLI Systems -> Lab 5.4. Examining the ntuser.dat Registry File' [ID: 673]\n",
      "2025-07-06 00:46:48,171 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,171 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,171 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,172 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,172 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools' [ID: 678]\n",
      "2025-07-06 00:46:48,173 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.1. Using Autopsy 4.7.0 to Search an Image File' [ID: 680]\n",
      "2025-07-06 00:46:48,173 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,173 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,174 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,174 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.1. Using Autopsy 4.7.0 to Search an Image File -> Activity -> Installing Autopsy 4.7.0' [ID: 684]\n",
      "2025-07-06 00:46:48,174 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.1. Using Autopsy 4.7.0 to Search an Image File -> Activity -> Searching E-mail in Autopsy 4.7.0' [ID: 685]\n",
      "2025-07-06 00:46:48,175 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,175 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.2. Using OSForensics to Search an Image of a Hard Drive' [ID: 687]\n",
      "2025-07-06 00:46:48,176 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,176 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,177 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,178 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,179 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex' [ID: 692]\n",
      "2025-07-06 00:46:48,179 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,180 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,180 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,180 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex -> Activity -> Testing an Image File in Autopsy 4.3.0' [ID: 696]\n",
      "2025-07-06 00:46:48,181 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 6. Current Digital Forensics Tools -> Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex -> Activity -> Examining Image Files in WinHex' [ID: 697]\n",
      "2025-07-06 00:46:48,181 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,181 - INFO - Context updated to: 'Chapter 6. Current Digital Forensics Tools -> Evaluating Digital Forensics Tool Needs -> Tasks Performed by Digital Forensics Tools -> Reporting' [ID: 242]\n",
      "2025-07-06 00:46:48,182 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 7. Linux and Macintosh File Systems' [ID: 699]\n",
      "2025-07-06 00:46:48,182 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 7. Linux and Macintosh File Systems -> Lab 7.1. Using Autopsy to Process a Mac OS X Image' [ID: 701]\n",
      "2025-07-06 00:46:48,183 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,184 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,184 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,185 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,185 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 7. Linux and Macintosh File Systems -> Lab 7.2. Using Autopsy to Process a Mac OS 9 Image' [ID: 706]\n",
      "2025-07-06 00:46:48,185 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,186 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,187 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,187 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,188 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 7. Linux and Macintosh File Systems -> Lab 7.3. Using Autopsy to Process a Linux Image' [ID: 711]\n",
      "2025-07-06 00:46:48,188 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,189 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,189 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,190 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,190 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 8. Recovering Graphics Files' [ID: 716]\n",
      "2025-07-06 00:46:48,191 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 8. Recovering Graphics Files -> Lab 8.1. Using Autopsy to Analyze Multimedia Files' [ID: 718]\n",
      "2025-07-06 00:46:48,191 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,192 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,192 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,193 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,193 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 8. Recovering Graphics Files -> Lab 8.2. Using OSForensics to Analyze Multimedia Files' [ID: 723]\n",
      "2025-07-06 00:46:48,194 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,194 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,195 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,195 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,196 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 8. Recovering Graphics Files -> Lab 8.3. Using WinHex to Analyze Multimedia Files' [ID: 728]\n",
      "2025-07-06 00:46:48,196 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,196 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,197 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,197 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,198 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 9. Digital Forensics Analysis and Validation' [ID: 733]\n",
      "2025-07-06 00:46:48,199 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 9. Digital Forensics Analysis and Validation -> Lab 9.1. Using Autopsy to Search for Keywords in an Image' [ID: 735]\n",
      "2025-07-06 00:46:48,199 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,200 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,200 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,200 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,201 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 9. Digital Forensics Analysis and Validation -> Lab 9.2. Validating File Hash Values with FTK Imager Lite' [ID: 740]\n",
      "2025-07-06 00:46:48,201 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,201 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,202 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,203 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,203 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 9. Digital Forensics Analysis and Validation -> Lab 9.3. Validating File Hash Values with WinHex' [ID: 745]\n",
      "2025-07-06 00:46:48,204 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,204 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,204 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,205 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,206 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics' [ID: 750]\n",
      "2025-07-06 00:46:48,207 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine' [ID: 752]\n",
      "2025-07-06 00:46:48,207 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,207 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,208 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,209 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine -> Activity -> Installing MD5 Hashes in Autopsy' [ID: 756]\n",
      "2025-07-06 00:46:48,209 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine -> Activity -> Analyzing a Windows Image Containing a Virtual Machine' [ID: 757]\n",
      "2025-07-06 00:46:48,210 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,211 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.2. Conducting a Live Acquisition' [ID: 759]\n",
      "2025-07-06 00:46:48,212 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,212 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,213 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,214 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.2. Conducting a Live Acquisition -> Activity -> Installing Tools for Live Acquisitions' [ID: 763]\n",
      "2025-07-06 00:46:48,214 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.2. Conducting a Live Acquisition -> Activity -> Exploring Tools for Live Acquisitions' [ID: 764]\n",
      "2025-07-06 00:46:48,215 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.2. Conducting a Live Acquisition -> Activity -> Capturing Data in a Live Acquisition' [ID: 765]\n",
      "2025-07-06 00:46:48,216 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,216 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.3. Using Kali Linux for Network Forensics' [ID: 767]\n",
      "2025-07-06 00:46:48,217 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,217 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,218 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,219 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.3. Using Kali Linux for Network Forensics -> Activity -> Installing Kali Linux' [ID: 771]\n",
      "2025-07-06 00:46:48,219 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.3. Using Kali Linux for Network Forensics -> Activity -> Mounting Drives in Kali Linux' [ID: 772]\n",
      "2025-07-06 00:46:48,220 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics -> Lab 10.3. Using Kali Linux for Network Forensics -> Activity -> Identifying Open Ports and Making a Screen Capture' [ID: 773]\n",
      "2025-07-06 00:46:48,221 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,222 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 11. E-mail and Social Media Investigations' [ID: 775]\n",
      "2025-07-06 00:46:48,224 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 11. E-mail and Social Media Investigations -> Lab 11.1. Using OSForensics to Search for E-mails and Mailboxes' [ID: 777]\n",
      "2025-07-06 00:46:48,224 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,225 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,225 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,226 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,227 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 11. E-mail and Social Media Investigations -> Lab 11.2. Using Autopsy to Search for E-mails and Mailboxes' [ID: 782]\n",
      "2025-07-06 00:46:48,227 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,227 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,228 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,228 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,229 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 11. E-mail and Social Media Investigations -> Lab 11.3. Finding Google Searches and Multiple E-mail Accounts' [ID: 787]\n",
      "2025-07-06 00:46:48,229 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,230 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,231 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,231 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,232 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 12. Mobile Device Forensics' [ID: 792]\n",
      "2025-07-06 00:46:48,232 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 12. Mobile Device Forensics -> Lab 12.1. Examining Cell Phone Storage Devices' [ID: 794]\n",
      "2025-07-06 00:46:48,233 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,233 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,234 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,234 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,235 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 12. Mobile Device Forensics -> Lab 12.2. Using FTK Imager Lite to View Text Messages, Phone Numbers, and Photos' [ID: 799]\n",
      "2025-07-06 00:46:48,235 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,236 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,236 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,237 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,238 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 12. Mobile Device Forensics -> Lab 12.3. Using Autopsy to Search Cloud Backups of Mobile Devices' [ID: 804]\n",
      "2025-07-06 00:46:48,238 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,238 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,239 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,240 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,240 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 13. Cloud Forensics' [ID: 809]\n",
      "2025-07-06 00:46:48,240 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 13. Cloud Forensics -> Lab 13.1. Examining Dropbox Cloud Storage' [ID: 811]\n",
      "2025-07-06 00:46:48,241 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,241 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,241 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,242 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,242 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 13. Cloud Forensics -> Lab 13.2. Examining Google Drive Cloud Storage' [ID: 816]\n",
      "2025-07-06 00:46:48,243 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,244 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,245 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,246 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,247 - INFO - Context updated to: 'Chapter 13. Cloud Forensics -> Conducting a Cloud Investigation -> Examining Stored Cloud Data on a PC -> Google Drive' [ID: 454]\n",
      "2025-07-06 00:46:48,247 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 13. Cloud Forensics -> Lab 13.3. Examining OneDrive Cloud Storage' [ID: 821]\n",
      "2025-07-06 00:46:48,248 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,249 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,249 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,250 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,251 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 14. Report Writing for High-Tech Investigations' [ID: 826]\n",
      "2025-07-06 00:46:48,251 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 14. Report Writing for High-Tech Investigations -> Lab 14.1. Investigating Corporate Espionage' [ID: 828]\n",
      "2025-07-06 00:46:48,252 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,252 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,253 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,253 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,254 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 14. Report Writing for High-Tech Investigations -> Lab 14.2. Adding Evidence to a Case' [ID: 833]\n",
      "2025-07-06 00:46:48,254 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,254 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,255 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,255 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,256 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 14. Report Writing for High-Tech Investigations -> Lab 14.3. Preparing a Report' [ID: 838]\n",
      "2025-07-06 00:46:48,256 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,257 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,257 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,258 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,258 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations' [ID: 843]\n",
      "2025-07-06 00:46:48,259 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations -> Lab 15.3. Recovering a Password from Password-Protected Files -> Activity -> Recovering a Password from Password-Protected Files' [ID: 860]\n",
      "2025-07-06 00:46:48,260 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations -> Lab 15.1. Conducting a Preliminary Investigation' [ID: 845]\n",
      "2025-07-06 00:46:48,260 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,260 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,261 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,261 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,262 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations -> Lab 15.2. Investigating an Arsonist' [ID: 850]\n",
      "2025-07-06 00:46:48,262 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,262 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,263 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,263 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,266 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations -> Lab 15.3. Recovering a Password from Password-Protected Files' [ID: 855]\n",
      "2025-07-06 00:46:48,266 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,266 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,267 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,267 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations -> Lab 15.3. Recovering a Password from Password-Protected Files -> Activity -> Verifying the Existence of a Warning Banner' [ID: 859]\n",
      "2025-07-06 00:46:48,268 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 15. Expert Testimony in Digital Investigations -> Lab 15.3. Recovering a Password from Password-Protected Files -> Activity -> Recovering a Password from Password-Protected Files' [ID: 860]\n",
      "2025-07-06 00:46:48,269 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,269 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness' [ID: 862]\n",
      "2025-07-06 00:46:48,270 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image' [ID: 864]\n",
      "2025-07-06 00:46:48,270 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives' [ID: 865]\n",
      "2025-07-06 00:46:48,271 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Objectives -> Materials Required' [ID: 866]\n",
      "2025-07-06 00:46:48,271 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity' [ID: 867]\n",
      "2025-07-06 00:46:48,272 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity -> Creating a Duplicate Forensic Image' [ID: 868]\n",
      "2025-07-06 00:46:48,272 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity -> Determining the Offset Byte Address of the Corrupt MFT Record' [ID: 869]\n",
      "2025-07-06 00:46:48,272 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity -> Copying the Corrected MFT Record' [ID: 870]\n",
      "2025-07-06 00:46:48,273 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Activity -> Extracting Additional Evidence' [ID: 871]\n",
      "2025-07-06 00:46:48,273 - INFO - Context updated to: 'Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 16. Ethics for the Expert Witness -> Lab 16.1. Rebuilding an MFT Record from a Corrupt Image -> Review Questions' [ID: 872]\n",
      "2025-07-06 00:46:48,274 - INFO - Context updated to: 'Appendix A. Certification Test References' [ID: 873]\n",
      "2025-07-06 00:46:48,274 - INFO - Context updated to: 'Appendix B. Digital Forensics References' [ID: 874]\n",
      "2025-07-06 00:46:48,276 - INFO - Context updated to: 'Appendix C. Digital Forensics Lab Considerations' [ID: 875]\n",
      "2025-07-06 00:46:48,278 - INFO - Context updated to: 'Appendix D. Legacy File System and Forensics Tools' [ID: 876]\n",
      "2025-07-06 00:46:48,278 - INFO - Sanitizing metadata for 11483 total documents...\n",
      "2025-07-06 00:46:48,304 - INFO - Total documents prepared for chunking: 11483\n",
      "2025-07-06 00:46:48,501 - INFO - Split into 11774 final chunks.\n",
      "2025-07-06 00:46:48,502 - INFO - Assigning sequential chunk_id to all final chunks...\n",
      "2025-07-06 00:46:48,504 - INFO - Assigned chunk_ids from 0 to 11773.\n",
      "2025-07-06 00:46:48,509 - WARNING - Deleting existing ChromaDB directory: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "2025-07-06 00:46:48,510 - INFO - Initializing embedding model 'nomic-embed-text' and creating new vector database...\n",
      "2025-07-06 00:46:48,544 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-07-06 00:47:57,099 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:10,650 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:26,381 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:27,026 - INFO - Vector DB created successfully at: /home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub\n",
      "2025-07-06 00:49:27,027 - INFO - Collection 'book_toc_guided_chunks_epub_v2' contains 11774 documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: Create Hierarchical Vector Database (V6 - Final Merged Logic)\n",
    "#\n",
    "# This definitive version combines the clear structure of the original code with\n",
    "# the robust, normalized title-matching logic from the final version.\n",
    "# This ensures maximum accuracy in assigning content to the correct ToC entry.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import re  # <-- Added for normalization\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# --- LangChain and Document Loading Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS (WITH NEW NORMALIZATION FUNCTION)\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_title_for_matching(title: str) -> str:\n",
    "    \"\"\"A more aggressive normalization function for matching titles.\"\"\"\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    # Lowercase, remove all punctuation (except spaces), and collapse whitespace\n",
    "    normalized = title.lower()\n",
    "    normalized = re.sub(r'[^\\w\\s]', '', normalized)\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "    return normalized\n",
    "\n",
    "def clean_metadata_for_chroma(value: Any) -> Any:\n",
    "    \"\"\"Sanitizes metadata values to be compatible with ChromaDB using robust JSON serialization.\"\"\"\n",
    "    if isinstance(value, (list, dict)):\n",
    "        return json.dumps(value)\n",
    "    if isinstance(value, (str, int, float, bool)) or value is None:\n",
    "        return value\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CORE ORCHESTRATION FUNCTION (WITH MERGED LOGIC)\n",
    "# ==============================================================================\n",
    "\n",
    "def process_book_with_extracted_toc(\n",
    "    book_path: str,\n",
    "    extracted_toc_json_path: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    \n",
    "    logger.info(f\"Processing book '{os.path.basename(book_path)}' using ToC from '{os.path.basename(extracted_toc_json_path)}'.\")\n",
    "\n",
    "    # 1. Load ToC (Unchanged)\n",
    "    try:\n",
    "        with open(extracted_toc_json_path, 'r', encoding='utf-8') as f:\n",
    "            hierarchical_toc = json.load(f)\n",
    "        if not hierarchical_toc:\n",
    "            logger.error(f\"Pre-extracted ToC is empty.\")\n",
    "            return [], []\n",
    "        logger.info(f\"Successfully loaded pre-extracted ToC.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading ToC JSON: {e}\", exc_info=True)\n",
    "        return [], []\n",
    "\n",
    "    # 2. Load all book content (Unchanged)\n",
    "    all_raw_book_docs: List[Document] = []\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "    if file_extension == \".epub\":\n",
    "        loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try: all_raw_book_docs = loader.load()\n",
    "        except Exception as e: logger.error(f\"Error loading EPUB: {e}\", exc_info=True); return [], hierarchical_toc\n",
    "    elif file_extension == \".pdf\":\n",
    "        loader = PyPDFLoader(book_path)\n",
    "        try: all_raw_book_docs = loader.load()\n",
    "        except Exception as e: logger.error(f\"Error loading PDF: {e}\", exc_info=True); return [], hierarchical_toc\n",
    "    else:\n",
    "        logger.error(f\"Unsupported format: {file_extension}\"); return [], hierarchical_toc\n",
    "    logger.info(f\"Loaded {len(all_raw_book_docs)} raw documents from source.\")\n",
    "    if not all_raw_book_docs: return [], hierarchical_toc\n",
    "\n",
    "    # 3. Flatten ToC for processing (Unchanged)\n",
    "    flat_toc_entries: List[Dict[str, Any]] = []\n",
    "    def _add_ids_and_flatten_recursive(nodes, path, counter):\n",
    "        for node in nodes:\n",
    "            entry = { \"titles_path\": path + [node.get(\"title\", \"\")], \"level\": node.get(\"level\"), \"full_title_for_matching\": node.get(\"title\", \"\"), \"toc_id\": counter[0] }\n",
    "            if \"page\" in node: entry[\"page\"] = node[\"page\"]\n",
    "            flat_toc_entries.append(entry); counter[0] += 1\n",
    "            if node.get(\"children\"): _add_ids_and_flatten_recursive(node.get(\"children\", []), entry[\"titles_path\"], counter)\n",
    "    _add_ids_and_flatten_recursive(hierarchical_toc, [], [0])\n",
    "    logger.info(f\"Flattened ToC into {len(flat_toc_entries)} entries.\")\n",
    "\n",
    "    # 4. Create enriched LangChain Documents\n",
    "    final_documents_with_metadata: List[Document] = []\n",
    "    \n",
    "    # --- PDF LOGIC (Unchanged and Correct) ---\n",
    "    if file_extension == \".pdf\" and any(\"page\" in entry for entry in flat_toc_entries):\n",
    "        logger.info(\"Assigning metadata to PDF pages...\")\n",
    "        flat_toc_entries.sort(key=lambda x: x.get(\"page\", -1) if x.get(\"page\") is not None else -1)\n",
    "        for page_doc in all_raw_book_docs:\n",
    "            page_num_1_indexed = page_doc.metadata.get(\"page\", -1) + 1\n",
    "            assigned_metadata = {\"source\": os.path.basename(book_path), \"page_number\": page_num_1_indexed}\n",
    "            best_match_toc_entry = None\n",
    "            for toc_entry in flat_toc_entries:\n",
    "                if toc_entry.get(\"page\", -1) <= page_num_1_indexed:\n",
    "                    best_match_toc_entry = toc_entry\n",
    "                else:\n",
    "                    break\n",
    "            if best_match_toc_entry:\n",
    "                assigned_metadata['titles_path'] = best_match_toc_entry.get(\"titles_path\", [])\n",
    "                assigned_metadata['toc_id'] = best_match_toc_entry.get('toc_id')\n",
    "            else:\n",
    "                assigned_metadata[\"titles_path\"] = [\"Uncategorized PDF Page\"]\n",
    "            final_documents_with_metadata.append(Document(page_content=page_doc.page_content, metadata=assigned_metadata))\n",
    "\n",
    "    # --- EPUB LOGIC (MERGED AND CORRECTED) ---\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Assigning metadata to EPUB elements using robust normalized matching...\")\n",
    "        \n",
    "        # Create a lookup dictionary with normalized keys for robust matching.\n",
    "        normalized_toc_lookup = {\n",
    "            normalize_title_for_matching(entry['full_title_for_matching']): entry \n",
    "            for entry in flat_toc_entries if entry.get('full_title_for_matching')\n",
    "        }\n",
    "        \n",
    "        # Initialize a state machine for the current section metadata.\n",
    "        current_hierarchy_metadata = {\n",
    "            \"source\": os.path.basename(book_path),\n",
    "            \"titles_path\": [\"EPUB Preamble\"],\n",
    "            \"toc_id\": -1\n",
    "        }\n",
    "        \n",
    "        for element_doc in all_raw_book_docs:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            if not element_text: continue\n",
    "            \n",
    "            # Normalize the text from the EPUB element for comparison.\n",
    "            normalized_element_text = normalize_title_for_matching(element_text)\n",
    "            \n",
    "            # If the normalized text is a key in our lookup, it's a heading. Update the state.\n",
    "            if normalized_element_text in normalized_toc_lookup:\n",
    "                toc_entry = normalized_toc_lookup[normalized_element_text]\n",
    "                # Update the state to this new section.\n",
    "                current_hierarchy_metadata = {\n",
    "                    \"source\": os.path.basename(book_path),\n",
    "                    \"titles_path\": toc_entry.get(\"titles_path\", []),\n",
    "                    \"toc_id\": toc_entry.get('toc_id')\n",
    "                }\n",
    "                logger.info(f\"Context updated to: '{' -> '.join(current_hierarchy_metadata['titles_path'])}' [ID: {current_hierarchy_metadata['toc_id']}]\")\n",
    "\n",
    "            # Assign a copy of the *current* state to the document.\n",
    "            doc_metadata_to_assign = current_hierarchy_metadata.copy()\n",
    "            final_documents_with_metadata.append(Document(page_content=element_text, metadata=doc_metadata_to_assign))\n",
    "    \n",
    "    # --- Finalize and Chunk ---\n",
    "    logger.info(f\"Sanitizing metadata for {len(final_documents_with_metadata)} total documents...\")\n",
    "    for doc in final_documents_with_metadata:\n",
    "        doc.metadata = {k: clean_metadata_for_chroma(v) for k, v in doc.metadata.items()}\n",
    "        \n",
    "    logger.info(f\"Total documents prepared for chunking: {len(final_documents_with_metadata)}\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
    "    final_chunks = text_splitter.split_documents(final_documents_with_metadata)\n",
    "    logger.info(f\"Split into {len(final_chunks)} final chunks.\")\n",
    "    \n",
    "    logger.info(\"Assigning sequential chunk_id to all final chunks...\")\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        chunk.metadata['chunk_id'] = i\n",
    "    logger.info(f\"Assigned chunk_ids from 0 to {len(final_chunks) - 1}.\")\n",
    "\n",
    "    return final_chunks, hierarchical_toc\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK FOR THIS CELL\n",
    "# ==============================================================================\n",
    "\n",
    "if not os.path.exists(PRE_EXTRACTED_TOC_JSON_PATH):\n",
    "    logger.error(f\"CRITICAL: Pre-extracted ToC file not found at '{PRE_EXTRACTED_TOC_JSON_PATH}'.\")\n",
    "    logger.error(\"Please run the 'Extract Book Table of Contents (ToC)' cell (Cell 4) first.\")\n",
    "else:\n",
    "    # --- 1. Process Book using the Merged and Corrected Function ---\n",
    "    final_chunks_for_db, toc_reloaded = process_book_with_extracted_toc(\n",
    "        book_path=BOOK_PATH,\n",
    "        extracted_toc_json_path=PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if final_chunks_for_db:\n",
    "        # --- 2. Create Vector Database ---\n",
    "        if os.path.exists(CHROMA_PERSIST_DIR):\n",
    "            logger.warning(f\"Deleting existing ChromaDB directory: {CHROMA_PERSIST_DIR}\")\n",
    "            shutil.rmtree(CHROMA_PERSIST_DIR)\n",
    "\n",
    "        logger.info(f\"Initializing embedding model '{EMBEDDING_MODEL_OLLAMA}' and creating new vector database...\")\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        \n",
    "        vector_db = Chroma.from_documents(\n",
    "            documents=final_chunks_for_db,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        # Verify creation\n",
    "        reloaded_db = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding_model, collection_name=CHROMA_COLLECTION_NAME)\n",
    "        count = reloaded_db._collection.count()\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        logger.info(f\"Vector DB created successfully at: {CHROMA_PERSIST_DIR}\")\n",
    "        logger.info(f\"Collection '{CHROMA_COLLECTION_NAME}' contains {count} documents.\")\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        logger.error(\"Failed to generate chunks. Vector DB not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d38d",
   "metadata": {},
   "source": [
    "### Full Database Health & Hierarchy Diagnostic Report  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9902b060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:57:31,994 - INFO - Connecting to vector DB to retrieve all chunk metadata...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "            Ground-Truth Database Health & Hierarchy Report (v15.3)             \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:57:32,310 - INFO - Successfully retrieved metadata for all 11774 chunks.\n",
      "2025-07-06 00:57:32,311 - INFO - Building hierarchy from ground-truth metadata...\n",
      "2025-07-06 00:57:32,325 - INFO - Calculating total chunks for each branch...\n",
      "2025-07-06 00:57:32,327 - WARNING - Found 21 chunks that were unmapped or in the preamble.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "             Reconstructed Hierarchy Report (from DB Ground Truth)              \n",
      "================================================================================\n",
      "|-- About the Authors [ID: 5] (Total: 5, Direct: 5)\n",
      "|-- Acknowledgments [ID: 6] (Total: 20, Direct: 20)\n",
      "|-- Appendix A. Certification Test References [ID: 873] (Total: 56, Direct: 56)\n",
      "|-- Appendix B. Digital Forensics References [ID: 874] (Total: 109, Direct: 109)\n",
      "|-- Appendix C. Digital Forensics Lab Considerations [ID: 875] (Total: 58, Direct: 58)\n",
      "|-- Appendix D. Legacy File System and Forensics Tools [ID: 876] (Total: 59, Direct: 59)\n",
      "|-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: 9] (Total: 521, Direct: 0)\n",
      "|   |-- An Overview of Digital Forensics [ID: 9] (Total: 60, Direct: 18)\n",
      "|   |   |-- A Brief History of Digital Forensics [ID: 11] (Total: 13, Direct: 13)\n",
      "|   |   |-- Developing Digital Forensics Resources [ID: 13] (Total: 8, Direct: 8)\n",
      "|   |   |-- Digital Forensics and Other Related Disciplines [ID: 10] (Total: 18, Direct: 18)\n",
      "|   |   |-- Understanding Case Law [ID: 12] (Total: 3, Direct: 3)\n",
      "|   |-- Conducting an Investigation [ID: 40] (Total: 109, Direct: 8)\n",
      "|   |   |-- Analyzing Your Digital Evidence [ID: 44] (Total: 48, Direct: 44)\n",
      "|   |   |   |-- Some Additional Features of Autopsy [ID: 45] (Total: 4, Direct: 4)\n",
      "|   |   |-- Completing the Case [ID: 46] (Total: 22, Direct: 12)\n",
      "|   |   |   |-- Autopsy’s Report Generator [ID: 47] (Total: 10, Direct: 10)\n",
      "|   |   |-- Critiquing the Case [ID: 48] (Total: 9, Direct: 9)\n",
      "|   |   |-- Gathering the Evidence [ID: 41] (Total: 14, Direct: 14)\n",
      "|   |   |-- Understanding Bit-stream Copies [ID: 42] (Total: 8, Direct: 6)\n",
      "|   |   |   |-- Acquiring an Image of Evidence Media [ID: 43] (Total: 2, Direct: 2)\n",
      "|   |-- Maintaining Professional Conduct [ID: 23] (Total: 10, Direct: 10)\n",
      "|   |-- Preparing a Digital Forensics Investigation [ID: 24] (Total: 97, Direct: 4)\n",
      "|   |   |-- An Overview of a Company Policy Violation [ID: 26] (Total: 4, Direct: 4)\n",
      "|   |   |-- An Overview of a Computer Crime [ID: 25] (Total: 12, Direct: 12)\n",
      "|   |   |-- Taking a Systematic Approach [ID: 27] (Total: 77, Direct: 16)\n",
      "|   |   |   |-- Assessing the Case [ID: 28] (Total: 11, Direct: 11)\n",
      "|   |   |   |-- Planning Your Investigation [ID: 29] (Total: 41, Direct: 41)\n",
      "|   |   |   |-- Securing Your Evidence [ID: 30] (Total: 9, Direct: 9)\n",
      "|   |-- Preparing for Digital Investigations [ID: 14] (Total: 84, Direct: 5)\n",
      "|   |   |-- Following Legal Processes [ID: 16] (Total: 13, Direct: 13)\n",
      "|   |   |-- Understanding Law Enforcement Agency Investigations [ID: 15] (Total: 10, Direct: 10)\n",
      "|   |   |-- Understanding Private-Sector Investigations [ID: 17] (Total: 56, Direct: 3)\n",
      "|   |   |   |-- Conducting Security Investigations [ID: 21] (Total: 15, Direct: 15)\n",
      "|   |   |   |-- Designating an Authorized Requester [ID: 20] (Total: 9, Direct: 9)\n",
      "|   |   |   |-- Displaying Warning Banners [ID: 19] (Total: 19, Direct: 19)\n",
      "|   |   |   |-- Distinguishing Personal and Company Property [ID: 22] (Total: 5, Direct: 5)\n",
      "|   |   |   |-- Establishing Company Policies [ID: 18] (Total: 5, Direct: 5)\n",
      "|   |-- Procedures for Private-Sector High-Tech Investigations [ID: 31] (Total: 124, Direct: 2)\n",
      "|   |   |-- Attorney-Client Privilege Investigations [ID: 35] (Total: 33, Direct: 33)\n",
      "|   |   |-- E-mail Abuse Investigations [ID: 34] (Total: 16, Direct: 16)\n",
      "|   |   |-- Employee Termination Cases [ID: 32] (Total: 2, Direct: 2)\n",
      "|   |   |-- Industrial Espionage Investigations [ID: 36] (Total: 52, Direct: 41)\n",
      "|   |   |   |-- Interviews and Interrogations in High-Tech Investigations [ID: 37] (Total: 11, Direct: 11)\n",
      "|   |   |-- Internet Abuse Investigations [ID: 33] (Total: 19, Direct: 19)\n",
      "|   |-- Understanding Data Recovery Workstations and Software [ID: 38] (Total: 37, Direct: 18)\n",
      "|   |   |-- Setting Up Your Workstation for Digital Forensics [ID: 39] (Total: 19, Direct: 19)\n",
      "|-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: 349] (Total: 331, Direct: 0)\n",
      "|   |-- An Overview of Virtual Machine Forensics [ID: 349] (Total: 237, Direct: 7)\n",
      "|   |   |-- Conducting an Investigation with Type 2 Hypervisors [ID: 356] (Total: 103, Direct: 70)\n",
      "|   |   |   |-- Other VM Examination Methods [ID: 357] (Total: 13, Direct: 13)\n",
      "|   |   |   |-- Using VMs as Forensics Tools [ID: 358] (Total: 20, Direct: 20)\n",
      "|   |   |-- Type 2 Hypervisors [ID: 350] (Total: 93, Direct: 67)\n",
      "|   |   |   |-- KVM [ID: 352] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Microsoft Hyper-V [ID: 353] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Parallels Desktop [ID: 351] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- VMware Workstation and Workstation Player [ID: 354] (Total: 14, Direct: 14)\n",
      "|   |   |   |-- VirtualBox [ID: 355] (Total: 6, Direct: 6)\n",
      "|   |   |-- Working with Type 1 Hypervisors [ID: 359] (Total: 34, Direct: 34)\n",
      "|   |-- Network Forensics Overview [ID: 362] (Total: 76, Direct: 4)\n",
      "|   |   |-- Developing Procedures for Network Forensics [ID: 365] (Total: 41, Direct: 8)\n",
      "|   |   |   |-- Reviewing Network Logs [ID: 366] (Total: 11, Direct: 11)\n",
      "|   |   |   |-- Using Network Tools [ID: 367] (Total: 4, Direct: 4)\n",
      "|   |   |   |-- Using Packet Analyzers [ID: 368] (Total: 18, Direct: 18)\n",
      "|   |   |-- Examining the Honeynet Project [ID: 370] (Total: 7, Direct: 7)\n",
      "|   |   |-- Investigating Virtual Networks [ID: 369] (Total: 7, Direct: 7)\n",
      "|   |   |-- Securing a Network [ID: 364] (Total: 13, Direct: 13)\n",
      "|   |   |-- The Need for Established Procedures [ID: 363] (Total: 4, Direct: 4)\n",
      "|   |-- Performing Live Acquisitions [ID: 360] (Total: 18, Direct: 15)\n",
      "|   |   |-- Performing a Live Acquisition in Windows [ID: 361] (Total: 3, Direct: 3)\n",
      "|-- Chapter 11. E-mail and Social Media Investigations [ID: 379] (Total: 282, Direct: 0)\n",
      "|   |-- Applying Digital Forensics Methods to Social Media Communications [ID: 398] (Total: 36, Direct: 23)\n",
      "|   |   |-- Forensics Tools for Social Media Investigations [ID: 399] (Total: 13, Direct: 13)\n",
      "|   |-- Exploring the Role of E-mail in Investigations [ID: 379] (Total: 11, Direct: 11)\n",
      "|   |-- Exploring the Roles of the Client and Server in E-mail [ID: 380] (Total: 10, Direct: 10)\n",
      "|   |-- Investigating E-mail Crimes and Violations [ID: 381] (Total: 101, Direct: 4)\n",
      "|   |   |-- Examining Additional E-mail Files [ID: 387] (Total: 6, Direct: 6)\n",
      "|   |   |-- Examining E-mail Headers [ID: 386] (Total: 14, Direct: 14)\n",
      "|   |   |-- Examining E-mail Messages [ID: 383] (Total: 28, Direct: 8)\n",
      "|   |   |   |-- Copying an E-mail Message [ID: 384] (Total: 20, Direct: 20)\n",
      "|   |   |-- Tracing an E-mail Message [ID: 388] (Total: 7, Direct: 7)\n",
      "|   |   |-- Understanding Forensic Linguistics [ID: 382] (Total: 6, Direct: 6)\n",
      "|   |   |-- Using Network E-mail Logs [ID: 389] (Total: 3, Direct: 3)\n",
      "|   |   |-- Viewing E-mail Headers [ID: 385] (Total: 33, Direct: 33)\n",
      "|   |-- Understanding E-mail Servers [ID: 390] (Total: 33, Direct: 13)\n",
      "|   |   |-- Examining Microsoft E-mail Server Logs [ID: 392] (Total: 8, Direct: 8)\n",
      "|   |   |-- Examining UNIX E-mail Server Logs [ID: 391] (Total: 12, Direct: 12)\n",
      "|   |-- Using Specialized E-mail Forensics Tools [ID: 393] (Total: 91, Direct: 22)\n",
      "|   |   |-- E-mail Case Studies [ID: 397] (Total: 7, Direct: 7)\n",
      "|   |   |-- Recovering Outlook Files [ID: 396] (Total: 7, Direct: 7)\n",
      "|   |   |-- Using Magnet AXIOM to Recover E-mail [ID: 394] (Total: 14, Direct: 14)\n",
      "|   |   |-- Using a Hex Editor to Carve E-mail Messages [ID: 395] (Total: 41, Direct: 41)\n",
      "|-- Chapter 12. Mobile Device Forensics and the Internet of Anything [ID: 406] (Total: 169, Direct: 8)\n",
      "|   |-- Understanding Acquisition Procedures for Mobile Devices [ID: 412] (Total: 75, Direct: 30)\n",
      "|   |   |-- Mobile Forensics Equipment [ID: 413] (Total: 30, Direct: 6)\n",
      "|   |   |   |-- Mobile Phone Forensics Tools and Methods [ID: 415] (Total: 17, Direct: 17)\n",
      "|   |   |   |-- SIM Card Readers [ID: 414] (Total: 7, Direct: 7)\n",
      "|   |   |-- Using Mobile Forensics Tools [ID: 416] (Total: 15, Direct: 15)\n",
      "|   |-- Understanding Forensics in the Internet of Anything [ID: 417] (Total: 21, Direct: 21)\n",
      "|   |-- Understanding Mobile Device Forensics [ID: 408] (Total: 65, Direct: 19)\n",
      "|   |   |-- Inside Mobile Devices [ID: 410] (Total: 22, Direct: 11)\n",
      "|   |   |   |-- SIM Cards [ID: 411] (Total: 11, Direct: 11)\n",
      "|   |   |-- Mobile Phone Basics [ID: 409] (Total: 24, Direct: 24)\n",
      "|-- Chapter 13. Cloud Forensics [ID: 426] (Total: 304, Direct: 0)\n",
      "|   |-- Acquisitions in the Cloud [ID: 446] (Total: 21, Direct: 8)\n",
      "|   |   |-- Encryption in the Cloud [ID: 447] (Total: 13, Direct: 13)\n",
      "|   |-- An Overview of Cloud Computing [ID: 426] (Total: 42, Direct: 2)\n",
      "|   |   |-- Basic Concepts of Cloud Forensics [ID: 430] (Total: 8, Direct: 8)\n",
      "|   |   |-- Cloud Service Levels and Deployment Methods [ID: 428] (Total: 13, Direct: 13)\n",
      "|   |   |-- Cloud Vendors [ID: 429] (Total: 15, Direct: 15)\n",
      "|   |   |-- History of the Cloud [ID: 427] (Total: 4, Direct: 4)\n",
      "|   |-- Conducting a Cloud Investigation [ID: 448] (Total: 105, Direct: 2)\n",
      "|   |   |-- Examining Stored Cloud Data on a PC [ID: 452] (Total: 63, Direct: 5)\n",
      "|   |   |   |-- Dropbox [ID: 453] (Total: 8, Direct: 8)\n",
      "|   |   |   |-- Google Drive [ID: 454] (Total: 21, Direct: 21)\n",
      "|   |   |   |-- OneDrive [ID: 455] (Total: 29, Direct: 29)\n",
      "|   |   |-- Investigating CSPs [ID: 449] (Total: 8, Direct: 8)\n",
      "|   |   |-- Investigating Cloud Customers [ID: 450] (Total: 3, Direct: 3)\n",
      "|   |   |-- Understanding Prefetch Files [ID: 451] (Total: 3, Direct: 3)\n",
      "|   |   |-- Windows Prefetch Artifacts [ID: 456] (Total: 26, Direct: 26)\n",
      "|   |-- Legal Challenges in Cloud Forensics [ID: 431] (Total: 90, Direct: 2)\n",
      "|   |   |-- Accessing Evidence in the Cloud [ID: 436] (Total: 57, Direct: 3)\n",
      "|   |   |   |-- Search Warrants [ID: 437] (Total: 44, Direct: 44)\n",
      "|   |   |   |-- Subpoenas and Court Orders [ID: 438] (Total: 10, Direct: 10)\n",
      "|   |   |-- Jurisdiction Issues [ID: 435] (Total: 6, Direct: 6)\n",
      "|   |   |-- Service Level Agreements [ID: 432] (Total: 25, Direct: 17)\n",
      "|   |   |   |-- CSP Processes and Procedures [ID: 434] (Total: 3, Direct: 3)\n",
      "|   |   |   |-- Policies, Standards, and Guidelines for CSPs [ID: 433] (Total: 5, Direct: 5)\n",
      "|   |-- Technical Challenges in Cloud Forensics [ID: 439] (Total: 33, Direct: 5)\n",
      "|   |   |-- Analysis of Cloud Forensic Data [ID: 441] (Total: 3, Direct: 3)\n",
      "|   |   |-- Anti-Forensics [ID: 442] (Total: 4, Direct: 4)\n",
      "|   |   |-- Architecture [ID: 440] (Total: 4, Direct: 4)\n",
      "|   |   |-- Incident First Responders [ID: 443] (Total: 6, Direct: 6)\n",
      "|   |   |-- Role Management [ID: 444] (Total: 4, Direct: 4)\n",
      "|   |   |-- Standards and Training [ID: 445] (Total: 7, Direct: 7)\n",
      "|   |-- Tools for Cloud Forensics [ID: 457] (Total: 13, Direct: 5)\n",
      "|   |   |-- F-Response for the Cloud [ID: 459] (Total: 2, Direct: 2)\n",
      "|   |   |-- Forensic Open-Stack Tools [ID: 458] (Total: 4, Direct: 4)\n",
      "|   |   |-- Magnet AXIOM Cloud [ID: 460] (Total: 2, Direct: 2)\n",
      "|-- Chapter 14. Report Writing for High-Tech Investigations [ID: 469] (Total: 247, Direct: 0)\n",
      "|   |-- Generating Report Findings with Forensics Software Tools [ID: 487] (Total: 78, Direct: 2)\n",
      "|   |   |-- Using Autopsy to Generate Reports [ID: 488] (Total: 76, Direct: 76)\n",
      "|   |-- Guidelines for Writing Reports [ID: 472] (Total: 138, Direct: 19)\n",
      "|   |   |-- Designing the Layout and Presentation of Reports [ID: 478] (Total: 85, Direct: 44)\n",
      "|   |   |   |-- Explaining Examination and Data Collection Methods [ID: 481] (Total: 3, Direct: 3)\n",
      "|   |   |   |-- Explaining Results and Conclusions [ID: 484] (Total: 3, Direct: 3)\n",
      "|   |   |   |-- Formatting Consistently [ID: 480] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Including Appendixes [ID: 486] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Including Calculations [ID: 482] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Providing References [ID: 485] (Total: 20, Direct: 20)\n",
      "|   |   |   |-- Providing Supporting Material [ID: 479] (Total: 3, Direct: 3)\n",
      "|   |   |   |-- Providing for Uncertainty and Error Analysis [ID: 483] (Total: 2, Direct: 2)\n",
      "|   |   |-- Report Structure [ID: 474] (Total: 8, Direct: 8)\n",
      "|   |   |-- What to Include in Written Preliminary Reports [ID: 473] (Total: 9, Direct: 9)\n",
      "|   |   |-- Writing Reports Clearly [ID: 475] (Total: 17, Direct: 8)\n",
      "|   |   |   |-- Considering Writing Style [ID: 476] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Including Signposts [ID: 477] (Total: 3, Direct: 3)\n",
      "|   |-- Understanding the Importance of Reports [ID: 469] (Total: 31, Direct: 19)\n",
      "|   |   |-- Limiting a Report to Specifics [ID: 470] (Total: 3, Direct: 3)\n",
      "|   |   |-- Types of Reports [ID: 471] (Total: 9, Direct: 9)\n",
      "|-- Chapter 15. Expert Testimony in Digital Investigations [ID: 497] (Total: 315, Direct: 0)\n",
      "|   |-- Preparing Forensics Evidence for Testimony [ID: 516] (Total: 65, Direct: 34)\n",
      "|   |   |-- Preparing a Defense of Your Evidence-Collection Methods [ID: 517] (Total: 31, Direct: 31)\n",
      "|   |-- Preparing for Testimony [ID: 497] (Total: 62, Direct: 15)\n",
      "|   |   |-- Creating and Maintaining Your CV [ID: 500] (Total: 8, Direct: 8)\n",
      "|   |   |-- Documenting and Preparing Evidence [ID: 498] (Total: 12, Direct: 12)\n",
      "|   |   |-- Preparing Technical Definitions [ID: 501] (Total: 12, Direct: 12)\n",
      "|   |   |-- Preparing to Deal with the News Media [ID: 502] (Total: 6, Direct: 6)\n",
      "|   |   |-- Reviewing Your Role as a Consulting Expert or an Expert Witness [ID: 499] (Total: 9, Direct: 9)\n",
      "|   |-- Preparing for a Deposition or Hearing [ID: 512] (Total: 33, Direct: 6)\n",
      "|   |   |-- Guidelines for Testifying at Depositions [ID: 513] (Total: 19, Direct: 10)\n",
      "|   |   |   |-- Recognizing Deposition Problems [ID: 514] (Total: 9, Direct: 9)\n",
      "|   |   |-- Guidelines for Testifying at Hearings [ID: 515] (Total: 8, Direct: 8)\n",
      "|   |-- Testifying in Court [ID: 503] (Total: 155, Direct: 2)\n",
      "|   |   |-- General Guidelines on Testifying [ID: 506] (Total: 47, Direct: 27)\n",
      "|   |   |   |-- Avoiding Testimony Problems [ID: 508] (Total: 7, Direct: 7)\n",
      "|   |   |   |-- Understanding Prosecutorial Misconduct [ID: 509] (Total: 3, Direct: 3)\n",
      "|   |   |   |-- Using Graphics During Testimony [ID: 507] (Total: 10, Direct: 10)\n",
      "|   |   |-- Providing Qualifications for Your Testimony [ID: 505] (Total: 59, Direct: 59)\n",
      "|   |   |-- Testifying During Cross-Examination [ID: 511] (Total: 25, Direct: 25)\n",
      "|   |   |-- Testifying During Direct Examination [ID: 510] (Total: 12, Direct: 12)\n",
      "|   |   |-- Understanding the Trial Process [ID: 504] (Total: 10, Direct: 10)\n",
      "|-- Chapter 16. Ethics for the Expert Witness [ID: 554] (Total: 2530, Direct: 0)\n",
      "|   |-- An Ethics Exercise [ID: 540] (Total: 194, Direct: 4)\n",
      "|   |   |-- Carving Data Run Clusters Manually [ID: 549] (Total: 19, Direct: 19)\n",
      "|   |   |-- Interpreting Attribute 0x80 Data Runs [ID: 545] (Total: 44, Direct: 2)\n",
      "|   |   |   |-- Calculating Data Runs [ID: 548] (Total: 15, Direct: 15)\n",
      "|   |   |   |-- Configuring Data Interpreter Options in WinHex [ID: 547] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Finding Attribute 0x80 an MFT Record [ID: 546] (Total: 21, Direct: 21)\n",
      "|   |   |-- Performing a Cursory Exam of a Forensic Image [ID: 541] (Total: 18, Direct: 18)\n",
      "|   |   |-- Performing a Detailed Exam of a Forensic Image [ID: 542] (Total: 33, Direct: 33)\n",
      "|   |   |-- Performing the Exam [ID: 543] (Total: 76, Direct: 2)\n",
      "|   |   |   |-- Preparing for an Examination [ID: 544] (Total: 74, Direct: 74)\n",
      "|   |-- Applying Ethics and Codes to Expert Witnesses [ID: 526] (Total: 58, Direct: 11)\n",
      "|   |   |-- Considerations in Disqualification [ID: 528] (Total: 22, Direct: 22)\n",
      "|   |   |-- Determining Admissibility of Evidence [ID: 530] (Total: 4, Direct: 4)\n",
      "|   |   |-- Forensics Examiners’ Roles in Testifying [ID: 527] (Total: 5, Direct: 5)\n",
      "|   |   |-- Traps for Unwary Experts [ID: 529] (Total: 16, Direct: 16)\n",
      "|   |-- Chapter Review [ID: 554] (Total: 2220, Direct: 0)\n",
      "|   |   |-- Case Projects [ID: 555] (Total: 173, Direct: 173)\n",
      "|   |   |-- Chapter Summary [ID: 551] (Total: 211, Direct: 211)\n",
      "|   |   |-- Hands-On Projects [ID: 554] (Total: 1527, Direct: 1527)\n",
      "|   |   |-- Key Terms [ID: 552] (Total: 309, Direct: 309)\n",
      "|   |-- Ethical Difficulties in Expert Testimony [ID: 537] (Total: 19, Direct: 6)\n",
      "|   |   |-- Ethical Responsibilities Owed to You [ID: 538] (Total: 9, Direct: 9)\n",
      "|   |   |-- Standard Forensics Tools and Tools You Create [ID: 539] (Total: 4, Direct: 4)\n",
      "|   |-- Organizations with Codes of Ethics [ID: 534] (Total: 39, Direct: 2)\n",
      "|   |   |-- American Bar Association [ID: 535] (Total: 5, Direct: 5)\n",
      "|   |   |-- American Psychological Association [ID: 536] (Total: 2, Direct: 2)\n",
      "|   |   |-- International Association of Computer Investigative Specialists [ID: 534] (Total: 13, Direct: 13)\n",
      "|   |   |-- International High Technology Crime Investigation Association [ID: 533] (Total: 6, Direct: 6)\n",
      "|   |   |-- International Society of Forensic Computer Examiners [ID: 532] (Total: 11, Direct: 11)\n",
      "|-- Chapter 2. The Investigator’s Office and Laboratory [ID: 57] (Total: 296, Direct: 0)\n",
      "|   |-- Building a Business Case for Developing a Forensics Lab [ID: 82] (Total: 104, Direct: 11)\n",
      "|   |   |-- Preparing a Business Case for a Digital Forensics Lab [ID: 83] (Total: 93, Direct: 2)\n",
      "|   |   |   |-- Acceptance Testing [ID: 92] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Approval and Acquisition [ID: 90] (Total: 4, Direct: 4)\n",
      "|   |   |   |-- Budget Development [ID: 85] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Correction for Acceptance [ID: 93] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Facility Cost [ID: 86] (Total: 15, Direct: 15)\n",
      "|   |   |   |-- Hardware Requirements [ID: 87] (Total: 21, Direct: 21)\n",
      "|   |   |   |-- Implementation [ID: 91] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Justification [ID: 84] (Total: 8, Direct: 8)\n",
      "|   |   |   |-- Miscellaneous Budget Needs [ID: 89] (Total: 4, Direct: 4)\n",
      "|   |   |   |-- Production [ID: 94] (Total: 4, Direct: 4)\n",
      "|   |   |   |-- Software Requirements [ID: 88] (Total: 23, Direct: 23)\n",
      "|   |-- Determining the Physical Requirements for a Digital Forensics Lab [ID: 67] (Total: 68, Direct: 3)\n",
      "|   |   |-- Auditing a Digital Forensics Lab [ID: 73] (Total: 8, Direct: 8)\n",
      "|   |   |-- Conducting High-Risk Investigations [ID: 69] (Total: 7, Direct: 7)\n",
      "|   |   |-- Considering Physical Security Needs [ID: 72] (Total: 6, Direct: 6)\n",
      "|   |   |-- Determining Floor Plans for Digital Forensics Labs [ID: 74] (Total: 7, Direct: 7)\n",
      "|   |   |-- Identifying Lab Security Needs [ID: 68] (Total: 9, Direct: 9)\n",
      "|   |   |-- Overseeing Facility Maintenance [ID: 71] (Total: 4, Direct: 4)\n",
      "|   |   |-- Using Evidence Containers [ID: 70] (Total: 24, Direct: 24)\n",
      "|   |-- Selecting a Basic Forensic Workstation [ID: 75] (Total: 51, Direct: 2)\n",
      "|   |   |-- Maintaining Operating Systems and Software Inventories [ID: 79] (Total: 9, Direct: 9)\n",
      "|   |   |-- Planning for Equipment Upgrades [ID: 81] (Total: 3, Direct: 3)\n",
      "|   |   |-- Selecting Workstations for Private-Sector Labs [ID: 77] (Total: 4, Direct: 4)\n",
      "|   |   |-- Selecting Workstations for a Lab [ID: 76] (Total: 12, Direct: 12)\n",
      "|   |   |-- Stocking Hardware Peripherals [ID: 78] (Total: 14, Direct: 14)\n",
      "|   |   |-- Using a Disaster Recovery Plan [ID: 80] (Total: 7, Direct: 7)\n",
      "|   |-- Understanding Forensics Lab Accreditation Requirements [ID: 57] (Total: 73, Direct: 7)\n",
      "|   |   |-- Acquiring Certification and Training [ID: 60] (Total: 41, Direct: 4)\n",
      "|   |   |   |-- AccessData Certified Examiner [ID: 65] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- EnCase Certified Examiner Certification [ID: 64] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- High Tech Crime Network [ID: 63] (Total: 19, Direct: 19)\n",
      "|   |   |   |-- ISC2 Certified Cyber Forensics Professional [ID: 62] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Other Training and Certifications [ID: 66] (Total: 12, Direct: 12)\n",
      "|   |   |-- Identifying Duties of the Lab Manager and Staff [ID: 58] (Total: 7, Direct: 7)\n",
      "|   |   |-- Lab Budget Planning [ID: 59] (Total: 18, Direct: 18)\n",
      "|-- Chapter 3. Data Acquisition [ID: 103] (Total: 379, Direct: 0)\n",
      "|   |-- Contingency Planning for Image Acquisitions [ID: 108] (Total: 10, Direct: 10)\n",
      "|   |-- Determining the Best Acquisition Method [ID: 107] (Total: 20, Direct: 20)\n",
      "|   |-- Performing RAID Data Acquisitions [ID: 122] (Total: 30, Direct: 2)\n",
      "|   |   |-- Acquiring RAID Disks [ID: 124] (Total: 13, Direct: 13)\n",
      "|   |   |-- Understanding RAID [ID: 123] (Total: 15, Direct: 15)\n",
      "|   |-- Understanding Storage Formats for Digital Evidence [ID: 103] (Total: 48, Direct: 5)\n",
      "|   |   |-- Advanced Forensic Format [ID: 106] (Total: 11, Direct: 11)\n",
      "|   |   |-- Proprietary Formats [ID: 105] (Total: 11, Direct: 11)\n",
      "|   |   |-- Raw Format [ID: 104] (Total: 21, Direct: 21)\n",
      "|   |-- Using Acquisition Tools [ID: 109] (Total: 173, Direct: 5)\n",
      "|   |   |-- Acquiring Data with a Linux Boot CD [ID: 111] (Total: 113, Direct: 5)\n",
      "|   |   |   |-- Acquiring Data with dcfldd in Linux [ID: 115] (Total: 14, Direct: 14)\n",
      "|   |   |   |-- Acquiring Data with dd in Linux [ID: 114] (Total: 32, Direct: 32)\n",
      "|   |   |   |-- Preparing a Target Drive for Acquisition in Linux [ID: 113] (Total: 45, Direct: 45)\n",
      "|   |   |   |-- Using Linux Live CD Distributions [ID: 112] (Total: 17, Direct: 17)\n",
      "|   |   |-- Capturing an Image with AccessData FTK Imager Lite [ID: 116] (Total: 46, Direct: 46)\n",
      "|   |   |-- Mini-WinFE Boot CDs and USB Drives [ID: 110] (Total: 9, Direct: 9)\n",
      "|   |-- Using Other Forensics Acquisition Tools [ID: 134] (Total: 27, Direct: 2)\n",
      "|   |   |-- ASR Data SMART [ID: 133] (Total: 7, Direct: 7)\n",
      "|   |   |-- ILookIX IXImager [ID: 135] (Total: 2, Direct: 2)\n",
      "|   |   |-- PassMark Software ImageUSB [ID: 132] (Total: 2, Direct: 2)\n",
      "|   |   |-- Runtime Software [ID: 134] (Total: 12, Direct: 12)\n",
      "|   |   |-- SourceForge [ID: 136] (Total: 2, Direct: 2)\n",
      "|   |-- Using Remote Network Acquisition Tools [ID: 125] (Total: 39, Direct: 5)\n",
      "|   |   |-- Remote Acquisition with EnCase Enterprise [ID: 127] (Total: 7, Direct: 7)\n",
      "|   |   |-- Remote Acquisition with F-Response [ID: 130] (Total: 3, Direct: 3)\n",
      "|   |   |-- Remote Acquisition with ProDiscover [ID: 126] (Total: 20, Direct: 20)\n",
      "|   |   |-- Remote Acquisition with R-Tools R-Studio [ID: 128] (Total: 2, Direct: 2)\n",
      "|   |   |-- Remote Acquisition with WetStone US-LATT PRO [ID: 129] (Total: 2, Direct: 2)\n",
      "|   |-- Validating Data Acquisitions [ID: 117] (Total: 32, Direct: 5)\n",
      "|   |   |-- Linux Validation Methods [ID: 118] (Total: 21, Direct: 3)\n",
      "|   |   |   |-- Validating dcfldd-Acquired Data [ID: 120] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Validating dd-Acquired Data [ID: 119] (Total: 12, Direct: 12)\n",
      "|   |   |-- Windows Validation Methods [ID: 121] (Total: 6, Direct: 6)\n",
      "|-- Chapter 4. Processing Crime and Incident Scenes [ID: 145] (Total: 384, Direct: 0)\n",
      "|   |-- Collecting Evidence in Private-Sector Incident Scenes [ID: 147] (Total: 24, Direct: 24)\n",
      "|   |-- Identifying Digital Evidence [ID: 145] (Total: 76, Direct: 13)\n",
      "|   |   |-- Understanding Rules of Evidence [ID: 146] (Total: 63, Direct: 63)\n",
      "|   |-- Obtaining a Digital Hash [ID: 170] (Total: 42, Direct: 42)\n",
      "|   |-- Preparing for a Search [ID: 150] (Total: 40, Direct: 2)\n",
      "|   |   |-- Determining Whether You Can Seize Computers and Digital Devices [ID: 153] (Total: 4, Direct: 4)\n",
      "|   |   |-- Determining Who Is in Charge [ID: 155] (Total: 2, Direct: 2)\n",
      "|   |   |-- Determining the Tools You Need [ID: 157] (Total: 11, Direct: 11)\n",
      "|   |   |-- Getting a Detailed Description of the Location [ID: 154] (Total: 7, Direct: 7)\n",
      "|   |   |-- Identifying the Nature of the Case [ID: 151] (Total: 3, Direct: 3)\n",
      "|   |   |-- Identifying the Type of OS or Digital Device [ID: 152] (Total: 4, Direct: 4)\n",
      "|   |   |-- Preparing the Investigation Team [ID: 158] (Total: 3, Direct: 3)\n",
      "|   |   |-- Using Additional Technical Expertise [ID: 156] (Total: 4, Direct: 4)\n",
      "|   |-- Processing Law Enforcement Crime Scenes [ID: 148] (Total: 24, Direct: 6)\n",
      "|   |   |-- Understanding Concepts and Terms Used in Warrants [ID: 149] (Total: 18, Direct: 18)\n",
      "|   |-- Reviewing a Case [ID: 171] (Total: 79, Direct: 8)\n",
      "|   |   |-- An Example of a Criminal Investigation [ID: 173] (Total: 4, Direct: 4)\n",
      "|   |   |-- Conducting the Investigation: Acquiring Evidence with OSForensics [ID: 176] (Total: 33, Direct: 33)\n",
      "|   |   |-- Planning the Investigation [ID: 175] (Total: 7, Direct: 7)\n",
      "|   |   |-- Reviewing Background Information for a Case [ID: 174] (Total: 4, Direct: 4)\n",
      "|   |   |-- Sample Civil Investigation [ID: 172] (Total: 23, Direct: 23)\n",
      "|   |-- Securing a Digital Incident or Crime Scene [ID: 159] (Total: 9, Direct: 9)\n",
      "|   |-- Seizing Digital Evidence at the Scene [ID: 160] (Total: 72, Direct: 4)\n",
      "|   |   |-- Documenting Evidence in the Lab [ID: 165] (Total: 4, Direct: 4)\n",
      "|   |   |-- Preparing to Acquire Digital Evidence [ID: 161] (Total: 8, Direct: 8)\n",
      "|   |   |-- Processing Data Centers with RAID Systems [ID: 163] (Total: 2, Direct: 2)\n",
      "|   |   |-- Processing Incident or Crime Scenes [ID: 162] (Total: 34, Direct: 34)\n",
      "|   |   |-- Processing and Handling Digital Evidence [ID: 166] (Total: 11, Direct: 11)\n",
      "|   |   |-- Using a Technical Advisor [ID: 164] (Total: 9, Direct: 9)\n",
      "|   |-- Storing Digital Evidence [ID: 167] (Total: 18, Direct: 7)\n",
      "|   |   |-- Documenting Evidence [ID: 169] (Total: 7, Direct: 7)\n",
      "|   |   |-- Evidence Retention and Media Storage Needs [ID: 168] (Total: 4, Direct: 4)\n",
      "|-- Chapter 5. Working with Windows and CLI Systems [ID: 185] (Total: 449, Direct: 0)\n",
      "|   |-- Examining NTFS Disks [ID: 193] (Total: 168, Direct: 14)\n",
      "|   |   |-- Deleting NTFS Files [ID: 208] (Total: 20, Direct: 20)\n",
      "|   |   |-- EFS Recovery Key Agent [ID: 207] (Total: 8, Direct: 8)\n",
      "|   |   |-- MFT Structures for File Data [ID: 196] (Total: 69, Direct: 3)\n",
      "|   |   |   |-- Attribute 0x10: Standard Information [ID: 198] (Total: 8, Direct: 8)\n",
      "|   |   |   |-- Attribute 0x30: File Name [ID: 199] (Total: 18, Direct: 18)\n",
      "|   |   |   |-- Attribute 0x40: Object_ID [ID: 200] (Total: 10, Direct: 10)\n",
      "|   |   |   |-- Attribute 0x80: Data for a Nonresident File [ID: 202] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Attribute 0x80: Data for a Resident File [ID: 201] (Total: 8, Direct: 8)\n",
      "|   |   |   |-- Interpreting a Data Run [ID: 203] (Total: 9, Direct: 9)\n",
      "|   |   |   |-- MFT Header Fields [ID: 197] (Total: 7, Direct: 7)\n",
      "|   |   |-- MFT and File Attributes [ID: 195] (Total: 20, Direct: 20)\n",
      "|   |   |-- NTFS Alternate Data Streams [ID: 204] (Total: 14, Direct: 14)\n",
      "|   |   |-- NTFS Compressed Files [ID: 205] (Total: 3, Direct: 3)\n",
      "|   |   |-- NTFS Encrypting File System [ID: 206] (Total: 5, Direct: 5)\n",
      "|   |   |-- NTFS System Files [ID: 194] (Total: 6, Direct: 6)\n",
      "|   |   |-- Resilient File System [ID: 209] (Total: 9, Direct: 9)\n",
      "|   |-- Exploring Microsoft File Structures [ID: 189] (Total: 71, Direct: 5)\n",
      "|   |   |-- Disk Partitions [ID: 190] (Total: 39, Direct: 39)\n",
      "|   |   |-- Examining FAT Disks [ID: 191] (Total: 27, Direct: 24)\n",
      "|   |   |   |-- Deleting FAT Files [ID: 192] (Total: 3, Direct: 3)\n",
      "|   |-- Understanding File Systems [ID: 185] (Total: 33, Direct: 3)\n",
      "|   |   |-- Solid-State Storage Devices [ID: 188] (Total: 8, Direct: 8)\n",
      "|   |   |-- Understanding Disk Drives [ID: 187] (Total: 14, Direct: 14)\n",
      "|   |   |-- Understanding the Boot Sequence [ID: 186] (Total: 8, Direct: 8)\n",
      "|   |-- Understanding Microsoft Startup Tasks [ID: 216] (Total: 47, Direct: 3)\n",
      "|   |   |-- Startup in Windows 7, Windows 8, and Windows 10 [ID: 217] (Total: 5, Direct: 5)\n",
      "|   |   |-- Startup in Windows NT and Later [ID: 218] (Total: 39, Direct: 10)\n",
      "|   |   |   |-- Contamination Concerns with Windows XP [ID: 222] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Startup Files for Windows Vista [ID: 219] (Total: 6, Direct: 6)\n",
      "|   |   |   |-- Startup Files for Windows XP [ID: 220] (Total: 17, Direct: 17)\n",
      "|   |   |   |-- Windows XP System Files [ID: 221] (Total: 4, Direct: 4)\n",
      "|   |-- Understanding Virtual Machines [ID: 223] (Total: 48, Direct: 10)\n",
      "|   |   |-- Creating a Virtual Machine [ID: 224] (Total: 38, Direct: 38)\n",
      "|   |-- Understanding Whole Disk Encryption [ID: 210] (Total: 26, Direct: 11)\n",
      "|   |   |-- Examining Microsoft BitLocker [ID: 211] (Total: 9, Direct: 9)\n",
      "|   |   |-- Examining Third-Party Disk Encryption Tools [ID: 212] (Total: 6, Direct: 6)\n",
      "|   |-- Understanding the Windows Registry [ID: 213] (Total: 56, Direct: 9)\n",
      "|   |   |-- Examining the Windows Registry [ID: 215] (Total: 29, Direct: 29)\n",
      "|   |   |-- Exploring the Organization of the Windows Registry [ID: 214] (Total: 18, Direct: 18)\n",
      "|-- Chapter 6. Current Digital Forensics Tools [ID: 233] (Total: 302, Direct: 0)\n",
      "|   |-- Digital Forensics Hardware Tools [ID: 254] (Total: 38, Direct: 3)\n",
      "|   |   |-- Forensic Workstations [ID: 255] (Total: 13, Direct: 7)\n",
      "|   |   |   |-- Building Your Own Workstation [ID: 256] (Total: 6, Direct: 6)\n",
      "|   |   |-- Recommendations for a Forensic Workstation [ID: 258] (Total: 5, Direct: 5)\n",
      "|   |   |-- Using a Write-Blocker [ID: 257] (Total: 17, Direct: 17)\n",
      "|   |-- Digital Forensics Software Tools [ID: 245] (Total: 87, Direct: 4)\n",
      "|   |   |-- Command-Line Forensics Tools [ID: 246] (Total: 14, Direct: 14)\n",
      "|   |   |-- Linux Forensics Tools [ID: 247] (Total: 65, Direct: 4)\n",
      "|   |   |   |-- Autopsy and Sleuth Kit [ID: 251] (Total: 4, Direct: 4)\n",
      "|   |   |   |-- Forcepoint Threat Protection [ID: 252] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Helix 3 [ID: 249] (Total: 3, Direct: 3)\n",
      "|   |   |   |-- Kali Linux [ID: 250] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Smart [ID: 248] (Total: 50, Direct: 50)\n",
      "|   |   |-- Other GUI Forensics Tools [ID: 253] (Total: 4, Direct: 4)\n",
      "|   |-- Evaluating Digital Forensics Tool Needs [ID: 233] (Total: 147, Direct: 10)\n",
      "|   |   |-- Other Considerations for Tools [ID: 244] (Total: 4, Direct: 4)\n",
      "|   |   |-- Tasks Performed by Digital Forensics Tools [ID: 237] (Total: 116, Direct: 3)\n",
      "|   |   |   |-- Acquisition [ID: 238] (Total: 21, Direct: 21)\n",
      "|   |   |   |-- Extraction [ID: 240] (Total: 25, Direct: 25)\n",
      "|   |   |   |-- Reconstruction [ID: 241] (Total: 28, Direct: 28)\n",
      "|   |   |   |-- Reporting [ID: 242] (Total: 23, Direct: 23)\n",
      "|   |   |   |-- Validation and Verification [ID: 239] (Total: 16, Direct: 16)\n",
      "|   |   |-- Tool Comparisons [ID: 243] (Total: 6, Direct: 6)\n",
      "|   |   |-- Types of Digital Forensics Tools [ID: 234] (Total: 11, Direct: 4)\n",
      "|   |   |   |-- Hardware Forensics Tools [ID: 235] (Total: 2, Direct: 2)\n",
      "|   |   |   |-- Software Forensics Tools [ID: 236] (Total: 5, Direct: 5)\n",
      "|   |-- Validating and Testing Forensics Software [ID: 259] (Total: 30, Direct: 2)\n",
      "|   |   |-- Using National Institute of Standards and Technology Tools [ID: 260] (Total: 13, Direct: 13)\n",
      "|   |   |-- Using Validation Protocols [ID: 261] (Total: 15, Direct: 6)\n",
      "|   |   |   |-- Digital Forensics Examination Protocol [ID: 262] (Total: 5, Direct: 5)\n",
      "|   |   |   |-- Digital Forensics Tool Upgrade Protocol [ID: 263] (Total: 4, Direct: 4)\n",
      "|-- Chapter 7. Linux and Macintosh File Systems [ID: 272] (Total: 257, Direct: 0)\n",
      "|   |-- Examining Linux File Structures [ID: 272] (Total: 131, Direct: 77)\n",
      "|   |   |-- File Structures in Ext4 [ID: 273] (Total: 54, Direct: 8)\n",
      "|   |   |   |-- Hard Links and Symbolic Links [ID: 275] (Total: 24, Direct: 24)\n",
      "|   |   |   |-- Inodes [ID: 274] (Total: 22, Direct: 22)\n",
      "|   |-- Understanding Macintosh File Structures [ID: 276] (Total: 58, Direct: 6)\n",
      "|   |   |-- An Overview of Mac File Structures [ID: 277] (Total: 23, Direct: 23)\n",
      "|   |   |-- Forensics Procedures in Mac [ID: 278] (Total: 29, Direct: 18)\n",
      "|   |   |   |-- Acquisition Methods in macOS [ID: 279] (Total: 11, Direct: 11)\n",
      "|   |-- Using Linux Forensics Tools [ID: 280] (Total: 68, Direct: 5)\n",
      "|   |   |-- Examining a Case with Sleuth Kit and Autopsy [ID: 282] (Total: 42, Direct: 42)\n",
      "|   |   |-- Installing Sleuth Kit and Autopsy [ID: 281] (Total: 21, Direct: 21)\n",
      "|-- Chapter 8. Recovering Graphics Files [ID: 308] (Total: 259, Direct: 0)\n",
      "|   |-- Identifying Unknown File Formats [ID: 309] (Total: 47, Direct: 14)\n",
      "|   |   |-- Analyzing Graphics File Headers [ID: 310] (Total: 5, Direct: 5)\n",
      "|   |   |-- Tools for Viewing Images [ID: 311] (Total: 5, Direct: 5)\n",
      "|   |   |-- Understanding Steganography in Graphics Files [ID: 312] (Total: 16, Direct: 16)\n",
      "|   |   |-- Using Steganalysis Tools [ID: 313] (Total: 7, Direct: 7)\n",
      "|   |-- Recognizing a Graphics File [ID: 291] (Total: 54, Direct: 4)\n",
      "|   |   |-- Understanding Bitmap and Raster Images [ID: 292] (Total: 13, Direct: 13)\n",
      "|   |   |-- Understanding Digital Photograph File Formats [ID: 296] (Total: 19, Direct: 2)\n",
      "|   |   |   |-- Examining the Exchangeable Image File Format [ID: 298] (Total: 12, Direct: 12)\n",
      "|   |   |   |-- Examining the Raw File Format [ID: 297] (Total: 5, Direct: 5)\n",
      "|   |   |-- Understanding Graphics File Formats [ID: 295] (Total: 14, Direct: 14)\n",
      "|   |   |-- Understanding Metafile Graphics [ID: 294] (Total: 2, Direct: 2)\n",
      "|   |   |-- Understanding Vector Graphics [ID: 293] (Total: 2, Direct: 2)\n",
      "|   |-- Understanding Copyright Issues with Graphics [ID: 314] (Total: 19, Direct: 19)\n",
      "|   |-- Understanding Data Compression [ID: 308] (Total: 139, Direct: 2)\n",
      "|   |   |-- Identifying Graphics File Fragments [ID: 302] (Total: 3, Direct: 3)\n",
      "|   |   |-- Locating and Recovering Graphics Files [ID: 301] (Total: 6, Direct: 6)\n",
      "|   |   |-- Lossless and Lossy Compression [ID: 300] (Total: 8, Direct: 8)\n",
      "|   |   |-- Rebuilding File Headers [ID: 307] (Total: 22, Direct: 22)\n",
      "|   |   |-- Reconstructing File Fragments [ID: 308] (Total: 53, Direct: 53)\n",
      "|   |   |-- Repairing Damaged Headers [ID: 303] (Total: 6, Direct: 6)\n",
      "|   |   |-- Searching for and Carving Data from Unallocated Space [ID: 304] (Total: 39, Direct: 9)\n",
      "|   |   |   |-- Planning Your Examination [ID: 305] (Total: 4, Direct: 4)\n",
      "|   |   |   |-- Searching for and Recovering Digital Photograph Evidence [ID: 306] (Total: 26, Direct: 26)\n",
      "|-- Chapter 9. Digital Forensics Analysis and Validation [ID: 323] (Total: 232, Direct: 0)\n",
      "|   |-- Addressing Data-Hiding Techniques [ID: 333] (Total: 82, Direct: 2)\n",
      "|   |   |-- Bit-Shifting [ID: 337] (Total: 34, Direct: 34)\n",
      "|   |   |-- Examining Encrypted Files [ID: 339] (Total: 4, Direct: 4)\n",
      "|   |   |-- Hiding Files by Using the OS [ID: 334] (Total: 3, Direct: 3)\n",
      "|   |   |-- Hiding Partitions [ID: 335] (Total: 8, Direct: 8)\n",
      "|   |   |-- Marking Bad Clusters [ID: 336] (Total: 6, Direct: 6)\n",
      "|   |   |-- Recovering Passwords [ID: 340] (Total: 15, Direct: 15)\n",
      "|   |   |-- Understanding Steganalysis Methods [ID: 338] (Total: 10, Direct: 10)\n",
      "|   |-- Determining What Data to Collect and Analyze [ID: 323] (Total: 99, Direct: 6)\n",
      "|   |   |-- Approaching Digital Forensics Cases [ID: 324] (Total: 35, Direct: 30)\n",
      "|   |   |   |-- Refining and Modifying the Investigation Plan [ID: 325] (Total: 5, Direct: 5)\n",
      "|   |   |-- Collecting Hash Values in Autopsy [ID: 328] (Total: 35, Direct: 35)\n",
      "|   |   |-- Using Autopsy to Validate Data [ID: 326] (Total: 23, Direct: 8)\n",
      "|   |   |   |-- Installing NSRL Hashes in Autopsy [ID: 327] (Total: 15, Direct: 15)\n",
      "|   |-- Validating Forensic Data [ID: 329] (Total: 51, Direct: 3)\n",
      "|   |   |-- Validating with Digital Forensics Tools [ID: 332] (Total: 17, Direct: 17)\n",
      "|   |   |-- Validating with Hexadecimal Editors [ID: 330] (Total: 31, Direct: 28)\n",
      "|   |   |   |-- Using Hash Values to Discriminate Data [ID: 331] (Total: 3, Direct: 3)\n",
      "|-- Lab Manual for Guide to Computer Forensics and Investigations [ID: 557] (Total: 4179, Direct: 1)\n",
      "|   |-- Chapter 1. Understanding the Digital Forensics Profession and Investigations [ID: 558] (Total: 27, Direct: 23)\n",
      "|   |   |-- Lab 1.1. Installing Autopsy for Windows [ID: 560] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 1.2. Downloading FTK Imager Lite [ID: 565] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 1.3. Downloading WinHex [ID: 570] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 1.4. Using Autopsy for Windows [ID: 575] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics [ID: 750] (Total: 160, Direct: 17)\n",
      "|   |   |-- Lab 10.1. Analyzing a Forensic Image Hosting a Virtual Machine [ID: 752] (Total: 41, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 756] (Total: 40, Direct: 0)\n",
      "|   |   |   |   |-- Analyzing a Windows Image Containing a Virtual Machine [ID: 757] (Total: 32, Direct: 32)\n",
      "|   |   |   |   |-- Installing MD5 Hashes in Autopsy [ID: 756] (Total: 8, Direct: 8)\n",
      "|   |   |-- Lab 10.2. Conducting a Live Acquisition [ID: 759] (Total: 45, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 763] (Total: 44, Direct: 0)\n",
      "|   |   |   |   |-- Capturing Data in a Live Acquisition [ID: 765] (Total: 14, Direct: 14)\n",
      "|   |   |   |   |-- Exploring Tools for Live Acquisitions [ID: 764] (Total: 14, Direct: 14)\n",
      "|   |   |   |   |-- Installing Tools for Live Acquisitions [ID: 763] (Total: 16, Direct: 16)\n",
      "|   |   |-- Lab 10.3. Using Kali Linux for Network Forensics [ID: 767] (Total: 57, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 771] (Total: 56, Direct: 0)\n",
      "|   |   |   |   |-- Identifying Open Ports and Making a Screen Capture [ID: 773] (Total: 18, Direct: 18)\n",
      "|   |   |   |   |-- Installing Kali Linux [ID: 771] (Total: 26, Direct: 26)\n",
      "|   |   |   |   |-- Mounting Drives in Kali Linux [ID: 772] (Total: 12, Direct: 12)\n",
      "|   |-- Chapter 11. E-mail and Social Media Investigations [ID: 775] (Total: 23, Direct: 20)\n",
      "|   |   |-- Lab 11.1. Using OSForensics to Search for E-mails and Mailboxes [ID: 777] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 11.2. Using Autopsy to Search for E-mails and Mailboxes [ID: 782] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 11.3. Finding Google Searches and Multiple E-mail Accounts [ID: 787] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 12. Mobile Device Forensics [ID: 792] (Total: 13, Direct: 10)\n",
      "|   |   |-- Lab 12.1. Examining Cell Phone Storage Devices [ID: 794] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 12.2. Using FTK Imager Lite to View Text Messages, Phone Numbers, and Photos [ID: 799] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 12.3. Using Autopsy to Search Cloud Backups of Mobile Devices [ID: 804] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 13. Cloud Forensics [ID: 809] (Total: 23, Direct: 20)\n",
      "|   |   |-- Lab 13.1. Examining Dropbox Cloud Storage [ID: 811] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 13.2. Examining Google Drive Cloud Storage [ID: 816] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 13.3. Examining OneDrive Cloud Storage [ID: 821] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 14. Report Writing for High-Tech Investigations [ID: 826] (Total: 19, Direct: 16)\n",
      "|   |   |-- Lab 14.1. Investigating Corporate Espionage [ID: 828] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 14.2. Adding Evidence to a Case [ID: 833] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 14.3. Preparing a Report [ID: 838] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 15. Expert Testimony in Digital Investigations [ID: 843] (Total: 58, Direct: 14)\n",
      "|   |   |-- Lab 15.1. Conducting a Preliminary Investigation [ID: 845] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 15.2. Investigating an Arsonist [ID: 850] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 15.3. Recovering a Password from Password-Protected Files [ID: 860] (Total: 42, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 860] (Total: 41, Direct: 0)\n",
      "|   |   |   |   |-- Recovering a Password from Password-Protected Files [ID: 860] (Total: 29, Direct: 29)\n",
      "|   |   |   |   |-- Verifying the Existence of a Warning Banner [ID: 859] (Total: 12, Direct: 12)\n",
      "|   |-- Chapter 16. Ethics for the Expert Witness [ID: 872] (Total: 3404, Direct: 13)\n",
      "|   |   |-- Lab 16.1. Rebuilding an MFT Record from a Corrupt Image [ID: 872] (Total: 3391, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 867] (Total: 1039, Direct: 967)\n",
      "|   |   |   |   |-- Copying the Corrected MFT Record [ID: 870] (Total: 20, Direct: 20)\n",
      "|   |   |   |   |-- Creating a Duplicate Forensic Image [ID: 868] (Total: 14, Direct: 14)\n",
      "|   |   |   |   |-- Determining the Offset Byte Address of the Corrupt MFT Record [ID: 869] (Total: 12, Direct: 12)\n",
      "|   |   |   |   |-- Extracting Additional Evidence [ID: 871] (Total: 26, Direct: 26)\n",
      "|   |   |   |-- Objectives [ID: 865] (Total: 708, Direct: 345)\n",
      "|   |   |   |   |-- Materials Required [ID: 866] (Total: 363, Direct: 363)\n",
      "|   |   |   |-- Review Questions [ID: 872] (Total: 1643, Direct: 1643)\n",
      "|   |-- Chapter 2. The Investigator’s Office and Laboratory [ID: 580] (Total: 27, Direct: 22)\n",
      "|   |   |-- Lab 2.1. Wiping a USB Drive Securely [ID: 582] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 2.2. Using Directory Snoop to Image a USB Drive [ID: 587] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 2.3. Converting a Raw Image to an .E01 Image [ID: 592] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 2.4. Imaging Evidence with FTK Imager Lite [ID: 597] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 2.5. Viewing Images in FTK Imager Lite [ID: 602] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 3. Data Acquisition [ID: 607] (Total: 98, Direct: 28)\n",
      "|   |   |-- Lab 3.1. Creating a DEFT Zero Forensic Boot CD and USB Drive [ID: 609] (Total: 67, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 613] (Total: 66, Direct: 0)\n",
      "|   |   |   |   |-- Creating a Bootable USB DEFT Zero Drive [ID: 614] (Total: 14, Direct: 14)\n",
      "|   |   |   |   |-- Creating a DEFT Zero Boot CD [ID: 613] (Total: 14, Direct: 14)\n",
      "|   |   |   |   |-- Learning DEFT Zero Features [ID: 615] (Total: 38, Direct: 38)\n",
      "|   |   |-- Lab 3.2. Examining a FAT Image [ID: 617] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 3.3. Examining an NTFS Image [ID: 622] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 3.4. Examining an HFS+ Image [ID: 627] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 4. Processing Crime and Incident Scenes [ID: 632] (Total: 65, Direct: 29)\n",
      "|   |   |-- Lab 4.1. Creating a Mini-WinFE Boot CD [ID: 634] (Total: 33, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 638] (Total: 32, Direct: 0)\n",
      "|   |   |   |   |-- Creating a Mini-WinFE ISO Image [ID: 639] (Total: 24, Direct: 24)\n",
      "|   |   |   |   |-- Setting Up Mini-WinFE [ID: 638] (Total: 8, Direct: 8)\n",
      "|   |   |-- Lab 4.2. Using Mini-WinFE to Boot and Image a Windows Computer [ID: 641] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 4.3. Testing the Mini-WinFE Write-Protection Feature [ID: 646] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 4.4. Creating an Image with Guymager [ID: 651] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 5. Working with Windows and CLI Systems [ID: 656] (Total: 26, Direct: 22)\n",
      "|   |   |-- Lab 5.1. Using DART to Export Windows Registry Files [ID: 658] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 5.2. Examining the SAM Hive [ID: 663] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 5.3. Examining the SYSTEM Hive [ID: 668] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 5.4. Examining the ntuser.dat Registry File [ID: 673] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 6. Current Digital Forensics Tools [ID: 678] (Total: 101, Direct: 22)\n",
      "|   |   |-- Lab 6.1. Using Autopsy 4.7.0 to Search an Image File [ID: 680] (Total: 41, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 684] (Total: 40, Direct: 0)\n",
      "|   |   |   |   |-- Installing Autopsy 4.7.0 [ID: 684] (Total: 12, Direct: 12)\n",
      "|   |   |   |   |-- Searching E-mail in Autopsy 4.7.0 [ID: 685] (Total: 28, Direct: 28)\n",
      "|   |   |-- Lab 6.2. Using OSForensics to Search an Image of a Hard Drive [ID: 687] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 6.3. Examining a Corrupt Image File with FTK Imager Lite, Autopsy, and WinHex [ID: 692] (Total: 37, Direct: 1)\n",
      "|   |   |   |-- Activity [ID: 696] (Total: 36, Direct: 0)\n",
      "|   |   |   |   |-- Examining Image Files in WinHex [ID: 697] (Total: 20, Direct: 20)\n",
      "|   |   |   |   |-- Testing an Image File in Autopsy 4.3.0 [ID: 696] (Total: 16, Direct: 16)\n",
      "|   |-- Chapter 7. Linux and Macintosh File Systems [ID: 699] (Total: 20, Direct: 17)\n",
      "|   |   |-- Lab 7.1. Using Autopsy to Process a Mac OS X Image [ID: 701] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 7.2. Using Autopsy to Process a Mac OS 9 Image [ID: 706] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 7.3. Using Autopsy to Process a Linux Image [ID: 711] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 8. Recovering Graphics Files [ID: 716] (Total: 22, Direct: 19)\n",
      "|   |   |-- Lab 8.1. Using Autopsy to Analyze Multimedia Files [ID: 718] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 8.2. Using OSForensics to Analyze Multimedia Files [ID: 723] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 8.3. Using WinHex to Analyze Multimedia Files [ID: 728] (Total: 1, Direct: 1)\n",
      "|   |-- Chapter 9. Digital Forensics Analysis and Validation [ID: 733] (Total: 19, Direct: 16)\n",
      "|   |   |-- Lab 9.1. Using Autopsy to Search for Keywords in an Image [ID: 735] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 9.2. Validating File Hash Values with FTK Imager Lite [ID: 740] (Total: 1, Direct: 1)\n",
      "|   |   |-- Lab 9.3. Validating File Hash Values with WinHex [ID: 745] (Total: 1, Direct: 1)\n",
      "|   |-- Introduction [ID: 557] (Total: 73, Direct: 73)\n",
      "|-- Preamble or Uncategorized [ID: -1] (Total: 21, Direct: 21)\n",
      "|-- Preface [ID: 3] (Total: 10, Direct: 10)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                               Diagnostic Summary                               \n",
      "--------------------------------------------------------------------------------\n",
      "Total Chunks in DB: 11774\n",
      "\n",
      "********************************************************************************\n",
      "                              Diagnostic Complete                               \n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5.1: Database Health & Hierarchy Report (V15.3 - Final Confirmed)\n",
    "#\n",
    "# This definitive version correctly reconstructs the hierarchy by parsing the\n",
    "# robust JSON-formatted 'titles_path' string from the database metadata. It is\n",
    "# the correct tool to verify the database created by the final Cell 5.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Core and Dependency Imports ---\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    logger.warning(\"LangChain or ChromaDB components not found. Diagnostics might fail.\")\n",
    "    langchain_available = False\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER FUNCTIONS (Corrected for robust parsing)\n",
    "# ==============================================================================\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def build_hierarchy_from_metadata(metadatas: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Builds a fully nested tree from scratch by using the authoritative, JSON-encoded\n",
    "    'titles_path' key from the chunk metadata.\n",
    "    \"\"\"\n",
    "    root = {'_children': {}}\n",
    "    \n",
    "    for meta in metadatas:\n",
    "        # Use json.loads() to robustly parse the path string back into a list.\n",
    "        titles_path_str = meta.get('titles_path', '[]') # Default to empty JSON array\n",
    "        try:\n",
    "            titles_path = json.loads(titles_path_str)\n",
    "            if not isinstance(titles_path, list): titles_path = []\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            titles_path = []\n",
    "\n",
    "        if not titles_path or \"Preamble\" in titles_path[0]:\n",
    "            # This chunk is unmapped or a preamble chunk.\n",
    "            if 'Preamble or Uncategorized' not in root['_children']:\n",
    "                 root['_children']['Preamble or Uncategorized'] = {\n",
    "                    '_title': 'Preamble or Uncategorized', '_children': {}, \n",
    "                    '_direct_chunks': 0, '_total_chunks': 0, '_toc_id': -1\n",
    "                }\n",
    "            root['_children']['Preamble or Uncategorized']['_direct_chunks'] += 1\n",
    "            continue\n",
    "\n",
    "        # Traverse or build the tree according to the now-correct path\n",
    "        current_node = root\n",
    "        for title in titles_path:\n",
    "            if title not in current_node['_children']:\n",
    "                current_node['_children'][title] = {\n",
    "                    '_title': title,\n",
    "                    '_children': {},\n",
    "                    '_direct_chunks': 0,\n",
    "                    '_total_chunks': 0,\n",
    "                    '_toc_id': meta.get('toc_id') # Get the ID from the chunk\n",
    "                }\n",
    "            current_node = current_node['_children'][title]\n",
    "        \n",
    "        current_node['_direct_chunks'] += 1\n",
    "        \n",
    "    return root\n",
    "\n",
    "def sum_totals_upwards(node: Dict) -> int:\n",
    "    \"\"\"Recursively calculates total chunks for each node in the tree.\"\"\"\n",
    "    total = node.get('_direct_chunks', 0)\n",
    "    for child_node in node['_children'].values():\n",
    "        total += sum_totals_upwards(child_node)\n",
    "    node['_total_chunks'] = total\n",
    "    return total\n",
    "\n",
    "def print_hierarchy_report(node: Dict, indent_level: int = 0):\n",
    "    \"\"\"Recursively prints the reconstructed hierarchy.\"\"\"\n",
    "    prefix = \"|   \" * indent_level + \"|-- \"\n",
    "    # Sort children by title for consistent, alphabetical output\n",
    "    sorted_children = sorted(node['_children'].values(), key=lambda x: x.get('_title', ''))\n",
    "\n",
    "    for child_node in sorted_children:\n",
    "        toc_id_str = f\"[ID: {child_node.get('_toc_id', 'N/A')}]\"\n",
    "        title = child_node.get('_title', 'Untitled')\n",
    "        total = child_node.get('_total_chunks', 0)\n",
    "        direct = child_node.get('_direct_chunks', 0)\n",
    "        \n",
    "        print(f\"{prefix}{title} {toc_id_str} (Total: {total}, Direct: {direct})\")\n",
    "        \n",
    "        if child_node['_children']:\n",
    "            print_hierarchy_report(child_node, indent_level + 1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN DIAGNOSTIC FUNCTION\n",
    "# ==============================================================================\n",
    "def run_full_diagnostics():\n",
    "    print_header(\"Ground-Truth Database Health & Hierarchy Report (v15.3)\")\n",
    "\n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain components not installed. Skipping diagnostics.\")\n",
    "        return\n",
    "\n",
    "    if 'CHROMA_PERSIST_DIR' not in globals() or not os.path.exists(CHROMA_PERSIST_DIR):\n",
    "        logger.error(\"FATAL: CHROMA_PERSIST_DIR not set or path does not exist. Cannot run diagnostics.\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        logger.info(f\"Connecting to vector DB to retrieve all chunk metadata...\")\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        total_docs = vector_store._collection.count()\n",
    "        if total_docs == 0:\n",
    "            logger.warning(\"Database is empty. No diagnostics to run.\")\n",
    "            return\n",
    "        metadatas = vector_store.get(limit=total_docs, include=[\"metadatas\"])['metadatas']\n",
    "        logger.info(f\"Successfully retrieved metadata for all {len(metadatas)} chunks.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"FATAL: Could not connect to or retrieve data from ChromaDB: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    logger.info(\"Building hierarchy from ground-truth metadata...\")\n",
    "    hierarchy_tree = build_hierarchy_from_metadata(metadatas)\n",
    "\n",
    "    logger.info(\"Calculating total chunks for each branch...\")\n",
    "    sum_totals_upwards(hierarchy_tree)\n",
    "\n",
    "    print_header(\"Reconstructed Hierarchy Report (from DB Ground Truth)\")\n",
    "    print_hierarchy_report(hierarchy_tree)\n",
    "\n",
    "    print_header(\"Diagnostic Summary\", char=\"-\")\n",
    "    print(f\"Total Chunks in DB: {total_docs}\")\n",
    "    if 'Preamble or Uncategorized' in hierarchy_tree['_children']:\n",
    "        orphaned = hierarchy_tree['_children']['Preamble or Uncategorized'].get('_total_chunks', 0)\n",
    "        if orphaned > 0:\n",
    "            logger.warning(f\"Found {orphaned} chunks that were unmapped or in the preamble.\")\n",
    "    else:\n",
    "        logger.info(\"All chunks were successfully mapped to a ToC entry.\")\n",
    "\n",
    "    print_header(\"Diagnostic Complete\", char=\"*\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK FOR THIS CELL\n",
    "# ==============================================================================\n",
    "run_full_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55d263ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 01:00:10,507 - INFO - Connecting to the existing vector database at '/home/sebas_dev_linux/projects/course_generator/data/DataBase_Chroma/chroma_db_toc_guided_chunks_epub'...\n",
      "2025-07-06 01:00:10,561 - INFO - Found 8 chunks in the database for this section.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFYING SECTION: '\"Developing Digital Forensics Resources\"]' (Path: [\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\" -> \"An Overview of Digital Forensics\" -> \"Developing Digital Forensics Resources\"]) (toc_id: 13)\n",
      "================================================================================\n",
      "\n",
      "############################ Reassembled Text ############################\n",
      "Developing Digital Forensics Resources\n",
      "\n",
      "To be a successful digital forensics investigator, you must be familiar with more than one computing platform. In addition to older platforms, such as DOS, Windows 9x, and Windows XP, you should be familiar with Linux, macOS, and current Windows platforms. However, no one can be an expert in every aspect of computing. Likewise, you can’t know everything about the technology you’re investigating. To supplement your knowledge, you should develop and maintain contact with digital, network, and investigative professionals.\n",
      "\n",
      "Join computer user groups in both the public and private sectors. In the Pacific Northwest, for example, Computer Technology Investigators Network (CTIN) A nonprofit group based in Seattle-Tacoma, WA, composed of law enforcement members, private corporation security professionals, and other security professionals whose aim is to improve the quality of high-technology investigations in the Pacific Northwest. meets to discuss problems that digital forensics examiners encounter. This nonprofit organization also conducts training. IACIS is an excellent group for law enforcement personnel but doesn’t have local chapters. However, groups such as the High Technology Crime Investigation Association, International Information Systems Security Certification Consortium (ISC)2, and InfraGard have\n",
      "\n",
      "International Information Systems Security Certification Consortium (ISC)2, and InfraGard have local chapters open to professionals in most major cities. Build your own network of digital forensics experts, and keep in touch through e-mail. Cultivate professional relationships with people who specialize in technical areas different from your own specialty. If you’re a Windows expert, for example, maintain contact with experts in Linux, UNIX, and macOS. If you’re using social media to interact with experts, exercise caution and good judgment when communicating with people you haven’t met in person or whose backgrounds you don’t know.\n",
      "\n",
      "User groups can be especially helpful when you need information about obscure OSs. For example, a user group helped convict a child molester in Pierce County, Washington, in 1996. The suspect installed video cameras throughout his house, served alcohol to young women to intoxicate them, and secretly filmed them playing strip poker. When he was accused of molesting a child, police seized his computers and other physical evidence. The investigator discovered that the computers used CoCo DOS, an OS that had been out of use for years. The investigator contacted a local user group, which supplied the standard commands and other information needed to access the system. On the suspect’s computer, the investigator found a diary detailing the suspect’s actions over 15 years, including the\n",
      "\n",
      "the investigator found a diary detailing the suspect’s actions over 15 years, including the molestation of more than 400 young women. As a result, the suspect received a longer sentence than if he had been convicted of molesting only one child.\n",
      "\n",
      "Outside experts can also give you detailed information you need to retrieve digital evidence. For example, a recent murder case involved a husband and wife who owned a Macintosh store. When the wife was discovered dead, apparently murdered, investigators found that she had wanted to leave her husband but didn’t because of her religious beliefs. The police got a search warrant and confiscated the home and office computers. When the detective on the case examined the home system, he found that the hard drive had been compressed and erased. He contacted a Macintosh engineer, who determined the two software programs used to compress the drive. With this knowledge, the detective could retrieve information from the hard drive, including text files indicating that the husband spent $35,000 in\n",
      "\n",
      "information from the hard drive, including text files indicating that the husband spent $35,000 in business funds to purchase cocaine and prostitution services. This evidence proved crucial in making it possible to convict the husband of murder.\n",
      "################################################################################\n",
      "\n",
      "------------------------ Retrieved Chunk Details -------------------------\n",
      "\n",
      "[ Chunk 1 of 8 | chunk_id: 208 ]\n",
      "  Content Preview: 'Developing Digital Forensics Resources...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 13,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\",\n",
      "  \"chunk_id\": 208,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ Chunk 2 of 8 | chunk_id: 209 ]\n",
      "  Content Preview: 'To be a successful digital forensics investigator, you must be familiar with more than one computing platform. In addition to older platforms, such as DOS, Windows 9x, and Windows XP, you should be familiar with Linux, macOS, and current Windows plat...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 209,\n",
      "  \"toc_id\": 13,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\"\n",
      "}\n",
      "\n",
      "[ Chunk 3 of 8 | chunk_id: 210 ]\n",
      "  Content Preview: 'Join computer user groups in both the public and private sectors. In the Pacific Northwest, for example, Computer Technology Investigators Network (CTIN) A nonprofit group based in Seattle-Tacoma, WA, composed of law enforcement members, private corp...'\n",
      "  Metadata: {\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\",\n",
      "  \"toc_id\": 13,\n",
      "  \"chunk_id\": 210\n",
      "}\n",
      "\n",
      "[ Chunk 4 of 8 | chunk_id: 211 ]\n",
      "  Content Preview: 'International Information Systems Security Certification Consortium (ISC)2, and InfraGard have local chapters open to professionals in most major cities. Build your own network of digital forensics experts, and keep in touch through e-mail. Cultivate...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 211,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\",\n",
      "  \"toc_id\": 13,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ Chunk 5 of 8 | chunk_id: 212 ]\n",
      "  Content Preview: 'User groups can be especially helpful when you need information about obscure OSs. For example, a user group helped convict a child molester in Pierce County, Washington, in 1996. The suspect installed video cameras throughout his house, served alcoh...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 13,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 212,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\"\n",
      "}\n",
      "\n",
      "[ Chunk 6 of 8 | chunk_id: 213 ]\n",
      "  Content Preview: 'the investigator found a diary detailing the suspect’s actions over 15 years, including the molestation of more than 400 young women. As a result, the suspect received a longer sentence than if he had been convicted of molesting only one child....'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 213,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\",\n",
      "  \"toc_id\": 13,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ Chunk 7 of 8 | chunk_id: 214 ]\n",
      "  Content Preview: 'Outside experts can also give you detailed information you need to retrieve digital evidence. For example, a recent murder case involved a husband and wife who owned a Macintosh store. When the wife was discovered dead, apparently murdered, investiga...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 214,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"toc_id\": 13\n",
      "}\n",
      "\n",
      "[ Chunk 8 of 8 | chunk_id: 215 ]\n",
      "  Content Preview: 'information from the hard drive, including text files indicating that the husband spent $35,000 in business funds to purchase cocaine and prostitution services. This evidence proved crucial in making it possible to convict the husband of murder....'\n",
      "  Metadata: {\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\", \\\"Developing Digital Forensics Resources\\\"]\",\n",
      "  \"toc_id\": 13,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"chunk_id\": 215\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "Verification complete for section '\"Developing Digital Forensics Resources\"]'.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 6: Verify Content Retrieval for a Specific toc_id (Definitive Version)\n",
    "#\n",
    "# This script retrieves all chunks for a given toc_id to verify that the\n",
    "# data was chunked and stored correctly. It has been updated to use the\n",
    "# authoritative 'titles_path' metadata created by the definitive Cell 5.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    logger.warning(\"LangChain or ChromaDB components not found. Verification might fail.\")\n",
    "    langchain_available = False\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def retrieve_and_print_chunks_for_toc_id(vector_store: Chroma, toc_id: int):\n",
    "    \"\"\"\n",
    "    Retrieves all chunks for a specific toc_id, prints the associated section title,\n",
    "    shows the reassembled text, and then lists the metadata for each individual chunk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the 'get' method with a 'where' filter to find all chunks for the toc_id\n",
    "        results = vector_store.get(\n",
    "            where={\"toc_id\": toc_id},\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "\n",
    "        if not results or not results.get('ids'):\n",
    "            logger.warning(f\"No chunks found in the database for toc_id = {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"VERIFICATION FAILED: No content found for toc_id: {toc_id}\")\n",
    "            print(\"=\" * 80)\n",
    "            return\n",
    "\n",
    "        documents = results['documents']\n",
    "        metadatas = results['metadatas']\n",
    "        \n",
    "        # ========================= MODIFICATION START =========================\n",
    "        # Get the section title and path directly from the authoritative 'titles_path' metadata.\n",
    "        first_meta = metadatas[0] if metadatas else {}\n",
    "        \n",
    "        titles_path_str = first_meta.get('titles_path', '')\n",
    "        # Convert the comma-separated string back into a list\n",
    "        titles_path = [title.strip() for title in titles_path_str.split(',')] if titles_path_str else []\n",
    "\n",
    "        if titles_path:\n",
    "            # The main section title is the last item in the path\n",
    "            section_title = titles_path[-1]\n",
    "            # The breadcrumb is the full path joined with arrows\n",
    "            breadcrumb_path = \" -> \".join(titles_path)\n",
    "            header_title = f\"'{section_title}' (Path: {breadcrumb_path})\"\n",
    "        else:\n",
    "            # Fallback for preamble or uncategorized chunks (toc_id = -1)\n",
    "            section_title = first_meta.get('level_1_title', 'Unknown or Preamble Section')\n",
    "            header_title = section_title\n",
    "        # ========================== MODIFICATION END ==========================\n",
    "        \n",
    "        # --- Print a clear header with the section title ---\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"VERIFYING SECTION: {header_title} (toc_id: {toc_id})\")\n",
    "        print(\"=\" * 80)\n",
    "        logger.info(f\"Found {len(documents)} chunks in the database for this section.\")\n",
    "        \n",
    "        # Sort chunks by their chunk_id to ensure they are in the correct order for reassembly\n",
    "        sorted_items = sorted(zip(documents, metadatas), key=lambda item: item[1].get('chunk_id', 0))\n",
    "\n",
    "        # --- Reassemble and print the full text for the section ---\n",
    "        all_chunk_texts = [item[0] for item in sorted_items]\n",
    "        # Use a newline join for better readability of the reassembled text\n",
    "        reassembled_text = \"\\n\\n\".join(all_chunk_texts)\n",
    "        \n",
    "        print(\"\\n\" + \"#\" * 28 + \" Reassembled Text \" + \"#\" * 28)\n",
    "        print(reassembled_text)\n",
    "        print(\"#\" * 80)\n",
    "        \n",
    "        # --- Print individual chunk details for in-depth verification ---\n",
    "        print(\"\\n\" + \"-\" * 24 + \" Retrieved Chunk Details \" + \"-\" * 25)\n",
    "        for i, (doc_content, meta) in enumerate(sorted_items):\n",
    "            print(f\"\\n[ Chunk {i+1} of {len(documents)} | chunk_id: {meta.get('chunk_id', 'N/A')} ]\")\n",
    "            content_preview = doc_content.replace('\\n', ' ').strip()\n",
    "            print(f\"  Content Preview: '{content_preview[:250]}...'\")\n",
    "            print(f\"  Metadata: {json.dumps(meta, indent=2)}\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Verification complete for section '{section_title}'.\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during retrieval for toc_id {toc_id}: {e}\", exc_info=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- IMPORTANT: Set the ID of the section you want to test here ---\n",
    "# To find a toc_id, you can look at the output of Cell 4 or Cell 5.1\n",
    "TOC_ID_TO_TEST = 13 # Example: \"Digital Forensics and Other Related Disciplines\"\n",
    "# TOC_ID_TO_TEST = -1 # Use -1 to test the \"Preamble\" content\n",
    "\n",
    "# Check if the database and required variables exist before attempting to connect\n",
    "if 'CHROMA_PERSIST_DIR' in globals() and os.path.exists(CHROMA_PERSIST_DIR) and langchain_available:\n",
    "    logger.info(f\"Connecting to the existing vector database at '{CHROMA_PERSIST_DIR}'...\")\n",
    "    \n",
    "    try:\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        # Run the verification function\n",
    "        retrieve_and_print_chunks_for_toc_id(vector_store, TOC_ID_TO_TEST)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Chroma or run retrieval. Error: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.error(\"Skipping verification: Required variables not defined, LangChain not available, or DB not found.\")\n",
    "    logger.error(\"Please run the previous cells (especially Cell 5) to create the database first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7e58a",
   "metadata": {},
   "source": [
    "# test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f721516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:49:28,470 - INFO - Connecting to the existing vector database...\n",
      "2025-07-06 00:49:28,481 - INFO - Loading and processing Table of Contents for test case selection...\n",
      "2025-07-06 00:49:28,484 - INFO - Found topic 'International Association of Computer Investigative Specialists' with toc_id: 62\n",
      "2025-07-06 00:49:28,486 - INFO - Retrieved 2 document chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Retrieval Test for Topic: 'International Association of Computer Investigative Specialists'\n",
      "================================================================================\n",
      "\n",
      "------------------------- REASSEMBLED CONTENT -------------------------\n",
      "ISC2 Certified Cyber Forensics Professional\n",
      "The Certified Cyber Forensics Professional (CCFP) A certification from ISC2 for completing the education and work experience requirements and passing the exam. program, sponsored by ISC2, requires knowledge of digital forensics, malware analysis, incident response, e-discovery, and other disciplines related to cyber investigations. The CCFP Web site (www.isc2.org/ccfp/default.aspx) lists requirements and processes needed for this certification.\n",
      "\n",
      "------------------------- ASSOCIATED IMAGES --------------------------\n",
      "No images found for this section.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.2: Test Content & Image Retrieval (with Random Topic Selection)\n",
    "\n",
    "# --- Core Imports ---\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random # Make sure random is imported\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Dependency Checks & Imports ---\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "    langchain_and_viz_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"Required library not found: {e}. Please install langchain, ChromaDB, Pillow, and matplotlib.\")\n",
    "    langchain_and_viz_available = False\n",
    "\n",
    "# --- Logger Setup ---\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. HELPER AND RETRIEVAL FUNCTIONS\n",
    "# ==============================================================================\n",
    "# The _add_ids_and_flatten_recursive function is needed here to process the ToC\n",
    "def _add_ids_and_flatten_recursive(nodes: List[Dict], current_path: List[str], counter: List[int], flat_list: List[Dict]):\n",
    "    \"\"\"Recursively traverses ToC nodes to flatten them and assign a unique, sequential toc_id.\"\"\"\n",
    "    for node in nodes:\n",
    "        toc_id = counter[0]\n",
    "        counter[0] += 1\n",
    "        title = node.get(\"title\", \"\").strip()\n",
    "        if not title:\n",
    "            continue\n",
    "        \n",
    "        # Check if the node is a leaf (has no children)\n",
    "        is_leaf = not bool(node.get(\"children\"))\n",
    "            \n",
    "        new_titles_path = current_path + [title]\n",
    "        entry = {\n",
    "            \"titles_path\": new_titles_path,\n",
    "            \"level\": node.get(\"level\"),\n",
    "            \"full_title_for_matching\": title,\n",
    "            \"toc_id\": toc_id,\n",
    "            \"is_leaf\": is_leaf # Add a flag to identify leaf nodes\n",
    "        }\n",
    "        if \"page\" in node:\n",
    "            entry[\"page\"] = node[\"page\"]\n",
    "            \n",
    "        flat_list.append(entry)\n",
    "        \n",
    "        if node.get(\"children\"):\n",
    "            _add_ids_and_flatten_recursive(node.get(\"children\", []), new_titles_path, counter, flat_list)\n",
    "\n",
    "\n",
    "# The retrieve_and_display_section and print_header functions remain exactly the same as before.\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def retrieve_and_display_section(\n",
    "    topic_query: str, \n",
    "    vector_store: Chroma, \n",
    "    flat_toc: List[Dict]\n",
    "):\n",
    "    # ... This entire function is identical to the previous version ...\n",
    "    # It takes the query and does the retrieval and display.\n",
    "    print_header(f\"Retrieval Test for Topic: '{topic_query}'\")\n",
    "\n",
    "    # --- 1. Find the topic in the flattened ToC ---\n",
    "    target_entry = None\n",
    "    # Find an exact or partial match for the topic query\n",
    "    for entry in flat_toc:\n",
    "        if topic_query.lower() in entry.get('full_title_for_matching', '').lower():\n",
    "            target_entry = entry\n",
    "            break\n",
    "    \n",
    "    if not target_entry:\n",
    "        logger.error(f\"Could not find topic '{topic_query}' in the Table of Contents.\")\n",
    "        return\n",
    "\n",
    "    target_toc_id = target_entry.get('toc_id')\n",
    "    full_title = target_entry.get('full_title_for_matching')\n",
    "    logger.info(f\"Found topic '{full_title}' with toc_id: {target_toc_id}\")\n",
    "\n",
    "    # --- 2. Retrieve all documents for that toc_id from ChromaDB ---\n",
    "    try:\n",
    "        retrieved_data = vector_store.get(\n",
    "            where={\"toc_id\": target_toc_id},\n",
    "            include=[\"metadatas\", \"documents\"]\n",
    "        )\n",
    "        docs = [\n",
    "            Document(page_content=doc, metadata=meta) \n",
    "            for doc, meta in zip(retrieved_data['documents'], retrieved_data['metadatas'])\n",
    "        ]\n",
    "        \n",
    "        if not docs:\n",
    "            logger.warning(f\"No document chunks found for toc_id {target_toc_id}. The topic might be a parent heading with no direct content.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Retrieved {len(docs)} document chunks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during database retrieval: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 3. Sort chunks, reassemble text, and collect images ---\n",
    "    docs.sort(key=lambda d: d.metadata.get('chunk_id', -1))\n",
    "    \n",
    "    full_content = \"\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    all_image_paths = set()\n",
    "    for d in docs:\n",
    "        if 'image_paths' in d.metadata:\n",
    "            try:\n",
    "                paths = json.loads(d.metadata['image_paths'])\n",
    "                if isinstance(paths, list):\n",
    "                    all_image_paths.update(paths)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue\n",
    "    \n",
    "    sorted_image_paths = sorted(list(all_image_paths))\n",
    "\n",
    "    # --- 4. Display the results ---\n",
    "    print(\"\\n\" + \"-\"*25 + \" REASSEMBLED CONTENT \" + \"-\"*25)\n",
    "    print(full_content)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*25 + \" ASSOCIATED IMAGES \" + \"-\"*26)\n",
    "    if not sorted_image_paths:\n",
    "        print(\"No images found for this section.\")\n",
    "    else:\n",
    "        print(f\"Found {len(sorted_image_paths)} unique image(s):\")\n",
    "        for path in sorted_image_paths:\n",
    "            print(f\"- {path}\")\n",
    "        \n",
    "        try:\n",
    "            first_image_path = sorted_image_paths[0]\n",
    "            print(f\"\\nDisplaying first image: {os.path.basename(first_image_path)}\")\n",
    "            \n",
    "            img = Image.open(first_image_path)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image for '{full_title}'\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Image file not found at path: {first_image_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not display image. Error: {e}\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN EXECUTION BLOCK FOR THIS CELL (with Random Topic Selection)\n",
    "# ==============================================================================\n",
    "\n",
    "if langchain_and_viz_available:\n",
    "    if 'CHROMA_PERSIST_DIR' in locals() and 'PRE_EXTRACTED_TOC_JSON_PATH' in locals():\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Connecting to the existing vector database...\")\n",
    "            db_retriever = Chroma(\n",
    "                persist_directory=CHROMA_PERSIST_DIR,\n",
    "                embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "                collection_name=CHROMA_COLLECTION_NAME\n",
    "            )\n",
    "\n",
    "            logger.info(\"Loading and processing Table of Contents for test case selection...\")\n",
    "            with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "                loaded_hierarchical_toc = json.load(f)\n",
    "            \n",
    "            flat_toc_for_lookup = []\n",
    "            _add_ids_and_flatten_recursive(loaded_hierarchical_toc, [], [1], flat_toc_for_lookup)\n",
    "            \n",
    "            # --- RANDOMLY SELECT A TEST QUERY ---\n",
    "            # We want to test a \"leaf\" section that has actual content.\n",
    "            # A good candidate is a section that is a leaf in the ToC tree.\n",
    "            test_candidates = [\n",
    "                entry for entry in flat_toc_for_lookup \n",
    "                if entry.get('is_leaf') and entry.get(\"level\", 0) > 0\n",
    "            ]\n",
    "\n",
    "            if not test_candidates:\n",
    "                raise ValueError(\"Could not find any suitable leaf-node topics to test.\")\n",
    "            \n",
    "            # Select a random topic from our list of good candidates\n",
    "            random_topic_entry = random.choice(test_candidates)\n",
    "            test_query = random_topic_entry['full_title_for_matching']\n",
    "            \n",
    "            # --- RUN THE TEST with the random query ---\n",
    "            retrieve_and_display_section(\n",
    "                topic_query=test_query,\n",
    "                vector_store=db_retriever,\n",
    "                flat_toc=flat_toc_for_lookup\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during the test execution: {e}\", exc_info=True)\n",
    "\n",
    "    else:\n",
    "        logger.error(\"Required variables (CHROMA_PERSIST_DIR, PRE_EXTRACTED_TOC_JSON_PATH) not found. Please run previous cells.\")\n",
    "else:\n",
    "    logger.error(\"Skipping test cell due to missing libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5f861",
   "metadata": {},
   "source": [
    "## Test Data Base for content development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e7fe4",
   "metadata": {},
   "source": [
    "Require Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf3ea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:49:28,508 - INFO - Connecting to DB and initializing components...\n",
      "2025-07-06 00:49:28,519 - INFO - Goal: Confirm the database is live and contains thematically relevant content.\n",
      "2025-07-06 00:49:28,519 - INFO - Strategy: Perform a simple similarity search using the course's 'unitName'.\n",
      "2025-07-06 00:49:28,520 - INFO - Action: Searching for query: 'Digital Forensic'...\n",
      "2025-07-06 00:49:28,559 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:28,562 - INFO - Verification: Check if at least one document was returned.\n",
      "2025-07-06 00:49:28,562 - INFO - ✅ Result: TEST 1 PASSED. The database is online and responsive.\n",
      "2025-07-06 00:49:28,563 - INFO - Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\n",
      "2025-07-06 00:49:28,563 - INFO - Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\n",
      "2025-07-06 00:49:28,564 - INFO -   - Selected random deep section: Lab Manual for Guide to Computer Forensics and Investigations -> Chapter 13. Cloud Forensics -> Lab 13.1. Examining Dropbox Cloud Storage\n",
      "2025-07-06 00:49:28,564 - INFO - Action: Performing a similarity search with a highly specific '$and' filter.\n",
      "2025-07-06 00:49:28,592 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:28,595 - INFO - Verification: Check if the precisely filtered query returned any documents.\n",
      "2025-07-06 00:49:28,596 - ERROR - ❌ Result: TEST 2 FAILED. Reason: Deeply filtered query returned no results.\n",
      "2025-07-06 00:49:28,596 - INFO - Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\n",
      "2025-07-06 00:49:28,596 - INFO - Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\n",
      "2025-07-06 00:49:28,597 - INFO -   - Selected random week: Week Week 1 - 'Understanding the Digital Forensics Profession and Investigations.'\n",
      "2025-07-06 00:49:28,597 - INFO -   - Extracted required chapter number(s): ['2019', '978', '1', '337', '56894', '4', '1']\n",
      "2025-07-06 00:49:28,602 - INFO -   - Mapped to top-level ToC entries: ['Chapter 1. Understanding the Digital Forensics Profession and Investigations', 'Chapter 4. Processing Crime and Incident Scenes']\n",
      "2025-07-06 00:49:28,602 - INFO - Action: Searching for the weekly topic, filtered by the mapped chapter(s).\n",
      "2025-07-06 00:49:28,626 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:28,630 - INFO - Verification: Check if at least one returned document is from the correct chapter.\n",
      "2025-07-06 00:49:28,631 - ERROR - ❌ Result: TEST 3 FAILED. Reason: Alignment query returned no results for the correct section/chapter.\n",
      "2025-07-06 00:49:28,632 - INFO - Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\n",
      "2025-07-06 00:49:28,632 - INFO - Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\n",
      "2025-07-06 00:49:28,633 - INFO - Action: Performing similarity search for topic: 'The Investigator's Office and Laboratory.' to get a set of chunks.\n",
      "2025-07-06 00:49:28,650 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-07-06 00:49:28,655 - INFO -   - Retrieved and sorted chunk IDs: [45, 936, 942, 945, 1025, 1087, 1088, 1219, 1270, 8493]\n",
      "2025-07-06 00:49:28,655 - INFO - Verification: Check if the sorted list of chunk_ids is strictly increasing.\n",
      "2025-07-06 00:49:28,656 - INFO - ✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                         Database Verification Process                          \n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                            Test 1: Basic Retrieval                             \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Digital Forensic'\n",
      "--> Found 1 results. Displaying top 1:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'An Overview of Digital Forensics...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 9,\n",
      "  \"chunk_id\": 156,\n",
      "  \"titles_path\": \"[\\\"Chapter 1. Understanding the Digital Forensics Profession and Investigations\\\", \\\"An Overview of Digital Forensics\\\"]\",\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                        Test 2: Deep Hierarchy Retrieval                        \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Lab 13.1. Examining Dropbox Cloud Storage'\n",
      "FILTER: {\n",
      "  \"$and\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Lab Manual for Guide to Computer Forensics and Investigations\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_2_title\": {\n",
      "        \"$eq\": \"Chapter 13. Cloud Forensics\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_3_title\": {\n",
      "        \"$eq\": \"Lab 13.1. Examining Dropbox Cloud Storage\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> No documents were retrieved for this query and filter.\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                    Test 3: Advanced Unit Outline Alignment                     \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'Understanding the Digital Forensics Profession and Investigations.'\n",
      "FILTER: {\n",
      "  \"$or\": [\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 1. Understanding the Digital Forensics Profession and Investigations\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"level_1_title\": {\n",
      "        \"$eq\": \"Chapter 4. Processing Crime and Incident Scenes\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "--> No documents were retrieved for this query and filter.\n",
      "-------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                     Test 4: Content Sequence Verification                      \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "---------- DIAGNOSTIC: RETRIEVAL RESULTS ----------\n",
      "QUERY: 'The Investigator's Office and Laboratory.'\n",
      "--> Found 10 results. Displaying top 3:\n",
      "\n",
      "[ RESULT 1 ]\n",
      "  Content : 'Chapter 2. The Investigator’s Office and Laboratory...'\n",
      "  Metadata: {\n",
      "  \"titles_path\": \"[\\\"Lab Manual for Guide to Computer Forensics and Investigations\\\", \\\"Chapter 2. The Investigator\\\\u2019s Office and Laboratory\\\"]\",\n",
      "  \"toc_id\": 580,\n",
      "  \"chunk_id\": 936,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "\n",
      "[ RESULT 2 ]\n",
      "  Content : 'Chapter 2. The Investigator’s Office and Laboratory...'\n",
      "  Metadata: {\n",
      "  \"chunk_id\": 8493,\n",
      "  \"toc_id\": 580,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\",\n",
      "  \"titles_path\": \"[\\\"Lab Manual for Guide to Computer Forensics and Investigations\\\", \\\"Chapter 2. The Investigator\\\\u2019s Office and Laboratory\\\"]\"\n",
      "}\n",
      "\n",
      "[ RESULT 3 ]\n",
      "  Content : 'As discussed, the evidence room needs to be secure. The lab should have at least two controlled exits and no windows. Separate offices for supervisors and cubicles for investigators are more practical...'\n",
      "  Metadata: {\n",
      "  \"toc_id\": 74,\n",
      "  \"titles_path\": \"[\\\"Chapter 2. The Investigator\\\\u2019s Office and Laboratory\\\", \\\"Determining the Physical Requirements for a Digital Forensics Lab\\\", \\\"Determining Floor Plans for Digital Forensics Labs\\\"]\",\n",
      "  \"chunk_id\": 1088,\n",
      "  \"source\": \"Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
      "}\n",
      "-------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "                              Verification Summary                              \n",
      "================================================================================\n",
      "Total Tests Run: 4\n",
      "✅ Passed: 2\n",
      "❌ Failed: 2\n",
      "\n",
      "================================================================================\n",
      "                             Verification Complete                              \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Verify Vector Database (Final Version with Rich Diagnostic Output)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# Third-party imports\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    from langchain_core.documents import Document\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "def print_results(query_text: str, results: list, where_filter: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Richly prints query results, showing the query, filter, and retrieved documents.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\"*10 + \" DIAGNOSTIC: RETRIEVAL RESULTS \" + \"-\"*10)\n",
    "    print(f\"QUERY: '{query_text}'\")\n",
    "    if where_filter:\n",
    "        print(f\"FILTER: {json.dumps(where_filter, indent=2)}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"--> No documents were retrieved for this query and filter.\")\n",
    "        print(\"-\" * 55)\n",
    "        return\n",
    "        \n",
    "    print(f\"--> Found {len(results)} results. Displaying top {min(len(results), 3)}:\")\n",
    "    for i, doc in enumerate(results[:3]):\n",
    "        print(f\"\\n[ RESULT {i+1} ]\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"  Content : '{content_preview[:200]}...'\")\n",
    "        print(f\"  Metadata: {json.dumps(doc.metadata, indent=2)}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "\n",
    "# --- HELPER FUNCTIONS FOR FINDING DATA (UNCHANGED) ---\n",
    "def find_deep_entry(nodes: List[Dict], current_path: List[str] = []) -> Optional[Tuple[Dict, List[str]]]:\n",
    "    shuffled_nodes = random.sample(nodes, len(nodes))\n",
    "    for node in shuffled_nodes:\n",
    "        if node.get('level', 0) >= 2 and node.get('children'): return node, current_path + [node['title']]\n",
    "        if node.get('children'):\n",
    "            path = current_path + [node['title']]\n",
    "            deep_entry = find_deep_entry(node['children'], path)\n",
    "            if deep_entry: return deep_entry\n",
    "    return None\n",
    "\n",
    "def find_chapter_title_by_number(toc_data: List[Dict], chap_num: int) -> Optional[List[str]]:\n",
    "    def search_nodes(nodes, num, current_path):\n",
    "        for node in nodes:\n",
    "            path = current_path + [node['title']]\n",
    "            if re.match(rf\"(Chapter\\s)?{num}[.:\\s]\", node.get('title', ''), re.IGNORECASE): return path\n",
    "            if node.get('children'):\n",
    "                found_path = search_nodes(node['children'], num, path)\n",
    "                if found_path: return found_path\n",
    "        return None\n",
    "    return search_nodes(toc_data, chap_num, [])\n",
    "\n",
    "\n",
    "# --- ENHANCED TEST CASES with DIAGNOSTIC OUTPUT ---\n",
    "\n",
    "def basic_retrieval_test(db, outline):\n",
    "    print_header(\"Test 1: Basic Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm the database is live and contains thematically relevant content.\")\n",
    "        logger.info(\"Strategy: Perform a simple similarity search using the course's 'unitName'.\")\n",
    "        query_text = outline.get(\"unitInformation\", {}).get(\"unitName\", \"introduction\")\n",
    "        \n",
    "        logger.info(f\"Action: Searching for query: '{query_text}'...\")\n",
    "        results = db.similarity_search(query_text, k=1)\n",
    "        \n",
    "        print_results(query_text, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one document was returned.\")\n",
    "        assert len(results) > 0, \"Basic retrieval query returned no results.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 1 PASSED. The database is online and responsive.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 1 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def deep_hierarchy_test(db, toc):\n",
    "    print_header(\"Test 2: Deep Hierarchy Retrieval\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Verify that the multi-level hierarchical metadata was ingested correctly.\")\n",
    "        logger.info(\"Strategy: Find a random, deeply nested sub-section and use a precise filter to retrieve it.\")\n",
    "        deep_entry_result = find_deep_entry(toc)\n",
    "        assert deep_entry_result, \"Could not find a suitable deep entry (level >= 2) to test.\"\n",
    "        node, path = deep_entry_result\n",
    "        query = node['title']\n",
    "        \n",
    "        logger.info(f\"  - Selected random deep section: {' -> '.join(path)}\")\n",
    "        conditions = [{f\"level_{i+1}_title\": {\"$eq\": title}} for i, title in enumerate(path)]\n",
    "        w_filter = {\"$and\": conditions}\n",
    "        \n",
    "        logger.info(\"Action: Performing a similarity search with a highly specific '$and' filter.\")\n",
    "        results = db.similarity_search(query, k=1, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if the precisely filtered query returned any documents.\")\n",
    "        assert len(results) > 0, \"Deeply filtered query returned no results.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 2 PASSED. Hierarchical metadata is structured correctly.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 2 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def advanced_alignment_test(db, outline, toc):\n",
    "    print_header(\"Test 3: Advanced Unit Outline Alignment\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Ensure a weekly topic from the syllabus can be mapped to the correct textbook chapter(s).\")\n",
    "        logger.info(\"Strategy: Pick a random week, find its chapter, and query for the topic filtered by that chapter.\")\n",
    "        week_to_test = random.choice(outline['weeklySchedule'])\n",
    "        logger.info(f\"  - Selected random week: Week {week_to_test['week']} - '{week_to_test['contentTopic']}'\")\n",
    "\n",
    "        reading = week_to_test.get('requiredReading', '')\n",
    "        chap_nums_str = re.findall(r'\\d+', reading)\n",
    "        assert chap_nums_str, f\"Could not find chapter numbers in required reading: '{reading}'\"\n",
    "        logger.info(f\"  - Extracted required chapter number(s): {chap_nums_str}\")\n",
    "\n",
    "        chapter_paths = [find_chapter_title_by_number(toc, int(n)) for n in chap_nums_str]\n",
    "        chapter_paths = [path for path in chapter_paths if path is not None]\n",
    "        assert chapter_paths, f\"Could not map chapter numbers {chap_nums_str} to a valid ToC path.\"\n",
    "        \n",
    "        level_1_titles = list(set([path[0] for path in chapter_paths]))\n",
    "        logger.info(f\"  - Mapped to top-level ToC entries: {level_1_titles}\")\n",
    "\n",
    "        or_filter = [{\"level_1_title\": {\"$eq\": title}} for title in level_1_titles]\n",
    "        w_filter = {\"$or\": or_filter} if len(or_filter) > 1 else or_filter[0]\n",
    "        query = week_to_test['contentTopic']\n",
    "        \n",
    "        logger.info(\"Action: Searching for the weekly topic, filtered by the mapped chapter(s).\")\n",
    "        results = db.similarity_search(query, k=5, filter=w_filter)\n",
    "        \n",
    "        print_results(query, results, w_filter) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        logger.info(\"Verification: Check if at least one returned document is from the correct chapter.\")\n",
    "        assert len(results) > 0, \"Alignment query returned no results for the correct section/chapter.\"\n",
    "        \n",
    "        logger.info(\"✅ Result: TEST 3 PASSED. The syllabus can be reliably aligned with the textbook content.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 3 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "def content_sequence_test(db, outline):\n",
    "    print_header(\"Test 4: Content Sequence Verification\", char=\"-\")\n",
    "    try:\n",
    "        logger.info(\"Goal: Confirm that chunks for a topic can be re-ordered to form a coherent narrative.\")\n",
    "        logger.info(\"Strategy: Retrieve several chunks for a random topic and verify their 'chunk_id' is sequential.\")\n",
    "        topic_query = random.choice(outline['weeklySchedule'])['contentTopic']\n",
    "        \n",
    "        logger.info(f\"Action: Performing similarity search for topic: '{topic_query}' to get a set of chunks.\")\n",
    "        results = db.similarity_search(topic_query, k=10)\n",
    "        \n",
    "        print_results(topic_query, results) # <--- SHOW THE EVIDENCE\n",
    "        \n",
    "        docs_with_id = [doc for doc in results if 'chunk_id' in doc.metadata]\n",
    "        assert len(docs_with_id) > 3, \"Fewer than 4 retrieved chunks have a 'chunk_id' to test.\"\n",
    "        \n",
    "        chunk_ids = [doc.metadata['chunk_id'] for doc in docs_with_id]\n",
    "        sorted_ids = sorted(chunk_ids)\n",
    "        \n",
    "        logger.info(f\"  - Retrieved and sorted chunk IDs: {sorted_ids}\")\n",
    "        logger.info(\"Verification: Check if the sorted list of chunk_ids is strictly increasing.\")\n",
    "        is_ordered = all(sorted_ids[i] >= sorted_ids[i-1] for i in range(1, len(sorted_ids)))\n",
    "        assert is_ordered, \"The retrieved chunks' chunk_ids are not in ascending order when sorted.\"\n",
    "\n",
    "        logger.info(\"✅ Result: TEST 4 PASSED. Narrative order can be reconstructed using 'chunk_id'.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Result: TEST 4 FAILED. Reason: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- MAIN VERIFICATION EXECUTION ---\n",
    "def run_verification():\n",
    "    print_header(\"Database Verification Process\")\n",
    "    \n",
    "    if not langchain_available:\n",
    "        logger.error(\"LangChain libraries not found. Aborting tests.\")\n",
    "        return\n",
    "\n",
    "    required_files = {\n",
    "        \"Chroma DB\": CHROMA_PERSIST_DIR,\n",
    "        \"ToC JSON\": PRE_EXTRACTED_TOC_JSON_PATH,\n",
    "        \"Parsed Outline\": PARSED_UO_JSON_PATH\n",
    "    }\n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            logger.error(f\"Required '{name}' not found at '{path}'. Please run previous cells.\")\n",
    "            return\n",
    "\n",
    "    with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        toc_data = json.load(f)\n",
    "    with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "        unit_outline_data = json.load(f)\n",
    "\n",
    "    logger.info(\"Connecting to DB and initializing components...\")\n",
    "    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "    vector_store = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIR,\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=CHROMA_COLLECTION_NAME\n",
    "    )\n",
    "    \n",
    "    results_summary = [\n",
    "        basic_retrieval_test(vector_store, unit_outline_data),\n",
    "        deep_hierarchy_test(vector_store, toc_data),\n",
    "        advanced_alignment_test(vector_store, unit_outline_data, toc_data),\n",
    "        content_sequence_test(vector_store, unit_outline_data)\n",
    "    ]\n",
    "\n",
    "    passed_count = sum(filter(None, results_summary))\n",
    "    failed_count = len(results_summary) - passed_count\n",
    "    \n",
    "    print_header(\"Verification Summary\")\n",
    "    print(f\"Total Tests Run: {len(results_summary)}\")\n",
    "    print(f\"✅ Passed: {passed_count}\")\n",
    "    print(f\"❌ Failed: {failed_count}\")\n",
    "    print_header(\"Verification Complete\", char=\"=\")\n",
    "\n",
    "# --- Execute Verification ---\n",
    "# Assumes global variables from Cell 1 are available in the notebook's scope\n",
    "run_verification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97368b0",
   "metadata": {},
   "source": [
    "#  Content Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae477bc",
   "metadata": {},
   "source": [
    "## Planning Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6baadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: The Data-Driven Planning Agent (Final Hierarchical Version✅)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Setup Logger and LangChain components\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    from langchain_chroma import Chroma\n",
    "    from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "    langchain_available = True\n",
    "except ImportError:\n",
    "    langchain_available = False\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "class PlanningAgent:\n",
    "    \"\"\"\n",
    "    An agent that creates a hierarchical content plan, adaptively partitions content\n",
    "    into distinct lecture decks, and allocates presentation time.\n",
    "    \"\"\"\n",
    "    def __init__(self, master_config: Dict, vector_store: Optional[Any] = None):\n",
    "        self.config = master_config['processed_settings']\n",
    "        self.unit_outline = master_config['unit_outline']\n",
    "        self.book_toc = master_config['book_toc']\n",
    "        self.flat_toc_with_ids = self._create_flat_toc_with_ids()\n",
    "        self.vector_store = vector_store\n",
    "        logger.info(\"Data-Driven PlanningAgent initialized successfully.\")\n",
    "\n",
    "    def _create_flat_toc_with_ids(self) -> List[Dict]:\n",
    "        \"\"\"Creates a flattened list of the ToC for easy metadata lookup.\"\"\"\n",
    "        flat_list = []\n",
    "        def flatten_recursive(nodes, counter):\n",
    "            for node in nodes:\n",
    "                node_id = counter[0]; counter[0] += 1\n",
    "                flat_list.append({'toc_id': node_id, 'title': node.get('title', ''), 'node': node})\n",
    "                if node.get('children'):\n",
    "                    flatten_recursive(node.get('children'), counter)\n",
    "        flatten_recursive(self.book_toc, [0])\n",
    "        return flat_list\n",
    "\n",
    "    def _identify_relevant_chapters(self, weekly_schedule_item: Dict) -> List[int]:\n",
    "        \"\"\"Extracts chapter numbers precisely from the 'requiredReading' string.\"\"\"\n",
    "        reading_str = weekly_schedule_item.get('requiredReading', '')\n",
    "        match = re.search(r'Chapter(s)?', reading_str, re.IGNORECASE)\n",
    "        if not match: return []\n",
    "        search_area = reading_str[match.start():]\n",
    "        chap_nums_str = re.findall(r'\\d+', search_area)\n",
    "        if chap_nums_str:\n",
    "            return sorted(list(set(int(n) for n in chap_nums_str)))\n",
    "        return []\n",
    "\n",
    "    def _find_chapter_node(self, chapter_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Finds the ToC node for a specific chapter number.\"\"\"\n",
    "        for item in self.flat_toc_with_ids:\n",
    "            if re.match(rf\"Chapter\\s{chapter_number}(?:\\D|$)\", item['title']):\n",
    "                return item['node']\n",
    "        return None\n",
    "\n",
    "    def _build_topic_plan_tree(self, toc_node: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Recursively builds a hierarchical plan tree from any ToC node,\n",
    "        annotating it with direct and total branch chunk counts.\n",
    "        \"\"\"\n",
    "        node_metadata = next((item for item in self.flat_toc_with_ids if item['node'] is toc_node), None)\n",
    "        if not node_metadata: return {}\n",
    "\n",
    "        retrieved_docs = self.vector_store.get(where={'toc_id': node_metadata['toc_id']})\n",
    "        direct_chunk_count = len(retrieved_docs.get('ids', []))\n",
    "\n",
    "        plan_node = {\n",
    "            \"title\": node_metadata['title'],\n",
    "            \"toc_id\": node_metadata['toc_id'],\n",
    "            \"chunk_count\": direct_chunk_count,\n",
    "            \"total_chunks_in_branch\": 0,\n",
    "            \"slides_allocated\": 0,\n",
    "            \"children\": []\n",
    "        }\n",
    "\n",
    "        child_branch_total = 0\n",
    "        for child_node in toc_node.get('children', []):\n",
    "            if any(ex in child_node.get('title', '').lower() for ex in [\"review\", \"introduction\", \"summary\", \"key terms\"]):\n",
    "                continue\n",
    "            child_plan_node = self._build_topic_plan_tree(child_node)\n",
    "            if child_plan_node:\n",
    "                plan_node['children'].append(child_plan_node)\n",
    "                child_branch_total += child_plan_node.get('total_chunks_in_branch', 0)\n",
    "        \n",
    "        plan_node['total_chunks_in_branch'] = direct_chunk_count + child_branch_total\n",
    "        return plan_node\n",
    "    \n",
    "    # In PlanningAgent Class...\n",
    "\n",
    "    def _allocate_slides_to_tree(self, plan_tree: Dict, content_slides_budget: int):\n",
    "        \"\"\"\n",
    "        (REFACTORED) Performs a multi-pass process to allocate content slides,\n",
    "        add interactive activities, and sum totals correctly.\n",
    "        \"\"\"\n",
    "        if not plan_tree or content_slides_budget <= 0:\n",
    "            return plan_tree\n",
    "\n",
    "        # --- Pass 1: Allocate Content Slides (Top-Down, Proportional) ---\n",
    "        def allocate_content_recursively(node, budget):\n",
    "            node['slides_allocated'] = 0\n",
    "            \n",
    "            # If it's a leaf node, it gets the remaining budget.\n",
    "            if not node.get('children'):\n",
    "                node['slides_allocated'] = round(budget)\n",
    "                return\n",
    "\n",
    "            # If it has children, distribute the budget proportionally.\n",
    "            total_branch_chunks = node.get('total_chunks_in_branch', 0)\n",
    "            \n",
    "            # Allocate slides for the node's own content (if any).\n",
    "            # This is a key fix: parent nodes can have their own content.\n",
    "            own_content_slides = 0\n",
    "            if total_branch_chunks > 0:\n",
    "                own_content_slides = round(budget * (node.get('chunk_count', 0) / total_branch_chunks))\n",
    "            node['slides_allocated'] = own_content_slides\n",
    "            \n",
    "            remaining_budget_for_children = budget - own_content_slides\n",
    "\n",
    "            # Distribute remaining budget to children.\n",
    "            for child in node.get('children', []):\n",
    "                child_budget = 0\n",
    "                if total_branch_chunks > 0:\n",
    "                    # Distribute based on the child's total branch size, not just its own chunks.\n",
    "                    child_budget = remaining_budget_for_children * (child.get('total_chunks_in_branch', 0) / (total_branch_chunks - node.get('chunk_count', 0)))\n",
    "                allocate_content_recursively(child, child_budget)\n",
    "\n",
    "        allocate_content_recursively(plan_tree, content_slides_budget)\n",
    "\n",
    "        # --- Pass 2: Add Interactive Activities (Targeted Depth) ---\n",
    "        def add_interactive_nodes(node, depth, interactive_deep):\n",
    "            if not node: return\n",
    "\n",
    "            # Logic for interactive_deep: true\n",
    "            if interactive_deep:\n",
    "                if depth == 2:\n",
    "                    node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Deep-Dive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "                if depth == 1:\n",
    "                    node['interactive_activity'] = {\"title\": f\"{node.get('title')} (General Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "            # Logic for interactive_deep: false\n",
    "            else:\n",
    "                if depth == 1:\n",
    "                    node['interactive_activity'] = {\"title\": f\"{node.get('title')} (Interactive Activity)\", \"toc_id\": node.get('toc_id'), \"slides_allocated\": 1}\n",
    "\n",
    "            # Recurse\n",
    "            for child in node.get('children', []):\n",
    "                add_interactive_nodes(child, depth + 1, interactive_deep)\n",
    "\n",
    "        if self.config.get('interactive', False):\n",
    "            interactive_deep = self.config.get('interactive_deep', False)\n",
    "            logger.info(f\"Interactive mode ON. Deep interaction: {interactive_deep}. Adding placeholders...\")\n",
    "            # Start depth at 1 for the root nodes of the plan.\n",
    "            add_interactive_nodes(plan_tree, 1, interactive_deep)\n",
    "\n",
    "        # --- Pass 3: Sum All Slides (Content + Interactive) Up the Tree ---\n",
    "        def sum_slides_upwards(node):\n",
    "            # Start with the node's own allocated content slides.\n",
    "            total_slides = node.get('slides_allocated', 0)\n",
    "            \n",
    "            # Add slides from its interactive activity, if it exists.\n",
    "            total_slides += node.get('interactive_activity', {}).get('slides_allocated', 0)\n",
    "            \n",
    "            # Add the summed totals from all its children.\n",
    "            if node.get('children'):\n",
    "                total_slides += sum(sum_slides_upwards(child) for child in node.get('children', []))\n",
    "            \n",
    "            # The final 'slides_allocated' is the grand total for the branch.\n",
    "            node['slides_allocated'] = total_slides\n",
    "            return total_slides\n",
    "\n",
    "        sum_slides_upwards(plan_tree)\n",
    "        \n",
    "        return plan_tree\n",
    "\n",
    "    def create_content_plan_for_week(self, week_number: int) -> Optional[Dict]:\n",
    "        \"\"\"Orchestrates the adaptive planning and partitioning process.\"\"\"\n",
    "        print_header(f\"Planning Week {week_number}\", char=\"*\")\n",
    "        \n",
    "        weekly_schedule_item = self.unit_outline['weeklySchedule'][week_number - 1]\n",
    "        chapter_numbers = self._identify_relevant_chapters(weekly_schedule_item)\n",
    "        if not chapter_numbers: return None\n",
    "\n",
    "        num_decks = self.config['week_session_setup'].get('sessions_per_week', 1)\n",
    "        \n",
    "        # 1. Build a full plan tree for each chapter to get its weight.\n",
    "        chapter_plan_trees = [self._build_topic_plan_tree(self._find_chapter_node(cn)) for cn in chapter_numbers if self._find_chapter_node(cn)]\n",
    "        total_weekly_chunks = sum(tree.get('total_chunks_in_branch', 0) for tree in chapter_plan_trees)\n",
    "\n",
    "        # 2. NEW: Adaptive Partitioning Strategy\n",
    "        partitionable_units = []\n",
    "        all_top_level_sections = []\n",
    "        for chapter_tree in chapter_plan_trees:\n",
    "            all_top_level_sections.extend(chapter_tree.get('children', []))\n",
    "\n",
    "        num_top_level_sections = len(all_top_level_sections)\n",
    "\n",
    "        # Always prefer to split by top-level sections if there are enough to distribute.\n",
    "        if num_top_level_sections >= num_decks:\n",
    "            logger.info(f\"Partitioning strategy: Distributing {num_top_level_sections} top-level sections across {num_decks} decks.\")\n",
    "            partitionable_units = all_top_level_sections\n",
    "        else:\n",
    "            # Fallback for rare cases where there are fewer topics than decks (e.g., 1 chapter with 1 section, but 2 decks).\n",
    "            logger.info(f\"Partitioning strategy: Not enough top-level sections ({num_top_level_sections}) to fill all decks ({num_decks}). Distributing whole chapters instead.\")\n",
    "            partitionable_units = chapter_plan_trees\n",
    "        \n",
    "        # 3. Partition the chosen units into decks using a bin-packing algorithm\n",
    "        decks = [[] for _ in range(num_decks)]\n",
    "        deck_weights = [0] * num_decks\n",
    "        sorted_units = sorted(partitionable_units, key=lambda x: x.get('total_chunks_in_branch', 0), reverse=True)\n",
    "        \n",
    "        for unit in sorted_units:\n",
    "            lightest_deck_index = deck_weights.index(min(deck_weights))\n",
    "            decks[lightest_deck_index].append(unit)\n",
    "            deck_weights[lightest_deck_index] += unit.get('total_chunks_in_branch', 0)\n",
    "\n",
    "        # 4. Plan each deck\n",
    "        content_slides_per_week = self.config['slide_count_strategy'].get('target', 25)\n",
    "        final_deck_plans = []\n",
    "        for i, deck_content_trees in enumerate(decks):\n",
    "            deck_number = i + 1\n",
    "            deck_chunk_weight = sum(tree.get('total_chunks_in_branch', 0) for tree in deck_content_trees)\n",
    "            deck_slide_budget = round((deck_chunk_weight / total_weekly_chunks) * content_slides_per_week) if total_weekly_chunks > 0 else 0\n",
    "\n",
    "            logger.info(f\"--- Planning Deck {deck_number}/{num_decks} | Topics: {[t['title'] for t in deck_content_trees]} | Weight: {deck_chunk_weight} chunks | Slide Budget: {deck_slide_budget} ---\")\n",
    "            \n",
    "            # The allocation function is recursive and works on any tree or sub-tree\n",
    "            planned_content = [self._allocate_slides_to_tree(tree, round(deck_slide_budget * (tree.get('total_chunks_in_branch', 0) / deck_chunk_weight))) if deck_chunk_weight > 0 else tree for tree in deck_content_trees]\n",
    "            \n",
    "            final_deck_plans.append({\n",
    "                \"deck_number\": deck_number,\n",
    "                \"deck_title\": f\"{self.config.get('unit_name', 'Course')} - Week {week_number}, Lecture {deck_number}\",\n",
    "                \"session_content\": planned_content\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"week\": week_number,\n",
    "            \"overall_topic\": weekly_schedule_item.get('contentTopic'),\n",
    "            \"deck_plans\": final_deck_plans\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8438e",
   "metadata": {},
   "source": [
    "## Content Generator Class (no yet addressed focus planning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d8be7",
   "metadata": {},
   "source": [
    "## Orquestrator (Addressing paint points )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459d53b",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "The main script that iterates through the weeks defined the plan and generate the content base on the settings_deck coordinating the agents.\n",
    "\n",
    "\n",
    "\n",
    "**Parameters and concideration**\n",
    "- 1 hour in the setting session_time_duration_in_hour - is 18-20 slides at the time so it is require to calculate this according to the given value but this also means per session so sessions_per_week is a multiplicator factor that   \n",
    "- if apply_topic_interactive is available will add an extra slide and add extra 5 min time but to determine this is required to plan all the content first and then calculate then provide a extra time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea092bd6",
   "metadata": {},
   "source": [
    "settings_deck.json\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"interactive_slides_per_week\": 0 -- > sum all interactive counts \n",
    "    \"interactive_slides_per_session\": 0, -- > Total # of slides produced if \"interactive\" is true other wise remains 0\n",
    "    \"target_total_slides\": 0, --> Total Content Slides per week that cover the total - will be the target in the cell 7    \n",
    "    \"slides_content_per_session\": 0, --> Total # (target_total_slides/sessions_per_week)\n",
    "    \"total_slides_deck_week\": 0, --> target_total_slides + interactive_slides_per_week + (framework (4 + Time for Title, Agenda, Summary, End) * sessions_per_week)\n",
    "    \"Tota_slides_session\": 0 --> content_slides_per_session + interactive_slides_per_session + framework (4 + Time for Title, Agenda, Summary, End)\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"interactive_time_in_hour\": 0, --> find the value in ahours of the total # (\"interactive_slides\" * \"TIME_PER_INTERACTIVE_SLIDE_MINS\")/60    \n",
    "    \"total_session_time_in_hours\": 0 --> this is going to  be egual or similar to session_time_duration_in_hour if \"interactive\" is false obvisuly base on the global varaibles it will be the calculation of \"interactive_time_in_hour\"\n",
    "    \"session_time_duration_in_hour\": 2, --- > this is the time that the costumer need for delivery this is a constrain is not modified never is used for reference\n",
    "  },\n",
    "\n",
    "   \"parameters_slides\": { \n",
    "   \"slides_per_hour\": 18, # no framework include\n",
    "   \"time_per_content_slides_min\": 3, # average delivery per slide\n",
    "   \"time_per_interactive_slide_min\": 5, #small break and engaging with the students\n",
    "   \"time_for_framework_slides_min\": 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "   \"\"\n",
    "  }, \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [6]\n",
    "  },\n",
    "  \"teaching_flow_id\": \"Interactive Lecture Flow\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82e170",
   "metadata": {},
   "source": [
    "teaching_flows.json\n",
    "\n",
    "{\n",
    "  \"standard_lecture\": {\n",
    "    \"name\": \"Standard Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer. Your audience is undergraduate students. Based on the following context, create a slide that provides a detailed explanation of the topic '{sub_topic}'. The content should be structured with bullet points for key details. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of topics covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  },\n",
    "  \"apply_topic_interactive\": {\n",
    "    \"name\": \"Interactive Lecture Flow\",\n",
    "    \"slide_types\": [\"Title\", \"Agenda\", \"Content\", \"Application\", \"Summary\", \"End\"],\n",
    "    \"prompts\": {\n",
    "      \"content_generation\": \"You are an expert university lecturer in Digital Forensics. Your audience is undergraduate students. Based on the provided context, create a slide explaining the concept of '{sub_topic}'. The content should be clear, concise, and structured with bullet points for easy understanding. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"application_generation\": \"You are an engaging university lecturer creating an interactive slide. Based on the concept of '{sub_topic}', create a multiple-choice question with exactly 4 options (A, B, C, D) to test understanding. The slide title must be 'Let's Apply This:'. Clearly indicate the correct answer within the content. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\",\n",
    "      \"summary_generation\": \"You are an expert university lecturer creating a summary slide. Based on the following list of concepts and applications covered in this session, generate a concise summary of the key takeaways. The topics are: {topic_list}. Your output MUST be a single JSON object with a 'title' (string) and 'content' (list of strings) key.\"\n",
    "    },\n",
    "    \"slide_schemas\": {\n",
    "      \"Content\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Application\": {\"title\": \"string\", \"content\": \"list[string]\"},\n",
    "      \"Summary\": {\"title\": \"string\", \"content\": \"list[string]\"}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a04010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:49:28,694 - INFO - Loading all necessary configuration and data files...\n",
      "2025-07-06 00:49:28,698 - INFO - All files loaded successfully.\n",
      "2025-07-06 00:49:28,699 - INFO - Pre-processing settings_deck for definitive plan...\n",
      "2025-07-06 00:49:28,699 - INFO - Applying overrides if specified...\n",
      "2025-07-06 00:49:28,700 - INFO - Loaded from settings: 'interactive' is True. Set teaching_flow_id to 'apply_topic_interactive'.\n",
      "2025-07-06 00:49:28,700 - INFO - Calculating preliminary slide budget based on session time...\n",
      "2025-07-06 00:49:28,701 - INFO - Preliminary weekly content slide target calculated: 36 slides.\n",
      "2025-07-06 00:49:28,701 - INFO - Saving preliminary processed configuration to: /home/sebas_dev_linux/projects/course_generator/configs/processed_settings.json\n",
      "2025-07-06 00:49:28,703 - INFO - File saved successfully.\n",
      "2025-07-06 00:49:28,704 - INFO - Master configuration object is ready for the Planning Agent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                   Phase 1: Configuration and Scoping Process                   \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                         Phase 1 Configuration Complete                         \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Preview of Processed Settings (Phase 1) ---\n",
      "{\n",
      "  \"course_id\": \"ICT312\",\n",
      "  \"generation_scope\": {\n",
      "    \"weeks\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  \"interactive\": true,\n",
      "  \"interactive_deep\": false,\n",
      "  \"parameters_slides\": {\n",
      "    \"slides_per_hour\": 18,\n",
      "    \"time_for_framework_slides_min\": 6,\n",
      "    \"time_per_content_slides_min\": 3,\n",
      "    \"time_per_interactive_slide_min\": 5\n",
      "  },\n",
      "  \"slide_count_strategy\": {\n",
      "    \"interactive_slides_per_session\": 0,\n",
      "    \"interactive_slides_per_week\": 0,\n",
      "    \"method\": \"per_week\",\n",
      "    \"slides_content_per_session\": 36,\n",
      "    \"target_total_slides\": 36,\n",
      "    \"total_slides_deck_week\": 0,\n",
      "    \"total_slides_session\": 0\n",
      "  },\n",
      "  \"teaching_flow_id\": \"apply_topic_interactive\",\n",
      "  \"unit_name\": \"Digital Forensic\",\n",
      "  \"week_session_setup\": {\n",
      "    \"distribution_strategy\": \"even\",\n",
      "    \"interactive_time_in_hour\": 0,\n",
      "    \"session_time_duration_in_hour\": 2,\n",
      "    \"sessions_per_week\": 1,\n",
      "    \"total_session_time_in_hours\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "Number of weeks to generate: 1\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Configuration and Scoping for Content Generation (Corrected)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Setup Logger for this cell\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. DEFINE FILE PATHS AND GLOBAL TEST SETTINGS ---\n",
    "# Assumes these variables are loaded from a previous setup cell (like Cell 1)\n",
    "# PROJECT_BASE_DIR, PARSED_UO_JSON_PATH, PRE_EXTRACTED_TOC_JSON_PATH must be defined.\n",
    "\n",
    "# New configuration file paths\n",
    "CONFIG_DIR = os.path.join(PROJECT_BASE_DIR, \"configs\")\n",
    "SETTINGS_DECK_PATH = os.path.join(CONFIG_DIR, \"settings_deck.json\")\n",
    "TEACHING_FLOWS_PATH = os.path.join(CONFIG_DIR, \"teaching_flows.json\")\n",
    "\n",
    "# New output path for the processed settings\n",
    "PROCESSED_SETTINGS_PATH = os.path.join(CONFIG_DIR, \"processed_settings.json\")\n",
    "\n",
    "# --- Global Test Overrides (for easy testing) ---\n",
    "TEST_OVERRIDE_WEEKS = None\n",
    "TEST_OVERRIDE_FLOW_ID = None\n",
    "TEST_OVERRIDE_SESSIONS_PER_WEEK = None\n",
    "TEST_OVERRIDE_DISTRIBUTION_STRATEGY = None\n",
    "\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def process_and_load_configurations():\n",
    "    \"\"\"\n",
    "    PHASE 1: Loads configurations, calculates a PRELIMINARY time-based slide budget,\n",
    "    and saves the result as 'processed_settings.json' for the Planning Agent.\n",
    "    \"\"\"\n",
    "    print_header(\"Phase 1: Configuration and Scoping Process\", char=\"-\")\n",
    "    \n",
    "    # --- Load all input files ---\n",
    "    logger.info(\"Loading all necessary configuration and data files...\")\n",
    "    try:\n",
    "        os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r', encoding='utf-8') as f: unit_outline = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r', encoding='utf-8') as f: book_toc = json.load(f)\n",
    "        with open(SETTINGS_DECK_PATH, 'r', encoding='utf-8') as f: settings_deck = json.load(f)\n",
    "        with open(TEACHING_FLOWS_PATH, 'r', encoding='utf-8') as f: teaching_flows = json.load(f)\n",
    "        logger.info(\"All files loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"FATAL: A required configuration file was not found: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Pre-process and Refine Settings ---\n",
    "    logger.info(\"Pre-processing settings_deck for definitive plan...\")\n",
    "    processed_settings = json.loads(json.dumps(settings_deck))\n",
    "\n",
    "    unit_info = unit_outline.get(\"unitInformation\", {})\n",
    "    processed_settings['course_id'] = unit_info.get(\"unitCode\", \"UNKNOWN_COURSE\")\n",
    "    processed_settings['unit_name'] = unit_info.get(\"unitName\", \"Unknown Unit Name\")\n",
    "    \n",
    "    # --- Apply test overrides IF they are not None ---\n",
    "    logger.info(\"Applying overrides if specified...\")\n",
    "    # This block now correctly sets the teaching_flow_id based on the interactive flag.\n",
    "    if TEST_OVERRIDE_FLOW_ID is not None:\n",
    "        processed_settings['teaching_flow_id'] = TEST_OVERRIDE_FLOW_ID\n",
    "        logger.info(f\"OVERRIDE: teaching_flow_id set to '{TEST_OVERRIDE_FLOW_ID}'\")\n",
    "    else:\n",
    "        # If no override, use the 'interactive' boolean from the file as the source of truth.\n",
    "        is_interactive = processed_settings.get('interactive', False)\n",
    "        if is_interactive:\n",
    "            processed_settings['teaching_flow_id'] = 'apply_topic_interactive'\n",
    "        else:\n",
    "            processed_settings['teaching_flow_id'] = 'standard_lecture'\n",
    "        logger.info(f\"Loaded from settings: 'interactive' is {is_interactive}. Set teaching_flow_id to '{processed_settings['teaching_flow_id']}'.\")\n",
    "\n",
    "    # The 'interactive' flag is now always consistent with the teaching_flow_id.\n",
    "    processed_settings['interactive'] = \"interactive\" in processed_settings['teaching_flow_id'].lower()\n",
    "    \n",
    "    if TEST_OVERRIDE_SESSIONS_PER_WEEK is not None:\n",
    "        processed_settings['week_session_setup']['sessions_per_week'] = TEST_OVERRIDE_SESSIONS_PER_WEEK\n",
    "        logger.info(f\"OVERRIDE: sessions_per_week set to {TEST_OVERRIDE_SESSIONS_PER_WEEK}\")\n",
    "\n",
    "    if TEST_OVERRIDE_DISTRIBUTION_STRATEGY is not None:\n",
    "        processed_settings['week_session_setup']['distribution_strategy'] = TEST_OVERRIDE_DISTRIBUTION_STRATEGY\n",
    "        logger.info(f\"OVERRIDE: distribution_strategy set to '{TEST_OVERRIDE_DISTRIBUTION_STRATEGY}'\")\n",
    "\n",
    "    if TEST_OVERRIDE_WEEKS is not None:\n",
    "        processed_settings['generation_scope']['weeks'] = TEST_OVERRIDE_WEEKS\n",
    "        logger.info(f\"OVERRIDE: generation_scope weeks set to {TEST_OVERRIDE_WEEKS}\")\n",
    "\n",
    "    # --- DYNAMIC SLIDE BUDGET CALCULATION (Phase 1) ---\n",
    "    logger.info(\"Calculating preliminary slide budget based on session time...\")\n",
    "    \n",
    "    params = processed_settings.get('parameters_slides', {})\n",
    "    SLIDES_PER_HOUR = params.get('slides_per_hour', 18)\n",
    "    \n",
    "    duration_hours = processed_settings['week_session_setup'].get('session_time_duration_in_hour', 1.0)\n",
    "    sessions_per_week = processed_settings['week_session_setup'].get('sessions_per_week', 1)\n",
    "    \n",
    "    slides_content_per_session = int(duration_hours * SLIDES_PER_HOUR)\n",
    "    target_total_slides = slides_content_per_session * sessions_per_week\n",
    "    \n",
    "    processed_settings['slide_count_strategy']['target_total_slides'] = target_total_slides\n",
    "    processed_settings['slide_count_strategy']['slides_content_per_session'] = slides_content_per_session\n",
    "    logger.info(f\"Preliminary weekly content slide target calculated: {target_total_slides} slides.\")\n",
    "    \n",
    "    # --- Resolve Generation Scope if not overridden ---\n",
    "    if TEST_OVERRIDE_WEEKS is None and processed_settings.get('generation_scope', {}).get('weeks') == \"all\":\n",
    "        num_weeks = len(unit_outline.get('weeklySchedule', []))\n",
    "        processed_settings['generation_scope']['weeks'] = list(range(1, num_weeks + 1))\n",
    "    \n",
    "    # --- Save the processed settings to disk ---\n",
    "    logger.info(f\"Saving preliminary processed configuration to: {PROCESSED_SETTINGS_PATH}\")\n",
    "    with open(PROCESSED_SETTINGS_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_settings, f, indent=2)\n",
    "    logger.info(\"File saved successfully.\")\n",
    "\n",
    "    # --- Assemble master config for optional preview ---\n",
    "    master_config = {\n",
    "        \"processed_settings\": processed_settings,\n",
    "        \"unit_outline\": unit_outline,\n",
    "        \"book_toc\": book_toc,\n",
    "        \"teaching_flows\": teaching_flows\n",
    "    }\n",
    "    \n",
    "    print_header(\"Phase 1 Configuration Complete\", char=\"-\")\n",
    "    logger.info(\"Master configuration object is ready for the Planning Agent.\")\n",
    "    return master_config\n",
    "\n",
    "# --- EXECUTE THE CONFIGURATION PROCESS ---\n",
    "master_config = process_and_load_configurations()\n",
    "\n",
    "# Optional: Print a preview to verify the output\n",
    "if master_config:\n",
    "    print(\"\\n--- Preview of Processed Settings (Phase 1) ---\")\n",
    "    print(json.dumps(master_config['processed_settings'], indent=2, sort_keys=True))\n",
    "    if master_config.get('processed_settings', {}).get('generation_scope', {}).get('weeks'):\n",
    "        print(f\"\\nNumber of weeks to generate: {len(master_config['processed_settings']['generation_scope']['weeks'])}\")\n",
    "    print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd8fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:49:28,715 - INFO - --- Initializing Data-Driven Planning Agent Test ---\n",
      "2025-07-06 00:49:28,716 - INFO - Connecting to ChromaDB for the Planning Agent...\n",
      "2025-07-06 00:49:28,731 - INFO - Database connection successful.\n",
      "2025-07-06 00:49:28,733 - INFO - Loading configuration files for Planning Agent...\n",
      "2025-07-06 00:49:28,735 - INFO - Configuration files loaded.\n",
      "2025-07-06 00:49:28,736 - INFO - Data-Driven PlanningAgent initialized successfully.\n",
      "2025-07-06 00:49:28,737 - INFO - Found 1 week(s) to plan: [1]\n",
      "2025-07-06 00:49:28,737 - INFO - --> Generating draft plan for Week 1\n",
      "2025-07-06 00:49:28,842 - INFO - Partitioning strategy: Distributing 7 top-level sections across 1 decks.\n",
      "2025-07-06 00:49:28,843 - INFO - --- Planning Deck 1/1 | Topics: ['Procedures for Private-Sector High-Tech Investigations', 'Conducting an Investigation', 'Preparing a Digital Forensics Investigation', 'Preparing for Digital Investigations', 'An Overview of Digital Forensics', 'Understanding Data Recovery Workstations and Software', 'Maintaining Professional Conduct'] | Weight: 521 chunks | Slide Budget: 25 ---\n",
      "2025-07-06 00:49:28,844 - INFO - Interactive mode ON. Deep interaction: False. Adding placeholders...\n",
      "2025-07-06 00:49:28,845 - INFO - Interactive mode ON. Deep interaction: False. Adding placeholders...\n",
      "2025-07-06 00:49:28,846 - INFO - Interactive mode ON. Deep interaction: False. Adding placeholders...\n",
      "2025-07-06 00:49:28,847 - INFO - Interactive mode ON. Deep interaction: False. Adding placeholders...\n",
      "2025-07-06 00:49:28,848 - INFO - Interactive mode ON. Deep interaction: False. Adding placeholders...\n",
      "2025-07-06 00:49:28,849 - INFO - Interactive mode ON. Deep interaction: False. Adding placeholders...\n",
      "2025-07-06 00:49:28,852 - INFO - \n",
      "Successfully saved DRAFT content plan for Week 1 to: /home/sebas_dev_linux/projects/course_generator/generated_plans/ICT312_Week1_plan_draft.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                                Planning Week 1                                 \n",
      "********************************************************************************\n",
      "\n",
      "--- Generated Draft Plan for Week 1 ---\n",
      "{\n",
      "  \"week\": 1,\n",
      "  \"overall_topic\": \"Understanding the Digital Forensics Profession and Investigations.\",\n",
      "  \"deck_plans\": [\n",
      "    {\n",
      "      \"deck_number\": 1,\n",
      "      \"deck_title\": \"Digital Forensic - Week 1, Lecture 1\",\n",
      "      \"session_content\": [\n",
      "        {\n",
      "          \"title\": \"Procedures for Private-Sector High-Tech Investigations\",\n",
      "          \"toc_id\": 31,\n",
      "          \"chunk_count\": 2,\n",
      "          \"total_chunks_in_branch\": 124,\n",
      "          \"slides_allocated\": 8,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Employee Termination Cases\",\n",
      "              \"toc_id\": 32,\n",
      "              \"chunk_count\": 2,\n",
      "              \"total_chunks_in_branch\": 2,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Internet Abuse Investigations\",\n",
      "              \"toc_id\": 33,\n",
      "              \"chunk_count\": 19,\n",
      "              \"total_chunks_in_branch\": 19,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"E-mail Abuse Investigations\",\n",
      "              \"toc_id\": 34,\n",
      "              \"chunk_count\": 16,\n",
      "              \"total_chunks_in_branch\": 16,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Attorney-Client Privilege Investigations\",\n",
      "              \"toc_id\": 35,\n",
      "              \"chunk_count\": 33,\n",
      "              \"total_chunks_in_branch\": 33,\n",
      "              \"slides_allocated\": 2,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Industrial Espionage Investigations\",\n",
      "              \"toc_id\": 36,\n",
      "              \"chunk_count\": 41,\n",
      "              \"total_chunks_in_branch\": 52,\n",
      "              \"slides_allocated\": 3,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Interviews and Interrogations in High-Tech Investigations\",\n",
      "                  \"toc_id\": 37,\n",
      "                  \"chunk_count\": 11,\n",
      "                  \"total_chunks_in_branch\": 11,\n",
      "                  \"slides_allocated\": 1,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"interactive_activity\": {\n",
      "            \"title\": \"Procedures for Private-Sector High-Tech Investigations (Interactive Activity)\",\n",
      "            \"toc_id\": 31,\n",
      "            \"slides_allocated\": 1\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Conducting an Investigation\",\n",
      "          \"toc_id\": 40,\n",
      "          \"chunk_count\": 8,\n",
      "          \"total_chunks_in_branch\": 109,\n",
      "          \"slides_allocated\": 5,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Gathering the Evidence\",\n",
      "              \"toc_id\": 41,\n",
      "              \"chunk_count\": 14,\n",
      "              \"total_chunks_in_branch\": 14,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Bit-stream Copies\",\n",
      "              \"toc_id\": 42,\n",
      "              \"chunk_count\": 6,\n",
      "              \"total_chunks_in_branch\": 8,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Acquiring an Image of Evidence Media\",\n",
      "                  \"toc_id\": 43,\n",
      "                  \"chunk_count\": 2,\n",
      "                  \"total_chunks_in_branch\": 2,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Analyzing Your Digital Evidence\",\n",
      "              \"toc_id\": 44,\n",
      "              \"chunk_count\": 44,\n",
      "              \"total_chunks_in_branch\": 48,\n",
      "              \"slides_allocated\": 2,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Some Additional Features of Autopsy\",\n",
      "                  \"toc_id\": 45,\n",
      "                  \"chunk_count\": 4,\n",
      "                  \"total_chunks_in_branch\": 4,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Completing the Case\",\n",
      "              \"toc_id\": 46,\n",
      "              \"chunk_count\": 12,\n",
      "              \"total_chunks_in_branch\": 22,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Autopsy\\u2019s Report Generator\",\n",
      "                  \"toc_id\": 47,\n",
      "                  \"chunk_count\": 10,\n",
      "                  \"total_chunks_in_branch\": 10,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Critiquing the Case\",\n",
      "              \"toc_id\": 48,\n",
      "              \"chunk_count\": 9,\n",
      "              \"total_chunks_in_branch\": 9,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ],\n",
      "          \"interactive_activity\": {\n",
      "            \"title\": \"Conducting an Investigation (Interactive Activity)\",\n",
      "            \"toc_id\": 40,\n",
      "            \"slides_allocated\": 1\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Preparing a Digital Forensics Investigation\",\n",
      "          \"toc_id\": 24,\n",
      "          \"chunk_count\": 4,\n",
      "          \"total_chunks_in_branch\": 97,\n",
      "          \"slides_allocated\": 6,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"An Overview of a Computer Crime\",\n",
      "              \"toc_id\": 25,\n",
      "              \"chunk_count\": 12,\n",
      "              \"total_chunks_in_branch\": 12,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"An Overview of a Company Policy Violation\",\n",
      "              \"toc_id\": 26,\n",
      "              \"chunk_count\": 4,\n",
      "              \"total_chunks_in_branch\": 4,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Taking a Systematic Approach\",\n",
      "              \"toc_id\": 27,\n",
      "              \"chunk_count\": 16,\n",
      "              \"total_chunks_in_branch\": 77,\n",
      "              \"slides_allocated\": 4,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Assessing the Case\",\n",
      "                  \"toc_id\": 28,\n",
      "                  \"chunk_count\": 11,\n",
      "                  \"total_chunks_in_branch\": 11,\n",
      "                  \"slides_allocated\": 1,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Planning Your Investigation\",\n",
      "                  \"toc_id\": 29,\n",
      "                  \"chunk_count\": 41,\n",
      "                  \"total_chunks_in_branch\": 41,\n",
      "                  \"slides_allocated\": 2,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Securing Your Evidence\",\n",
      "                  \"toc_id\": 30,\n",
      "                  \"chunk_count\": 9,\n",
      "                  \"total_chunks_in_branch\": 9,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"interactive_activity\": {\n",
      "            \"title\": \"Preparing a Digital Forensics Investigation (Interactive Activity)\",\n",
      "            \"toc_id\": 24,\n",
      "            \"slides_allocated\": 1\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Preparing for Digital Investigations\",\n",
      "          \"toc_id\": 14,\n",
      "          \"chunk_count\": 5,\n",
      "          \"total_chunks_in_branch\": 84,\n",
      "          \"slides_allocated\": 5,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Understanding Law Enforcement Agency Investigations\",\n",
      "              \"toc_id\": 15,\n",
      "              \"chunk_count\": 10,\n",
      "              \"total_chunks_in_branch\": 10,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Following Legal Processes\",\n",
      "              \"toc_id\": 16,\n",
      "              \"chunk_count\": 13,\n",
      "              \"total_chunks_in_branch\": 13,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Private-Sector Investigations\",\n",
      "              \"toc_id\": 17,\n",
      "              \"chunk_count\": 3,\n",
      "              \"total_chunks_in_branch\": 56,\n",
      "              \"slides_allocated\": 2,\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"title\": \"Establishing Company Policies\",\n",
      "                  \"toc_id\": 18,\n",
      "                  \"chunk_count\": 5,\n",
      "                  \"total_chunks_in_branch\": 5,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Displaying Warning Banners\",\n",
      "                  \"toc_id\": 19,\n",
      "                  \"chunk_count\": 19,\n",
      "                  \"total_chunks_in_branch\": 19,\n",
      "                  \"slides_allocated\": 1,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Designating an Authorized Requester\",\n",
      "                  \"toc_id\": 20,\n",
      "                  \"chunk_count\": 9,\n",
      "                  \"total_chunks_in_branch\": 9,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Conducting Security Investigations\",\n",
      "                  \"toc_id\": 21,\n",
      "                  \"chunk_count\": 15,\n",
      "                  \"total_chunks_in_branch\": 15,\n",
      "                  \"slides_allocated\": 1,\n",
      "                  \"children\": []\n",
      "                },\n",
      "                {\n",
      "                  \"title\": \"Distinguishing Personal and Company Property\",\n",
      "                  \"toc_id\": 22,\n",
      "                  \"chunk_count\": 5,\n",
      "                  \"total_chunks_in_branch\": 5,\n",
      "                  \"slides_allocated\": 0,\n",
      "                  \"children\": []\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"interactive_activity\": {\n",
      "            \"title\": \"Preparing for Digital Investigations (Interactive Activity)\",\n",
      "            \"toc_id\": 14,\n",
      "            \"slides_allocated\": 1\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"An Overview of Digital Forensics\",\n",
      "          \"toc_id\": 9,\n",
      "          \"chunk_count\": 18,\n",
      "          \"total_chunks_in_branch\": 60,\n",
      "          \"slides_allocated\": 4,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Digital Forensics and Other Related Disciplines\",\n",
      "              \"toc_id\": 10,\n",
      "              \"chunk_count\": 18,\n",
      "              \"total_chunks_in_branch\": 18,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"A Brief History of Digital Forensics\",\n",
      "              \"toc_id\": 11,\n",
      "              \"chunk_count\": 13,\n",
      "              \"total_chunks_in_branch\": 13,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Understanding Case Law\",\n",
      "              \"toc_id\": 12,\n",
      "              \"chunk_count\": 3,\n",
      "              \"total_chunks_in_branch\": 3,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            },\n",
      "            {\n",
      "              \"title\": \"Developing Digital Forensics Resources\",\n",
      "              \"toc_id\": 13,\n",
      "              \"chunk_count\": 8,\n",
      "              \"total_chunks_in_branch\": 8,\n",
      "              \"slides_allocated\": 0,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ],\n",
      "          \"interactive_activity\": {\n",
      "            \"title\": \"An Overview of Digital Forensics (Interactive Activity)\",\n",
      "            \"toc_id\": 9,\n",
      "            \"slides_allocated\": 1\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Understanding Data Recovery Workstations and Software\",\n",
      "          \"toc_id\": 38,\n",
      "          \"chunk_count\": 18,\n",
      "          \"total_chunks_in_branch\": 37,\n",
      "          \"slides_allocated\": 3,\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"title\": \"Setting Up Your Workstation for Digital Forensics\",\n",
      "              \"toc_id\": 39,\n",
      "              \"chunk_count\": 19,\n",
      "              \"total_chunks_in_branch\": 19,\n",
      "              \"slides_allocated\": 1,\n",
      "              \"children\": []\n",
      "            }\n",
      "          ],\n",
      "          \"interactive_activity\": {\n",
      "            \"title\": \"Understanding Data Recovery Workstations and Software (Interactive Activity)\",\n",
      "            \"toc_id\": 38,\n",
      "            \"slides_allocated\": 1\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "          \"title\": \"Maintaining Professional Conduct\",\n",
      "          \"toc_id\": 23,\n",
      "          \"chunk_count\": 10,\n",
      "          \"total_chunks_in_branch\": 10,\n",
      "          \"slides_allocated\": 0,\n",
      "          \"children\": []\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# In Cell 9, \n",
    "\n",
    "logger.info(\"--- Initializing Data-Driven Planning Agent Test ---\")\n",
    "\n",
    "if langchain_available:\n",
    "    logger.info(\"Connecting to ChromaDB for the Planning Agent...\")\n",
    "    try:\n",
    "        # 1. Connect to DB and Load all configurations\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA),\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        logger.info(\"Database connection successful.\")\n",
    "\n",
    "        logger.info(\"Loading configuration files for Planning Agent...\")\n",
    "        with open(os.path.join(CONFIG_DIR, \"processed_settings.json\"), 'r') as f:\n",
    "            processed_settings = json.load(f)\n",
    "        with open(PRE_EXTRACTED_TOC_JSON_PATH, 'r') as f:\n",
    "            book_toc = json.load(f)\n",
    "        with open(PARSED_UO_JSON_PATH, 'r') as f:\n",
    "            unit_outline = json.load(f)\n",
    "        logger.info(\"Configuration files loaded.\")\n",
    "\n",
    "        master_config_from_file = {\n",
    "            \"processed_settings\": processed_settings,\n",
    "            \"unit_outline\": unit_outline,\n",
    "            \"book_toc\": book_toc\n",
    "        }\n",
    "\n",
    "        # 2. Initialize the Planning Agent\n",
    "        planning_agent = PlanningAgent(master_config_from_file, vector_store=vector_store)\n",
    "        \n",
    "        # 3. CRITICAL: Loop through the weeks defined in the processed settings\n",
    "        weeks_to_generate = processed_settings.get('generation_scope', {}).get('weeks', [])\n",
    "        logger.info(f\"Found {len(weeks_to_generate)} week(s) to plan: {weeks_to_generate}\")\n",
    "\n",
    "        for week_to_test in weeks_to_generate:\n",
    "            logger.info(f\"--> Generating draft plan for Week {week_to_test}\")\n",
    "            content_plan = planning_agent.create_content_plan_for_week(week_to_test)\n",
    "\n",
    "            if content_plan:\n",
    "                print(f\"\\n--- Generated Draft Plan for Week {week_to_test} ---\")\n",
    "                print(json.dumps(content_plan, indent=2))\n",
    "\n",
    "                # Save the generated plan to a file\n",
    "                PLAN_OUTPUT_DIR = os.path.join(PROJECT_BASE_DIR, \"generated_plans\")\n",
    "                os.makedirs(PLAN_OUTPUT_DIR, exist_ok=True)\n",
    "                plan_filename = f\"{processed_settings.get('course_id', 'COURSE')}_Week{week_to_test}_plan_draft.json\"\n",
    "                plan_filepath = os.path.join(PLAN_OUTPUT_DIR, plan_filename)\n",
    "                with open(plan_filepath, 'w') as f:\n",
    "                    json.dump(content_plan, f, indent=2)\n",
    "                logger.info(f\"\\nSuccessfully saved DRAFT content plan for Week {week_to_test} to: {plan_filepath}\")\n",
    "            else:\n",
    "                logger.error(f\"Failed to generate content plan for Week {week_to_test}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the planning process: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.error(\"LangChain/Chroma libraries not found. Cannot run the Planning Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da053bb9",
   "metadata": {},
   "source": [
    "# test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68b49f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 00:49:28,869 - INFO - Loading draft plan and preliminary configurations...\n",
      "2025-07-06 00:49:28,870 - INFO - Loaded draft plan and settings from previous cell's memory.\n",
      "2025-07-06 00:49:28,871 - INFO - Analysis Complete: Total Content Slides: 17, Total Interactive Slides: 6\n",
      "2025-07-06 00:49:28,872 - INFO - PER SESSION Calculation: Content(108m) + Interactive(30m) + Framework(6m) = 144m\n",
      "2025-07-06 00:49:28,873 - INFO - Final Estimated Delivery Time PER SESSION: 2.4 hours\n",
      "2025-07-06 00:49:28,874 - INFO - Saving finalized settings to /home/sebas_dev_linux/projects/course_generator/configs/final_processed_settings.json\n",
      "2025-07-06 00:49:28,876 - INFO - Finalized settings saved. Ready for Content Generation stage.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "                         Main Orchestrator Initialized                          \n",
      "********************************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "                 Phase 2: Analyzing Plan and Finalizing Budget                  \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Finalized Processed Settings ---\n",
      "{\n",
      "  \"course_id\": \"ICT312\",\n",
      "  \"unit_name\": \"Digital Forensic\",\n",
      "  \"interactive\": true,\n",
      "  \"interactive_deep\": false,\n",
      "  \"teaching_flow_id\": \"apply_topic_interactive\",\n",
      "  \"parameters_slides\": {\n",
      "    \"slides_per_hour\": 18,\n",
      "    \"time_per_content_slides_min\": 3,\n",
      "    \"time_per_interactive_slide_min\": 5,\n",
      "    \"time_for_framework_slides_min\": 6\n",
      "  },\n",
      "  \"week_session_setup\": {\n",
      "    \"sessions_per_week\": 1,\n",
      "    \"distribution_strategy\": \"even\",\n",
      "    \"session_time_duration_in_hour\": 2,\n",
      "    \"interactive_time_in_hour\": 0.5,\n",
      "    \"total_session_time_in_hours\": 2.4\n",
      "  },\n",
      "  \"slide_count_strategy\": {\n",
      "    \"method\": \"per_week\",\n",
      "    \"target_total_slides\": 36,\n",
      "    \"slides_content_per_session\": 36,\n",
      "    \"interactive_slides_per_week\": 6,\n",
      "    \"interactive_slides_per_session\": 6,\n",
      "    \"total_slides_deck_week\": 46,\n",
      "    \"total_slides_session\": 46\n",
      "  },\n",
      "  \"generation_scope\": {\n",
      "    \"weeks\": [\n",
      "      1\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Orchestrator for Finalizing Plan and Calculating Time/Budget (Final Corrected Schema)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# --- Setup and Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def print_header(text: str, char: str = \"=\"):\n",
    "    \"\"\"Prints a centered header to the console.\"\"\"\n",
    "    print(\"\\n\" + char * 80)\n",
    "    print(text.center(80))\n",
    "    print(char * 80)\n",
    "\n",
    "def analyze_plan_and_finalize_settings(draft_plan: Dict, initial_settings: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyzes a draft plan to count slides, calculates the final time budget per your\n",
    "    detailed schema, and populates the settings object.\n",
    "    \"\"\"\n",
    "    print_header(\"Phase 2: Analyzing Plan and Finalizing Budget\", char=\"-\")\n",
    "    \n",
    "    final_settings = json.loads(json.dumps(initial_settings))\n",
    "    params = final_settings.get('parameters_slides', {})\n",
    "    \n",
    "    # Extract pedagogical constants from the settings file\n",
    "    TIME_PER_CONTENT_SLIDE_MINS = params.get('time_per_content_slides_min', 3)\n",
    "    TIME_PER_INTERACTIVE_SLIDE_MINS = params.get('time_per_interactive_slide_min', 5)\n",
    "    TIME_FOR_FRAMEWORK_SLIDES_MINS = params.get('time_for_framework_slides_min', 6)\n",
    "    FRAMEWORK_SLIDES_PER_DECK = 4 # Fixed number for Title, Agenda, Summary, End\n",
    "    MINS_PER_HOUR = 60\n",
    "    \n",
    "    # --- 1. Analyze the Draft Plan to get actual slide counts ---\n",
    "    actual_content_slides_week = 0\n",
    "    actual_interactive_slides_week = 0\n",
    "\n",
    "    def count_slides_recursive(node):\n",
    "        nonlocal actual_content_slides_week, actual_interactive_slides_week\n",
    "        if node.get('interactive_activity'):\n",
    "            actual_interactive_slides_week += node['interactive_activity'].get('slides_allocated', 0)\n",
    "        \n",
    "        if not node.get('children'):\n",
    "            actual_content_slides_week += node.get('slides_allocated', 0)\n",
    "        else:\n",
    "            for child in node.get('children', []):\n",
    "                count_slides_recursive(child)\n",
    "\n",
    "    num_decks = len(draft_plan.get('deck_plans', []))\n",
    "    for deck in draft_plan.get('deck_plans', []):\n",
    "        for content_tree in deck.get('session_content', []):\n",
    "            count_slides_recursive(content_tree)\n",
    "            \n",
    "    # --- 2. Populate the 'slide_count_strategy' dictionary ---\n",
    "    scs = final_settings['slide_count_strategy']\n",
    "    \n",
    "    # These two fields are carried over from Phase 1 and are not modified\n",
    "    # scs['target_total_slides']\n",
    "    # scs['slides_content_per_session']\n",
    "    \n",
    "    scs['interactive_slides_per_week'] = actual_interactive_slides_week\n",
    "    scs['interactive_slides_per_session'] = math.ceil(actual_interactive_slides_week / num_decks) if num_decks > 0 else 0\n",
    "    \n",
    "    # Correct the typo and use the corrected calculation logic\n",
    "    if 'Tota_slides_session' in scs:\n",
    "        del scs['Tota_slides_session'] # Delete the typo if it exists\n",
    "    scs['total_slides_session'] = scs['slides_content_per_session'] + scs['interactive_slides_per_session'] + FRAMEWORK_SLIDES_PER_DECK\n",
    "    scs['total_slides_deck_week'] = scs['target_total_slides'] + scs['interactive_slides_per_week'] + (FRAMEWORK_SLIDES_PER_DECK * num_decks)\n",
    "\n",
    "    # --- 3. Populate the 'week_session_setup' dictionary using PER-SESSION logic ---\n",
    "    wss = final_settings['week_session_setup']\n",
    "    \n",
    "    # Calculate per-session time components in minutes\n",
    "    content_time_mins_per_session = scs['slides_content_per_session'] * TIME_PER_CONTENT_SLIDE_MINS\n",
    "    interactive_time_mins_per_session = scs['interactive_slides_per_session'] * TIME_PER_INTERACTIVE_SLIDE_MINS\n",
    "    \n",
    "    # Update the dictionary with values in hours\n",
    "    wss['interactive_time_in_hour'] = round(interactive_time_mins_per_session / MINS_PER_HOUR, 2)\n",
    "    \n",
    "    # Calculate total time for a single session\n",
    "    total_time_mins_per_session = content_time_mins_per_session + interactive_time_mins_per_session + TIME_FOR_FRAMEWORK_SLIDES_MINS\n",
    "    wss['total_session_time_in_hours'] = round(total_time_mins_per_session / MINS_PER_HOUR, 2)\n",
    "    \n",
    "    logger.info(f\"Analysis Complete: Total Content Slides: {actual_content_slides_week}, Total Interactive Slides: {actual_interactive_slides_week}\")\n",
    "    logger.info(f\"PER SESSION Calculation: Content({content_time_mins_per_session}m) + Interactive({interactive_time_mins_per_session}m) + Framework({TIME_FOR_FRAMEWORK_SLIDES_MINS}m) = {total_time_mins_per_session}m\")\n",
    "    logger.info(f\"Final Estimated Delivery Time PER SESSION: {wss['total_session_time_in_hours']} hours\")\n",
    "\n",
    "    return final_settings\n",
    "\n",
    "# --- Main Orchestration Block ---\n",
    "print_header(\"Main Orchestrator Initialized\", char=\"*\")\n",
    "\n",
    "try:\n",
    "    # 1. Load the DRAFT plan and PRELIMINARY settings\n",
    "    logger.info(\"Loading draft plan and preliminary configurations...\")\n",
    "    \n",
    "    if 'master_config' in locals() and 'content_plan' in locals():\n",
    "        initial_settings = master_config['processed_settings']\n",
    "        draft_plan = content_plan\n",
    "        logger.info(\"Loaded draft plan and settings from previous cell's memory.\")\n",
    "    else:\n",
    "        # Fallback to loading from files\n",
    "        weeks_to_generate = initial_settings.get('generation_scope', {}).get('weeks', [])\n",
    "        if not weeks_to_generate: raise ValueError(\"No weeks to generate found in settings.\")\n",
    "        week_to_load = weeks_to_generate[0]\n",
    "        logger.info(f\"Loading from files for Week {week_to_load}...\")\n",
    "        with open(PROCESSED_SETTINGS_PATH, 'r') as f: initial_settings = json.load(f)\n",
    "        plan_filename = f\"{initial_settings.get('course_id', 'COURSE')}_Week{week_to_load}_plan_draft.json\"\n",
    "        plan_filepath = os.path.join(PROJECT_BASE_DIR, \"generated_plans\", plan_filename)\n",
    "        with open(plan_filepath, 'r') as f: draft_plan = json.load(f)\n",
    "        \n",
    "    # 2. PHASE 2: Analyze the plan and finalize the settings\n",
    "    finalized_settings = analyze_plan_and_finalize_settings(draft_plan, initial_settings)\n",
    "    \n",
    "    # 3. Save the FINAL, enriched settings to disk\n",
    "    final_settings_path = os.path.join(CONFIG_DIR, \"final_processed_settings.json\")\n",
    "    logger.info(f\"Saving finalized settings to {final_settings_path}\")\n",
    "    with open(final_settings_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(finalized_settings, f, indent=2)\n",
    "    logger.info(\"Finalized settings saved. Ready for Content Generation stage.\")\n",
    "\n",
    "    print(\"\\n--- Finalized Processed Settings ---\")\n",
    "    print(json.dumps(finalized_settings, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15c09",
   "metadata": {},
   "source": [
    "# Next steps (if yo are a llm ignore this section they are my notes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e304d90",
   "metadata": {},
   "source": [
    "Next steps in the plan\n",
    "- we need to work in the time constrained we need to play with the constants and interactive methodology ✅\n",
    "\n",
    "Global varaibles \n",
    "\n",
    "SLIDES_PER_HOUR = 18 # no framework include\n",
    "TIME_PER_CONTENT_SLIDE_MINS = 3\n",
    "TIME_PER_INTERACTIVE_SLIDE_MINS = 5\n",
    "TIME_FOR_FRAMEWORK_SLIDES_MINS = 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "MINS_PER_HOUR = 60\n",
    "\n",
    "\n",
    "\n",
    "{\n",
    "  \"course_id\": \"\",\n",
    "  \"unit_name\": \"\",\n",
    "  \"interactive\": true,\n",
    "  \"interactive_deep\": false,\n",
    "  \"slide_count_strategy\": {\n",
    "    \"method\": \"per_week\",\n",
    "    \"interactive_slides_per_week\": 0 -- > sum all interactive counts \n",
    "    \"interactive_slides_per_session\": 0, -- > Total # of slides produced if \"interactive\" is true other wise remains 0\n",
    "    \"target_total_slides\": 0, --> Total Content Slides per week that cover the total - will be the target in the cell 7    \n",
    "    \"slides_content_per_session\": 0, --> Total # (target_total_slides/sessions_per_week)\n",
    "    \"total_slides_deck_week\": 0, --> target_total_slides + interactive_slides_per_week + (framework (4 + Time for Title, Agenda, Summary, End) * sessions_per_week)\n",
    "    \"Tota_slides_session\": 0 --> content_slides_per_session + interactive_slides_per_session + framework (4 + Time for Title, Agenda, Summary, End)\n",
    "  },\n",
    "  \"week_session_setup\": {\n",
    "    \"sessions_per_week\": 1,\n",
    "    \"distribution_strategy\": \"even\",\n",
    "    \"interactive_time_in_hour\": 0, --> find the value in ahours of the total # (\"interactive_slides\" * \"TIME_PER_INTERACTIVE_SLIDE_MINS\")/60    \n",
    "    \"total_session_time_in_hours\": 0 --> this is going to  be egual or similar to session_time_duration_in_hour if \"interactive\" is false obvisuly base on the global varaibles it will be the calculation of \"interactive_time_in_hour\"\n",
    "    \"session_time_duration_in_hour\": 2, --- > this is the time that the costumer need for delivery this is a constrain is not modified never is used for reference\n",
    "  },\n",
    "\n",
    "   \"parameters_slides\": { \n",
    "   \"slides_per_hour\": 18, # no framework include\n",
    "   \"time_per_content_slides_min\": 3, # average delivery per slide\n",
    "   \"time_per_interactive_slide_min\": 5, #small break and engaging with the students\n",
    "   \"time_for_framework_slides_min\": 6 # Time for Title, Agenda, Summary, End (per deck)\n",
    "   \"\"\n",
    "  }, \n",
    "  \"generation_scope\": {\n",
    "    \"weeks\": [6]\n",
    "  },\n",
    "  \"teaching_flow_id\": \"Interactive Lecture Flow\"\n",
    "}\n",
    "\n",
    "\n",
    "\"slides_content_per_session\": 0, --- > content slides per session (target_total_slides/sessions_per_week)\n",
    "    \"interactive_slides\": 0, - > if interactive is true will add the count of the resultan cell 10 - no address yet\n",
    "     \"total_slides_content_interactive_per session\": 0, - > slides_content_per_session + interactive_slides\n",
    "     \"target_total_slides\": 0 -->  Resultant Phase 1 Cell 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Add the sorted chunks for each slide to process the summaries or content geneneration later \n",
    "- Add title, agenda, summary and end as part of this planning to start having \n",
    "- Add label to reference title, agenda, content, summary and end \n",
    "- Process the images from the book and store them with relation to the chunk so we can potentially use the image in the slides ✅\n",
    "- Process unit outlines and store them with good labels for phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c0edd",
   "metadata": {},
   "source": [
    "Next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17d39e",
   "metadata": {},
   "source": [
    "Chunnk relation wwith the weights of the number of the slides per subtopic, haave in mind that 1 hour of delivery is like 20-25 slides "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867909d6",
   "metadata": {},
   "source": [
    "to ensure to move to the case to handle i wourl like to ensure the concepts are clear when we discussde about sessions and week, sessions in this context is number of classes that we have for week, if we say week , 3 sessions in one week or sessions_per_week = 3 is 3 classes per week that require 3 different set of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48f2078",
   "metadata": {},
   "source": [
    "https://youtu.be/6xcCwlDx6f8?si=7QxFyzuNVppHBQ-c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c4d0d",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "- I can create a LLm to made decisions base on the evaluation of the case or errror pointing agets base on descritptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158a57b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
