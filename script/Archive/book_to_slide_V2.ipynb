{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# --- LangChain Core Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser # Keep StrOutputParser for debugging\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# --- LangChain Community/Vendor Imports ---\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# --- LangChain Text Splitter & Retriever ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration ---\n",
    "BOOK_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "UNIT_OUTLINE_JSON_PATH = \"/home/sebas_dev_linux/projects/course_generator/results/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\"\n",
    "\n",
    "CHROMA_PERSIST_DIR_CHILD_GLOBAL = \"./chroma_db_global_child_chunks_strat3_v3\" # For PDR to find parent chapters\n",
    "CHROMA_COLLECTION_NAME_CHILD_GLOBAL = \"global_book_child_chunks_strat3_v3\"\n",
    "\n",
    "OUTPUT_STRUCTURED_JSON_DIR = \"./structured_weekly_content_toc_iterated_v3\"\n",
    "os.makedirs(OUTPUT_STRUCTURED_JSON_DIR, exist_ok=True)\n",
    "\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "GENERATION_MODEL_OLLAMA = \"mistral:latest\"\n",
    "\n",
    "CHILD_CHUNK_SIZE_GLOBAL = 700 # For PDR's initial search for parent chapters\n",
    "CHILD_CHUNK_OVERLAP_GLOBAL = 100\n",
    "\n",
    "# For splitting a retrieved parent chapter for subtopic search\n",
    "SUBTOPIC_CONTEXT_CHUNK_SIZE = 500\n",
    "SUBTOPIC_CONTEXT_CHUNK_OVERLAP = 50\n",
    "TOP_K_SUBTOPIC_CONTEXT = 3 # How many child chunks to get for a specific subtopic\n",
    "\n",
    "# --- Pydantic Models (Same) ---\n",
    "class LearningModule(BaseModel):\n",
    "    subtopicTitle: str = Field(description=\"A clear and concise title for this sub-topic or learning objective, often derived from chapter sections.\")\n",
    "    elaboratedContent: List[str] = Field(description=\"A list of detailed paragraphs or comprehensive bullet points explaining the subtopic.\")\n",
    "\n",
    "class WeeklyContent(BaseModel):\n",
    "    mainWeeklyTopic: str = Field(description=\"The main overarching topic for the week.\")\n",
    "    learningModules: List[LearningModule] = Field(description=\"A list of learning modules covering distinct sub-topics with detailed content.\")\n",
    "\n",
    "# --- Helper Functions (load_unit_outline, clean_metadata_value, load_book_into_parent_documents - largely same) ---\n",
    "def load_unit_outline(json_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "        logger.info(f\"Successfully loaded unit outline from: {json_path}\"); return data\n",
    "    except Exception as e: logger.error(f\"Error loading unit outline {json_path}: {e}\", exc_info=True); raise\n",
    "\n",
    "def clean_metadata_value(value: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Cleans metadata values to be Chroma-compatible (str, int, float, bool, or None).\n",
    "    Lists are converted to comma-separated strings.\n",
    "    Dicts are converted to JSON strings.\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        # Always convert lists to a comma-separated string for ChromaDB metadata\n",
    "        return \", \".join(map(str, value)) \n",
    "    elif isinstance(value, dict):\n",
    "        return json.dumps(value) # Convert dict to a JSON string\n",
    "    elif isinstance(value, (str, int, float, bool)) or value is None:\n",
    "        return value\n",
    "    else:\n",
    "        # For any other type, attempt to convert to string as a fallback\n",
    "        return str(value)\n",
    "\n",
    "def load_book_into_parent_documents(book_path: str) -> List[Document]:\n",
    "    # (Same as your last working version that extracts chapter_toc for EPUBs)\n",
    "    logger.info(f\"Loading book from '{book_path}' to create parent documents (chapters with ToC).\")\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "    parent_documents: List[Document] = []\n",
    "\n",
    "    if file_extension == \".pdf\":\n",
    "        logger.warning(\"PDF processing: Treating each page as a parent document. Chapter/ToC segmentation is not robustly implemented here.\")\n",
    "        pdf_loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            pages_as_parents = pdf_loader.load()\n",
    "            for i, page_doc in enumerate(pages_as_parents):\n",
    "                page_num = page_doc.metadata.get('page', i + 1)\n",
    "                parent_id = f\"pdf_page_{page_num}\"\n",
    "                cleaned_metadata = {\n",
    "                    \"source\": os.path.basename(page_doc.metadata.get(\"source\", book_path)),\n",
    "                    \"page_number\": page_num, \"chapter_number\": page_num,\n",
    "                    \"chapter_title\": f\"PDF Page {page_num}\", \"chapter_toc\": [],\n",
    "                    \"document_type\": \"parent\", \"parent_id\": parent_id\n",
    "                }\n",
    "                parent_documents.append(Document(page_content=page_doc.page_content, metadata=cleaned_metadata))\n",
    "            logger.info(f\"Loaded {len(parent_documents)} pages as parent documents from PDF.\")\n",
    "        except Exception as e: logger.error(f\"Error loading PDF: {e}\", exc_info=True); raise\n",
    "\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Using UnstructuredEPubLoader for EPUB to identify chapters and their ToCs.\")\n",
    "        epub_loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            raw_lc_documents = epub_loader.load()\n",
    "            if not raw_lc_documents: raise ValueError(f\"No elements loaded from EPUB {book_path}\")\n",
    "            logger.info(f\"Loaded {len(raw_lc_documents)} raw elements as LangChain Documents from EPUB.\")\n",
    "        except Exception as e: logger.error(f\"Error loading EPUB: {e}\", exc_info=True); raise\n",
    "\n",
    "        current_chapter_number = 0\n",
    "        current_chapter_title = \"Preface or Introduction\"\n",
    "        current_chapter_toc: List[str] = []\n",
    "        current_chapter_content_accumulator: List[str] = []\n",
    "        parent_id_counter = 0\n",
    "\n",
    "        for i, element_doc in enumerate(raw_lc_documents):\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            element_category = element_doc.metadata.get(\"category\")\n",
    "            new_chapter_detected = False\n",
    "\n",
    "            if element_category == \"Title\" and element_text and len(element_text) < 150:\n",
    "                chapter_match = re.match(r\"(?i)^(chapter\\s+(\\d+|[IVXLCDM]+)\\b|part\\s+[A-Z0-9]+|appendix\\s+[A-Z])\", element_text)\n",
    "                if chapter_match: new_chapter_detected = True\n",
    "            \n",
    "            if new_chapter_detected and current_chapter_content_accumulator:\n",
    "                parent_id_counter += 1; parent_doc_content = \"\\n\".join(current_chapter_content_accumulator).strip()\n",
    "                if parent_doc_content:\n",
    "                    parent_metadata = {\"source\": os.path.basename(book_path), \"chapter_number\": current_chapter_number,\n",
    "                                       \"chapter_title\": current_chapter_title, \"chapter_toc\": current_chapter_toc.copy(),\n",
    "                                       \"document_type\": \"parent\", \"parent_id\": f\"epub_ch_{parent_id_counter}\"}\n",
    "                    parent_documents.append(Document(page_content=parent_doc_content, metadata={k: clean_metadata_value(v) for k, v in parent_metadata.items() if v is not None}))\n",
    "                    logger.info(f\"Created parent doc for Ch {current_chapter_number}: '{current_chapter_title}' with {len(current_chapter_toc)} ToC items.\")\n",
    "                current_chapter_content_accumulator = []; current_chapter_toc = []\n",
    "\n",
    "            if new_chapter_detected:\n",
    "                current_chapter_number +=1; current_chapter_title = element_text\n",
    "                if element_text not in current_chapter_toc: current_chapter_toc.append(element_text)\n",
    "            elif element_category == \"Title\" and element_text and current_chapter_number > 0: # Subheading\n",
    "                if element_text not in current_chapter_toc: current_chapter_toc.append(element_text)\n",
    "            if element_text: current_chapter_content_accumulator.append(element_text)\n",
    "        \n",
    "        if current_chapter_content_accumulator: # Last chapter\n",
    "            parent_id_counter += 1; parent_doc_content = \"\\n\".join(current_chapter_content_accumulator).strip()\n",
    "            if parent_doc_content:\n",
    "                parent_metadata = {\"source\": os.path.basename(book_path), \"chapter_number\": current_chapter_number,\n",
    "                                   \"chapter_title\": current_chapter_title, \"chapter_toc\": current_chapter_toc.copy(),\n",
    "                                   \"document_type\": \"parent\", \"parent_id\": f\"epub_ch_{parent_id_counter}\"}\n",
    "                parent_documents.append(Document(page_content=parent_doc_content, metadata={k: clean_metadata_value(v) for k, v in parent_metadata.items() if v is not None}))\n",
    "                logger.info(f\"Created parent doc for last Ch {current_chapter_number}: '{current_chapter_title}' with {len(current_chapter_toc)} ToC items.\")\n",
    "    else: raise ValueError(f\"Unsupported book file format: {file_extension}\")\n",
    "    if not parent_documents: logger.error(f\"No parent documents created from {book_path}.\")\n",
    "    else: logger.info(f\"Created {len(parent_documents)} parent documents with ToC metadata (if EPUB).\")\n",
    "    return parent_documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW FUNCTION for subtopic-specific context ---\n",
    "def get_subtopic_specific_context_from_parent(\n",
    "    subtopic_query: str,\n",
    "    parent_chapter_document: Document,\n",
    "    embedding_model: OllamaEmbeddings,\n",
    "    k: int = TOP_K_SUBTOPIC_CONTEXT\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Splits a given parent chapter document into small chunks,\n",
    "    creates a temporary in-memory vector store for these chunks,\n",
    "    and retrieves the most relevant chunks for the subtopic_query.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating temporary vectorstore for parent chapter: '{parent_chapter_document.metadata.get('chapter_title', 'Unknown Chapter')}' to find context for subtopic: '{subtopic_query}'\")\n",
    "    \n",
    "    # Split the single parent document's content\n",
    "    sub_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=SUBTOPIC_CONTEXT_CHUNK_SIZE,\n",
    "        chunk_overlap=SUBTOPIC_CONTEXT_CHUNK_OVERLAP\n",
    "    )\n",
    "    # Create Document objects from the text splits of the parent,\n",
    "    # they can inherit parent's metadata for reference if needed, but not strictly necessary for this temp store\n",
    "    parent_content_chunks = sub_splitter.split_text(parent_chapter_document.page_content)\n",
    "    \n",
    "    if not parent_content_chunks:\n",
    "        logger.warning(\"Parent chapter content yielded no chunks for temporary vectorstore.\")\n",
    "        return \"No specific textual context found within the chapter for this subtopic after chunking.\"\n",
    "\n",
    "    # Create LangChain Documents from these text chunks\n",
    "    temp_docs_for_subtopic_search = [Document(page_content=text) for text in parent_content_chunks]\n",
    "    \n",
    "    # Create a temporary, in-memory Chroma vector store for these specific chunks\n",
    "    try:\n",
    "        temp_vectorstore = Chroma.from_documents(\n",
    "            documents=temp_docs_for_subtopic_search,\n",
    "            embedding=embedding_model\n",
    "            # No persistence needed for this temporary store\n",
    "        )\n",
    "        retrieved_sub_chunks = temp_vectorstore.similarity_search(subtopic_query, k=k)\n",
    "        logger.info(f\"Retrieved {len(retrieved_sub_chunks)} child chunks from parent chapter for subtopic '{subtopic_query}'.\")\n",
    "        return format_docs(retrieved_sub_chunks) # format_docs is your existing helper\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating/querying temporary vectorstore for subtopic '{subtopic_query}': {e}\", exc_info=True)\n",
    "        return f\"Error retrieving specific context for subtopic: {str(e)}\"\n",
    "\n",
    "# --- NEW FUNCTION for generating content for a single subtopic ---\n",
    "def generate_elaborated_content_for_subtopic(\n",
    "    subtopic_title: str,\n",
    "    subtopic_context: str,\n",
    "    generation_llm: ChatOllama\n",
    ") -> List[str]:\n",
    "    logger.info(f\"Generating elaborated content for subtopic: '{subtopic_title}'\")\n",
    "    # Using Pydantic model for structured list output\n",
    "    class SubtopicElaboration(BaseModel):\n",
    "        points: List[str] = Field(description=\"A list of detailed paragraphs or comprehensive bullet points explaining the subtopic.\")\n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=SubtopicElaboration)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    You are an academic writer. Given a specific subtopic title and highly relevant text excerpts\n",
    "    from a textbook chapter, provide a detailed explanation of ONLY that subtopic.\n",
    "\n",
    "    Subtopic to Explain: \"{subtopic_title}\"\n",
    "\n",
    "    Relevant Textbook Excerpts for this Subtopic:\n",
    "    ---CONTEXT START---\n",
    "    {subtopic_context}\n",
    "    ---CONTEXT END---\n",
    "\n",
    "    Based ONLY on the provided Textbook Excerpts for this subtopic:\n",
    "    1. Generate a detailed explanation of the subtopic.\n",
    "    2. The explanation should include key definitions, principles, and examples if found in the context.\n",
    "    3. Present your output as a JSON object with a single key \"points\", where the value is a list of strings. Each string in the list should be a full paragraph or a comprehensive bullet point.\n",
    "    4. Aim for substantial detail for the subtopic. Ensure your explanation is focused and directly supported by the provided context.\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template,\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "    chain = prompt | generation_llm | parser\n",
    "\n",
    "    try:\n",
    "        llm_response_dict = chain.invoke({\n",
    "            \"subtopic_title\": subtopic_title,\n",
    "            \"subtopic_context\": subtopic_context\n",
    "        })\n",
    "        content_list = llm_response_dict.get(\"points\", [])\n",
    "        if not content_list and subtopic_context and \"Error\" not in subtopic_context: # If context was there but no points generated\n",
    "            logger.warning(f\"LLM generated no points for subtopic '{subtopic_title}' despite context. Using placeholder.\")\n",
    "            content_list = [f\"Content for '{subtopic_title}' should be elaborated based on chapter knowledge.\"]\n",
    "        elif not content_list: # No context and no points\n",
    "             content_list = [f\"No specific context available to elaborate on '{subtopic_title}'.\"]\n",
    "\n",
    "        logger.info(f\"Successfully generated {len(content_list)} content points for subtopic '{subtopic_title}'.\")\n",
    "        return content_list\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating content for subtopic '{subtopic_title}': {e}\", exc_info=True)\n",
    "        # Log raw output if parsing fails\n",
    "        try:\n",
    "            raw_output = (prompt | generation_llm | StrOutputParser()).invoke({\n",
    "                \"subtopic_title\": subtopic_title, \"subtopic_context\": subtopic_context\n",
    "            })\n",
    "            logger.error(f\"LLM Raw Output for subtopic '{subtopic_title}':\\n{raw_output}\")\n",
    "        except Exception as raw_e:\n",
    "            logger.error(f\"Failed to get raw LLM output for subtopic after error: {raw_e}\")\n",
    "        return [f\"Error generating content for {subtopic_title}: {str(e)}\"]\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    logger.info(\"Starting ENHANCED Content Structuring RAG pipeline (Iterate ToC Subtopics)...\")\n",
    "    # (Initial setup: paths, load unit outline, select week, init LLMs - same as before)\n",
    "    if not os.path.exists(BOOK_PATH): logger.error(f\"Book file not found: {BOOK_PATH}\"); return\n",
    "    if not os.path.exists(UNIT_OUTLINE_JSON_PATH): logger.error(f\"Unit outline JSON file not found: {UNIT_OUTLINE_JSON_PATH}\"); return\n",
    "\n",
    "    unit_outline_data = load_unit_outline(UNIT_OUTLINE_JSON_PATH)\n",
    "    weekly_schedule = unit_outline_data.get(\"weeklySchedule\", [])\n",
    "    if not weekly_schedule: logger.error(\"No weekly schedule in unit outline.\"); return\n",
    "\n",
    "    WEEK_TO_PROCESS_INDEX = 0 \n",
    "    if WEEK_TO_PROCESS_INDEX >= len(weekly_schedule): logger.error(f\"Week index {WEEK_TO_PROCESS_INDEX} out of bounds.\"); return\n",
    "    \n",
    "    selected_week_info = weekly_schedule[WEEK_TO_PROCESS_INDEX]\n",
    "    main_weekly_topic_title_from_outline = selected_week_info.get(\"topic\") or selected_week_info.get(\"contentTopic\")\n",
    "    week_identifier = selected_week_info.get(\"week\", f\"Idx_{WEEK_TO_PROCESS_INDEX}\")\n",
    "    full_main_topic_name = f\"Week {week_identifier}: {main_weekly_topic_title_from_outline}\"\n",
    "\n",
    "    if not main_weekly_topic_title_from_outline: \n",
    "        logger.error(f\"Selected week (Idx {WEEK_TO_PROCESS_INDEX}) has no valid topic title.\"); return\n",
    "    logger.info(f\"Processing selected week: {full_main_topic_name}\")\n",
    "\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        # Generation LLM for subtopic content needs to be good at focused explanation and JSON list output\n",
    "        generation_llm_for_subtopics = ChatOllama(model=GENERATION_MODEL_OLLAMA, temperature=0.2, format=\"json\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Ollama models: {e}. Ensure Ollama is running.\", exc_info=True); return\n",
    "\n",
    "    parent_documents = load_book_into_parent_documents(BOOK_PATH)\n",
    "    if not parent_documents: logger.error(\"Failed to load book into parent documents.\"); return\n",
    "\n",
    "    # Setup ParentDocumentRetriever (for finding the main chapter(s) for the week)\n",
    "    docstore_for_parents = InMemoryStore()\n",
    "    parent_doc_ids_for_store = [doc.metadata.get(\"parent_id\", f\"fallback_pid_{i}\") for i, doc in enumerate(parent_documents)]\n",
    "    docstore_for_parents.mset(list(zip(parent_doc_ids_for_store, parent_documents)))\n",
    "    logger.info(f\"Parent documents ({len(parent_documents)}) added to in-memory docstore.\")\n",
    "\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR_CHILD_GLOBAL):\n",
    "        logger.info(f\"Deleting existing GLOBAL child chunk DB: {CHROMA_PERSIST_DIR_CHILD_GLOBAL}\")\n",
    "        shutil.rmtree(CHROMA_PERSIST_DIR_CHILD_GLOBAL)\n",
    "        \n",
    "    global_child_vectorstore = Chroma(\n",
    "        collection_name=CHROMA_COLLECTION_NAME_CHILD_GLOBAL,\n",
    "        embedding_function=embedding_model,\n",
    "        persist_directory=CHROMA_PERSIST_DIR_CHILD_GLOBAL\n",
    "    )\n",
    "    global_child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE_GLOBAL, chunk_overlap=CHILD_CHUNK_OVERLAP_GLOBAL)\n",
    "    \n",
    "    logger.info(\"Initializing ParentDocumentRetriever for chapter lookup...\")\n",
    "    parent_retriever_for_chapter_lookup = ParentDocumentRetriever(\n",
    "        vectorstore=global_child_vectorstore, docstore=docstore_for_parents, child_splitter=global_child_splitter\n",
    "    )\n",
    "    parent_retriever_for_chapter_lookup.add_documents(parent_documents, ids=parent_doc_ids_for_store, add_to_docstore=False) # Already added\n",
    "    logger.info(\"Global ParentDocumentRetriever initialized, child chunks added to its vector store.\")\n",
    "\n",
    "    # --- 1. Retrieve the PRIMARY Parent Document(s) (Chapter) for the Week's Main Topic ---\n",
    "    logger.info(f\"Retrieving primary parent chapter(s) for: '{main_weekly_topic_title_from_outline}'\")\n",
    "    try:\n",
    "        retrieved_primary_chapters = parent_retriever_for_chapter_lookup.invoke(main_weekly_topic_title_from_outline)\n",
    "        if not retrieved_primary_chapters:\n",
    "            logger.error(f\"No parent chapter(s) found for main topic: '{main_weekly_topic_title_from_outline}'. Cannot proceed.\"); return\n",
    "        logger.info(f\"Retrieved {len(retrieved_primary_chapters)} primary parent chapter(s). Focusing on the first.\")\n",
    "        # For simplicity, we'll focus on the ToC of the *first* retrieved chapter.\n",
    "        # A more advanced approach might merge ToCs or process multiple relevant chapters.\n",
    "        primary_chapter_doc = retrieved_primary_chapters[0]\n",
    "        chapter_title = primary_chapter_doc.metadata.get(\"chapter_title\", \"Unknown Chapter\")\n",
    "        chapter_number_meta = primary_chapter_doc.metadata.get(\"chapter_number\", \"NA\")\n",
    "        \n",
    "        chapter_toc_raw = primary_chapter_doc.metadata.get(\"chapter_toc\", [])\n",
    "        chapter_toc_list = []\n",
    "        if isinstance(chapter_toc_raw, str): # Handle if ToC was stringified\n",
    "            try: chapter_toc_list = json.loads(chapter_toc_raw) if chapter_toc_raw.startswith(\"[\") else [t.strip() for t in chapter_toc_raw.split(',') if t.strip()]\n",
    "            except: chapter_toc_list = [t.strip() for t in chapter_toc_raw.split(',') if t.strip()]\n",
    "        elif isinstance(chapter_toc_raw, list):\n",
    "            chapter_toc_list = chapter_toc_raw\n",
    "        \n",
    "        if not chapter_toc_list:\n",
    "            logger.warning(f\"No ToC found or ToC is empty in retrieved chapter '{chapter_title}'. Will attempt to generate content for the chapter as a whole or use a generic subtopic.\")\n",
    "            # Fallback: Treat the whole chapter title as the only subtopic, or use a generic prompt\n",
    "            chapter_toc_list = [f\"Overview of {chapter_title}\"] if chapter_title != \"Unknown Chapter\" else [\"Main concepts of the week\"]\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving primary chapter or its ToC: {e}\", exc_info=True); return\n",
    "\n",
    "    # --- 2. Iterate through Chapter ToC Subtopics ---\n",
    "    learning_modules_for_week: List[LearningModule] = []\n",
    "    logger.info(f\"Processing {len(chapter_toc_list)} subtopics from ToC of chapter '{chapter_title}':\")\n",
    "\n",
    "    for subtopic_from_toc in chapter_toc_list:\n",
    "        if not subtopic_from_toc.strip(): continue\n",
    "        logger.info(f\"-- Processing Subtopic from ToC: '{subtopic_from_toc}' --\")\n",
    "\n",
    "        # 3a. Targeted RAG: Get specific context for THIS subtopic from WITHIN the primary_chapter_doc\n",
    "        subtopic_specific_context_str = get_subtopic_specific_context_from_parent(\n",
    "            subtopic_query=subtopic_from_toc,\n",
    "            parent_chapter_document=primary_chapter_doc, # Pass the whole chapter doc\n",
    "            embedding_model=embedding_model # For the temporary vector store\n",
    "        )\n",
    "\n",
    "        if not subtopic_specific_context_str.strip() or \"Error retrieving specific context\" in subtopic_specific_context_str :\n",
    "            logger.warning(f\"No specific context found for subtopic '{subtopic_from_toc}'. Using placeholder or skipping detailed generation.\")\n",
    "            # If using placeholder, it means the LLM will try to generate based on title alone\n",
    "            # Forcing some context, even if generic, might be better if LLM struggles with no context\n",
    "            subtopic_specific_context_str = f\"General information pertaining to {subtopic_from_toc} within the broader context of {chapter_title}.\" if not subtopic_specific_context_str.strip() else subtopic_specific_context_str\n",
    "\n",
    "\n",
    "        # 3b. LLM Content Generation for this specific subtopic\n",
    "        elaborated_content_for_subtopic = generate_elaborated_content_for_subtopic(\n",
    "            subtopic_title=subtopic_from_toc,\n",
    "            subtopic_context=subtopic_specific_context_str,\n",
    "            generation_llm=generation_llm_for_subtopics\n",
    "        )\n",
    "        \n",
    "        learning_modules_for_week.append(\n",
    "            LearningModule(subtopicTitle=subtopic_from_toc, elaboratedContent=elaborated_content_for_subtopic)\n",
    "        )\n",
    "\n",
    "    # 4. Structure the final output\n",
    "    final_weekly_output = WeeklyContent(\n",
    "        mainWeeklyTopic=full_main_topic_name,\n",
    "        learningModules=learning_modules_for_week\n",
    "    )\n",
    "\n",
    "    # 5. Save JSON\n",
    "    output_filename = os.path.join(OUTPUT_STRUCTURED_JSON_DIR, f\"week_{week_identifier}_content_iterated_toc_ch{chapter_number_meta}.json\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_weekly_output.dict(), f, indent=2, ensure_ascii=False) # .dict() for Pydantic model\n",
    "        logger.info(f\"Successfully saved detailed subtopic content to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving structured JSON: {e}\", exc_info=True)\n",
    "\n",
    "    logger.info(\"ENHANCED Content Structuring RAG pipeline (Iterate ToC Subtopics) finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"YOUR_BOOK.pdf\" in BOOK_PATH:\n",
    "        logger.error(\"!!! PLEASE UPDATE 'BOOK_PATH' with the actual path to your book. !!!\")\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb52671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 21:31:05,279 - INFO - RAG Pipeline (ParentDocRetriever + LLM ToC for Parents + Iterate LLM-ToC Subtopics)...\n",
      "2025-06-15 21:31:05,279 - INFO - Successfully loaded unit outline: /home/sebas_dev_linux/projects/course_generator/results/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\n",
      "2025-06-15 21:31:05,280 - INFO - Processing: Week 1: Understanding the Digital Forensics Profession and Investigations.\n",
      "2025-06-15 21:31:05,310 - INFO - Loading book '/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub' for parent docs & full ToC (LLM-assisted for chapters).\n",
      "2025-06-15 21:31:05,311 - INFO - Processing EPUB for chapters and generating detailed ToCs using LLM.\n",
      "2025-06-15 21:31:07,497 - INFO - Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "2025-06-15 21:31:07,498 - INFO - NumExpr defaulting to 16 threads.\n",
      "[WARNING] Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "2025-06-15 21:31:12,938 - WARNING - Could not load translations for en-US\n",
      "  data file translations/en.yaml not found\n",
      "[WARNING] The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-15 21:31:12,938 - WARNING - The term Abstract has no translation defined.\n",
      "\n",
      "2025-06-15 21:31:16,259 - INFO - Generating ToC with LLM for chapter: 'Preface or Introduction' (text length: 31594)\n",
      "2025-06-15 21:31:16,278 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,279 - ERROR - LLM ToC generation error for 'Preface or Introduction': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,282 - INFO - Created parent doc Ch 0: 'Preface or Introduction' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,283 - INFO - Generating ToC with LLM for chapter: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations' (text length: 150355)\n",
      "2025-06-15 21:31:16,285 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,286 - ERROR - LLM ToC generation error for 'Chapter 1. Understanding the Digital Forensics Profession and Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,286 - INFO - Created parent doc Ch 1: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,287 - INFO - Generating ToC with LLM for chapter: 'Chapter 2. The Investigator’s Office and Laboratory' (text length: 73873)\n",
      "2025-06-15 21:31:16,289 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,290 - ERROR - LLM ToC generation error for 'Chapter 2. The Investigator’s Office and Laboratory': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,290 - INFO - Created parent doc Ch 2: 'Chapter 2. The Investigator’s Office and Laboratory' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,291 - INFO - Generating ToC with LLM for chapter: 'Chapter 3. Data Acquisition' (text length: 92745)\n",
      "2025-06-15 21:31:16,293 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,294 - ERROR - LLM ToC generation error for 'Chapter 3. Data Acquisition': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,295 - INFO - Created parent doc Ch 3: 'Chapter 3. Data Acquisition' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,295 - INFO - Generating ToC with LLM for chapter: 'Chapter 4. Processing Crime and Incident Scenes' (text length: 129892)\n",
      "2025-06-15 21:31:16,298 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,299 - ERROR - LLM ToC generation error for 'Chapter 4. Processing Crime and Incident Scenes': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,299 - INFO - Created parent doc Ch 4: 'Chapter 4. Processing Crime and Incident Scenes' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,300 - INFO - Generating ToC with LLM for chapter: 'Chapter 5. Working with Windows and CLI Systems' (text length: 123169)\n",
      "2025-06-15 21:31:16,303 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,303 - ERROR - LLM ToC generation error for 'Chapter 5. Working with Windows and CLI Systems': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,304 - INFO - Created parent doc Ch 5: 'Chapter 5. Working with Windows and CLI Systems' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,305 - INFO - Generating ToC with LLM for chapter: 'Chapter 6. Current Digital Forensics Tools' (text length: 83084)\n",
      "2025-06-15 21:31:16,307 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,307 - ERROR - LLM ToC generation error for 'Chapter 6. Current Digital Forensics Tools': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,308 - INFO - Created parent doc Ch 6: 'Chapter 6. Current Digital Forensics Tools' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,309 - INFO - Generating ToC with LLM for chapter: 'Chapter 7. Linux and Macintosh File Systems' (text length: 67069)\n",
      "2025-06-15 21:31:16,311 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,311 - ERROR - LLM ToC generation error for 'Chapter 7. Linux and Macintosh File Systems': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,312 - INFO - Created parent doc Ch 7: 'Chapter 7. Linux and Macintosh File Systems' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,313 - INFO - Generating ToC with LLM for chapter: 'Chapter 8. Recovering Graphics Files' (text length: 75885)\n",
      "2025-06-15 21:31:16,315 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,315 - ERROR - LLM ToC generation error for 'Chapter 8. Recovering Graphics Files': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,316 - INFO - Created parent doc Ch 8: 'Chapter 8. Recovering Graphics Files' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,317 - INFO - Generating ToC with LLM for chapter: 'Chapter 9. Digital Forensics Analysis and Validation' (text length: 68766)\n",
      "2025-06-15 21:31:16,319 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,319 - ERROR - LLM ToC generation error for 'Chapter 9. Digital Forensics Analysis and Validation': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,320 - INFO - Created parent doc Ch 9: 'Chapter 9. Digital Forensics Analysis and Validation' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,320 - INFO - Generating ToC with LLM for chapter: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics' (text length: 70595)\n",
      "2025-06-15 21:31:16,322 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,323 - ERROR - LLM ToC generation error for 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,324 - INFO - Created parent doc Ch 10: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,324 - INFO - Generating ToC with LLM for chapter: 'Chapter 11. E-mail and Social Media Investigations' (text length: 86117)\n",
      "2025-06-15 21:31:16,327 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,327 - ERROR - LLM ToC generation error for 'Chapter 11. E-mail and Social Media Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,328 - INFO - Created parent doc Ch 11: 'Chapter 11. E-mail and Social Media Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,329 - INFO - Generating ToC with LLM for chapter: 'Chapter 12. Mobile Device Forensics and the Internet of Anything' (text length: 64257)\n",
      "2025-06-15 21:31:16,331 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,332 - ERROR - LLM ToC generation error for 'Chapter 12. Mobile Device Forensics and the Internet of Anything': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,332 - INFO - Created parent doc Ch 12: 'Chapter 12. Mobile Device Forensics and the Internet of Anything' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,333 - INFO - Generating ToC with LLM for chapter: 'Chapter 13. Cloud Forensics' (text length: 84700)\n",
      "2025-06-15 21:31:16,335 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,336 - ERROR - LLM ToC generation error for 'Chapter 13. Cloud Forensics': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,336 - INFO - Created parent doc Ch 13: 'Chapter 13. Cloud Forensics' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,337 - INFO - Generating ToC with LLM for chapter: 'Chapter 14. Report Writing for High-Tech Investigations' (text length: 61746)\n",
      "2025-06-15 21:31:16,340 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,340 - ERROR - LLM ToC generation error for 'Chapter 14. Report Writing for High-Tech Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,341 - INFO - Created parent doc Ch 14: 'Chapter 14. Report Writing for High-Tech Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,341 - INFO - Generating ToC with LLM for chapter: 'Chapter 15. Expert Testimony in Digital Investigations' (text length: 90704)\n",
      "2025-06-15 21:31:16,344 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,345 - ERROR - LLM ToC generation error for 'Chapter 15. Expert Testimony in Digital Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,345 - INFO - Created parent doc Ch 15: 'Chapter 15. Expert Testimony in Digital Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,346 - INFO - Generating ToC with LLM for chapter: 'Chapter 16. Ethics for the Expert Witness' (text length: 92855)\n",
      "2025-06-15 21:31:16,348 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,349 - ERROR - LLM ToC generation error for 'Chapter 16. Ethics for the Expert Witness': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,349 - INFO - Created parent doc Ch 16: 'Chapter 16. Ethics for the Expert Witness' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,350 - INFO - Generating ToC with LLM for chapter: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations' (text length: 18087)\n",
      "2025-06-15 21:31:16,352 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,353 - ERROR - LLM ToC generation error for 'Chapter 1. Understanding the Digital Forensics Profession and Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,353 - INFO - Created parent doc Ch 17: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,354 - INFO - Generating ToC with LLM for chapter: 'Chapter 2. The Investigator’s Office and Laboratory' (text length: 23050)\n",
      "2025-06-15 21:31:16,356 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,357 - ERROR - LLM ToC generation error for 'Chapter 2. The Investigator’s Office and Laboratory': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,357 - INFO - Created parent doc Ch 18: 'Chapter 2. The Investigator’s Office and Laboratory' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,358 - INFO - Generating ToC with LLM for chapter: 'Chapter 3. Data Acquisition' (text length: 25177)\n",
      "2025-06-15 21:31:16,360 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,361 - ERROR - LLM ToC generation error for 'Chapter 3. Data Acquisition': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,361 - INFO - Created parent doc Ch 19: 'Chapter 3. Data Acquisition' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,362 - INFO - Generating ToC with LLM for chapter: 'Chapter 4. Processing Crime and Incident Scenes' (text length: 24768)\n",
      "2025-06-15 21:31:16,364 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,365 - ERROR - LLM ToC generation error for 'Chapter 4. Processing Crime and Incident Scenes': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,365 - INFO - Created parent doc Ch 20: 'Chapter 4. Processing Crime and Incident Scenes' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,365 - INFO - Generating ToC with LLM for chapter: 'Chapter 5. Working with Windows and CLI Systems' (text length: 15473)\n",
      "2025-06-15 21:31:16,367 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,368 - ERROR - LLM ToC generation error for 'Chapter 5. Working with Windows and CLI Systems': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,368 - INFO - Created parent doc Ch 21: 'Chapter 5. Working with Windows and CLI Systems' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,369 - INFO - Generating ToC with LLM for chapter: 'Chapter 6. Current Digital Forensics Tools' (text length: 21151)\n",
      "2025-06-15 21:31:16,371 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,372 - ERROR - LLM ToC generation error for 'Chapter 6. Current Digital Forensics Tools': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,372 - INFO - Created parent doc Ch 22: 'Chapter 6. Current Digital Forensics Tools' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,373 - INFO - Generating ToC with LLM for chapter: 'Chapter 7. Linux and Macintosh File Systems' (text length: 12533)\n",
      "2025-06-15 21:31:16,375 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,375 - ERROR - LLM ToC generation error for 'Chapter 7. Linux and Macintosh File Systems': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,376 - INFO - Created parent doc Ch 23: 'Chapter 7. Linux and Macintosh File Systems' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,376 - INFO - Generating ToC with LLM for chapter: 'Chapter 8. Recovering Graphics Files' (text length: 17015)\n",
      "2025-06-15 21:31:16,379 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,379 - ERROR - LLM ToC generation error for 'Chapter 8. Recovering Graphics Files': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,380 - INFO - Created parent doc Ch 24: 'Chapter 8. Recovering Graphics Files' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,380 - INFO - Generating ToC with LLM for chapter: 'Chapter 9. Digital Forensics Analysis and Validation' (text length: 12018)\n",
      "2025-06-15 21:31:16,382 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,383 - ERROR - LLM ToC generation error for 'Chapter 9. Digital Forensics Analysis and Validation': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,383 - INFO - Created parent doc Ch 25: 'Chapter 9. Digital Forensics Analysis and Validation' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,384 - INFO - Generating ToC with LLM for chapter: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics' (text length: 28597)\n",
      "2025-06-15 21:31:16,386 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,387 - ERROR - LLM ToC generation error for 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,387 - INFO - Created parent doc Ch 26: 'Chapter 10. Virtual Machine Forensics, Live Acquisitions, and Network Forensics' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,388 - INFO - Generating ToC with LLM for chapter: 'Chapter 11. E-mail and Social Media Investigations' (text length: 10938)\n",
      "2025-06-15 21:31:16,390 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,390 - ERROR - LLM ToC generation error for 'Chapter 11. E-mail and Social Media Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,391 - INFO - Created parent doc Ch 27: 'Chapter 11. E-mail and Social Media Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,391 - INFO - Generating ToC with LLM for chapter: 'Chapter 12. Mobile Device Forensics' (text length: 12070)\n",
      "2025-06-15 21:31:16,394 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,394 - ERROR - LLM ToC generation error for 'Chapter 12. Mobile Device Forensics': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,395 - INFO - Created parent doc Ch 28: 'Chapter 12. Mobile Device Forensics' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,395 - INFO - Generating ToC with LLM for chapter: 'Chapter 13. Cloud Forensics' (text length: 13502)\n",
      "2025-06-15 21:31:16,397 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,398 - ERROR - LLM ToC generation error for 'Chapter 13. Cloud Forensics': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,398 - INFO - Created parent doc Ch 29: 'Chapter 13. Cloud Forensics' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,398 - INFO - Generating ToC with LLM for chapter: 'Chapter 14. Report Writing for High-Tech Investigations' (text length: 18274)\n",
      "2025-06-15 21:31:16,401 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,401 - ERROR - LLM ToC generation error for 'Chapter 14. Report Writing for High-Tech Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,402 - INFO - Created parent doc Ch 30: 'Chapter 14. Report Writing for High-Tech Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,402 - INFO - Generating ToC with LLM for chapter: 'Chapter 15. Expert Testimony in Digital Investigations' (text length: 22024)\n",
      "2025-06-15 21:31:16,404 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,405 - ERROR - LLM ToC generation error for 'Chapter 15. Expert Testimony in Digital Investigations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,405 - INFO - Created parent doc Ch 31: 'Chapter 15. Expert Testimony in Digital Investigations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,406 - INFO - Generating ToC with LLM for chapter: 'Chapter 16. Ethics for the Expert Witness' (text length: 14277)\n",
      "2025-06-15 21:31:16,408 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,409 - ERROR - LLM ToC generation error for 'Chapter 16. Ethics for the Expert Witness': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,409 - INFO - Created parent doc Ch 32: 'Chapter 16. Ethics for the Expert Witness' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,409 - INFO - Generating ToC with LLM for chapter: 'Appendix A. Certification Test References' (text length: 9079)\n",
      "2025-06-15 21:31:16,412 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,412 - ERROR - LLM ToC generation error for 'Appendix A. Certification Test References': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,413 - INFO - Created parent doc Ch 33: 'Appendix A. Certification Test References' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,413 - INFO - Generating ToC with LLM for chapter: 'Appendix B. Digital Forensics References' (text length: 12139)\n",
      "2025-06-15 21:31:16,416 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,416 - ERROR - LLM ToC generation error for 'Appendix B. Digital Forensics References': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,417 - INFO - Created parent doc Ch 34: 'Appendix B. Digital Forensics References' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,417 - INFO - Generating ToC with LLM for chapter: 'Appendix C. Digital Forensics Lab Considerations' (text length: 13818)\n",
      "2025-06-15 21:31:16,419 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,420 - ERROR - LLM ToC generation error for 'Appendix C. Digital Forensics Lab Considerations': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,420 - INFO - Created parent doc Ch 35: 'Appendix C. Digital Forensics Lab Considerations' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,421 - INFO - Generating ToC with LLM for chapter: 'Appendix D. Legacy File System and Forensics Tools' (text length: 12120)\n",
      "2025-06-15 21:31:16,423 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 404 Not Found\"\n",
      "2025-06-15 21:31:16,423 - ERROR - LLM ToC generation error for 'Appendix D. Legacy File System and Forensics Tools': model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_927980/2589378533.py\", line 97, in generate_toc_for_chapter_text\n",
      "    response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 741, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 678, in _chat_stream_with_aggregation\n",
      "    for chunk in self._iterate_over_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 763, in _iterate_over_stream\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/langchain_ollama/chat_models.py\", line 665, in _create_chat_stream\n",
      "    yield from self._client.chat(**chat_params)\n",
      "  File \"/home/sebas_dev_linux/miniconda3/envs/tensor2/lib/python3.10/site-packages/ollama/_client.py\", line 170, in inner\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model \"mistral:7b-instruct-q4_K_M\" not found, try pulling it first (status code: 404)\n",
      "2025-06-15 21:31:16,424 - INFO - Created parent doc last Ch 36: 'Appendix D. Legacy File System and Forensics Tools' (LLM ToC items: 1)\n",
      "2025-06-15 21:31:16,424 - INFO - Created 37 parent documents.\n",
      "2025-06-15 21:31:16,424 - INFO - Generated hierarchical book ToC with 36 top-level entries.\n",
      "2025-06-15 21:31:16,436 - INFO - Full book hierarchical Table of Contents saved to: ./book_table_of_contents.json\n",
      "2025-06-15 21:31:16,451 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-15 21:31:59,050 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:32:01,990 - INFO - Global ParentDocumentRetriever initialized for chapter lookup.\n",
      "2025-06-15 21:32:01,991 - INFO - Retrieving primary parent chapter(s) for: 'Understanding the Digital Forensics Profession and Investigations.'\n",
      "2025-06-15 21:32:02,031 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:32:02,037 - WARNING - Invalid/empty LLM-generated ToC for 'Chapter 1. Understanding the Digital Forensics Profession and Investigations'. Using fallback.\n",
      "2025-06-15 21:32:02,037 - INFO - Processing 1 subtopics from LLM-generated ToC of chapter 'Chapter 1. Understanding the Digital Forensics Profession and Investigations':\n",
      "2025-06-15 21:32:02,038 - INFO - -- Processing LLM-ToC Subtopic: 'Overview of Chapter 1. Understanding the Digital Forensics Profession and Investigations' --\n",
      "2025-06-15 21:32:02,038 - INFO - Temp VS for parent: 'Chapter 1. Understanding the Digital Forensics Profession and Investigations' for subtopic: 'Overview of Chapter 1. Understanding the Digital Forensics Profession and Investigations'\n",
      "2025-06-15 21:32:02,047 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-15 21:32:07,277 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:32:07,559 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:32:07,574 - INFO - Retrieved 3 child chunks from parent chapter for subtopic 'Overview of Chapter 1. Understanding the Digital Forensics Profession and Investigations'.\n",
      "2025-06-15 21:32:07,575 - INFO - Generating elaborated content for subtopic: 'Overview of Chapter 1. Understanding the Digital Forensics Profession and Investigations'\n",
      "2025-06-15 21:32:12,616 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-15 21:32:13,216 - INFO - Generated 3 content points for subtopic 'Overview of Chapter 1. Understanding the Digital Forensics Profession and Investigations'.\n",
      "/tmp/ipykernel_927980/2589378533.py:363: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  with open(output_filename, 'w', encoding='utf-8') as f: json.dump(final_weekly_output.dict(), f, indent=2, ensure_ascii=False)\n",
      "2025-06-15 21:32:13,217 - INFO - Saved detailed subtopic content to ./structured_weekly_content_toc_iterated_v4_fulltoc/week_1_content_llm_toc_ch1.json\n",
      "2025-06-15 21:32:13,217 - INFO - Pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Optional, Tuple # Added Tuple\n",
    "\n",
    "# --- LangChain Core Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from pydantic import BaseModel, Field # Using direct pydantic import\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# --- LangChain Community/Vendor Imports ---\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# --- LangChain Text Splitter & Retriever ---\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "# --- Setup Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration ---\n",
    "BOOK_PATH = \"/home/sebas_dev_linux/projects/course_generator/data/books/Bill Nelson, Amelia Phillips, Christopher Steuart - Guide to Computer Forensics and Investigations_ Processing Digital Evidence-Cengage Learning (2018).epub\"\n",
    "UNIT_OUTLINE_JSON_PATH = \"/home/sebas_dev_linux/projects/course_generator/results/Parse_UO/ICT312 Digital Forensic_Final_parsed.json\"\n",
    "\n",
    "CHROMA_PERSIST_DIR_CHILD_GLOBAL = \"./chroma_db_global_child_chunks_strat3_v4_fulltoc\"\n",
    "CHROMA_COLLECTION_NAME_CHILD_GLOBAL = \"global_book_child_chunks_strat3_v4_fulltoc\"\n",
    "\n",
    "OUTPUT_STRUCTURED_JSON_DIR = \"./structured_weekly_content_toc_iterated_v4_fulltoc\"\n",
    "OUTPUT_BOOK_TOC_JSON_PATH = \"./book_table_of_contents.json\" # New output for the book's ToC\n",
    "os.makedirs(OUTPUT_STRUCTURED_JSON_DIR, exist_ok=True)\n",
    "\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"\n",
    "CONTENT_GENERATION_MODEL_OLLAMA = \"mistral:latest\"\n",
    "TOC_GENERATION_MODEL_OLLAMA = \"mistral:7b-instruct-q4_K_M\"\n",
    "\n",
    "CHILD_CHUNK_SIZE_GLOBAL = 700\n",
    "CHILD_CHUNK_OVERLAP_GLOBAL = 100\n",
    "SUBTOPIC_CONTEXT_CHUNK_SIZE = 500\n",
    "SUBTOPIC_CONTEXT_CHUNK_OVERLAP = 50\n",
    "TOP_K_SUBTOPIC_CONTEXT = 3\n",
    "\n",
    "# --- Pydantic Models (Same) ---\n",
    "class LearningModule(BaseModel):\n",
    "    subtopicTitle: str = Field(description=\"A clear and concise title for this sub-topic/objective.\")\n",
    "    elaboratedContent: List[str] = Field(description=\"Detailed paragraphs/bullets explaining the subtopic.\")\n",
    "\n",
    "class WeeklyContent(BaseModel):\n",
    "    mainWeeklyTopic: str = Field(description=\"The main weekly topic.\")\n",
    "    learningModules: List[LearningModule] = Field(description=\"List of learning modules.\")\n",
    "\n",
    "class ChapterToCOutput(BaseModel): # For LLM ToC generation for individual chapters\n",
    "    table_of_contents: List[str] = Field(description=\"List of concise section headings for the chapter.\")\n",
    "\n",
    "# --- Helper Functions (load_unit_outline, clean_metadata_value, generate_toc_for_chapter_text - same) ---\n",
    "def load_unit_outline(json_path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "        logger.info(f\"Successfully loaded unit outline: {json_path}\"); return data\n",
    "    except Exception as e: logger.error(f\"Error loading outline {json_path}: {e}\", exc_info=True); raise\n",
    "\n",
    "def clean_metadata_value(value: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Cleans metadata values to be Chroma-compatible (str, int, float, bool, or None).\n",
    "    Lists are converted to comma-separated strings.\n",
    "    Dicts are converted to JSON strings.\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        # Always convert lists to a comma-separated string for ChromaDB metadata\n",
    "        return \", \".join(map(str, value)) \n",
    "    elif isinstance(value, dict):\n",
    "        return json.dumps(value) # Convert dict to a JSON string\n",
    "    elif isinstance(value, (str, int, float, bool)) or value is None:\n",
    "        return value\n",
    "    else:\n",
    "        # For any other type, attempt to convert to string as a fallback\n",
    "        return str(value)\n",
    "\n",
    "def generate_toc_for_chapter_text(chapter_text: str, chapter_title: str, toc_llm: ChatOllama) -> List[str]:\n",
    "    # (Same as your last working version)\n",
    "    logger.info(f\"Generating ToC with LLM for chapter: '{chapter_title}' (text length: {len(chapter_text)})\")\n",
    "    parser = JsonOutputParser(pydantic_object=ChapterToCOutput)\n",
    "    MAX_CHARS_FOR_TOC_LLM = 28000 \n",
    "    chapter_text_for_llm = chapter_text[:MAX_CHARS_FOR_TOC_LLM] + \"\\n... [TRUNCATED]\" if len(chapter_text) > MAX_CHARS_FOR_TOC_LLM else chapter_text\n",
    "    if not chapter_text_for_llm.strip(): return [f\"Content for chapter '{chapter_title}' was empty.\"]\n",
    "    prompt_template = \"\"\"You are an expert technical writer... Return ONLY a JSON object... {format_instructions}\"\"\" # Your full prompt\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template, partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "    chain = prompt | toc_llm | parser\n",
    "    try:\n",
    "        response_dict = chain.invoke({\"chapter_title\": chapter_title, \"chapter_content\": chapter_text_for_llm})\n",
    "        toc_list = response_dict.get(\"table_of_contents\", [])\n",
    "        logger.info(f\"LLM generated ToC for '{chapter_title}': Count {len(toc_list)}\")\n",
    "        return toc_list if toc_list else [f\"No ToC generated by LLM for {chapter_title}\"]\n",
    "    except Exception as e: logger.error(f\"LLM ToC generation error for '{chapter_title}': {e}\", exc_info=True); return [f\"Error generating ToC for {chapter_title}\"]\n",
    "\n",
    "\n",
    "def load_book_into_parent_documents(book_path: str, toc_generation_llm: ChatOllama) -> Tuple[List[Document], List[Dict[str, Any]]]:\n",
    "    logger.info(f\"Loading book '{book_path}' for parent docs & full ToC (LLM-assisted for chapters).\")\n",
    "    _, file_extension = os.path.splitext(book_path.lower())\n",
    "    parent_documents: List[Document] = []\n",
    "    hierarchical_toc: List[Dict[str, Any]] = []\n",
    "\n",
    "    # --- PDF Section (remains the same basic handling) ---\n",
    "    if file_extension == \".pdf\":\n",
    "        # ... (your existing PDF handling code) ...\n",
    "        logger.warning(\"PDF processing: Pages as parents. Full hierarchical ToC extraction is basic for PDF.\")\n",
    "        pdf_loader = PyPDFLoader(book_path)\n",
    "        try:\n",
    "            pages_as_parents = pdf_loader.load()\n",
    "            current_chapter_entry_for_toc = None # For the global ToC\n",
    "            for i, page_doc in enumerate(pages_as_parents):\n",
    "                page_num = page_doc.metadata.get('page', i + 1)\n",
    "                parent_id = f\"pdf_page_{page_num}\"\n",
    "                page_title = f\"PDF Page {page_num}\"\n",
    "                \n",
    "                if not current_chapter_entry_for_toc: # Create a single \"chapter\" for all PDF pages\n",
    "                    current_chapter_entry_for_toc = {\n",
    "                        \"title\": \"PDF Content (Page by Page)\", \"level\": 1, \n",
    "                        \"number\": \"PDF\", \"sections\": []\n",
    "                    }\n",
    "                    hierarchical_toc.append(current_chapter_entry_for_toc)\n",
    "                current_chapter_entry_for_toc[\"sections\"].append({\"title\": page_title, \"level\": 2, \"subsections\": []})\n",
    "\n",
    "                parent_metadata = {\n",
    "                    \"source\": os.path.basename(page_doc.metadata.get(\"source\", book_path)),\n",
    "                    \"page_number\": page_num, \"chapter_number\": page_num, \n",
    "                    \"chapter_title\": page_title, \"chapter_toc\": [], \n",
    "                    \"document_type\": \"parent\", \"parent_id\": parent_id\n",
    "                }\n",
    "                parent_documents.append(Document(page_content=page_doc.page_content, metadata={k: clean_metadata_value(v) for k,v in parent_metadata.items() if v is not None}))\n",
    "            logger.info(f\"Loaded {len(parent_documents)} PDF pages as parent documents.\")\n",
    "        except Exception as e: logger.error(f\"Error loading PDF: {e}\", exc_info=True); raise\n",
    "\n",
    "\n",
    "    # --- EPUB Section (Refined) ---\n",
    "    elif file_extension == \".epub\":\n",
    "        logger.info(\"Processing EPUB for chapters and generating detailed ToCs using LLM.\")\n",
    "        epub_loader = UnstructuredEPubLoader(book_path, mode=\"elements\", strategy=\"fast\")\n",
    "        try:\n",
    "            raw_lc_documents = epub_loader.load()\n",
    "            if not raw_lc_documents: raise ValueError(f\"No elements from EPUB {book_path}\")\n",
    "        except Exception as e: logger.error(f\"Error loading EPUB: {e}\", exc_info=True); raise\n",
    "\n",
    "        current_chapter_number_val = 0\n",
    "        current_chapter_title_str = \"Preface or Introduction\" # Default for content before Ch1\n",
    "        current_chapter_content_acc: List[str] = []\n",
    "        current_chapter_sections_for_global_toc: List[Dict[str, Any]] = []\n",
    "        parent_id_counter = 0\n",
    "        \n",
    "        # Titles that are likely not main section headings for the global ToC\n",
    "        IGNORE_TITLE_PREFIXES_FOR_GLOBAL_TOC = (\"Note:\", \"Tip:\", \"Figure:\", \"Table:\", \"Listing:\")\n",
    "        # Specific title to look for as introduction\n",
    "        CHAPTER_INTRODUCTION_TITLE = \"Chapter Introduction\"\n",
    "\n",
    "        for element_doc in raw_lc_documents:\n",
    "            element_text = element_doc.page_content.strip() if element_doc.page_content else \"\"\n",
    "            element_category = element_doc.metadata.get(\"category\")\n",
    "            is_new_chapter_boundary = False\n",
    "\n",
    "            if element_category == \"Title\" and element_text and len(element_text) < 150:\n",
    "                if re.match(r\"(?i)^(chapter\\s+(\\d+|[IVXLCDM]+)\\b|part\\s+[A-Z0-9]+|appendix\\s+[A-Z])\", element_text):\n",
    "                    is_new_chapter_boundary = True\n",
    "            \n",
    "            if is_new_chapter_boundary and current_chapter_content_acc:\n",
    "                parent_id_counter += 1\n",
    "                parent_doc_content = \"\\n\".join(current_chapter_content_acc).strip()\n",
    "                if parent_doc_content:\n",
    "                    llm_toc = generate_toc_for_chapter_text(parent_doc_content, current_chapter_title_str, toc_generation_llm)\n",
    "                    \n",
    "                    # Add completed chapter to hierarchical_toc\n",
    "                    if current_chapter_number_val > 0: # Avoid adding preface/intro as a numbered chapter if it was processed\n",
    "                         hierarchical_toc.append({\n",
    "                             \"title\": current_chapter_title_str, \"level\": 1,\n",
    "                             \"number\": str(current_chapter_number_val),\n",
    "                             \"sections\": current_chapter_sections_for_global_toc\n",
    "                         })\n",
    "\n",
    "                    parent_metadata = {\"source\": os.path.basename(book_path), \"chapter_number\": current_chapter_number_val,\n",
    "                                       \"chapter_title\": current_chapter_title_str, \n",
    "                                       \"chapter_toc\": llm_toc,\n",
    "                                       \"document_type\": \"parent\", \"parent_id\": f\"epub_ch_{parent_id_counter}\"}\n",
    "                    parent_documents.append(Document(page_content=parent_doc_content, metadata={k: clean_metadata_value(v) for k, v in parent_metadata.items() if v is not None}))\n",
    "                    logger.info(f\"Created parent doc Ch {current_chapter_number_val}: '{current_chapter_title_str}' (LLM ToC items: {len(llm_toc)})\")\n",
    "                current_chapter_content_acc = []\n",
    "                current_chapter_sections_for_global_toc = [] \n",
    "\n",
    "            if is_new_chapter_boundary:\n",
    "                current_chapter_number_val +=1\n",
    "                current_chapter_title_str = element_text\n",
    "                # Don't add the chapter title itself to its own list of sections for global ToC here.\n",
    "                # The hierarchical_toc structure has a dedicated \"title\" for the chapter.\n",
    "            elif element_category == \"Title\" and element_text and current_chapter_number_val >= 0: # Allow for Preface/Intro (Ch0) sections\n",
    "                # Check if this title should be ignored for the global ToC's section list\n",
    "                is_ignorable_title = any(element_text.lower().startswith(p.lower()) for p in IGNORE_TITLE_PREFIXES_FOR_GLOBAL_TOC)\n",
    "                if not is_ignorable_title and len(element_text) < 150 : # Also check length for section titles\n",
    "                    # Only add if it's not a generic/ignorable title and it's a reasonable length for a section title\n",
    "                    current_chapter_sections_for_global_toc.append({\"title\": element_text, \"level\": 2, \"subsections\": []})\n",
    "            \n",
    "            if element_text: current_chapter_content_acc.append(element_text)\n",
    "        \n",
    "        if current_chapter_content_acc: # Last chapter\n",
    "            parent_id_counter += 1; parent_doc_content = \"\\n\".join(current_chapter_content_acc).strip()\n",
    "            if parent_doc_content:\n",
    "                llm_toc = generate_toc_for_chapter_text(parent_doc_content, current_chapter_title_str, toc_generation_llm)\n",
    "                # Add the last chapter to hierarchical_toc\n",
    "                if current_chapter_number_val >= 0: # Include preface/intro if it has content\n",
    "                    hierarchical_toc.append({\n",
    "                        \"title\": current_chapter_title_str, \"level\": 1,\n",
    "                        \"number\": str(current_chapter_number_val) if current_chapter_number_val > 0 else \"0\", # Use \"0\" for preface\n",
    "                        \"sections\": current_chapter_sections_for_global_toc\n",
    "                    })\n",
    "                parent_metadata = {\"source\": os.path.basename(book_path), \"chapter_number\": current_chapter_number_val,\n",
    "                                   \"chapter_title\": current_chapter_title_str, \"chapter_toc\": llm_toc,\n",
    "                                   \"document_type\": \"parent\", \"parent_id\": f\"epub_ch_{parent_id_counter}\"}\n",
    "                parent_documents.append(Document(page_content=parent_doc_content, metadata={k: clean_metadata_value(v) for k, v in parent_metadata.items() if v is not None}))\n",
    "                logger.info(f\"Created parent doc last Ch {current_chapter_number_val}: '{current_chapter_title_str}' (LLM ToC items: {len(llm_toc)})\")\n",
    "    else: raise ValueError(f\"Unsupported book file format: {file_extension}\")\n",
    "\n",
    "    if not parent_documents: logger.error(f\"No parent documents created.\")\n",
    "    else: logger.info(f\"Created {len(parent_documents)} parent documents.\")\n",
    "    \n",
    "    if not hierarchical_toc: logger.warning(\"Hierarchical Table of Contents for the book is empty.\")\n",
    "    else: logger.info(f\"Generated hierarchical book ToC with {len(hierarchical_toc)} top-level entries.\")\n",
    "    \n",
    "    return parent_documents, hierarchical_toc\n",
    "# format_docs_with_toc, get_subtopic_specific_context_from_parent, generate_elaborated_content_for_subtopic\n",
    "# (These remain largely the same as your last correct version)\n",
    "def format_docs_with_toc(docs: List[Document]) -> str:\n",
    "    # (Same as your last version - uses doc.metadata.get(\"chapter_toc\"))\n",
    "    formatted_strings = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        content_parts = []; title = doc.metadata.get(\"chapter_title\", f\"Retrieved Doc {i+1}\")\n",
    "        toc_items = doc.metadata.get(\"chapter_toc\", []) \n",
    "        content_parts.append(f\"--- Start of Content for: {title} ---\")\n",
    "        if toc_items and isinstance(toc_items, list) and len(toc_items) > 0 and not (len(toc_items)==1 and \"Error\" in toc_items[0]):\n",
    "            content_parts.append(\"Chapter Table of Contents (LLM Generated):\")\n",
    "            for item in toc_items: content_parts.append(f\"- {item}\")\n",
    "            content_parts.append(\"---\")\n",
    "        if doc.page_content: content_parts.append(doc.page_content)\n",
    "        content_parts.append(f\"--- End of Content for: {title} ---\")\n",
    "        formatted_strings.append(\"\\n\".join(content_parts))\n",
    "    return \"\\n\\n\".join(formatted_strings)\n",
    "\n",
    "def format_docs_simple(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n---\\n\\n\".join([doc.page_content for doc in docs if doc.page_content is not None])\n",
    "\n",
    "def get_subtopic_specific_context_from_parent(subtopic_query: str, parent_chapter_document: Document, embedding_model: OllamaEmbeddings, k: int = TOP_K_SUBTOPIC_CONTEXT) -> str:\n",
    "    # (Same as your last version)\n",
    "    logger.info(f\"Temp VS for parent: '{parent_chapter_document.metadata.get('chapter_title', 'Unk')}' for subtopic: '{subtopic_query}'\")\n",
    "    sub_splitter = RecursiveCharacterTextSplitter(chunk_size=SUBTOPIC_CONTEXT_CHUNK_SIZE, chunk_overlap=SUBTOPIC_CONTEXT_CHUNK_OVERLAP)\n",
    "    parent_content_chunks_text = sub_splitter.split_text(parent_chapter_document.page_content)\n",
    "    if not parent_content_chunks_text: return \"Parent chapter yielded no text chunks.\"\n",
    "    temp_docs = [Document(page_content=text) for text in parent_content_chunks_text]\n",
    "    try:\n",
    "        temp_vectorstore = Chroma.from_documents(documents=temp_docs, embedding=embedding_model)\n",
    "        retrieved_sub_chunks = temp_vectorstore.similarity_search(subtopic_query, k=k)\n",
    "        logger.info(f\"Retrieved {len(retrieved_sub_chunks)} child chunks from parent chapter for subtopic '{subtopic_query}'.\")\n",
    "        return format_docs_simple(retrieved_sub_chunks) \n",
    "    except Exception as e: logger.error(f\"Error in temp VS for subtopic '{subtopic_query}': {e}\", exc_info=True); return f\"Error: {str(e)}\"\n",
    "\n",
    "def generate_elaborated_content_for_subtopic(subtopic_title: str, subtopic_context: str, generation_llm: ChatOllama) -> List[str]:\n",
    "    # (Same as your last version)\n",
    "    logger.info(f\"Generating elaborated content for subtopic: '{subtopic_title}'\")\n",
    "    class SubtopicElaboration(BaseModel): points: List[str] = Field(description=\"List of detailed paragraphs/bullets.\")\n",
    "    parser = JsonOutputParser(pydantic_object=SubtopicElaboration)\n",
    "    template = \"\"\"You are an academic writer...{format_instructions}\"\"\" # Your full detailed prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template, partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
    "    chain = prompt | generation_llm | parser\n",
    "    try:\n",
    "        llm_response_dict = chain.invoke({\"subtopic_title\": subtopic_title, \"subtopic_context\": subtopic_context})\n",
    "        content_list = llm_response_dict.get(\"points\", [])\n",
    "        if not content_list: content_list = [f\"No specific elaboration for '{subtopic_title}' based on context.\"]\n",
    "        logger.info(f\"Generated {len(content_list)} content points for subtopic '{subtopic_title}'.\")\n",
    "        return content_list\n",
    "    except Exception as e: logger.error(f\"LLM error for subtopic '{subtopic_title}': {e}\", exc_info=True); return [f\"Error: {str(e)}\"]\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    logger.info(\"RAG Pipeline (ParentDocRetriever + LLM ToC for Parents + Iterate LLM-ToC Subtopics)...\")\n",
    "    # (File checks, unit outline, week selection - same)\n",
    "    if not os.path.exists(BOOK_PATH): logger.error(f\"Book not found: {BOOK_PATH}\"); return\n",
    "    if not os.path.exists(UNIT_OUTLINE_JSON_PATH): logger.error(f\"Outline not found: {UNIT_OUTLINE_JSON_PATH}\"); return\n",
    "    unit_outline_data = load_unit_outline(UNIT_OUTLINE_JSON_PATH)\n",
    "    weekly_schedule = unit_outline_data.get(\"weeklySchedule\", [])\n",
    "    if not weekly_schedule: logger.error(\"No weekly schedule.\"); return\n",
    "    WEEK_TO_PROCESS_INDEX = 0\n",
    "    if WEEK_TO_PROCESS_INDEX >= len(weekly_schedule): logger.error(\"Week index out of bounds.\"); return\n",
    "    selected_week_info = weekly_schedule[WEEK_TO_PROCESS_INDEX]\n",
    "    main_weekly_topic_title = selected_week_info.get(\"topic\") or selected_week_info.get(\"contentTopic\")\n",
    "    week_identifier = selected_week_info.get(\"week\", f\"Idx_{WEEK_TO_PROCESS_INDEX}\")\n",
    "    full_main_topic_name = f\"Week {week_identifier}: {main_weekly_topic_title}\"\n",
    "    if not main_weekly_topic_title: logger.error(\"Selected week has no topic title.\"); return\n",
    "    logger.info(f\"Processing: {full_main_topic_name}\")\n",
    "\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        content_gen_llm = ChatOllama(model=CONTENT_GENERATION_MODEL_OLLAMA, temperature=0.2, format=\"json\")\n",
    "        toc_gen_llm = ChatOllama(model=TOC_GENERATION_MODEL_OLLAMA, temperature=0.1, format=\"json\") # LLM for ToC\n",
    "    except Exception as e: logger.error(f\"LLM init error: {e}\", exc_info=True); return\n",
    "\n",
    "    # --- Load Parent Documents and Generate Full Book Hierarchical ToC ---\n",
    "    parent_documents, book_hierarchical_toc = load_book_into_parent_documents(BOOK_PATH, toc_generation_llm=toc_gen_llm)\n",
    "    if not parent_documents: logger.error(\"No parent documents loaded/created.\"); return\n",
    "\n",
    "    # --- Save the Full Book Hierarchical ToC ---\n",
    "    try:\n",
    "        with open(OUTPUT_BOOK_TOC_JSON_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump(book_hierarchical_toc, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Full book hierarchical Table of Contents saved to: {OUTPUT_BOOK_TOC_JSON_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving book hierarchical ToC: {e}\", exc_info=True)\n",
    "\n",
    "    # --- Setup ParentDocumentRetriever (same as before) ---\n",
    "    docstore_for_parents = InMemoryStore()\n",
    "    parent_doc_ids = [doc.metadata.get(\"parent_id\", f\"pid_{i}\") for i, doc in enumerate(parent_documents)]\n",
    "    for i, doc in enumerate(parent_documents): doc.metadata[\"parent_id\"] = parent_doc_ids[i]\n",
    "    docstore_for_parents.mset(list(zip(parent_doc_ids, parent_documents)))\n",
    "    if os.path.exists(CHROMA_PERSIST_DIR_CHILD_GLOBAL): shutil.rmtree(CHROMA_PERSIST_DIR_CHILD_GLOBAL)\n",
    "    global_child_vectorstore = Chroma(collection_name=CHROMA_COLLECTION_NAME_CHILD_GLOBAL, embedding_function=embedding_model, persist_directory=CHROMA_PERSIST_DIR_CHILD_GLOBAL)\n",
    "    global_child_splitter = RecursiveCharacterTextSplitter(chunk_size=CHILD_CHUNK_SIZE_GLOBAL, chunk_overlap=CHILD_CHUNK_OVERLAP_GLOBAL)\n",
    "    parent_retriever_for_chapters = ParentDocumentRetriever(vectorstore=global_child_vectorstore, docstore=docstore_for_parents, child_splitter=global_child_splitter)\n",
    "    parent_retriever_for_chapters.add_documents(parent_documents, ids=parent_doc_ids, add_to_docstore=False)\n",
    "    logger.info(\"Global ParentDocumentRetriever initialized for chapter lookup.\")\n",
    "\n",
    "    # --- Retrieve Primary Chapter & Iterate its LLM-Generated ToC for Content Gen ---\n",
    "    logger.info(f\"Retrieving primary parent chapter(s) for: '{main_weekly_topic_title}'\")\n",
    "    try:\n",
    "        retrieved_primary_chapters = parent_retriever_for_chapters.invoke(main_weekly_topic_title)\n",
    "        if not retrieved_primary_chapters: logger.error(f\"No parent chapter found for: {main_weekly_topic_title}\"); return\n",
    "        primary_chapter_doc = retrieved_primary_chapters[0]\n",
    "        chapter_title = primary_chapter_doc.metadata.get(\"chapter_title\", \"Unknown Chapter\")\n",
    "        chapter_number_meta = str(primary_chapter_doc.metadata.get(\"chapter_number\", \"NA\"))\n",
    "        # THIS IS KEY: Use the LLM-generated ToC from the parent document's metadata\n",
    "        llm_generated_chapter_toc_list = primary_chapter_doc.metadata.get(\"chapter_toc\", []) \n",
    "        if not (isinstance(llm_generated_chapter_toc_list, list) and llm_generated_chapter_toc_list and not (len(llm_generated_chapter_toc_list) == 1 and \"Error\" in llm_generated_chapter_toc_list[0])):\n",
    "            logger.warning(f\"Invalid/empty LLM-generated ToC for '{chapter_title}'. Using fallback.\")\n",
    "            llm_generated_chapter_toc_list = [f\"Overview of {chapter_title}\"] if chapter_title != \"Unknown Chapter\" else [\"Main concepts\"]\n",
    "    except Exception as e: logger.error(f\"Error retrieving primary chapter/LLM-ToC: {e}\", exc_info=True); return\n",
    "\n",
    "    learning_modules_for_week: List[LearningModule] = []\n",
    "    logger.info(f\"Processing {len(llm_generated_chapter_toc_list)} subtopics from LLM-generated ToC of chapter '{chapter_title}':\")\n",
    "    for subtopic_from_llm_toc in llm_generated_chapter_toc_list:\n",
    "        if not subtopic_from_llm_toc.strip(): continue\n",
    "        logger.info(f\"-- Processing LLM-ToC Subtopic: '{subtopic_from_llm_toc}' --\")\n",
    "        subtopic_specific_context_str = get_subtopic_specific_context_from_parent(\n",
    "            subtopic_query=subtopic_from_llm_toc, parent_chapter_document=primary_chapter_doc, embedding_model=embedding_model)\n",
    "        \n",
    "        elaborated_content = generate_elaborated_content_for_subtopic(\n",
    "            subtopic_title=subtopic_from_llm_toc, subtopic_context=subtopic_specific_context_str, generation_llm=content_gen_llm)\n",
    "        learning_modules_for_week.append(LearningModule(subtopicTitle=subtopic_from_llm_toc, elaboratedContent=elaborated_content))\n",
    "\n",
    "    final_weekly_output = WeeklyContent(mainWeeklyTopic=full_main_topic_name, learningModules=learning_modules_for_week)\n",
    "    output_filename = os.path.join(OUTPUT_STRUCTURED_JSON_DIR, f\"week_{week_identifier}_content_llm_toc_ch{chapter_number_meta}.json\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f: json.dump(final_weekly_output.dict(), f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Saved detailed subtopic content to {output_filename}\")\n",
    "    except Exception as e: logger.error(f\"Error saving JSON: {e}\", exc_info=True)\n",
    "    logger.info(\"Pipeline finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"YOUR_BOOK.pdf\" in BOOK_PATH: logger.error(\"!!! PLEASE UPDATE 'BOOK_PATH' !!!\")\n",
    "    else: main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f577a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 00:24:02,544 - INFO - Starting slide content generation for topic: 'Linux Validation Methods'\n",
      "2025-06-16 00:24:02,570 - INFO - Loading ChromaDB from: ./chroma_db_book_toc_guided_chunks_v1 with collection: book_toc_guided_chunks_v1\n",
      "2025-06-16 00:24:02,577 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-06-16 00:24:02,758 - INFO - Successfully loaded ChromaDB. Collection count: 11774\n",
      "2025-06-16 00:24:02,758 - INFO - Searching for relevant documents with query: 'Linux Validation Methods'\n",
      "2025-06-16 00:24:03,957 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-06-16 00:24:03,966 - INFO - Retrieved 5 documents.\n",
      "2025-06-16 00:24:03,979 - INFO - Initialized LLM: mistral:latest\n",
      "2025-06-16 00:24:03,986 - INFO - Generating slide content using LLM...\n",
      "2025-06-16 00:24:09,049 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-06-16 00:24:23,778 - INFO - Successfully generated slide content.\n",
      "2025-06-16 00:24:23,780 - INFO - Slide content saved to: ./linux_validation_slides_content.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Presentation Summary ---\n",
      "Topic: Linux Validation Methods\n",
      "Number of Slides: 5\n",
      "  Slide 1: Introduction to Linux Validation Methods\n",
      "  Slide 2: Validation and Verification in Linux Forensics\n",
      "  Slide 3: Using Validation Protocols in Linux Forensics\n",
      "  Slide 4: Evaluating Digital Forensics Tools for Linux Validation\n",
      "  Slide 5: Conclusion: Best Practices for Linux Validation Methods\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# --- LangChain & Pydantic Imports ---\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Setup Logging (can be in a shared setup cell or here) ---\n",
    "if not logging.getLogger().hasHandlers(): # Avoid adding handlers multiple times in Jupyter\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger_slides = logging.getLogger(__name__ + \"_slides\") # Use a specific logger name\n",
    "logger_slides.setLevel(logging.INFO)\n",
    "\n",
    "# --- Configuration for this Cell ---\n",
    "CHROMA_PERSIST_DIR = \"./chroma_db_book_toc_guided_chunks_v1\" # From previous script\n",
    "CHROMA_COLLECTION_NAME = \"book_toc_guided_chunks_v1\"   # From previous script\n",
    "EMBEDDING_MODEL_OLLAMA = \"nomic-embed-text\"           # Must match DB creation\n",
    "CONTENT_GENERATION_MODEL_OLLAMA = \"mistral:latest\"\n",
    "OUTPUT_JSON_SLIDES_PATH = \"./linux_validation_slides_content.json\"\n",
    "\n",
    "SEARCH_QUERY = \"Linux Validation Methods\"\n",
    "NUM_DOCS_TO_RETRIEVE = 5 # Number of relevant documents to fetch for context\n",
    "MAX_CONTEXT_LENGTH = 7000 # Max characters for context to avoid overly long prompts\n",
    "\n",
    "# --- Pydantic Models for Structured Output ---\n",
    "class Slide(BaseModel):\n",
    "    slide_number: int = Field(description=\"Sequential number of the slide\")\n",
    "    title: str = Field(description=\"The main title for this slide.\")\n",
    "    key_points: List[str] = Field(description=\"A list of bullet points or key takeaways for the slide.\")\n",
    "    speaker_notes: Optional[str] = Field(None, description=\"Additional notes or details for the speaker for this slide.\")\n",
    "\n",
    "class Presentation(BaseModel):\n",
    "    overall_topic: str = Field(description=\"The main topic of the presentation.\")\n",
    "    slides: List[Slide] = Field(description=\"A list of slide objects.\")\n",
    "\n",
    "def generate_slide_content():\n",
    "    logger_slides.info(f\"Starting slide content generation for topic: '{SEARCH_QUERY}'\")\n",
    "\n",
    "    # 1. Initialize Embedding Model and Load ChromaDB\n",
    "    try:\n",
    "        embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL_OLLAMA)\n",
    "        logger_slides.info(f\"Loading ChromaDB from: {CHROMA_PERSIST_DIR} with collection: {CHROMA_COLLECTION_NAME}\")\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=CHROMA_PERSIST_DIR,\n",
    "            embedding_function=embedding_model,\n",
    "            collection_name=CHROMA_COLLECTION_NAME\n",
    "        )\n",
    "        if vector_db._collection.count() == 0:\n",
    "            logger_slides.error(\"ChromaDB collection is empty. Cannot generate content.\")\n",
    "            return\n",
    "        logger_slides.info(f\"Successfully loaded ChromaDB. Collection count: {vector_db._collection.count()}\")\n",
    "    except Exception as e:\n",
    "        logger_slides.error(f\"Error loading ChromaDB: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # 2. Retrieve Relevant Documents\n",
    "    try:\n",
    "        logger_slides.info(f\"Searching for relevant documents with query: '{SEARCH_QUERY}'\")\n",
    "        retrieved_docs_with_score = vector_db.similarity_search_with_score(SEARCH_QUERY, k=NUM_DOCS_TO_RETRIEVE)\n",
    "        \n",
    "        if not retrieved_docs_with_score:\n",
    "            logger_slides.warning(\"No relevant documents found in the database for the query.\")\n",
    "            context_text = \"No specific information found in the provided documents. Please generate general content based on the topic.\"\n",
    "        else:\n",
    "            logger_slides.info(f\"Retrieved {len(retrieved_docs_with_score)} documents.\")\n",
    "            context_parts = []\n",
    "            current_length = 0\n",
    "            for i, (doc, score) in enumerate(retrieved_docs_with_score):\n",
    "                doc_info = f\"Source Document {i+1} (Relevance Score: {score:.4f}):\\n\"\n",
    "                doc_info += f\"  Section Titles: \"\n",
    "                titles = []\n",
    "                for level in range(1, 6): # Assuming up to 5 levels of titles\n",
    "                    title = doc.metadata.get(f\"level_{level}_title\")\n",
    "                    if title:\n",
    "                        titles.append(title)\n",
    "                doc_info += \" > \".join(titles) if titles else \"N/A\"\n",
    "                doc_info += f\"\\n  Page (if PDF): {doc.metadata.get('page_number', 'N/A')}\\n\"\n",
    "                doc_info += f\"  Content Snippet:\\n{doc.page_content}\\n---\\n\"\n",
    "                \n",
    "                if current_length + len(doc_info) > MAX_CONTEXT_LENGTH:\n",
    "                    logger_slides.warning(f\"Context length limit ({MAX_CONTEXT_LENGTH} chars) reached. Truncating context.\")\n",
    "                    break\n",
    "                context_parts.append(doc_info)\n",
    "                current_length += len(doc_info)\n",
    "            context_text = \"\\n\".join(context_parts)\n",
    "            logger_slides.debug(f\"Context Text for LLM:\\n{context_text[:500]}...\") # Log a snippet\n",
    "\n",
    "    except Exception as e:\n",
    "        logger_slides.error(f\"Error retrieving documents: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # 3. Initialize Content Generation LLM\n",
    "    try:\n",
    "        llm = ChatOllama(\n",
    "            model=CONTENT_GENERATION_MODEL_OLLAMA,\n",
    "            temperature=0.3, # Lower temperature for more factual, less creative slides\n",
    "            # format=\"json\" # If the model and Ollama version support direct JSON output reliably\n",
    "        )\n",
    "        logger_slides.info(f\"Initialized LLM: {CONTENT_GENERATION_MODEL_OLLAMA}\")\n",
    "    except Exception as e:\n",
    "        logger_slides.error(f\"Error initializing LLM: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # 4. Define Prompt and Output Parser\n",
    "    json_parser = JsonOutputParser(pydantic_object=Presentation)\n",
    "\n",
    "    prompt_template_str = \"\"\"\n",
    "    You are an expert instructional designer tasked with creating presentation slides.\n",
    "    Your goal is to generate content for a presentation on the topic: \"{query}\"\n",
    "\n",
    "    Use the following context from relevant documents to inform the slide content.\n",
    "    Focus on extracting and synthesizing key information related to the query.\n",
    "    If the context is sparse or unhelpful, rely on your general knowledge about the topic.\n",
    "\n",
    "    Context:\n",
    "    ---\n",
    "    {context}\n",
    "    ---\n",
    "\n",
    "    Presentation Topic: \"{query}\"\n",
    "\n",
    "    Please generate content for approximately 3-5 slides.\n",
    "    For each slide, provide:\n",
    "    1.  A clear `title`.\n",
    "    2.  A list of `key_points` (bullet points, 3-5 per slide).\n",
    "    3.  Optional `speaker_notes` for further explanation.\n",
    "    4.  A sequential `slide_number` starting from 1.\n",
    "\n",
    "    The output MUST be a single, valid JSON object that conforms to the following Pydantic schema:\n",
    "    ```json\n",
    "    {json_schema}\n",
    "    ```\n",
    "\n",
    "    Ensure your entire response is ONLY the JSON object. Do not include any text before or after the JSON.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=prompt_template_str,\n",
    "        partial_variables={\"json_schema\": json_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    # 5. Create LangChain Chain and Generate Content\n",
    "    chain = prompt | llm | json_parser\n",
    "    logger_slides.info(\"Generating slide content using LLM...\")\n",
    "    \n",
    "    try:\n",
    "        # Retry logic could be added here if needed for LLM flakiness\n",
    "        generated_presentation_data = chain.invoke({\n",
    "            \"query\": SEARCH_QUERY,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "        # The output from JsonOutputParser should already be a Pydantic model instance or dict\n",
    "        # If it's a dict, Pydantic model can validate it:\n",
    "        # presentation_obj = Presentation(**generated_presentation_data)\n",
    "\n",
    "        logger_slides.info(\"Successfully generated slide content.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger_slides.error(f\"Error during LLM content generation or JSON parsing: {e}\", exc_info=True)\n",
    "        \n",
    "        # Attempt to get raw output for debugging if parsing fails\n",
    "        # This part is more complex if the error is before the parser\n",
    "        try:\n",
    "            logger_slides.info(\"Attempting to get raw output from LLM for debugging...\")\n",
    "            raw_output_chain = prompt | llm | StrOutputParser()\n",
    "            raw_output = raw_output_chain.invoke({\n",
    "                \"query\": SEARCH_QUERY,\n",
    "                \"context\": context_text\n",
    "            })\n",
    "            logger_slides.error(f\"LLM Raw Output:\\n{raw_output}\")\n",
    "        except Exception as e_raw:\n",
    "            logger_slides.error(f\"Could not get raw output: {e_raw}\")\n",
    "        return\n",
    "\n",
    "    # 6. Save Output to JSON\n",
    "    try:\n",
    "        # If generated_presentation_data is a Pydantic model, convert to dict for json.dump\n",
    "        if isinstance(generated_presentation_data, BaseModel):\n",
    "            output_dict = generated_presentation_data.model_dump()\n",
    "        else: # Should be a dict if JsonOutputParser worked correctly\n",
    "            output_dict = generated_presentation_data\n",
    "\n",
    "        with open(OUTPUT_JSON_SLIDES_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_dict, f, indent=4, ensure_ascii=False)\n",
    "        logger_slides.info(f\"Slide content saved to: {OUTPUT_JSON_SLIDES_PATH}\")\n",
    "        \n",
    "        # Print a summary of the generated content\n",
    "        print(\"\\n--- Generated Presentation Summary ---\")\n",
    "        print(f\"Topic: {output_dict.get('overall_topic')}\")\n",
    "        if 'slides' in output_dict:\n",
    "            print(f\"Number of Slides: {len(output_dict['slides'])}\")\n",
    "            for i, slide_data in enumerate(output_dict['slides']):\n",
    "                print(f\"  Slide {slide_data.get('slide_number', i+1)}: {slide_data.get('title')}\")\n",
    "        print(\"------------------------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger_slides.error(f\"Error saving JSON output: {e}\", exc_info=True)\n",
    "\n",
    "# --- Execute the generation ---\n",
    "if __name__ == \"__main__\": # This block allows running as a script\n",
    "    generate_slide_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
